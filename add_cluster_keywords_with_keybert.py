"""
Add Cluster Keywords Using KeyBERT + TF-IDF Filtering

This script enhances per-year NLP topic cluster files by extracting representative keywords
from paper titles using KeyBERT, followed by TF-IDF-based filtering to retain unique and 
meaningful terms.

ğŸ“Œ Key Features:
- Uses KeyBERT (`all-MiniLM-L6-v2`) to extract candidate keywords from paper titles in each cluster
- Filters out domain-generic stopwords and English stopwords
- Applies TF-IDF filtering to keep distinctive terms across clusters
- Retains top-N (e.g., 5) final keywords per cluster
- Saves enriched cluster files with a new `"cluster_keywords"` field

ğŸ“ Input:
- Folder: `cluster_results_by_year/`
- JSON files with format:
  {
    "window": "2017_2019",
    "cluster_details": {
        "0": [{"title": "...", ...}, ...],
        ...
    }
  }

ğŸ“ Output:
- Folder: `cluster_with_keywords_by_year/`
- Same structure with additional:
  {
    "cluster_keywords": {
        "0": ["transformer", "qa", "dialogue", ...],
        ...
    }
  }

âš™ï¸ Settings:
- CANDIDATE_KEYWORDS: Number of keywords extracted from KeyBERT (default: 15)
- FINAL_KEYWORDS: Number of final keywords saved after TF-IDF filtering (default: 5)
- DOMAIN_STOPWORDS: Custom NLP domain stopword set

Usage:
- Run with: `python add_cluster_keywords_with_keybert.py`
"""

import json
import os
from tqdm import tqdm
from keybert import KeyBERT
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# ğŸ”§ ì„¤ì •
INPUT_DIR = "cluster_results_by_year"     # ì—°ë„ë³„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í´ë”
OUTPUT_DIR = "cluster_with_keywords_by_year"   # í‚¤ì›Œë“œ ì¶”ê°€ ê²°ê³¼ ì €ì¥ í´ë”
CANDIDATE_KEYWORDS = 15  # í›„ë³´ í‚¤ì›Œë“œ ìˆ˜ ì¦ê°€
FINAL_KEYWORDS = 5

# âœ… ë„ë©”ì¸ ë¶ˆìš©ì–´ í•„í„°ë§
DOMAIN_STOPWORDS = {
    "neural", "language", "learning", "text", "model", "data", "approach",
    "paper", "method", "methods", "study", "results", "system"
}

# ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ğŸ”¹ KeyBERT ëª¨ë¸ ë¡œë“œ
kw_model = KeyBERT("all-MiniLM-L6-v2")

# ğŸ“‚ ì…ë ¥ íŒŒì¼ ìˆœíšŒ
for fname in sorted(os.listdir(INPUT_DIR)):
    if not fname.endswith(".json"):
        continue

    input_path = os.path.join(INPUT_DIR, fname)
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    cluster_details = data.get("cluster_details", {})
    cluster_keywords_raw = {}
    corpus = []
    cluster_texts = {}

    for cluster_id, papers in tqdm(cluster_details.items(), desc=f"[KeyBERT] {fname}"):
        titles = [paper["title"] for paper in papers if paper.get("title")]
        text = " ".join(titles)
        cluster_texts[cluster_id] = text
        corpus.append(text)

        if not text.strip():
            cluster_keywords_raw[cluster_id] = []
            continue

        keywords = kw_model.extract_keywords(
            text,
            top_n=CANDIDATE_KEYWORDS + 10,
            stop_words='english',
            use_maxsum=True,
            nr_candidates=30,
            keyphrase_ngram_range=(1, 1)
        )
        filtered_keywords = [
            kw for kw, _ in keywords if kw.lower() not in DOMAIN_STOPWORDS
        ][:CANDIDATE_KEYWORDS]
        cluster_keywords_raw[cluster_id] = filtered_keywords

    # TF-IDF ë¶„ì„ ì¤€ë¹„
    all_keywords = set(kw for kws in cluster_keywords_raw.values() for kw in kws)
    vectorizer = TfidfVectorizer(vocabulary=list(all_keywords))
    tfidf_matrix = vectorizer.fit_transform(corpus)
    feature_names = vectorizer.get_feature_names_out()

    cluster_keywords_final = {}
    cluster_ids = list(cluster_texts.keys())

    for idx, cluster_id in enumerate(cluster_ids):
        row = tfidf_matrix[idx].toarray().flatten()
        keyword_scores = {feature_names[i]: row[i] for i in range(len(feature_names))}
        # ì „ì²´ í‰ê· ë³´ë‹¤ ë†’ì€ TF-IDFë¥¼ ê°–ëŠ” ë‹¨ì–´ë§Œ ë‚¨ê¸°ê¸° (ê°•í•œ í•„í„°ë§)
        mean_score = np.mean(tfidf_matrix.toarray(), axis=0)
        unusual_keywords = [
            kw for kw in cluster_keywords_raw[cluster_id]
            if kw in keyword_scores and keyword_scores[kw] > mean_score[feature_names.tolist().index(kw)]
        ]
        # ìŠ¤ì½”ì–´ ê¸°ì¤€ ìƒìœ„ FINAL_KEYWORDSê°œ ì„ íƒ
        sorted_keywords = sorted(
            unusual_keywords,
            key=lambda kw: keyword_scores[kw],
            reverse=True
        )
        cluster_keywords_final[cluster_id] = sorted_keywords[:FINAL_KEYWORDS]

    # ğŸ”¹ ì›ë³¸ì— í‚¤ì›Œë“œ ì¶”ê°€
    data["cluster_keywords"] = cluster_keywords_final

    # ğŸ”¹ ì €ì¥
    output_path = os.path.join(OUTPUT_DIR, fname)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
