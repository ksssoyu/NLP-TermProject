paper_id,title,year,eigenvector,cluster_id
W2963341956,,2019,0.3057482474247895,2
W2965373594,RoBERTa: A Robustly Optimized BERT Pretraining Approach,2019,0.263587349680793,2
W2896457183,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,2018,0.2294335787833057,2
W2963748441,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",2016,0.16824785806074793,2
W2963846996,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,2018,0.11606012397011038,2
W2970597249,XLNet: Generalized Autoregressive Pretraining for Language Understanding,2019,0.10640586986213661,2
W2923014074,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,2018,0.10511063118635335,2
W4288089799,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019,0.1030408979179676,2
W1840435438,A large annotated corpus for learning natural language inference,2015,0.09462771120155818,2
W2996428491,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2019,0.09462038577943165,2
W4292779060,Language Models are Few-Shot Learners,2020,0.0937627896978875,2
W2963310665,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,2018,0.0875223247343881,2
W2912924812,Natural Questions: A Benchmark for Question Answering Research,2019,0.07239885833743356,2
W2963323070,Know What You Don’t Know: Unanswerable Questions for SQuAD,2018,0.0715556145778451,2
W2963339397,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,2017,0.06955776103891634,2
W2979826702,Transformers: State-of-the-Art Natural Language Processing,2020,0.06931437040335171,2
W2978017171,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",2019,0.06784167535649195,2
W2970641574,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,2019,0.0658818800029945,2
W2970476646,Language Models as Knowledge Bases?,2019,0.06333232610311514,2
W2980282514,HuggingFace's Transformers: State-of-the-art Natural Language Processing,2019,0.0602892454308526,2
W2889787757,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",2018,0.056155747171929296,2
W3099700870,Dense Passage Retrieval for Open-Domain Question Answering,2020,0.0543065089667367,2
W2963026768,Universal Language Model Fine-tuning for Text Classification,2018,0.053601419831729814,2
W3104033643,SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,2017,0.052601083639831146,2
W2962985038,Reading Wikipedia to Answer Open-Domain Questions,2017,0.049796034644264145,2
W3118485687,A Primer in BERTology: What We Know About How BERT Works,2020,0.049602031566316505,2
W2943552823,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,2019,0.04855200000567703,2
W3173777717,Making Pre-trained Language Models Better Few-shot Learners,2021,0.04771534582215114,2
W2551396370,Bidirectional Attention Flow for Machine Comprehension,2016,0.046909698166860136,2
W3011574394,Pre-trained models for natural language processing: A survey,2020,0.046572632894917344,2
W2951286828,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,2019,0.045987466891467745,2
W2911489562,BioBERT: a pre-trained biomedical language representation model for biomedical text mining,2019,0.04569323084026413,2
W2963918774,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,2017,0.04536889320658818,2
W2962736243,Annotation Artifacts in Natural Language Inference Data,2018,0.04493459308991426,2
W3034850762,Adversarial NLI: A New Benchmark for Natural Language Understanding,2020,0.04475295678506647,2
W3011411500,SpanBERT: Improving Pre-training by Representing and Predicting Spans,2020,0.04469990497274785,2
W2606964149,RACE: Large-scale ReAding Comprehension Dataset From Examinations,2017,0.043568927317420965,2
W2953356739,ERNIE: Enhanced Language Representation with Informative Entities,2019,0.042556004866088216,2
W3102659883,How Much Knowledge Can You Pack Into the Parameters of a Language Model?,2020,0.04239108437224859,2
W3105966348,TinyBERT: Distilling BERT for Natural Language Understanding,2020,0.042091505736969306,2
W2978670439,Neural Network Acceptability Judgments,2019,0.041518789682522664,2
W2963159690,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,2018,0.040867401294833,2
W2951534261,MS MARCO: A Human Generated MAchine Reading COmprehension Dataset,2016,0.04038477910541072,2
W2963969878,Adversarial Examples for Evaluating Reading Comprehension Systems,2017,0.04021407207565429,2
W3030163527,Language Models are Few-Shot Learners,2020,0.04003854321829458,2
W1566289585,Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books,2015,0.04001870102417784,2
W2951434086,Latent Retrieval for Weakly Supervised Open Domain Question Answering,2019,0.0399707858055456,2
W3153427360,Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference,2021,0.03981112832651218,2
W3156636935,SimCSE: Simple Contrastive Learning of Sentence Embeddings,2021,0.03916901090205113,2
W2946417913,BERT Rediscovers the Classical NLP Pipeline,2019,0.03793176096350356,2
W3099655892,UNIFIEDQA: Crossing Format Boundaries with a Single QA System,2020,0.03776592915707968,2
W3205717164,SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer,2022,0.03763106900922211,2
W3034238904,Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,2020,0.03746029438629093,2
W3007672467,REALM: Retrieval-Augmented Language Model Pre-Training,2020,0.03738524733822396,2
W2963854351,Multi-Task Deep Neural Networks for Natural Language Understanding,2019,0.036463092531011806,2
W2964303116,Linguistic Knowledge and Transferability of Contextual Representations,2019,0.036408662567533646,2
W2919420119,,2019,0.036193281622865316,2
W3172642864,It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners,2021,0.03612280434012051,2
W2972324944,What Does BERT Look at? An Analysis of BERT’s Attention,2019,0.036035345071267086,2
W2964223283,CoQA: A Conversational Question Answering Challenge,2019,0.03511841614318246,2
W2946659172,,2019,0.03489756871125468,2
W3122890974,DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION,2021,0.034581516321363484,2
W2557764419,NewsQA: A Machine Comprehension Dataset,2017,0.03453125583041112,2
W2981852735,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,2019,0.03417385058200263,2
W3098267758,AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,2020,0.03412131111241465,2
W2962718483,Simple and Effective Multi-Paragraph Reading Comprehension,2018,0.03367221085887814,2
W3027879771,Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,2020,0.03337211502668921,2
W3035204084,SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization,2020,0.03317850523629156,2
W3205068155,Multitask Prompted Training Enables Zero-Shot Task Generalization,2021,0.03277391809914442,2
W4303648884,State-of-the-art generalisation research in NLP: A taxonomy and review,2022,0.03257380946854609,2
W3104667152,Which *BERT? A Survey Organizing Contextualized Encoders,2020,0.032570590009147714,2
W2948947170,What Does BERT Learn about the Structure of Language?,2019,0.03248139381721035,2
W2963995027,,2019,0.032427127943804625,2
W3098824823,Transformers: State-of-the-Art Natural Language Processing,2020,0.032374324774997375,2
W3105816068,On the Sentence Embeddings from Pre-trained Language Models,2020,0.03233986321971899,2
W2990704537,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,2019,0.032323755091540535,2
W3174770825,Prefix-Tuning: Optimizing Continuous Prompts for Generation,2021,0.032239859617380774,2
W3034457371,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,2020,0.03218080579772116,2
W2970771982,SciBERT: A Pretrained Language Model for Scientific Text,2019,0.03199973352568519,2
W2970454332,Patient Knowledge Distillation for BERT Model Compression,2019,0.031993097030363245,2
W4205991051,The Power of Scale for Parameter-Efficient Prompt Tuning,2021,0.03185681259749581,2
W2890894339,Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering,2018,0.031696202221166356,2
W3044438666,How Can We Know What Language Models Know?,2020,0.03147896613076758,2
W4253205364,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,2019,0.03141648806422039,2
W2898700502,Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks,2018,0.030593980048213797,2
W3024566755,Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond,2020,0.030249868784944327,2
W2962843521,Hypothesis Only Baselines in Natural Language Inference,2018,0.03020457209888803,2
W3004346089,What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models,2020,0.02992833229017088,2
W3185341429,"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",2022,0.029825396962490338,2
W2970062726,Social IQa: Commonsense Reasoning about Social Interactions,2019,0.029472576601011793,2
W2970986510,Knowledge Enhanced Contextual Word Representations,2019,0.02946685176216048,2
W2804897457,Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences,2018,0.029353769233856745,2
W2970780738,Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning,2019,0.02929944789089001,2
W3174702398,Parameter-Efficient Transfer Learning with Diff Pruning,2021,0.029246050345135773,2
W2988421999,MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension,2019,0.029235687913007936,2
W2963866616,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,2018,0.029037800568011946,2
W2963961878,FEVER: a Large-scale Dataset for Fact Extraction and VERification,2018,0.028912494417428346,2
W3173783447,DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations,2021,0.028540275101021233,2
W3151929433,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,2021,0.028046548643547633,2
W3169283738,KILT: a Benchmark for Knowledge Intensive Language Tasks,2021,0.028009527943400864,2
W2938830017,ERNIE: Enhanced Representation through Knowledge Integration,2019,0.027802631026727146,2
W3006881356,A Primer in BERTology: What we know about how BERT works,2020,0.027740829923280932,2
W4296557505,QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension,2022,0.02756707779146753,2
W3152515526,CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP,2021,0.027473841689527052,2
W3008374555,MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers,2020,0.02737331203248309,2
W3111372685,oLMpics-On What Language Model Pre-training Captures,2020,0.027340397237728704,2
W3163832451,The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures,2021,0.02729504656187658,2
W2983995706,KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning,2019,0.027230224694485747,2
W2888302696,QuAC: Question Answering in Context,2018,0.027136229219746864,2
W2946359678,,2019,0.027094450853922376,2
W3105662186,Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension,2020,0.02706826120293922,2
W3156789018,Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering,2021,0.026861882764873654,2
W2950339735,COMET: Commonsense Transformers for Automatic Knowledge Graph Construction,2019,0.026744596673260016,2
W3171654528,Dynabench: Rethinking Benchmarking in NLP,2021,0.026742431028726765,2
W3184751871,QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension,2021,0.026505902383506218,2
W3035267217,How Can We Accelerate Progress Towards Human-like Linguistic Generalization?,2020,0.02640252533498202,2
W3202712981,Measuring and Improving Consistency in Pretrained Language Models,2021,0.026235274865243994,2
W3201363102,STraTA: Self-Training with Task Augmentation for Better Few-shot Learning,2021,0.025967783048926177,2
W3175362188,ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer,2021,0.025773028854856047,2
W2962833140,Explain Yourself! Leveraging Language Models for Commonsense Reasoning,2019,0.025646051395951632,2
W2963756346,Learned in translation: contextualized word vectors,2017,0.025346812407743672,2
W3131870090,COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining,2021,0.02534589233408123,2
W2990928880,Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering,2019,0.02511866275627225,2
W2998369048,A Survey on Machine Reading Comprehension Systems,2020,0.02504589480327684,2
W2413794162,A Decomposable Attention Model for Natural Language Inference,2016,0.025034647832854393,2
W3176047188,Self-Guided Contrastive Learning for BERT Sentence Representations,2021,0.02495100847732877,2
W3162296828,KLUE: Korean Language Understanding Evaluation,2021,0.024827285632943986,2
W3034255912,Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?,2020,0.024804966586987012,2
W3035507081,Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,2020,0.024687050799920913,2
W3046375318,Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing,2021,0.024666313239189934,2
W3207878700,Towards Efficient NLP: A Standard Evaluation and A Strong Baseline,2022,0.02459771332466693,2
W4221145545,Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models,2022,0.024581002550118653,2
W3102762626,Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering,2020,0.024470291072979076,2
W3033187248,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,2020,0.024467362613768393,2
W3174784402,WARP: Word-level Adversarial ReProgramming,2021,0.02441807193981997,2
W3118999024,Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering,2021,0.024328622329425724,2
W3152996058,Relational World Knowledge Representation in Contextual Language Models: A Review,2021,0.024123461958523332,2
W3111289154,Reference Knowledgeable Network for Machine Reading Comprehension,2022,0.024107955053328852,2
W3105055324,Hierarchical Graph Network for Multi-hop Question Answering,2020,0.02408513205335117,2
W3166986030,Factual Probing Is [MASK]: Learning vs. Learning to Recall,2021,0.02404191972565014,2
W1486649854,Skip-thought vectors,2015,0.024039145803190977,2
W2946609015,HellaSwag: Can a Machine Really Finish Your Sentence?,2019,0.023988925296870578,2
W4242776313,Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,2019,0.02394378767928508,2
W4242952243,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations,2021,0.023936637039765056,2
W3152698349,Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little,2021,0.02390303261944207,2
W2963963993,The NarrativeQA Reading Comprehension Challenge,2018,0.02388952600347948,2
W2609826708,SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine,2017,0.023845081662365444,2
W4221153690,LinkBERT: Pretraining Language Models with Document Links,2022,0.023817330388337353,2
W2962753370,SpanBERT: Improving Pre-training by Representing and Predicting Spans,2019,0.023810850444839052,2
W2952984539,Probing Neural Network Comprehension of Natural Language Arguments,2019,0.023808416204526037,2
W3173169192,ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning,2021,0.023779053146705115,2
W2995638926,Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering,2020,0.023753986456312,2
W2950813464,XLNet: Generalized Autoregressive Pretraining for Language Understanding,2019,0.023725430317569785,2
W2998385486,K-BERT: Enabling Language Representation with Knowledge Graph,2020,0.023672720699486855,2
W3098613713,Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually),2020,0.02360868116271389,2
W3104415840,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,2020,0.023578520238799857,2
W2975059944,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,2019,0.02349501245529925,2
W4382246105,Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey,2023,0.023445587415402522,2
W3203309275,Compressing Large-Scale Transformer-Based Models: A Case Study on BERT,2021,0.023414323420967748,2
W4324108774,An Overview on Language Models: Recent Developments and Outlook,2023,0.02331444455124186,2
W2998617917,PIQA: Reasoning about Physical Commonsense in Natural Language,2020,0.023232270236363727,2
W3035599451,"To Test Machine Comprehension, Start by Defining Comprehension",2020,0.023212886118342633,2
W3001393026,Retrospective Reader for Machine Reading Comprehension,2021,0.023211953400164756,2
W3166846774,Learning How to Ask: Querying LMs with Mixtures of Soft Prompts,2021,0.02311774084455176,2
W3175204434,Comparing Test Sets with Item Response Theory,2021,0.023092016650295387,2
W2898662126,ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,2018,0.02304051524916437,2
W2990752173,"Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches",2019,0.023026568830406925,2
W3207976211,Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey,2021,0.02295380464065741,2
W2963691697,AllenNLP: A Deep Semantic Natural Language Processing Platform,2018,0.02292665848238885,2
W2910243263,Assessing BERT's Syntactic Abilities.,2019,0.02290269238583415,2
W3034408878,Pretrained Transformers Improve Out-of-Distribution Robustness,2020,0.02283581701965139,2
W2805206884,A Simple Method for Commonsense Reasoning,2018,0.0228057145889624,2
W3156572742,Benchmarking Machine Reading Comprehension: A Psychological Perspective,2021,0.022728132167893406,2
W2963121782,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,2019,0.022708371397336606,2
W2982756474,Universal Adversarial Triggers for Attacking and Analyzing NLP,2019,0.022628491991649465,2
W3099793224,AdapterHub: A Framework for Adapting Transformers,2020,0.02261489997466127,2
W3213730158,"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",2021,0.02248176087237886,2
W3034995113,"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly",2020,0.02244974453274144,2
W3098068947,Exploring and Predicting Transferability across NLP Tasks,2020,0.02239584345295157,2
W3168194750,What Will it Take to Fix Benchmarking in Natural Language Understanding?,2021,0.022287664281230423,2
W3202120412,A Survey of Knowledge Enhanced Pre-trained Models,2021,0.02218979743693086,2
W2608787653,Enhanced LSTM for Natural Language Inference,2017,0.022078943935942306,2
W2988217457,"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",2019,0.022061243909171302,2
W3099950029,LEGAL-BERT: The Muppets straight out of Law School,2020,0.022015116949491827,2
W4385574293,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,2022,0.022014274265014015,2
W3176828726,BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models,2022,0.02199487172124316,2
W3103368673,What Happens To BERT Embeddings During Fine-tuning?,2020,0.02196795960827579,2
W3139958517,WhiteningBERT: An Easy Unsupervised Sentence Embedding Approach,2021,0.021872145532639723,2
W3037624666,The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding,2020,0.021869861113847096,2
W3103092622,New Protocols and Negative Results for Textual Entailment Data Collection,2020,0.021840976837343942,2
W2970745243,Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning,2019,0.021835850794398936,2
W3174544005,MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers,2021,0.021801643445393835,2
W2970862333,Designing and Interpreting Probes with Control Tasks,2019,0.021799565652946124,2
W2964204621,What you can cram into a single $&amp;!#* vector: Probing sentence embeddings for linguistic properties,2018,0.021768592231627384,2
W2898695519,CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge,2018,0.02168565142701783,2
W3175606037,UnNatural Language Inference,2021,0.02163642437743593,2
W3211686893,DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing,2021,0.021598762054198032,2
W2970442950,Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets,2019,0.02159733979693479,2
W3186095502,Can Transformer Language Models Predict Psychometric Properties?,2021,0.021574600151087597,2
W3013571468,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,2020,0.021551818479021455,2
W3176198948,When Do You Need Billions of Words of Pretraining Data?,2021,0.021485579171531745,2
W3127227595,Measuring and Improving Consistency in Pretrained Language Models,2021,0.021462742102810805,2
W4205137784,A Survey on Machine Reading Comprehension Systems,2022,0.021434487289030385,2
W2970352191,To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks,2019,0.021386252175767256,2
W2985436116,CLER: Cross-task Learning with Expert Representation to Generalize Reading and Understanding,2019,0.021341327938023742,2
W3101066076,Contrastive Distillation on Intermediate Representations for Language Model Compression,2020,0.021335586884350548,2
W3114916066,CoLAKE: Contextualized Language and Knowledge Embedding,2020,0.021302102081714944,2
W3099911888,OCNLI: Original Chinese Natural Language Inference,2020,0.021263349547739317,2
W2994915912,Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model,2019,0.021202686503678903,2
W2970168256,A Discrete Hard EM Approach for Weakly Supervised Question Answering,2019,0.021191051594198602,2
W3115965961,Improving Commonsense Question Answering by Graph-based Iterative Retrieval over Multiple Knowledge Sources,2020,0.021178107906313193,2
W3035038672,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,2020,0.02116397506059597,2
W3102725307,Revisiting Pre-Trained Models for Chinese Natural Language Processing,2020,0.021102744317386863,2
W2798665661,Breaking NLI Systems with Sentences that Require Simple Lexical Inferences,2018,0.021086765999640614,2
W3198080531,Biomedical Question Answering: A Survey of Approaches and Challenges,2022,0.02101342453744088,2
W4385571175,Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering,2023,0.0209985753437673,2
W3213511181,Review and Arrange: Curriculum Learning for Natural Language Understanding,2021,0.02096700361987611,2
W2888329843,Dissecting Contextual Word Embeddings: Architecture and Representation,2018,0.02096347377062258,2
W2170973209,Semi-supervised Sequence Learning,2015,0.0209570388028053,2
W2788496822,SciTaiL: A Textual Entailment Dataset from Science Question Answering,2018,0.020952477462920928,2
W2991223644,Evaluating Commonsense in Pre-Trained Language Models,2020,0.020952147298859348,2
W2974875810,TinyBERT: Distilling BERT for Natural Language Understanding,2019,0.020921300455279623,2
W3188542058,Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification,2022,0.020903689098190494,2
W3092952717,Pretrained Transformers for Text Ranking: BERT and Beyond,2020,0.020887528676537,2
W3097986428,Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering,2020,0.020825847960146457,2
W3198002980,Finetuned Language Models Are Zero-Shot Learners,2021,0.020818902762878694,2
W3153675281,AdapterFusion: Non-Destructive Task Composition for Transfer Learning,2021,0.020775888858060476,2
W4388691863,Language Model Behavior: A Comprehensive Survey,2023,0.02073653068802735,2
W3135638847,Natural language understanding for argumentative dialogue systems in the opinion building domain,2022,0.020723823445043917,2
W2987669390,Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering,2019,0.020695213203223144,2
W3042631625,SBERT-WK: A Sentence Embedding Method by Dissecting BERT-Based Word Models,2020,0.020686339219487537,2
W2987553933,Reasoning Over Paragraph Effects in Situations,2019,0.020677968481801458,2
W3177265267,Q8BERT: Quantized 8Bit BERT,2019,0.02065834013707473,2
W2970161131,Commonsense Knowledge Mining from Pretrained Models,2019,0.020609375610974288,2
W4224909334,A survey of methods for revealing and overcoming weaknesses of data-driven Natural Language Understanding,2022,0.020458431966201666,2
W4298324482,Biomedical Question Answering: A Survey of Approaches and Challenges,2021,0.020446472083574873,2
W4383340139,LogiQA 2.0—An Improved Dataset for Logical Reasoning in Natural Language Understanding,2023,0.020441567898786785,2
W3184402450,Domain-matched Pre-training Tasks for Dense Retrieval,2022,0.02038955404909133,2
W3213241618,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,2021,0.020382994480632355,2
W3205349317,A Few More Examples May Be Worth Billions of Parameters,2022,0.02031845743289087,2
W3210129272,OpenPrompt: An Open-source Framework for Prompt-learning,2022,0.020198315253456858,2
W2906152891,Analysis Methods in Neural Language Processing: A Survey,2019,0.020195528982900755,2
W3007759824,UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training,2020,0.020155551039597663,2
W3169948074,Contextualized Perturbation for Textual Adversarial Attack,2021,0.02014043824433378,2
W4392402185,Datasets for Large Language Models: A Comprehensive Survey,2024,0.02013192896946837,2
W2964120615,The Web as a Knowledge-Base for Answering Complex Questions,2018,0.0201224790181127,2
W2951561177,Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension,2019,0.020112265869088458,2
W3189197390,From LSAT: The Progress and Challenges of Complex Reasoning,2022,0.020102737691102462,2
W3205235328,Pre-trained Language Models in Biomedical Domain: A Systematic Survey,2021,0.02007909248230631,2
W3198599617,Do Prompt-Based Models Really Understand the Meaning of Their Prompts?,2022,0.020059716510439463,2
W3209721572,Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey,2021,0.020057423191674653,2
W3035556416,QuASE: Question-Answer Driven Sentence Encoding,2020,0.020054669052349327,2
W3112776819,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,2021,0.020028199587152143,2
W4386566517,Trained on 100 million words and still in shape: BERT meets British National Corpus,2023,0.020004331739647534,2
W3168867926,LoRA: Low-Rank Adaptation of Large Language Models,2021,0.019948830962696045,2
W4385572001,GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective,2023,0.01992202863939699,2
W3034487470,Similarity Analysis of Contextual Word Representation Models,2020,0.01990266437685808,2
W3047171714,ConvBERT: Improving BERT with Span-based Dynamic Convolution,2020,0.01989247753784057,2
W3015233032,Poor Man's BERT: Smaller and Faster Transformer Models.,2020,0.01988271392365485,2
W3212781905,Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI,2021,0.019879816534896008,2
W4385571038,Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts),2023,0.01987578343902237,2
W2996035354,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,2020,0.01987309648413031,2
W3087148478,Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data,2020,0.019870521776183613,2
W3100353583,Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers,2020,0.01984533899384377,2
W2912817604,End-to-End Open-Domain Question Answering with,2019,0.019817741116946,2
W3196295870,CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade,2021,0.019793710712976427,2
W3138301265,Infusing Finetuning with Semantic Dependencies,2021,0.019776079463419215,2
W2950681488,ELI5: Long Form Question Answering,2019,0.01974290106631915,2
W2975381464,Reducing Transformer Depth on Demand with Structured Dropout,2019,0.019741421566778525,2
W3105645800,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,2020,0.019731562190970275,2
W3211777899,SituatedQA: Incorporating Extra-Linguistic Contexts into QA,2021,0.019730880470534246,2
W2995923603,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,2019,0.019723202433592758,2
W3210877910,Answering Open-Domain Questions of Varying Reasoning Steps from Text,2021,0.01971049778303061,2
W3020268419,Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,2020,0.01970328541480518,2
W3105956114,Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space,2020,0.01967867526602279,2
W3199246732,Distilling Linguistic Context for Language Model Compression,2021,0.01966990453087136,2
W3035030897,FastBERT: a Self-distilling BERT with Adaptive Inference Time,2020,0.019663522544333123,2
W3176549216,How effective is BERT without word ordering? Implications for language understanding and data privacy,2021,0.019652443619896005,2
W3038012435,Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,2020,0.019647140830839033,2
W3174730533,AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models,2021,0.01962577674150678,2
W2911681509,Learning and Evaluating General Linguistic Intelligence,2019,0.019609198813129648,2
W3201531807,Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding,2021,0.01959502455492631,2
W3090656107,JAKET: Joint Pre-training of Knowledge Graph and Language Understanding,2022,0.01958911897526035,2
W4206121183,Condenser: a Pre-training Architecture for Dense Retrieval,2021,0.019548003721744818,2
W3034723486,"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",2020,0.019528004744386433,2
W3106061119,Pre-Training Transformers as Energy-Based Cloze Models,2020,0.01952232031281416,2
W3176807265,Inspecting the concept knowledge graph encoded by modern language models,2021,0.0195094147413702,2
W3169934659,Question Answering Infused Pre-training of General-Purpose Contextualized Representations,2022,0.01950548645953751,2
W4287891024,On Transferability of Prompt Tuning for Natural Language Processing,2022,0.019490055040830186,2
W3115729981,Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity,2020,0.019487762948398935,2
W3015777882,Deep Learning Based Text Classification: A Comprehensive Review,2020,0.019476528037996745,2
W3128560087,Biomedical Question Answering: A Comprehensive Review.,2021,0.019461462418649857,2
W3099624838,BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance,2020,0.019431120671672273,2
W3017131202,Coreferential Reasoning Learning for Language Representation,2020,0.01941261448837656,2
W3105771185,EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering,2020,0.019404477902050462,2
W2740747242,Gated Self-Matching Networks for Reading Comprehension and Question Answering,2017,0.019385988394448536,2
W2996164352,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,2020,0.019371802430132982,2
W2953163841,Compositional Questions Do Not Necessitate Multi-hop Reasoning,2019,0.0193717810495467,2
W2971044268,Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs,2019,0.01936728117226777,2
W4241900798,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,2020,0.01934123532512235,2
W3204669968,Single-dataset Experts for Multi-dataset Question Answering,2021,0.019338869282591514,2
W4385573612,Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE),2022,0.01929786011304255,2
W3103754749,"When BERT Plays the Lottery, All Tickets Are Winning",2020,0.019291466721825617,2
W3174114019,Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval,2021,0.01928788546087876,2
W3099524945,IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,2020,0.01928348397890578,2
W3100124407,Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,2020,0.019281451125368384,2
W3172335055,QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,2021,0.019254167982407037,2
W4205412789,English Machine Reading Comprehension Datasets: A Survey,2021,0.019251529276936453,2
W3213963881,Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference,2021,0.01925034830327492,2
W3100452485,Learning from Task Descriptions,2020,0.019238790505003024,2
W3104939451,Calibration of Pre-trained Transformers,2020,0.019206837800894664,2
W2963101081,ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning,2019,0.019205400222960474,2
W4393684362,Understanding Large Language Models : Towards Rigorous and Targeted Interpretability Using Probing Classifiers and Self-Rationalisation,2024,0.019188362453034312,2
W4391407054,Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling,2024,0.019173757794514055,2
W3101850416,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,2020,0.01916426649084672,2
W2787560479,Deep contextualized word representations,2018,0.019121878692884817,2
W4205924343,Can NLI Models Verify QA Systems’ Predictions?,2021,0.01911329471690165,2
W4206636317,Generating Datasets with Pretrained Language Models,2021,0.01911146510551772,2
W3113529090,Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks,2020,0.019104898646844042,2
W3104136798,Analyzing Redundancy in Pretrained Transformer Models,2020,0.019052057361152494,2
W2963716420,Publicly Available Clinical,2019,0.019046382926084453,2
W2971258845,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,2019,0.019045177134353792,2
W3100475986,Assessing Phrasal Representation and Composition in Transformers,2020,0.01904457195518993,2
W3163322517,Logic-Driven Context Extension and Data Augmentation for Logical Reasoning of Text,2022,0.019032614567002303,2
W3035275890,INFOTABS: Inference on Tables as Semi-structured Data,2020,0.018954937597699832,2
W3101007570,Training Question Answering Models From Synthetic Data,2020,0.018951324940147805,2
W2983102021,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,2019,0.01894790630259103,2
W3155287831,Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models,2022,0.018927005844659842,2
W3034623328,Curriculum Learning for Natural Language Understanding,2020,0.018923994773855354,2
W3102372184,Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text,2020,0.018893022200985835,2
W3020908159,UnifiedQA: Crossing Format Boundaries With a Single QA System,2020,0.018886353919016934,2
W2950618399,Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction,2019,0.018871561438686485,2
W3174429158,What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?,2021,0.018846194996147383,2
W3099403624,Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks,2020,0.0188457829942835,2
W3033406728,DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations,2020,0.018835438160318806,2
W3173591450,Knowing More About Questions Can Help: Improving Calibration in Question Answering,2021,0.018828434224696874,2
W4285160596,Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts,2022,0.0188168353472809,2
W3136149525,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,2021,0.01880273390169165,2
W3017003177,Adversarial Training for Large Neural Language Models,2020,0.01878491460029436,2
W4385573007,Reasoning Like Program Executors,2022,0.018742890480697286,2
W4287854458,Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning,2022,0.018706877906696263,2
W3005441132,K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters,2020,0.018706605536807898,2
W4385571512,Decouple knowledge from paramters for plug-and-play language modeling,2023,0.018695611223910853,2
W4285294723,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,2022,0.01868146043083916,2
W4386275705,A Survey of Knowledge Enhanced Pre-Trained Language Models,2023,0.018648280453092293,2
W3211511509,"Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema",2021,0.01864741974252329,2
W3111364951,Semantics Altering Modifications for Evaluating Comprehension in Machine Reading,2021,0.018637380294786756,2
W3206240757,Virtual Augmentation Supported Contrastive Learning of Sentence Representations,2022,0.018615468799087858,2
W3213868621,Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning,2021,0.01861290930739227,2
W2986728023,Evidence Sentence Extraction for Machine Reading Comprehension,2019,0.01859532142930196,2
W2963453233,BAM! Born-Again Multi-Task Networks for Natural Language Understanding,2019,0.01859367672666725,2
W3136363192,On the effect of dropping layers of pre-trained transformer models,2022,0.018561571331661406,2
W3177323791,Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning,2021,0.018540273464982333,2
W3006963874,Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation,2023,0.018534180149604226,2
W3171291687,ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding,2021,0.018518802220872602,2
W3021533447,How Context Affects Language Models' Factual Predictions,2020,0.01850932751369808,2
W3100452049,Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art,2020,0.018506585453414465,2
W2549835527,Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies,2016,0.01849913721710065,2
W3174782183,Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference,2021,0.01848787960502457,2
W3173826007,On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study,2021,0.01847873597605931,2
W3095771422,CharBERT: Character-aware Pre-trained Language Model,2020,0.0184773880698419,2
W2963804993,Learning Distributed Representations of Sentences from Unlabelled Data,2016,0.018471934471833432,2
W2962922117,Cognitive Graph for Multi-Hop Reading Comprehension at Scale,2019,0.01846113967596845,2
W3018732874,Contextualized Representations Using Textual Encyclopedic Knowledge,2020,0.01844333693055593,2
W4385569751,Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories,2023,0.018435636724281897,2
W3156470785,Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity,2022,0.018399200480856095,2
W3176756782,REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training,2021,0.018394734299526105,2
W3154229486,Whitening Sentence Representations for Better Semantics and Faster Retrieval,2021,0.018381425495624003,2
W3173673636,Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases,2021,0.01837486547996581,2
W3176450819,Towards Semantics-Enhanced Pre-Training: Can Lexicon Definitions Help Learning Sentence Meanings?,2021,0.018369723130527586,2
W3100985894,GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference,2020,0.018344560758510695,2
W2998374885,Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering,2020,0.018335012510529092,2
W3195893957,Prompt-learning for Fine-grained Entity Typing,2022,0.018326603404849397,2
W3172318343,Noise Stability Regularization for Improving BERT Fine-tuning,2021,0.01832250195696287,2
W3092557781,Overview of the Transformer-based Models for NLP Tasks,2020,0.01830053067246741,2
W3173788106,Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks,2021,0.01827098720027745,2
W3177365697,AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization,2021,0.018266583903371852,2
W3103188966,SLM: Learning a Discourse Language Representation with Sentence Unshuffling,2020,0.01826067650895027,2
W3005296017,Pre-training Tasks for Embedding-based Large-scale Retrieval,2020,0.018248861913342577,2
W3175627818,Learning Dense Representations of Phrases at Scale,2021,0.018246240126821862,2
W3099080236,TaxiNLI: Taking a Ride up the NLU Hill,2020,0.01822711666491302,2
W3016473712,MPNet: Masked and Permuted Pre-training for Language Understanding,2020,0.018195332946505723,2
W3197901717,Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression,2021,0.01819188034098688,2
W2962809918,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,2016,0.01818525203540892,2
W3166508187,BioELECTRA:Pretrained Biomedical text Encoder using Discriminators,2021,0.01815878347534149,2
W2970482702,PubMedQA: A Dataset for Biomedical Research Question Answering,2019,0.018155555818630043,2
W3100468923,Question Answering with Long Multiple-Span Answers,2020,0.01815018308959972,2
W3176119108,Few-Shot Question Answering by Pretraining Span Selection,2021,0.018133103367593995,2
W3130347092,Using Prior Knowledge to Guide BERT’s Attention in Semantic Textual Matching Tasks,2021,0.018114514042867062,2
W3204846429,Revisiting Self-training for Few-shot Learning of Language Model,2021,0.018109757803250057,2
W2997200074,ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding,2020,0.01807210993597116,2
W3176647794,Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization,2021,0.018058038065338344,2
W3180942591,AutoBERT-Zero: Evolving BERT Backbone from Scratch,2022,0.018049904653892467,2
W3210923133,The neural architecture of language: Integrative modeling converges on predictive processing,2021,0.01802763055530088,2
W3194309076,Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections,2021,0.018024642243937836,2
W3103621845,Look at the First Sentence: Position Bias in Question Answering,2020,0.018015730696497857,2
W2971105107,Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering,2019,0.018009291533743244,2
W3034987253,RikiNet: Reading Wikipedia Pages for Natural Question Answering,2020,0.017993639778973693,2
W2941666437,Probing What Different NLP Tasks Teach Machines about Function Word Comprehension,2019,0.01798209092684445,2
W3105082862,Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning,2020,0.017950098582182477,2
W2962727366,How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks,2018,0.017947992891940243,2
W3035628711,NILE : Natural Language Inference with Faithful Natural Language Explanations,2020,0.01792769508009382,2
W3126960149,Making Pre-trained Language Models Better Few-shot Learners,2020,0.017902359524430887,2
W3035183289,On the Robustness of Language Encoders against Grammatical Errors,2020,0.01789302071207437,2
W2970069267,EntEval: A Holistic Evaluation Benchmark for Entity Representations,2019,0.017838222796119907,2
W2962881743,Zero-Shot Relation Extraction via Reading Comprehension,2017,0.017834848941560243,2
W3007685714,FreeLB: Enhanced Adversarial Training for Natural Language Understanding,2019,0.01782899302465342,2
W2970139579,Revealing the Importance of Semantic Retrieval for Machine Reading at Scale,2019,0.017827154730724968,2
W2964222271,Improving Machine Reading Comprehension with General Reading Strategies,2019,0.017821468878027838,2
W4386566688,MetaQA: Combining Expert Agents for Multi-Skill Question Answering,2023,0.01781683984939942,2
W3105183573,Adversarial Self-Supervised Data-Free Distillation for Text Classification,2020,0.017790736042619507,2
W3100652389,An Unsupervised Sentence Embedding Method by Mutual Information Maximization,2020,0.017775127251470194,2
W4385570329,Complex Reasoning in Natural Languag,2023,0.01776948746461693,2
W3035153870,SenseBERT: Driving Some Sense into BERT,2020,0.017762945564358642,2
W3118741274,Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation,2021,0.017756381794271588,2
W2986532682,An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering,2019,0.017753669493614302,2
W3174082608,BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?,2021,0.017751219467206963,2
W2996159613,Reducing Transformer Depth on Demand with Structured Dropout,2020,0.01772042070930948,2
W4385573468,Improved Universal Sentence Embeddings with Prompt-based Contrastive Learning and Energy-based Learning,2022,0.017705144639468113,2
W2949134692,Multi-hop Reading Comprehension through Question Decomposition and Rescoring,2019,0.01767772646033182,2
W3159795318,KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding,2021,0.017674817707762934,2
W2998183051,Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT,2020,0.01766298818049999,2
W3158542464,Bidirectional Language Modeling: A Systematic Literature Review,2021,0.017651128331613954,2
W3034417881,Span Selection Pre-training for Question Answering,2020,0.017649564838364086,2
W3104499181,Unsupervised Commonsense Question Answering with Self-Talk,2020,0.017639492627803583,2
W4385573618,Towards Unified Prompt Tuning for Few-shot Text Classification,2022,0.01762286972806789,2
W3170739233,SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval,2021,0.01761973710989309,2
W3103291112,Generative Data Augmentation for Commonsense Reasoning,2020,0.017600733131877055,2
W3035428952,Injecting Numerical Reasoning Skills into Language Models,2020,0.01758141680019293,2
W4401717597,Toward Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models,2024,0.01757250651543561,2
W4389523725,"Out-of-Distribution Generalization in Natural Language Processing: Past, Present, and Future",2023,0.01756547445564509,2
W3199348444,Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers,2021,0.01754793214337375,2
W3134929386,Enhancing Transformer-based language models with commonsense representations for knowledge-driven machine comprehension,2021,0.017546885505516502,2
W3164896303,Knowledge Inheritance for Pre-trained Language Models,2022,0.017542975017136083,2
W3168921237,Self-training Improves Pre-training for Natural Language Understanding,2021,0.017529164750194805,2
W2945067664,Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark,2019,0.017526411408793702,2
W4386566428,Transformers with Learnable Activation Functions,2023,0.01752135868603024,2
W3154710343,HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing,2021,0.01751793959539027,2
W3101082165,HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data,2020,0.017499595470309073,2
W3106070274,Pruning Redundant Mappings in Transformer Models via Spectral-Normalized Identity Prior,2020,0.017491564146377668,2
W2968908603,"Align, Mask and Select: A Simple Method for Incorporating Commonsense Knowledge into Language Representation Models",2019,0.017477775787506297,2
W3023419341,Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?,2020,0.017469073216812805,2
W2989312920,Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering,2019,0.01746858419399819,2
W3139145960,Local Interpretations for Explainable Natural Language Processing: A Survey,2021,0.01746627590839805,2
W2952484617,Multi-Hop Paragraph Retrieval for Open-Domain Question Answering,2019,0.01746480364480467,2
W3137305332,Semantic Models for the First-Stage Retrieval: A Comprehensive Review,2022,0.01745619678077315,2
W2953369973,What do you learn from context? Probing for sentence structure in contextualized word representations,2019,0.01742349543487057,2
W3133407825,Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge,2021,0.017419874940414887,2
W3099944244,Open-Retrieval Conversational Question Answering,2020,0.01741175735740742,2
W3169841173,Hurdles to Progress in Long-form Question Answering,2021,0.01738899063386146,2
W4224296706,MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation,2022,0.01737891838225472,2
W3152258780,What Will it Take to Fix Benchmarking in Natural Language Understanding,2021,0.017373224428665804,2
W2973154008,Open Sesame: Getting inside BERT’s Linguistic Knowledge,2019,0.017361958272023244,2
W4385456320,Pre-trained Language Models in Biomedical Domain: A Systematic Survey,2023,0.017325773786223228,2
W4385570731,Dynamic Heterogeneous-Graph Reasoning with Language Models and Knowledge Representation Learning for Commonsense Question Answering,2023,0.017313266225736906,2
W4385573694,Finding Skill Neurons in Pre-trained Transformer-based Language Models,2022,0.01731010379238543,2
W3035408713,DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering,2020,0.01728975262887217,2
W3035139434,End-to-End Bias Mitigation by Modelling Biases in Corpora,2020,0.017273681533566083,2
W3217374296,DKPLM: Decomposable Knowledge-Enhanced Pre-trained Language Model for Natural Language Understanding,2022,0.017259928840391115,2
W3098576111,TernaryBERT: Distillation-aware Ultra-low Bit BERT,2020,0.017258114731224412,2
W2998230451,Semantics-Aware BERT for Language Understanding,2020,0.01725132875629772,2
W3174169056,Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?,2021,0.01721402746045562,2
W3200593933,Comprehensive analysis of embeddings and pre-training in NLP,2021,0.01718525992500495,2
W3213415226,TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification,2021,0.01716917834194691,2
W4285214521,Data Augmentation for Biomedical Factoid Question Answering,2022,0.017154993717393555,2
W3170180819,Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence,2021,0.017139954291662308,2
W3104215796,Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,2020,0.01713144693434637,2
W3201734921,FewNLU: Benchmarking State-of-the-Art Methods for Few-Shot Natural Language Understanding,2022,0.017127998640081316,2
W3173188814,Bootstrapped Unsupervised Sentence Representation Learning,2021,0.017114833160569897,2
W3114219454,ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning,2020,0.01710804577454835,2
W3105424285,Utility is in the Eye of the User: A Critique of NLP Leaderboards,2020,0.017096523935952058,2
W3166204619,"BioM-Transformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA",2021,0.017096433831479043,2
W2986266667,Do NLP Models Know Numbers? Probing Numeracy in Embeddings,2019,0.017091471700900116,2
W2945290257,A Surprisingly Robust Trick for the Winograd Schema Challenge,2019,0.017084759801127915,2
W4385571291,Pre-trained Language Models Can be Fully Zero-Shot Learners,2023,0.017071796444258054,2
W3102844651,Entities as Experts: Sparse Memory Access with Entity Supervision,2020,0.01707048243258697,2
W4285280220,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,2022,0.017049082202718682,2
W2799007037,Semantically Equivalent Adversarial Rules for Debugging NLP models,2018,0.017032576353363155,2
W3205810519,Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models,2022,0.017003087031934143,2
W2971155257,PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text,2019,0.016976989747991552,2
W3177028045,Benchmarking Robustness of Machine Reading Comprehension Models,2021,0.016958471537145543,2
W3015372447,Multi-Step Inference for Reasoning Over Paragraphs,2020,0.01695752900257257,2
W3212706150,On Transferability of Prompt Tuning for Natural Language Understanding,2021,0.016950809625056698,2
W3207539211,Dict-BERT: Enhancing Language Model Pre-training with Dictionary,2022,0.01690765237442782,2
W3163018411,Sentence Similarity Based on Contexts,2022,0.016903791020833333,2
W3174708387,"Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search",2021,0.016900071647374115,2
W3035324702,SPECTER: Document-level Representation Learning using Citation-informed Transformers,2020,0.016884705799722382,2
W3022845607,Self-supervised Knowledge Triplet Learning for Zero-shot Question Answering,2020,0.016860162390766164,2
W3100292568,AmbigQA: Answering Ambiguous Open-domain Questions,2020,0.01685802861993071,2
W2970023150,Answering Complex Open-domain Questions Through Iterative Query Generation,2019,0.016835873185762823,2
W3034775979,Masked Language Model Scoring,2020,0.016827093581531952,2
W3118608099,BERT & Family Eat Word Salad: Experiments with Text Understanding,2021,0.016820331602313186,2
W3153839026,SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning,2021,0.016806757971971033,2
W3092448486,On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers,2020,0.01680575175782532,2
W4385573097,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,2022,0.016801339494361606,2
W4287887877,Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022),2022,0.0167993864890754,2
W4313908941,SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Cross-lingual Focused Evaluation,2017,0.016791037374474554,2
W3035599593,Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition,2020,0.016789831812940633,2
W2970283086,Show Your Work: Improved Reporting of Experimental Results,2019,0.01678678438223662,2
W3093582159,ANLIzing the Adversarial Natural Language Inference Dataset.,2020,0.01677426824752534,2
W3176693010,On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation,2021,0.01675970772890895,2
W3194782062,Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models,2022,0.016757189867154163,2
W3175910413,UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark,2021,0.016724432182327994,2
W3034520363,Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering,2020,0.016719068757760747,2
W3213074177,TaCL: Improving BERT Pre-training with Token-aware Contrastive Learning,2022,0.016695823580925792,2
W2996851481,Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,2020,0.01669171602966374,2
W3104417388,Cross-Thought for Sentence Encoder Pre-training,2020,0.016681602920361253,2
W3098467034,What Can We Learn from Collective Human Opinions on Natural Language Inference Data?,2020,0.016681208523989112,2
W3039017601,Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering,2020,0.016672285440329396,2
W3175277088,On Commonsense Cues in BERT for Solving Commonsense Tasks,2021,0.016664362971071097,2
W3192478068,A Robustly Optimized BERT Pre-training Approach with Post-training,2021,0.016664242960635272,2
W3035441651,Selective Question Answering under Domain Shift,2020,0.0166561904507115,2
W3122838366,Semantic Re-tuning with Contrastive Tension,2021,0.016639342195818387,2
W2790235966,SentEval: An Evaluation Toolkit for Universal Sentence Representations,2018,0.016635526201965584,2
W3102999298,Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models,2020,0.01663290632683427,2
W3035129496,Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset,2020,0.016629902756471564,2
W4385570792,MVP: Multi-task Supervised Pre-training for Natural Language Generation,2023,0.01661030405933358,2
W3175287561,COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences,2021,0.01660464794619311,2
W2973049837,CTRL: A Conditional Transformer Language Model for Controllable Generation,2019,0.01659800160324713,2
W3173581459,Natural Language Inference in Context - Investigating Contextual Reasoning over Long Texts,2021,0.01659595309697837,2
W3015253856,CLUE: A Chinese Language Understanding Evaluation Benchmark,2020,0.016550596941963175,2
W4390692489,Unifying Large Language Models and Knowledge Graphs: A Roadmap,2024,0.016549054227500252,2
W3170719901,Are Pretrained Convolutions Better than Pretrained Transformers?,2021,0.016532915540187213,2
W2963662654,Question Answering by Reasoning Across Documents with Graph Convolutional Networks,2019,0.016530862264481708,2
W2991265431,Do Attention Heads in BERT Track Syntactic Dependencies?,2019,0.016529991821513414,2
W3210120707,Pre-Training With Whole Word Masking for Chinese BERT,2021,0.01652591897893869,2
W4206778668,ReasonBERT: Pre-trained to Reason with Distant Supervision,2021,0.01652402955160899,2
W3035503910,ERASER: A Benchmark to Evaluate Rationalized NLP Models,2020,0.01652111082346326,2
W2924902521,Distilling Task-Specific Knowledge from BERT into Simple Neural Networks,2019,0.016499171661712716,2
W3098757391,MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,2020,0.01648878824010191,2
W3100055476,Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering,2019,0.016484771354159714,2
W3197287583,Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension,2021,0.01648102684168353,2
W3110846353,Reinforced Multi-Teacher Selection for Knowledge Distillation,2021,0.016464032443233256,2
W3035172163,The Sensitivity of Language Models and Humans to Winograd Schema Perturbations,2020,0.016434754820130095,2
W3186799149,What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams,2021,0.016408530824540097,2
W2133458109,"SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability",2015,0.016405972178794494,2
W3104216863,Structured Pruning of Large Language Models,2020,0.016388969839166136,2
W3199064974,CSS-LM: A Contrastive Framework for Semi-Supervised Fine-Tuning of Pre-Trained Language Models,2021,0.016388796715087025,2
W3022969335,"When BERT Plays the Lottery, All Tickets Are Winning",2020,0.01638827191892915,2
W3110664450,MASKER: Masked Keyword Regularization for Reliable Text Classification,2021,0.01635955114225146,2
W3170790933,"Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models",2021,0.016356861145998804,2
W3173195958,GhostBERT: Generate More Features with Cheap Operations for BERT,2021,0.016355253786781366,2
W3175505246,EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets,2021,0.016350529010512836,2
W4287854436,LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework,2022,0.016342734729383697,2
W4282945631,Improving Pretrained Language Model Fine-Tuning With Noise Stability Regularization,2023,0.016336968418912655,2
W3099414857,Know What You Don't Need: Single-Shot Meta-Pruning for Attention Heads,2020,0.01633457868619246,2
W2997759614,"Select, Answer and Explain: Interpretable Multi-Hop Reading Comprehension over Multiple Documents",2020,0.016315305775231936,2
W4385567255,InforMask: Unsupervised Informative Masking for Language Model Pretraining,2022,0.01630896638310991,2
W3115295967,CLEAR: Contrastive Learning for Sentence Representation,2020,0.016291608837421152,2
W3101855539,Natural Language Inference in Context -- Investigating Contextual Reasoning over Long Texts,2020,0.016279090509932603,2
W3160106041,Elbert: Fast Albert with Confidence-Window Based Early Exit,2021,0.016273586442293885,2
W3099843385,An Analysis of Natural Language Inference Benchmarks through the Lens of Negation,2020,0.016269174385354416,2
W3098057198,Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering,2018,0.016256089672836926,2
W3100501376,With Little Power Comes Great Responsibility,2020,0.016253472866423483,2
W3100980998,Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning,2020,0.016249547662114897,2
W3015721781,What do Models Learn from Question Answering Datasets?,2020,0.016221709416123508,2
W3100618740,Question Directed Graph Attention Network for Numerical Reasoning over Text,2020,0.016218330290716215,2
W3022717752,What Happens To BERT Embeddings During Fine-tuning?,2020,0.016210485469969983,2
W4385574313,Leveraging QA Datasets to Improve Generative Data Augmentation,2022,0.016203189512029738,2
W4225104598,LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval,2022,0.016193996733466574,2
W4285123797,Tracing Origins: Coreference-aware Machine Reading Comprehension,2022,0.01617005389859544,2
W2996728628,BLiMP: The Benchmark of Linguistic Minimal Pairs for English,2020,0.01616539924647638,2
W3177071108,DynaSent: A Dynamic Benchmark for Sentiment Analysis,2021,0.016163549090708045,2
W3169726359,Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge,2021,0.01615892747000822,2
W4386566638,Should You Mask 15% in Masked Language Modeling?,2023,0.016111291845356056,2
W4385570371,A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets,2023,0.016103663594904394,2
W3115947671,Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps,2020,0.01609480207388294,2
W3168251909,Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization,2021,0.01609479945100406,2
W3202125623,DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings,2021,0.016091083677338622,2
W3015440086,Knowledge Fusion and Semantic Knowledge Ranking for Open Domain Question Answering,2020,0.016081979207839416,2
W2996848635,QASC: A Dataset for Question Answering via Sentence Composition,2020,0.016078578573862624,2
W4285107336,Prompt-free and Efficient Few-shot Learning with Language Models,2022,0.01605339976629495,2
W3207964656,Interpreting Deep Learning Models in Natural Language Processing: A Review,2021,0.01604657734699851,2
W3113389843,REM-Net: Recursive Erasure Memory Network for Commonsense Evidence Refinement,2021,0.016040578012826184,2
W3207095490,PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them,2021,0.016040148015749482,2
W2462305634,"SemEval-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",2016,0.016025898705497393,2
W4385756485,Learning to Perturb for Contrastive Learning of Unsupervised Sentence Representations,2023,0.016019150014007725,2
W3032553678,Neural entity linking: A survey of models based on deep learning,2022,0.016009754009928583,2
W3034292689,The Right Tool for the Job: Matching Model and Instance Complexities,2020,0.016009252266625865,2
W4385570610,MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning,2023,0.01600883627250212,2
W4229019932,Improving In-Context Few-Shot Learning via Self-Supervised Training,2022,0.016008717681253313,2
W3098987177,ConjNLI: Natural Language Inference Over Conjunctive Sentences,2020,0.016002782162417253,2
W3101284630,Pretrained Language Model Embryology: The Birth of ALBERT,2020,0.01599452938691577,2
W3176793246,Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?,2021,0.01597530320062026,2
W2971033911,Visualizing and Understanding the Effectiveness of BERT,2019,0.01596530164395544,2
W3163458033,QAConv: Question Answering on Informative Conversations,2022,0.015953400110607634,2
W3199893015,Pairwise Supervised Contrastive Learning of Sentence Representations,2021,0.015943559092083625,2
W4280550797,Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence,2022,0.01593949432598931,2
W3211384762,Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models,2021,0.015931859970289636,2
W4389519222,Findings of the BabyLM Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora,2023,0.015924107746349902,2
W3103884771,BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance,2020,0.015920848731771953,2
W3101449015,BERT-ATTACK: Adversarial Attack Against BERT Using BERT,2020,0.01591877453915216,2
W3164972323,True Few-Shot Learning with Language Models,2021,0.015917655379788273,2
W3206210649,Analyzing Dynamic Adversarial Training Data in the Limit,2022,0.01591656507032546,2
W3205544360,bert2BERT: Towards Reusable Pretrained Language Models,2022,0.01591270509621155,2
W3199467273,On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain,2022,0.015905182047120464,2
W3100283070,E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT,2020,0.015904034891338382,2
W3199958362,How Can We Know <i>When</i> Language Models Know? On the Calibration of Language Models for Question Answering,2021,0.01589571933146483,2
W4367680667,Contrastive Learning Models for Sentence Representations,2023,0.015869495239235497,2
W3008219293,TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing,2020,0.01585399478535666,2
W4385570763,Downstream Datasets Make Surprisingly Good Pretraining Corpora,2023,0.015852614032651646,2
W3106109117,Text Classification Using Label Names Only: A Language Model Self-Training Approach,2020,0.015838277962251835,2
W4285126771,Knowledge-Augmented Methods for Natural Language Processing,2022,0.015835220411819773,2
W3098300729,On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers,2020,0.015822397014716657,2
W3101056292,ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning,2020,0.015814045296588506,2
W4287855143,Learning to Retrieve Passages without Supervision,2022,0.0158083311683627,2
W3184516261,Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models,2021,0.015791745972605777,2
W3174531908,Fusing Context Into Knowledge Graph for Commonsense Question Answering,2021,0.015791004422765122,2
W3015298864,MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices,2020,0.015753570537941877,2
W3146844750,"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",2021,0.015749314818337145,2
W3028836324,Multi-task learning for natural language processing in the 2020s: Where are we going?,2020,0.01574403439467238,2
W4285171787,Right for the Right Reason: Evidence Extraction for Trustworthy Tabular Reasoning,2022,0.015743764289038082,2
W2963012544,Character-level Convolutional Networks for Text Classification,2015,0.015742761862487662,2
W3114537677,Universal Sentence Representation Learning with Conditional Masked Language Model,2021,0.015742404861281284,2
W3156389890,Consistency Training with Virtual Adversarial Discrete Perturbation,2022,0.015721824216166237,2
W3105892552,"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition",2020,0.015715684992297228,2
W4385567149,Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?,2022,0.015711165067017616,2
W3015609966,DynaBERT: Dynamic BERT with Adaptive Width and Depth,2020,0.015698789317018534,2
W3212725701,Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder,2021,0.01569472981748514,2
W2998072062,Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets,2020,0.01568950431805046,2
W3202028501,Understanding and Overcoming the Challenges of Efficient Transformer Quantization,2021,0.01568799250163994,2
W3103227045,Context-Aware Answer Extraction in Question Answering,2020,0.015680537075916868,2
W3036463250,SqueezeBERT: What can computer vision teach NLP about efficient neural networks?,2020,0.015679792614217664,2
W3034651559,A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction,2020,0.015670938539077608,2
W3115413024,Aspect-based Document Similarity for Research Papers,2020,0.015665904233142135,2
W3169341408,CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark,2022,0.01566045618166547,2
W2971236147,“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding,2019,0.015654195562733834,2
W3117422238,Conversational Machine Comprehension: a Literature Review,2020,0.015653409590574912,2
W3087922520,RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms,2021,0.015645370635315112,2
W4385574363,It Is Not Easy To Detect Paraphrases: Analysing Semantic Similarity With Antonyms and Negation Using the New SemAntoNeg Benchmark,2022,0.01563283209812714,2
W3106339673,PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Context-conditioned Generation,2020,0.015625611795645818,2
W3174731106,"MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation",2021,0.015617106431174445,2
W3035352537,Syntactic Data Augmentation Increases Robustness to Inference Heuristics,2020,0.0156087298858575,2
W2889468083,Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation,2018,0.015589065443774381,2
W3170037207,Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models,2021,0.015574327735491836,2
W3034878914,Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order,2020,0.015573790997713596,2
W3154463028,"Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks",2021,0.015572946716093425,2
W3202586005,Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics,2021,0.015568526874601979,2
W4385573898,ASQA: Factoid Questions Meet Long-Form Answers,2022,0.015565702710811478,2
W3174057701,"Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models",2021,0.015563137903005968,2
W2950336186,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,2019,0.015554603481292404,2
W2971141916,Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations,2019,0.015548567915357915,2
W3182778088,Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills,2022,0.0155359744644984,2
W4382239116,SKDBERT: Compressing BERT via Stochastic Knowledge Distillation,2023,0.015528825653302504,2
W3199446690,Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-training,2022,0.015526238845283473,2
W2891177506,Universal Sentence Encoder for English,2018,0.015522564813816848,2
W2950729111,Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index,2019,0.015513166736253234,2
W3102554603,TextHide: Tackling Data Privacy in Language Understanding Tasks,2020,0.015510375946433043,2
W3207796810,KG-FiD: Infusing Knowledge Graph in Fusion-in-Decoder for Open-Domain Question Answering,2022,0.015493572098345154,2
W4377820893,Improving Pre-trained Language Models,2023,0.015491738917357552,2
W3208646411,"Flexibly Focusing on Supporting Facts, Using Bridge Links, and Jointly Training Specialized Modules for Multi-Hop Question Answering",2021,0.015484732250635288,2
W3113425182,SemEval-2020 Task 4: Commonsense Validation and Explanation,2020,0.015476810955927551,2
W3094629756,Transfer fine-tuning of BERT with phrasal paraphrases,2020,0.015458957603838286,2
W3089631405,Evaluating Models' Local Decision Boundaries via Contrast Sets,2020,0.01545192212984236,2
W3138794547,Visualizing Transformers for NLP: A Brief Survey,2020,0.015427155042600265,2
W3104350794,Analyzing Individual Neurons in Pre-trained Language Models,2020,0.015423468641275388,2
W4385573003,RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning,2022,0.015423152464951833,2
W3034510440,A Systematic Assessment of Syntactic Generalization in Neural Language Models,2020,0.015421433887285563,2
W2970678056,Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks,2019,0.015420460080983661,2
W4385572466,Masked Latent Semantic Modeling: an Efficient Pre-training Alternative to Masked Language Modeling,2023,0.015417623338326691,2
W3035102548,Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings,2020,0.015412169415010231,2
W3198757395,How much pretraining data do language models need to learn syntax?,2021,0.015408137278196971,2
W3008686018,How Much Knowledge Can You Pack Into the Parameters of a Language Model,2020,0.015405290542249544,2
W2970200208,"Benchmarking Zero-shot Text Classification: Datasets, Evaluation and Entailment Approach",2019,0.015396583185162436,2
W3096580779,Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification,2020,0.015394834672312188,2
W3100355250,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,2020,0.015387693388847644,2
W3202031169,Paradigm Shift in Natural Language Processing,2022,0.015381652803794279,2
W3185070134,Demystifying Neural Language Models' Insensitivity to Word-Order,2021,0.015379652617313257,2
W3205668414,Distilling Relation Embeddings from Pretrained Language Models,2021,0.015378657643962356,2
W2126209950,The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations,2015,0.015376497108346597,2
W3212093422,Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering,2021,0.01536160691501958,2
W3173617765,Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models,2022,0.015348166105791785,2
W4385573323,Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis,2022,0.015344529088167889,2
W4385572787,EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing,2022,0.015341043464569869,2
W3152409010,Exploring the Role of BERT Token Representations to Explain Sentence Probing Results,2021,0.015339975116185116,2
W3173618889,The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing,2021,0.015337544142332852,2
W4385571706,Dynamic Transformers Provide a False Sense of Efficiency,2023,0.015325238764249733,2
W3118568258,I-BERT: Integer-only BERT Quantization,2021,0.01532326106577365,2
W4384302764,AI-based Question Answering Assistance for Analyzing Natural-language Requirements,2023,0.01531670633917895,2
W3174657338,EBERT: Efficient BERT Inference with Dynamic Structured Pruning,2021,0.015313217240679685,2
W3117312003,Out of Order: How Important Is The Sequential Order of Words in a Sentence in Natural Language Understanding Tasks?,2020,0.015307610511117259,2
W3197979077,Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning,2021,0.01529830450330579,2
W2979196189,Multi-hop Question Answering via Reasoning Chains,2019,0.015284202061429976,2
W4281660701,Mokey,2022,0.015284114113449834,2
W3152801999,QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,2021,0.01527028023368226,2
W3099756172,FQuAD: French Question Answering Dataset,2020,0.015266918576094657,2
W3099215402,TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification,2020,0.015262091905955857,2
W4223560628,A Comparative Study of Pre-trained Encoders for Low-Resource Named Entity Recognition,2022,0.015261803527238374,2
W2996657743,TabFact: A Large-scale Dataset for Table-based Fact Verification,2020,0.015260583338879316,2
W3037763555,Commonsense Reasoning for Natural Language Processing,2020,0.015256445589688396,2
W3119821914,Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting,2021,0.01525455557480494,2
W2963430447,Unsupervised Question Answering by Cloze Translation,2019,0.015251834160876401,2
W4205537036,Probing Across Time: What Does RoBERTa Know and When?,2021,0.015249060622383473,2
W4382463788,A Survey on Model Compression and Acceleration for Pretrained Language Models,2023,0.015245181175672487,2
W3171446474,DAGN: Discourse-Aware Graph Network for Logical Reasoning,2021,0.015236970014056883,2
W4389977189,A survey of GPT-3 family large language models including ChatGPT and GPT-4,2023,0.015222184888108033,2
W4249573750,CERT: Contrastive Self-supervised Learning for Language Understanding,2020,0.015204688832081117,2
W3114100246,CS-NLP Team at SemEval-2020 Task 4: Evaluation of State-of-the-art NLP Deep Learning Architectures on Commonsense Reasoning Task,2020,0.015201791702870468,2
W3093452197,Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models,2020,0.015196919123714793,2
W2799054028,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,2018,0.01518981019077975,2
W4206118214,Editing Factual Knowledge in Language Models,2021,0.015188835651940175,2
W2914526845,Multi-Task Deep Neural Networks for Natural Language Understanding,2019,0.015181287658023703,2
W2963515589,Multi-style Generative Reading Comprehension,2019,0.01517318187970126,2
W3173374050,BinaryBERT: Pushing the Limit of BERT Quantization,2021,0.015169177782706622,2
W3174088532,How transfer learning impacts linguistic knowledge in deep NLP models?,2021,0.01515733357015017,2
W3162385798,How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering,2020,0.015117464405083375,2
W4320463733,Acquiring and Modelling Abstract Commonsense Knowledge via Conceptualization,2022,0.015111677402774606,2
W3007728469,The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding,2020,0.015105795160237068,2
W3188635098,"Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining",2021,0.015103474562272326,2
W2963829073,Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge,2018,0.015094806255556344,2
W3212176791,TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation,2021,0.01508908860778625,2
W4221142828,Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View,2022,0.01508144917313919,2
W4385574101,Contrastive Demonstration Tuning for Pre-trained Language Models,2022,0.015080826220901221,2
W2997561853,MMM: Multi-Stage Multi-Task Learning for Multi-Choice Reading Comprehension,2020,0.015068674689991615,2
W3099876468,A Simple Yet Strong Pipeline for HotpotQA,2020,0.015061628499797633,2
W3186545525,UniK-QA: Unified Representations of Structured and Unstructured Knowledge for Open-Domain Question Answering,2022,0.015061307787714423,2
W4224313754,DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings,2022,0.015052323640514922,2
W3188983256,Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval,2022,0.01504151357275322,2
W2809324505,The Natural Language Decathlon: Multitask Learning as Question Answering,2018,0.015038460823281322,2
W3155638556,Subsentence Extraction from Text Using Coverage-Based Deep Learning Language Models,2021,0.0150376305325369,2
W3152956381,The Power of Scale for Parameter-Efficient Prompt Tuning,2021,0.015027422342480966,2
W3169394946,Are we there yet? Exploring clinical domain knowledge of BERT models,2021,0.015002004681646353,2
W3119636502,UnNatural Language Inference,2020,0.014995494987336619,2
W4385573679,"Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models",2022,0.01498042146811923,2
W3205949070,Towards a Unified View of Parameter-Efficient Transfer Learning,2021,0.014968335371894858,2
W3088049945,SberQuAD – Russian Reading Comprehension Dataset: Description and Analysis,2020,0.014963599423632172,2
W2949847757,"Retrieve, Read, Rerank: Towards End-to-End Multi-Document Reading Comprehension",2019,0.014959842545339847,2
W4385574298,Prompt Consistency for Zero-Shot Task Generalization,2022,0.014959041953125772,2
W4220967417,AMMU: A survey of transformer-based biomedical pretrained language models,2021,0.014957183120284549,2
W3200092677,How Can the [MASK] Know? The Sources and Limitations of Knowledge in BERT,2021,0.014954154113554361,2
W4285296644,A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple-wise Perspective in Angular Space,2022,0.014952314696879108,2
W3139164912,Complex Factoid Question Answering with a Free-Text Knowledge Graph,2020,0.014938573758714328,2
W4226136917,A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation,2022,0.014937456387392332,2
W4385573170,PromptBERT: Improving BERT Sentence Embeddings with Prompts,2022,0.014933470579897416,2
W3124034626,Predicting Inductive Biases of Pre-Trained Models,2021,0.014929128792057877,2
W3162922479,What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams,2021,0.014927463424841103,2
W3184200018,MinD at SemEval-2021 Task 6: Propaganda Detection using Transfer Learning and Multimodal Fusion,2021,0.014926746149019138,2
W4294969216,BERT &amp; Family Eat Word Salad: Experiments with Text Understanding,2021,0.014924776992227598,2
W3113521481,Read and Reason with MuSeRC and RuCoS: Datasets for Machine Reading Comprehension for Russian,2020,0.01491977483449917,2
W3204112174,LexGLUE: A Benchmark Dataset for Legal Language Understanding in English,2022,0.014906104225069564,2
W3157374291,Entailment as Few-Shot Learner,2021,0.014904767222577033,2
W4281262985,Are Prompt-based Models Clueless?,2022,0.014900453442496587,2
W3103136066,Linguistically-Informed Transformations (LIT): A Method for Automatically Generating Contrast Sets,2020,0.014895006375471158,2
W2997789497,Probing Natural Language Inference Models through Semantic Fragments,2020,0.014872147157241921,2
W2984256198,Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual,2019,0.014868850767934242,2
W3103629016,Why do you think that? Exploring Faithful Sentence-Level Rationales Without Supervision,2020,0.014868801169885022,2
W2951365061,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,2019,0.014865464555975953,2
W3138008297,Identifying Machine-Paraphrased Plagiarism,2023,0.014855500849303968,2
W2985797697,Understanding Commonsense Inference Aptitude of Deep Contextual Representations,2019,0.014855421515887379,2
W3173361789,Learn to Resolve Conversational Dependency: A Consistency Training Framework for Conversational Question Answering,2021,0.01485205418607491,2
W4286955624,Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations,2021,0.01483310762156975,2
W4389520167,RobustEmbed: Robust Sentence Embeddings Using Self-Supervised Contrastive Pre-Training,2023,0.014831737522223557,2
W3182352988,CokeBERT: Contextual knowledge selection and embedding towards enhanced pre-trained language models,2021,0.014827938669668144,2
W4386566763,Don’t Blame the Annotator: Bias Already Starts in the Annotation Instructions,2023,0.014811632130958945,2
W3175352680,End-to-End Self-Debiasing Framework for Robust NLU Training,2021,0.014806399990686678,2
W3155807546,Retrieval Augmentation Reduces Hallucination in Conversation,2021,0.014783506313823058,2
W3174986053,TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance,2021,0.01477974666124603,2
W4385571730,PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification,2023,0.01477766964369394,2
W3173902720,Positional Artefacts Propagate Through Masked Language Model Embeddings,2021,0.014770937389704948,2
W2946794439,"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",2019,0.014769908860242346,2
W4385564918,Elaboration-Generating Commonsense Question Answering at Scale,2023,0.014765782066134782,2
W3136275640,Customizing Contextualized Language Models for Legal Document Reviews,2020,0.01475784515967665,2
W3175270222,RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge,2021,0.0147395487988735,2
W3093543164,TweetBERT: A Pretrained Language Representation Model for Twitter Text Analysis,2020,0.014739314256835747,2
W2997090102,TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection,2020,0.014729823618622184,2
W2981320891,MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension,2019,0.014729201350900974,2
W3209632425,A Simple and Effective Positional Encoding for Transformers,2021,0.014705636312819481,2
W3034503989,Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT,2020,0.014705532969914352,2
W3176693244,Evaluating Entity Disambiguation and the Role of Popularity in Retrieval-Based NLP,2021,0.014701494281369156,2
W4385573342,Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective,2022,0.014694618027295147,2
W3212748247,Few-Shot Self-Rationalization with Natural Language Prompts,2022,0.014691093564577325,2
W4385574466,ConGen: Unsupervised Control and Generalization Distillation For Sentence Representation,2022,0.014674686644125759,2
W3113909409,Asking Crowdworkers to Write Entailment Examples: The Best of Bad Options,2020,0.01467222487506905,2
W4285269381,Multi-Granularity Structural Knowledge Distillation for Language Model Compression,2022,0.014670416677888097,2
W4385570864,KILM: Knowledge Injection into Encoder-Decoder Language Models,2023,0.014659873627315773,2
W3034174970,Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering,2020,0.014651791891454392,2
W2984178847,Comprehensive Multi-Dataset Evaluation of Reading Comprehension,2019,0.014648135557553075,2
W3021774381,Exploring and Predicting Transferability across NLP Tasks,2020,0.014647501999439134,2
W2982295985,Adversarial NLI: A New Benchmark for Natural Language Understanding,2019,0.014628883565930081,2
W3176514068,Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning.,2021,0.01462781646176712,2
W3212522196,jurBERT: A Romanian BERT Model for Legal Judgement Prediction,2021,0.01462388391524412,2
W4385572514,Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS,2023,0.014623784866119906,2
W2953044594,Explicit Utilization of General Knowledge in Machine Reading Comprehension,2019,0.014619711349196122,2
W4391099575,Towards Faithful Model Explanation in NLP: A Survey,2024,0.014596703998063306,2
W2970746059,A Logic-Driven Framework for Consistency of Neural Models,2019,0.01459008437415685,2
W2970819455,BiPaR: A Bilingual Parallel Dataset for Multilingual and Cross-lingual Reading Comprehension on Novels,2019,0.01458382959523927,2
W2788448041,R 3 : Reinforced Ranker-Reader for Open-Domain Question Answering.,2018,0.014582232381564612,2
W2949849869,Synthetic QA Corpora Generation with Roundtrip Consistency,2019,0.01457001840113583,2
W4389523827,AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification,2023,0.014565456013174393,2
W3176825161,Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering,2021,0.014563084480646924,2
W2888922637,Targeted Syntactic Evaluation of Language Models,2018,0.014557471967745558,2
W2949961827,Learning to Ask Unanswerable Questions for Machine Reading Comprehension,2019,0.01455591103032676,2
W4245255589,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,2019,0.014546216392835359,2
W3099142828,Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data,2020,0.014542684362640338,2
W2970863760,AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models,2019,0.014542215001679244,2
W4386576728,Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding,2023,0.014536977972417589,2
W4385571407,DMLM: Descriptive Masked Language Modeling,2023,0.014536311408183972,2
W3167266074,NAS-BERT,2021,0.01453322010111684,2
W3028930880,A Framework for Evaluation of Machine Reading Comprehension Gold Standards,2020,0.014532698913993808,2
W3167354871,On Attention Redundancy: A Comprehensive Study,2021,0.014495833994549375,2
W3188892795,Is My Model Using the Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning,2022,0.014493334580390878,2
W2971096647,What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering,2019,0.01448714809372486,2
W3154449322,Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees,2021,0.014467924407193456,2
W3104578551,BioMegatron: Larger Biomedical Domain Language Model,2020,0.014454144111100988,2
W2963928014,DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension,2018,0.014447961387166244,2
W2994934025,Learning The Difference That Makes A Difference With Counterfactually-Augmented Data,2020,0.0144370898328129,2
W4206674056,DUMA: Reading Comprehension With Transposition Thinking,2021,0.014428798465108932,2
W4385570444,Nonparametric Masked Language Modeling,2023,0.014424262104956934,2
W3020206637,Recent Trends in Deep Learning Based Open-Domain Textual Question Answering Systems,2020,0.014423521164202403,2
W4389523985,UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation,2023,0.014395094783092522,2
W4205513945,KFCNet: Knowledge Filtering and Contrastive Learning for Generative Commonsense Reasoning,2021,0.014390946863180296,2
W3144194608,CausaLM: Causal Model Explanation Through Counterfactual Language Models,2021,0.014389576973890966,2
W3166574921,Know what you don't need: Single-Shot Meta-Pruning for attention heads,2021,0.014380505115078188,2
W4381435827,Leveraging Symbolic Knowledge Bases for Commonsense Natural Language Inference using Pattern Theory,2023,0.014362707229854705,2
W3175183722,Unsupervised Energy-based Adversarial Domain Adaptation for Cross-domain Text Classification,2021,0.01434723362121157,2
W3128630643,BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems,2021,0.014324763101887114,2
W2963769536,Stochastic Answer Networks for Machine Reading Comprehension,2018,0.014322434498284358,2
W4285160452,CoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations,2022,0.014320892108098997,2
W3041897721,Latent Retrieval for Large-Scale Fact-Checking and Question Answering with NLI training,2020,0.014316230626127868,2
W3035140194,TaPas: Weakly Supervised Table Parsing via Pre-training,2020,0.014309635729832824,2
W2982206942,What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?,2019,0.014304430117793206,2
W3175148050,A Multi-Level Attention Model for Evidence-Based Fact Checking,2021,0.014293568691400495,2
W3176182290,Generation-Augmented Retrieval for Open-Domain Question Answering,2021,0.014292958930526687,2
W3002535714,Dual Multi-head Co-attention for Multi-choice Reading Comprehension.,2020,0.014284916608285408,2
W3176017841,LeeBERT: Learned Early Exit for BERT with cross-level optimization,2021,0.014282515762477532,2
W4287891006,Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology,2022,0.014275894231750108,2
W3046423960,Improving Subject-Area Question Answering with External Knowledge,2019,0.014262153122202624,2
W3176443126,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,2021,0.01425685466085546,2
W3102927624,KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding,2020,0.014248581257199975,2
W2909544278,Passage Re-ranking with BERT,2019,0.014239835162846003,2
W4285227756,Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge,2022,0.014236000129581096,2
W2998696444,oLMpics -- On what Language Model Pre-training Captures,2019,0.01421733614538563,2
W3205192296,Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention,2022,0.014201824330410322,2
W3106210592,Improve Transformer Models with Better Relative Position Embeddings,2020,0.01420095529367921,2
W4285286749,Prototypical Verbalizer for Prompt-based Few-shot Tuning,2022,0.014193037535558539,2
W4206178588,AdapterDrop: On the Efficiency of Adapters in Transformers,2021,0.014193005067454452,2
W4385571571,ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval,2023,0.014179802218776669,2
W4206136559,How to Train BERT with an Academic Budget,2021,0.014178549947394904,2
W2998099211,Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks,2020,0.014176629489922361,2
W3199904696,"Extract, Integrate, Compete: Towards Verification Style Reading Comprehension",2021,0.01417487126903106,2
W3123161422,HyperGrid Transformers: Towards A Single Model for Multiple Tasks,2021,0.014171385708608417,2
W2962808855,Reinforced Mnemonic Reader for Machine Reading Comprehension,2018,0.014159313869477694,2
W4385570140,Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge,2023,0.01415771630828624,2
W2971869958,Language Models as Knowledge Bases,2019,0.014149803494452223,2
W4385574373,Contrastive Learning with Prompt-derived Virtual Semantic Prototypes for Unsupervised Sentence Embedding,2022,0.014130329683757856,2
W3202546170,Sorting through the noise: Testing robustness of information processing in pre-trained language models,2021,0.014127616550802122,2
W3185146124,Rethinking search,2021,0.014127377099745483,2
W3176355530,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering,2021,0.014114936098762283,2
W4385573853,Mixed-modality Representation Learning and Pre-training for Joint Table-and-Text Retrieval in OpenQA,2022,0.014103349080526274,2
W3101204082,Are Pretrained Language Models Symbolic Reasoners over Knowledge?,2020,0.014091846686708105,2
W3110879614,On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT,2020,0.0140833812885428,2
W4385567113,XPrompt: Exploring the Extreme of Prompt Tuning,2022,0.014068486912072143,2
W3104223418,Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,2020,0.01405539255481422,2
W4292474994,Transformer models used for text-based question answering systems,2022,0.014049354984730686,2
W3038035611,exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models,2020,0.014048065918742044,2
W4385572504,What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary,2023,0.014046771163362316,2
W3098324846,A Simple and Effective Model for Answering Multi-span Questions,2020,0.014030632200962838,2
W3153094109,Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets,2021,0.014024530939732436,2
W2970379526,Towards Debiasing Fact Verification Models,2019,0.014019894481699005,2
W4285204619,SkipBERT: Efficient Inference with Shallow Layer Skipping,2022,0.01401960186276712,2
W3002104146,Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference,2020,0.01401645894552102,2
W4385573325,ZeroGen: Efficient Zero-shot Learning via Dataset Generation,2022,0.014010211746331235,2
W4226265091,Zero-Shot Commonsense Question Answering with Cloze Translation and Consistency Optimization,2022,0.014005869718763561,2
W4287854682,Paragraph-based Transformer Pre-training for Multi-Sentence Inference,2022,0.014000925838596804,2
W3168386607,Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications,2021,0.013997302659188233,2
W4385718025,Arithmetic-Based Pretraining Improving Numeracy of Pretrained Language Models,2023,0.013997268172986634,2
W4385570355,Fact-Checking Complex Claims with Program-Guided Reasoning,2023,0.013995743221151125,2
W3164123300,Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering,2021,0.01399418927401751,2
W3197298549,NSP-BERT: A Prompt-based Few-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction,2021,0.013993111162546984,2
W3085177480,It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners,2020,0.013990498636880844,2
W3090325631,LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,2020,0.013985389783766148,2
W4386576705,COMPS: Conceptual Minimal Pair Sentences for testing Robust Property Knowledge and its Inheritance in Pre-trained Language Models,2023,0.01397013888823375,2
W3166035055,RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering,2021,0.013964118982213272,2
W3045958725,MKQA: A Linguistically Diverse Benchmark for Multilingual Open Domain Question Answering,2021,0.013963485209504481,2
W4280637601,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,2022,0.013948555463052043,2
W3130089296,Variational Information Bottleneck for Effective Low-Resource Fine-Tuning,2021,0.013946997482611656,2
W3015773279,Unsupervised Commonsense Question Answering with Self-Talk,2020,0.013946378968388082,2
W4285157416,SCD: Self-Contrastive Decorrelation of Sentence Embeddings,2022,0.013943546321009373,2
W4385571824,Relational Sentence Embedding for Flexible Semantic Matching,2023,0.013938591969095745,2
W3116216579,Do Neural Language Models Overcome Reporting Bias?,2020,0.01392332479766934,2
W2980360762,A Mutual Information Maximization Perspective of Language Representation Learning,2019,0.013923292000917282,2
W3162462834,Evaluation of BERT and ALBERT Sentence Embedding Performance on Downstream NLP Tasks,2021,0.013923047070873846,2
W4389523933,CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering,2023,0.013922863974436202,2
W2987241137,D-NET: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension,2019,0.013922182797709407,2
W3023532425,Benchmarking Robustness of Machine Reading Comprehension Models,2020,0.013905554635506797,2
W3114326827,Automatic Detection of Machine Generated Text: A Critical Survey,2020,0.013903422062563432,2
W2988022164,Natural Language Generation for Effective Knowledge Distillation,2019,0.013893262999234261,2
W3022104542,IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization,2021,0.013885376826436637,2
W2963957489,DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications,2018,0.013884029173259997,2
W4213132190,Proceedings of the Third Workshop on Narrative Understanding,2021,0.013883700729834202,2
W2970742161,A Multi-Type Multi-Span Network for Reading Comprehension that Requires Discrete Reasoning,2019,0.013882603880916243,2
W3100103516,Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks,2020,0.013880731851424312,2
W4285255684,E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning,2022,0.013874293118806579,2
W3095594087,Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models,2020,0.013873521212258528,2
W3170113752,A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models,2021,0.013872781329596354,2
W2925863688,Publicly Available Clinical BERT Embeddings,2019,0.013863002416162479,2
W3103616906,Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning,2020,0.013862654591842131,2
W2963159735,Efficient and Robust Question Answering from Minimal Context over Documents,2018,0.013844036184088455,2
W3153705853,Exploring Transitivity in Neural NLI Models through Veridicality,2021,0.013839088112695437,2
W4398976621,InA: Inhibition Adaption on pre-trained language models,2024,0.013838120307258398,2
W3113816342,SemGloVe: Semantic Co-Occurrences for GloVe From BERT,2022,0.013827498261743976,2
W2892280852,Commonsense for Generative Multi-Hop Question Answering Tasks,2018,0.013823347111408242,2
W3130196849,Teach Me to Explain: A Review of Datasets for Explainable NLP.,2021,0.01382273997173945,2
W2962718684,Generating Natural Language Adversarial Examples,2018,0.013821880187147431,2
W3194118282,Regularizing transformers with deep probabilistic layers,2023,0.013816278652235088,2
W3165327186,Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?,2021,0.013803701207215193,2
W3092510499,DiPair: Fast and Accurate Distillation for Trillion-Scale Text Matching and Pair Modeling,2020,0.013790010432444613,2
W3034808961,Reasoning Over Semantic-Level Graph for Fact Checking,2020,0.013789104605588614,2
W3106031450,A Span-Extraction Dataset for Chinese Machine Reading Comprehension,2019,0.013787861786886185,2
W4281743053,Improving Contrastive Learning of Sentence Embeddings with Case-Augmented Positives and Retrieved Negatives,2022,0.013787740639921555,2
W4386566791,PECO: Examining Single Sentence Label Leakage in Natural Language Inference Datasets through Progressive Evaluation of Cluster Outliers,2023,0.013787635763182466,2
W3105350614,SRLGRN: Semantic Role Labeling Graph Reasoning Network,2020,0.013784464540815675,2
W4389524381,Prompting with Pseudo-Code Instructions,2023,0.013775859956776148,2
W4287891464,Learning To Retrieve Prompts for In-Context Learning,2022,0.01377539683017864,2
W3023528699,AdapterFusion: Non-Destructive Task Composition for Transfer Learning,2020,0.013766955806758074,2
W3124424060,Open Question Answering over Tables and Text,2021,0.01376566671799489,2
W4385572733,Retrieval Augmentation for Commonsense Reasoning: A Unified Approach,2022,0.013765048364322295,2
W3175423875,SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining,2021,0.013759034915770713,2
W3132730484,PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them,2021,0.013757898581704956,2
W3007595536,From static to dynamic word representations: a survey,2020,0.013753604057308402,2
W3035668167,How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,2020,0.013749037819867032,2
W3120490999,"On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",2021,0.013732538772207842,2
W4393152852,Fact-Driven Logical Reasoning for Machine Reading Comprehension,2024,0.01373074136138632,2
W3175475697,Multi-Task Retrieval for Knowledge-Intensive Tasks,2021,0.01372895905919541,2
W3174223793,Corpus-Level Evaluation for Event QA: The IndiaPoliceEvents Corpus Covering the 2002 Gujarat Violence,2021,0.01371576085088171,2
W4394773691,Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering,2024,0.01371443170054758,2
W2970986790,"Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering",2019,0.013709791595572346,2
W2950576363,Careful Selection of Knowledge to Solve Open Book Question Answering,2019,0.01370767276187759,2
W4283815582,Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey,2022,0.013705630464988999,2
W4385572429,"Don’t Retrain, Just Rewrite: Countering Adversarial Perturbations by Rewriting Text",2023,0.013700488672399369,2
W3174672330,Issues with Entailment-based Zero-shot Text Classification,2021,0.013688605358001309,2
W3174370755,Weakly Supervised Pre-Training for Multi-Hop Retriever,2021,0.013680573898742023,2
W3169445878,EntityBERT: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain,2021,0.01367354188830818,2
W3171847983,Posterior Differential Regularization with f-divergence for Improving Model Robustness,2021,0.013673094526904657,2
W4386187806,"GPT understands, too",2023,0.013668211759755897,2
W2936615257,Quizbowl: The Case for Incremental Question Answering.,2019,0.013667254641602303,2
W3103667349,Understanding tables with intermediate pre-training,2020,0.013663366555872985,2
W3199761064,Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning,2021,0.013650493602803696,2
W4285288079,SciNLI: A Corpus for Natural Language Inference on Scientific Text,2022,0.013646294614399274,2
W3198802556,On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets,2022,0.013644843943352556,2
W4285148079,Local Structure Matters Most: Perturbation Study in NLU,2022,0.013642152026205308,2
W3100436891,Unsupervised Question Decomposition for Question Answering,2020,0.013640995012883279,2
W3176443840,ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation,2021,0.013639668478071477,2
W2427527485,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",2016,0.013638154127814547,2
W3136035550,Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2,2021,0.013637864673094196,2
W3174776104,Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models,2021,0.013637254792705422,2
W3173446720,Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering,2021,0.013636365235174037,2
W2984402309,Domain-agnostic Question-Answering with Adversarial Training,2019,0.013631924699436068,2
W3112158532,Multilingual Transfer Learning for QA using Translation as Data Augmentation,2021,0.013620380909015677,2
W3176265725,HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability,2021,0.013618490901526462,2
W3186289945,"<i>Break, Perturb, Build</i>: Automatic Perturbation of Reasoning Paths Through Question Decomposition",2022,0.013605776027031153,2
W2986836624,Generalizing Question Answering System with Pre-trained Language Model Fine-tuning,2019,0.013605363943463941,2
W3105057865,Interpretable Entity Representations through Large-Scale Typing,2020,0.013601459997998224,2
W4389520067,DistillCSE: Distilled Contrastive Learning for Sentence Embeddings,2023,0.013600460938896154,2
W3037965442,Adversarial Training for Commonsense Inference,2020,0.013597829313584837,2
W3086388855,Overview of BioASQ 2021: The Ninth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering,2021,0.013596324528002851,2
W2989536007,Hierarchical Graph Network for Multi-hop Question Answering,2019,0.013591978484371835,2
W3099577420,RecoBERT: A Catalog Language Model for Text-Based Recommendations,2020,0.013591348138110353,2
W3217305727,ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,2022,0.013579977653048917,2
W3119866685,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.,2021,0.013577985813405892,2
W4385572035,Improving Contrastive Learning of Sentence Embeddings from AI Feedback,2023,0.01357490968145416,2
W3104163040,X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,2020,0.01357099440014767,2
W2934842096,,2019,0.013564024113949585,2
W3106290101,A matter of framing: The impact of linguistic formalism on probing results,2020,0.013560424529705295,2
W2966892770,"StructBERT: Incorporating Language Structures into Pre-training for Deep
  Language Understanding",2019,0.013553570868506714,2
W4382202657,On the Effectiveness of Parameter-Efficient Fine-Tuning,2023,0.013546418843778558,2
W3213189520,SimCSE: Simple Contrastive Learning of Sentence Embeddings,2021,0.01354198863149885,2
W3106916526,Deep learning based question answering system in Bengali,2020,0.013538169657331811,2
W3107315802,CPM: A Large-scale Generative Chinese Pre-trained Language Model,2020,0.013533592168971006,2
W4385573374,Pre-training Language Models with Deterministic Factual Knowledge,2022,0.013525802101101984,2
W3198711991,It’s not Rocket Science: Interpreting Figurative Language in Narratives,2022,0.01352063980187258,2
W4212902066,Representation Learning for Natural Language Processing,2020,0.013516648191525593,2
W3104108820,KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations,2020,0.013516446442234797,2
W3035446106,"MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization",2020,0.013508654611167247,2
W3200808010,Dynamic Knowledge Distillation for Pre-trained Language Models,2021,0.013508054960638984,2
W3095789240,"A Survey on Machine Reading Comprehension—Tasks, Evaluation Metrics and Benchmark Datasets",2020,0.013505942063620819,2
W3173511996,Obtaining Better Static Word Embeddings Using Contextual Embedding Models,2021,0.013495831507502765,2
W4285999563,ScienceQA: a novel resource for question answering on scholarly articles,2022,0.013490476754991244,2
W3100894295,Undersensitivity in Neural Reading Comprehension,2020,0.013488881815302231,2
W3015982254,Structured Pruning of a BERT-based Question Answering Model,2019,0.013487280297598232,2
W4281564455,Phrase-level Textual Adversarial Attack with Label Preservation,2022,0.01347773410601926,2
W2515741950,Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks,2016,0.01346504970025585,2
W3158209167,Paraphrastic Representations at Scale,2022,0.013464628825409474,2
W3102839769,BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA,2020,0.013461674565724781,2
W3015770160,Explaining Question Answering Models through Text Generation,2020,0.013455300788512354,2
W4285263440,Probing as Quantifying Inductive Bias,2022,0.013454137474619066,2
W3172119680,COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List,2021,0.013435210545204652,2
W3177448743,Learning to Rationalize for Nonmonotonic Reasoning with Distant Supervision,2021,0.013427800351017946,2
W2995628494,Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension,2020,0.013400438431562547,2
W2951036431,Multi-task Learning with Sample Re-weighting for Machine Reading Comprehension,2019,0.013400029439559622,2
W2984147501,Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking,2019,0.013399086102007128,2
W3040558716,Knowledge-Aware Language Model Pretraining,2020,0.013392813051962094,2
W3161820423,A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,2021,0.013384795906873545,2
W4385570594,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings",2023,0.013384646217111912,2
W3034212969,The Cascade Transformer: an Application for Efficient Answer Sentence Selection,2020,0.013377274560390827,2
W4389520455,Text Classification via Large Language Models,2023,0.013360448358577644,2
W3105261549,Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning,2020,0.01335405805985852,2
W3196538463,Challenges in Generalization in Open Domain Question Answering,2022,0.013342083422536809,2
W3035027743,Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models,2020,0.013338744673470416,2
W4224863259,KALA: Knowledge-Augmented Language Model Adaptation,2022,0.013326368810162978,2
W2982346747,Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering,2019,0.013321810967201012,2
W3035290244,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning,2020,0.01332032930561891,2
W3154863804,Generating Datasets with Pretrained Language Models,2021,0.013317242382824186,2
W4384916163,HSM-QA: Question Answering System Based on Hierarchical Semantic Matching,2023,0.013316142215121307,2
W2951335443,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,2018,0.013305233793322414,2
W2980708516,How to Fine-Tune BERT for Text Classification?,2019,0.013295214221519325,2
W3174445427,TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models,2021,0.013293221471811615,2
W4385573369,CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation,2022,0.013292471764288038,2
W3198690080,Evaluating the Robustness of Neural Language Models to Input Perturbations,2021,0.013289504655503437,2
W3024171804,Movement Pruning: Adaptive Sparsity by Fine-Tuning,2020,0.013289128629213899,2
W4389519341,FACTIFY3M: A benchmark for multimodal fact verification with explainability through 5W Question-Answering,2023,0.013287855811852996,2
W3186134690,SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning,2021,0.013285821093162259,2
W4385573057,Improving Passage Retrieval with Zero-Shot Question Generation,2022,0.013275388569196052,2
W4285287265,Context Matters: A Pragmatic Study of PLMs’ Negation Understanding,2022,0.013274760756065311,2
W4296965999,Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation,2023,0.013269841449719804,2
W2888296173,CoQA: A Conversational Question Answering Challenge,2018,0.013268243450115671,2
W4403048625,Extracting Sentence Embeddings from Pretrained Transformer Models,2024,0.013259602435608994,2
W3137492414,Inductive Relation Prediction by BERT,2022,0.013259198845206356,2
W3033182847,GMAT: Global Memory Augmentation for Transformers,2020,0.013256599867115339,2
W4297253404,BioGPT: generative pre-trained transformer for biomedical text generation and mining,2022,0.013251574103673454,2
W4385570749,Gradient-based Intra-attention Pruning on Pre-trained Language Models,2023,0.013224831737360766,2
W2979928633,FriendsQA: Open-Domain Question Answering on TV Show Transcripts,2019,0.013223381352382754,2
W4385570586,MeetingQA: Extractive Question-Answering on Meeting Transcripts,2023,0.013222068310358116,2
W4386947867,Multi-hop question answering using sparse graphs,2023,0.013220554459886525,2
W2932893307,,2019,0.013218367440886345,2
W3173520982,Overview of BioASQ 2020: The Eighth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering,2020,0.013217342237310598,2
W3011150943,Coarse and Fine Granularity Graph Reasoning for Interpretable Multi-Hop Question Answering,2020,0.013217043116758309,2
W4385569878,DISCO: Distilling Counterfactuals with Large Language Models,2023,0.01321565129596791,2
W3017022649,Training with Quantization Noise for Extreme Model Compression,2020,0.013211304873881086,2
W4385573687,Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering,2022,0.013208760092724342,2
W3116605602,How Relevant Are Selectional Preferences for Transformer-based Language Models?,2020,0.013207899713039574,2
W4285134706,MoEfication: Transformer Feed-forward Layers are Mixtures of Experts,2022,0.013207506580092955,2
W3105959138,How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?,2020,0.013197970637269215,2
W3176108833,CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding,2021,0.013196748816212989,2
W3171434230,Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training,2021,0.0131910791326472,2
W4281479158,Prompt Tuning for Discriminative Pre-trained Language Models,2022,0.01318471895521278,2
W4385570391,A Survey of Deep Learning for Mathematical Reasoning,2023,0.013184303853973149,2
W3099965312,Table Search Using a Deep Contextualized Language Model,2020,0.013183718870114856,2
W3174464510,(Comet-) Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs,2021,0.013180322781438505,2
W3155744586,Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?,2021,0.013174141321855857,2
W3199619722,BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology,2021,0.013165894718158393,2
W4389523666,Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture,2023,0.013163330910703335,2
W4206555128,"Phrase Retrieval Learns Passage Retrieval, Too",2021,0.013163201390825696,2
W3121904249,Measuring Massive Multitask Language Understanding,2021,0.013162265816998045,2
W3174781392,An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models,2021,0.013158630159791358,2
W2950733407,Improving the Robustness of Question Answering Systems to Question Paraphrasing,2019,0.013157074465171219,2
W3161374759,Understanding by Understanding Not: Modeling Negation in Language Models,2021,0.01315659184370605,2
W3215599159,Semantic Structure in Deep Learning,2021,0.013154825671835563,2
W4389520746,APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models,2023,0.01315239505181369,2
W4385565410,CREPE: Open-Domain Question Answering with False Presuppositions,2023,0.013150674646205852,2
W3101717721,Asking without Telling: Exploring Latent Ontologies in Contextual Representations,2020,0.013146276771385662,2
W4386566768,Can Pretrained Language Models (Yet) Reason Deductively?,2023,0.013144874144070074,2
W3101662419,"The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models",2020,0.013142673928919914,2
W3197680083,"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning",2021,0.01314253095549483,2
W3172224317,Probing Contextual Language Models for Common Ground with Visual Representations,2021,0.013141745700739584,2
W3167783161,Unsupervised Multi-hop Question Answering by Question Generation,2021,0.01313463978589075,2
W3212893438,Few-Shot Text Generation with Natural Language Instructions,2021,0.013133295603724646,2
W3175591618,Explanations for CommonsenseQA: New Dataset and Models,2021,0.013127568097770504,2
W3152665347,Advanced Semantics for Commonsense Knowledge Extraction,2021,0.013123112762058948,2
W4385573404,Holistic Sentence Embeddings for Better Out-of-Distribution Detection,2022,0.013116915747557628,2
W3105698638,ReQA: An Evaluation for End-to-End Answer Retrieval Models,2019,0.013101288786945584,2
W2969515962,Patient Knowledge Distillation for BERT Model Compression,2019,0.01309784790181761,2
W3156359583,Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering,2021,0.013096407799876777,2
W4385570325,ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models,2023,0.013091476375453888,2
W3104738015,Early Exiting BERT for Efficient Document Ranking,2020,0.01308777879113092,2
W3034364750,Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension,2020,0.013081451642243043,2
W3176574162,Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation,2021,0.01307197451677336,2
W2962816513,Pathologies of Neural Models Make Interpretations Difficult,2018,0.013068038077379718,2
W3187018546,CPM: A large-scale generative Chinese Pre-trained language model,2021,0.013065913950595389,2
W3004117589,Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction,2020,0.01306032629115529,2
W3103816537,Exploring BERT’s Sensitivity to Lexical Cues using Tests from Semantic Priming,2020,0.013050880280995285,2
W3101244625,Pre-training Text-to-Text Transformers for Concept-centric Common Sense,2020,0.013047375547531943,2
W3087094142,FarsTail: a Persian natural language inference dataset,2023,0.013022251641827473,2
W3207166518,Symbolic Knowledge Distillation: from General Language Models to Commonsense Models,2022,0.013019952016238213,2
W2979949198,exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models,2019,0.01300837345839529,2
W4389777721,An Efficient Self-Supervised Cross-View Training For Sentence Embedding,2023,0.013007627823312416,2
W3198490223,Label Verbalization and Entailment for Effective Zero and Few-Shot Relation Extraction,2021,0.012994864859664189,2
W3088181395,Attention Meets Perturbations: Robust and Interpretable Attention With Adversarial Training,2021,0.012984002992335137,2
W3169937871,XOR QA: Cross-lingual Open-Retrieval Question Answering,2021,0.012974769463622991,2
W3156869386,Extremely Small BERT Models from Mixed-Vocabulary Training,2021,0.012972418994803815,2
W3167166933,ERNIE-NLI: Analyzing the Impact of Domain-Specific External Knowledge on Enhanced Representations for NLI,2021,0.012971094732952278,2
W4210894209,Framework for Deep Learning-Based Language Models Using Multi-Task Learning in Natural Language Understanding: A Systematic Literature Review and Future Directions,2022,0.01296778457450912,2
W3099180151,How Decoding Strategies Affect the Verifiability of Generated Text,2020,0.012961948674422762,2
W4378421851,Discourse-Aware Graph Networks for Textual Logical Reasoning,2023,0.01296185088937774,2
W3104566548,Teaching Machine Comprehension with Compositional Explanations,2020,0.012958830766463914,2
W2972498556,Analyzing the Structure of Attention in a Transformer Language Model,2019,0.012955323223671704,2
W3167829962,Towards BERT-based Automatic ICD Coding: Limitations and Opportunities,2021,0.01294825230941376,2
W2977944219,On Identifiability in Transformers,2019,0.012947603874782956,2
W4385571554,Enhancing text comprehension for Question Answering with Contrastive Learning,2023,0.012947255381639898,2
W4281254837,Modeling Multi-hop Question Answering as Single Sequence Prediction,2022,0.01294175238507749,2
W3100554158,Train No Evil: Selective Masking for Task-Guided Pre-Training,2020,0.012941713662086443,2
W3156128319,TransferNet: An Effective and Transparent Framework for Multi-hop Question Answering over Relation Graph,2021,0.012940899666241278,2
W4226217623,Adaptable Closed-Domain Question Answering Using Contextualized CNN-Attention Models and Question Expansion,2022,0.012915342284303116,2
W3176546569,Zero-shot Fact Verification by Claim Generation,2021,0.012910254669158664,2
W3105391665,Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs,2020,0.012909574457966746,2
W4382119231,Uncertainty-Driven Knowledge Distillation for Language Model Compression,2023,0.012908984912099039,2
W2997710335,Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,2020,0.012906654517644365,2
W3161801106,e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks,2021,0.012902305494514094,2
W4389524424,Not all layers are equally as important: Every Layer Counts BERT,2023,0.012887370346825432,2
W3104186312,BERTweet: A pre-trained language model for English Tweets,2020,0.012885393205699758,2
W4392903952,Sensi-Bert: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient Language Model,2024,0.012877715318086297,2
W4385571795,Preserving Commonsense Knowledge from Pre-trained Language Models via Causal Inference,2023,0.01287494242604257,2
W2964165804,SentEval: An Evaluation Toolkit for Universal Sentence Representations,2018,0.012874572807618001,2
W2984450720,Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks,2019,0.012873532496088705,2
W4225410153,AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks,2022,0.012871315637855348,2
W4385574060,Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens,2022,0.012868787448750736,2
W3207523779,Sharpness-Aware Minimization Improves Language Model Generalization,2022,0.012868481728367548,2
W3153592532,NoiseQA: Challenge Set Evaluation for User-Centric Question Answering,2021,0.012863436052431934,2
W4225795249,"Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices",2021,0.012857660477408309,2
W4385573119,The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models,2022,0.012854789674082887,2
W3105451204,BERT with History Answer Embedding for Conversational Question Answering,2019,0.012854642727438644,2
W4225377340,Clues Before Answers: Generation-Enhanced Multiple-Choice QA,2022,0.012850127520539621,2
W3123123873,Pre-training Text-to-Text Transformers for Concept-centric Common Sense,2021,0.012846572597820826,2
W3176495666,TIMEDIAL: Temporal Commonsense Reasoning in Dialog,2021,0.012841988537585763,2
W3201087948,RoR: Read-over-Read for Long Document Machine Reading Comprehension,2021,0.012834228618666817,2
W4385574088,Prompting ELECTRA: Few-Shot Learning with Discriminative Pre-Trained Models,2022,0.01282607452631454,2
W3198507920,FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models,2021,0.012821403718208654,2
W3037310559,Entity-Enriched Neural Models for Clinical Question Answering,2020,0.012818521228745943,2
W3099617520,SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness,2020,0.012816909970105896,2
W1525961042,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks,2015,0.012814554222679145,2
W3099008231,exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources,2020,0.012808899879702467,2
W3182414670,FewCLUE: A Chinese Few-shot Learning Evaluation Benchmark,2021,0.012807830157084898,2
W3115242847,Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks,2020,0.012807714637941338,2
W3213002786,Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference,2021,0.012802834441146252,2
W4200313831,"Question Answering Survey: Directions, Challenges, Datasets, Evaluation Matrices",2021,0.01280121428731561,2
W4206529673,Discrete and Soft Prompting for Multilingual Models,2021,0.012799085563278212,2
W3173736278,Improving Paraphrase Detection with the Adversarial Paraphrasing Task,2021,0.012786119107538699,2
W3175752238,BERT Busters: Outlier Dimensions that Disrupt Transformers,2021,0.012779441877219166,2
W4309483299,Semantic matching in machine reading comprehension: An empirical study,2022,0.01277910598004851,2
W4311431702,Federated Few-Shot Learning for Mobile NLP,2023,0.012768462570982558,2
W3104208618,Reevaluating Adversarial Examples in Natural Language,2020,0.012766285633301407,2
W4361205837,Can language representation models think in bets?,2023,0.012762490111678527,2
W2983049545,Beyond English-Only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian,2019,0.012760470037467197,2
W2466175319,A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories,2016,0.012758235837682,2
W4382449327,Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval,2023,0.01275134770984288,2
W3095156104,ABNIRML: Analyzing the Behavior of Neural IR Models,2022,0.012749557555286345,2
W3193647133,Natural Language Understanding with Privacy-Preserving BERT,2021,0.012746306842737745,2
W3166444100,Differentiable Open-Ended Commonsense Reasoning,2021,0.012745727900921355,2
W4389519036,Improving Language Models’ Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary,2023,0.012745726193432205,2
W3008851394,"Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",2020,0.012742959299340337,2
W3167525829,DReCa: A General Task Augmentation Strategy for Few-Shot Natural Language Inference,2021,0.012724034261690418,2
W4385571485,Composition-contrastive Learning for Sentence Embeddings,2023,0.012721416725911363,2
W4389519042,Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark,2023,0.01271921270691477,2
W3156587088,Language Models for Lexical Inference in Context,2021,0.012707914942244304,2
W3102127365,Domain Adversarial Fine-Tuning as an Effective Regularizer,2020,0.012699705407174815,2
W3154903254,BERTese: Learning to Speak to BERT,2021,0.012697200366541053,2
W4293304858,Match-Prompt,2022,0.012694144129675174,2
W4385572149,Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,2023,0.012680441509529773,2
W4246183800,TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning,2021,0.01266974906825808,2
W3175870271,A Closer Look at How Fine-tuning Changes BERT,2022,0.012660039908689458,2
W3153451655,GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation,2021,0.012645986025103232,2
W3096150021,Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?,2020,0.012643331955903175,2
W3034446185,MuTual: A Dataset for Multi-Turn Dialogue Reasoning,2020,0.012643172056729574,2
W3091783050,"Optimizing Transformers with Approximate Computing for Faster, Smaller and more Accurate NLP Models",2021,0.012636487058996235,2
W3035391452,Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension,2020,0.012631771429737221,2
W3035101152,FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining,2020,0.012630461809777067,2
W2963547127,Read + Verify: Machine Reading Comprehension with Unanswerable Questions,2019,0.012620660657844904,2
W4309811444,PTR: Prompt Tuning with Rules for Text Classification,2022,0.012608818424999422,2
W4229038710,Refined Commonsense Knowledge from Large-Scale Web Contents,2022,0.012606051038507904,2
W3174510164,One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers,2021,0.012603797557228897,2
W3037101098,Efficient Automatic Punctuation Restoration Using Bidirectional Transformers with Robust Inference,2020,0.012601104331668427,2
W4385574161,Generative Prompt Tuning for Relation Classification,2022,0.012598420050864798,2
W3038495045,Transferability of Natural Language Inference to Biomedical Question Answering,2020,0.012595306928610996,2
W3017779903,Quantifying the Contextualization of Word Representations with Semantic Class Probing,2020,0.012590333236582445,2
W3174821868,End-to-End Training of Neural Retrievers for Open-Domain Question Answering,2021,0.012588351225067922,2
W3116594510,Compressing Pre-trained Language Models by Matrix Decomposition,2020,0.012574811887507708,2
W3022579746,How Can We Accelerate Progress Towards Human-like Linguistic Generalization?.,2020,0.012570563762551513,2
W3175560839,How is BERT surprised? Layerwise detection of linguistic anomalies,2021,0.012564358844335603,2
W3113790969,A Vietnamese Dataset for Evaluating Machine Reading Comprehension,2020,0.01256385262289954,2
W2951181836,TWEETQA: A Social Media Focused Question Answering Dataset,2019,0.012561334140237579,2
W3031912764,Beyond Leaderboards: A survey of methods for revealing weaknesses in Natural Language Inference data and models,2020,0.012560742912399125,2
W4387892109,DistillCSE: Distilled Contrastive Learning for Sentence Embeddings,2023,0.012560575845379509,2
W3034457116,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,2020,0.012555267113333034,2
W4389520508,Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers,2023,0.012555067332333577,2
W4205456754,R2-D2: A Modular Baseline for Open-Domain Question Answering,2021,0.012551087873686672,2
W3168875417,RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering,2021,0.01254833655119223,2
W3172806051,Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks,2021,0.012548172912339107,2
W4287887774,ZusammenQA: Data Augmentation with Specialized Models for Cross-lingual Open-retrieval Question Answering System,2022,0.012546450676192134,2
W4389518664,Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data,2023,0.012541932424610696,2
W3175603587,Reordering Examples Helps during Priming-based Few-Shot Learning,2021,0.012530333344664519,2
W3115462295,CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters,2020,0.01252807402714241,2
W2970960342,Neural Arabic Question Answering,2019,0.012524492811399614,2
W2983004086,"What do you mean, BERT? Assessing BERT as a Distributional Semantics Model",2019,0.012523882670194289,2
W4385574114,Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning,2022,0.012523793226575416,2
W2998665041,SG-Net: Syntax-Guided Machine Reading Comprehension,2020,0.012517725245853411,2
W4385570901,Self-Evolution Learning for Discriminative Language Model Pretraining,2023,0.012515349430767242,2
W4293812374,Stable Contrastive Learning for Self-Supervised Sentence Embeddings With Pseudo-Siamese Mutual Learning,2022,0.012501379280404675,2
W4388829218,Bringing order into the realm of Transformer-based language models for artificial intelligence and law,2023,0.012492830635726862,2
W3184978204,Dimensions of commonsense knowledge,2021,0.01249235696751092,2
W3154295049,"Back to Square One: Bias Detection, Training and Commonsense Disentanglement in the Winograd Schema",2021,0.012490949727257965,2
W3116466566,Hy-NLI: a Hybrid system for Natural Language Inference,2020,0.012488087414354481,2
W4389518925,Automatic Evaluation of Attribution by Large Language Models,2023,0.012484127803022322,2
W3209717315,Dense Hierarchical Retrieval for Open-domain Question Answering,2021,0.012480630297298406,2
W4285606726,Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention,2022,0.012464722912623901,2
W3093871960,Language Models are Open Knowledge Graphs,2020,0.012463800099514007,2
W4223492536,BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model,2022,0.012462983087410664,2
W4220785642,Greedy-layer pruning: Speeding up transformer models for natural language processing,2022,0.012460217026247762,2
W4287888899,LM-CORE: Language Models with Contextually Relevant External Knowledge,2022,0.012449915450690099,2
W4385505449,Analysis of the evolution of advanced transformer-based language models: experiments on opinion mining,2023,0.012446288008988777,2
W3106156541,Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension,2020,0.012445139874608655,2
W4399168671,Hyperbolic Pre-Trained Language Model,2024,0.012437424476735098,2
W4385734218,Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering,2023,0.012436158539917802,2
W4389518835,MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension,2023,0.01243493998625259,2
W3195729941,"Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models",2021,0.012432616583308586,2
W3208821253,Pretrained Transformers for Text Ranking: BERT and Beyond,2021,0.0124296128129142,2
W3034560159,AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search,2020,0.012427725213178194,2
W4385571047,ContraCLM: Contrastive Learning For Causal Language Model,2023,0.012423409411953478,2
W3115393336,I Know What You Asked: Graph Path Learning using AMR for Commonsense Reasoning,2020,0.012415927479609226,2
W3174931845,MLMLM: Link Prediction with Mean Likelihood Masked Language Model,2021,0.012414816755099789,2
W3208933101,Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models,2021,0.012410428468235033,2
W3049366647,Is Supervised Syntactic Parsing Beneficial for Language Understanding? An Empirical Investigation,2020,0.012406609279389778,2
W4287887900,PCEE-BERT: Accelerating BERT Inference via Patient and Confident Early Exiting,2022,0.012406459285508152,2
W2983309655,Evaluating Question Answering Evaluation,2019,0.012403524803428306,2
W3167972416,Self-Supervised Test-Time Learning for Reading Comprehension,2021,0.012395501089416457,2
W4283792669,Supervising Model Attention with Human Explanations for Robust Natural Language Inference,2022,0.012391516619219911,2
W3125238517,Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval,2020,0.012388002432469862,2
W4385572770,RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder,2022,0.012383666086280725,2
W3036737707,New Vietnamese Corpus for Machine Reading Comprehension of Health News Articles,2022,0.012383080333836335,2
W3155431862,Competency Problems: On Finding and Removing Artifacts in Language Data,2021,0.012378361989801061,2
W3202099651,Towards Continual Knowledge Learning of Language Models,2021,0.01237737839922566,2
W4224115290,IDPG: An Instance-Dependent Prompt Generation Method,2022,0.012377250051115602,2
W2911435132,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,2019,0.012374092099143938,2
W3159630167,Contrastive Out-of-Distribution Detection for Pretrained Transformers,2021,0.012371741095370604,2
W3200130628,Conditional probing: measuring usable information beyond a baseline,2021,0.012371073839608608,2
W3114725666,Text Classification by Contrastive Learning and Cross-lingual Data Augmentation for Alzheimer’s Disease Detection,2020,0.012367839791028816,2
W3097977265,Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation,2020,0.012358749028957644,2
W4285107714,Data Contamination: From Memorization to Exploitation,2022,0.012354576807048651,2
W3211022409,Emerging trends: A gentle introduction to fine-tuning,2021,0.012343146228050755,2
W3209749143,FacTeR-Check: Semi-automated fact-checking through semantic similarity and natural language inference,2022,0.012341275554938445,2
W4404295242,PSSOP: A Prompt-style Approach for Few-Shot Text Classification,2024,0.012338961614268647,2
W3201369838,Artificial Text Detection via Examining the Topology of Attention Maps,2021,0.012338332980597827,2
W3035094063,Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering,2020,0.012327237795868482,2
W3091864705,Fact Extraction and VERification -- The FEVER case: An Overview,2020,0.012322547362013398,2
W3022870761,Question Rewriting for Conversational Question Answering,2020,0.012322182766063635,2
W2937297214,Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding,2019,0.012318769907938667,2
W4285309087,An Analysis of Negation in Natural Language Understanding Corpora,2022,0.012317883934090388,2
W3035097102,Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?,2020,0.0123143047704934,2
W4386566597,Neural Ranking with Weak Supervision for Open-Domain Question Answering : A Survey,2023,0.012313334071896758,2
W3034838723,Commonsense Evidence Generation and Injection in Reading Comprehension,2020,0.012312822580150611,2
W3210951978,Backdoor Pre-trained Models Can Transfer to All,2021,0.012308208042750493,2
W2912904516,DREAM: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension,2019,0.012302153279082363,2
W4221152557,Just Rank: Rethinking Evaluation with Word and Sentence Similarities,2022,0.012298359931902224,2
W3116423158,Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base,2020,0.012291420030095911,2
W3110414860,CAiRE-COVID: A Question Answering and Query-focused Multi-Document Summarization System for COVID-19 Scholarly Information Management,2020,0.012288491557552171,2
W3145602566,A Practical Survey on Faster and Lighter Transformers,2023,0.012288157769219387,2
W3166890286,Representing Numbers in NLP: a Survey and a Vision,2021,0.012287826339810558,2
W3084742463,Accelerating Real-Time Question Answering via Question Generation,2020,0.012284449097573003,2
W3097252660,BERTnesia: Investigating the capture and forgetting of knowledge in BERT,2020,0.012276439377594869,2
W4210550054,KerasBERT: Modeling the Keras Language,2021,0.012273985375762832,2
W2937845937,ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission,2019,0.012269591916137487,2
W3203176827,Swiss-Judgment-Prediction: A Multilingual Legal Judgment Prediction Benchmark,2021,0.012262362218151173,2
W3164054899,ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer,2021,0.012261192440064865,2
W3101190870,Examining the rhetorical capacities of neural language models,2020,0.012256206761323182,2
W2970900584,NumNet: Machine Reading Comprehension with Numerical Reasoning,2019,0.012241157293574886,2
W4385572490,Learning Better Masking for Better Language Model Pre-training,2023,0.012240027425744347,2
W4256093969,IndoNLI: A Natural Language Inference Dataset for Indonesian,2021,0.012231040062938112,2
W2626154462,Making Neural QA as Simple as Possible but not Simpler,2017,0.012226949163031964,2
W4390619884,Contrastive classification: A label-independent generalization model for text classification,2024,0.012218258839889607,2
W3135970112,Improving Skip-Gram Embeddings Using BERT,2021,0.012217598545167224,2
W4385570929,Chain-of-Skills: A Configurable Model for Open-Domain Question Answering,2023,0.012216403801711788,2
W4385573520,SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters,2022,0.012206535121061408,2
W3099576124,FastFormers: Highly Efficient Transformer Models for Natural Language Understanding,2020,0.012205815429679233,2
W3166441238,Pretrained Transformers for Text Ranking: BERT and Beyond,2021,0.01220502033290403,2
W4393147190,Labels Need Prompts Too: Mask Matching for Natural Language Understanding Tasks,2024,0.01219920092064773,2
W2998259147,ReCO: A Large Scale Chinese Reading Comprehension Dataset on Opinion,2020,0.012198669237759181,2
W4294833327,Conversational question answering: a survey,2022,0.012188039075145941,2
W4385573917,Generative Language Models for Paragraph-Level Question Generation,2022,0.01218323642735491,2
W4287888039,"QuALITY: Question Answering with Long Input Texts, Yes!",2022,0.012172204172098145,2
W4389519983,Impact of Co-occurrence on Factual Knowledge of Large Language Models,2023,0.012170611943215748,2
W3100107515,Document Ranking with a Pretrained Sequence-to-Sequence Model,2020,0.01216765993943673,2
W3089695206,A Survey on Explainability in Machine Reading Comprehension,2020,0.012167262181521785,2
W2984812384,EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference,2019,0.012158743090281261,2
W3015883388,Dense Passage Retrieval for Open-Domain Question Answering,2020,0.012158607025175421,2
W3105601320,LIMIT-BERT : Linguistics Informed Multi-Task BERT,2020,0.012156451448366924,2
W3199824684,CPT: a pre-trained unbalanced transformer for both Chinese language understanding and generation,2024,0.012152908194678328,2
W3202088367,FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks,2022,0.01215051739436758,2
W3126258442,Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches.,2021,0.012149938619895193,2
W4396982340,CoSENT: Consistent Sentence Embedding via Similarity Ranking,2024,0.012145988518688297,2
W4382202728,Adversarial Self-Attention for Language Understanding,2023,0.0121449547654918,2
W4385573338,BioReader: a Retrieval-Enhanced Text-to-Text Transformer for Biomedical Literature,2022,0.012144160790688276,2
W3022116759,The Effect of Natural Distribution Shift on Question Answering Models,2020,0.012135869848436667,2
W2799124508,What you can cram into a single \$&!#* vector: Probing sentence embeddings for linguistic properties,2018,0.012131249100564541,2
W3169890186,On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies,2021,0.012130644999699193,2
W4385570673,Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models,2023,0.012124975071892643,2
W3081031588,AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization,2020,0.012102382326738858,2
W4385574376,Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation,2022,0.012101718789823884,2
W4287887938,Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks,2022,0.012091898732062173,2
W3207663303,Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning.,2021,0.012086111074219991,2
W3207576321,Knowledge Enhanced Fact Checking and Verification,2021,0.012080422313587373,2
W3104597016,A Wrong Answer or a Wrong Question? An Intricate Relationship between Question Reformulation and Answer Selection in Conversational Question Answering,2020,0.012077791035723257,2
W4287888456,MultiSpanQA: A Dataset for Multi-Span Question Answering,2022,0.012077624910065858,2
W3176390156,Ecco: An Open Source Library for the Explainability of Transformer Language Models,2021,0.012073886589723106,2
W3111711122,Generate Your Counterfactuals: Towards Controlled Counterfactual Generation for Text,2021,0.012073463516739899,2
W3014568172,FastBERT: a Self-distilling BERT with Adaptive Inference Time,2020,0.012070715334787454,2
W3092171032,"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition",2020,0.012065262426397466,2
W3198950523,DADgraph: A Discourse-aware Dialogue Graph Neural Network for Multiparty Dialogue Machine Reading Comprehension,2021,0.012063387099187082,2
W4210497109,Testing Your Question Answering Software via Asking Recursively,2021,0.012060010427556379,2
W4285270385,Sentence-aware Contrastive Learning for Open-Domain Passage Retrieval,2022,0.012056854497193892,2
W3092642435,InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective,2020,0.01205084102095188,2
W3166642164,Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge,2021,0.012043024405506057,2
W3170894870,Automatic Story Generation: Challenges and Attempts,2021,0.012039692693612039,2
W3156836409,Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation,2021,0.012036002805303321,2
W4372270459,Weighted Sampling for Masked Language Modeling,2023,0.012023058993229061,2
W4312108492,A survey on complex factual question answering,2022,0.012021914665157852,2
W3154922002,Consistent Accelerated Inference via Confident Adaptive Transformers,2021,0.012014350989094527,2
W2998604177,Towards a Robust Deep Neural Network in Texts: A Survey,2019,0.012003118528496866,2
W3090789254,Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media,2020,0.01199984023118359,2
W3165773193,External features enriched model for biomedical question answering,2021,0.01199104154810154,2
W3102187933,CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning,2020,0.011986855184432444,2
W3172399575,Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach,2021,0.011985926006164098,2
W3169436637,DirectProbe: Studying Representations without Classifiers,2021,0.011984904174237573,2
W4285119699,Clickbait Spoiling via Question Answering and Passage Retrieval,2022,0.011984373490924849,2
W3105639882,RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,2020,0.011978810816435072,2
W4225992558,Evidentiality-guided Generation for Knowledge-Intensive NLP Tasks,2022,0.01197391484998545,2
W2972167903,KG-BERT: BERT for Knowledge Graph Completion,2019,0.011972015902321106,2
W4381487157,Beyond semantic distance: Automated scoring of divergent thinking greatly improves with large language models,2023,0.011965959341975873,2
W4386566485,Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models,2023,0.011965299108047904,2
W3150055540,Recommending metamodel concepts during modeling activities with pre-trained language models,2022,0.0119617408643616,2
W3196927838,Multistage BiCross encoder for multilingual access to COVID-19 health information,2021,0.011958480428958824,2
W3098275893,When is a bishop not like a rook? When it’s like a rabbi! Multi-prototype BERT embeddings for estimating semantic relationships,2020,0.011947123124878778,2
W3171391618,Empirical Evaluation of Pre-trained Transformers for Human-Level NLP: The Role of Sample Size and Dimensionality,2021,0.011938892051355152,2
W4385572256,Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data,2023,0.011926511865657219,2
W4389518961,MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization,2023,0.01192389851079661,2
W4400644218,A Survey on Symbolic Knowledge Distillation of Large Language Models,2024,0.011921081486802095,2
W3196642073,Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners,2021,0.011918799792368423,2
W3198691721,Dealing with Typos for BERT-based Passage Retrieval and Ranking,2021,0.011913227949712571,2
W2971193649,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,2019,0.011897470002327047,2
W2956105246,,2019,0.011896551988396632,2
W4376864968,Augmenting Low-Resource Text Classification with Graph-Grounded Pre-training and Prompting,2023,0.011885732455626618,2
W3101682885,HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification,2020,0.011883851985823179,2
W3173295223,KuiLeiXi: a Chinese Open-Ended Text Adventure Game,2021,0.011881088907252006,2
W3113677131,A Graph Reasoning Network for Multi-turn Response Selection via Customized Pre-training,2021,0.011872451752140719,2
W3106483960,Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,2020,0.011871779951068057,2
W3121309507,On Position Embeddings in BERT,2021,0.011867235252453558,2
W4401208806,Exploring Continual Learning of Compositional Generalization in NLI,2024,0.011858002718409021,2
W3103520839,diagNNose: A Library for Neural Activation Analysis,2020,0.01185254923469952,2
W3178926106,Trusting RoBERTa over BERT: Insights from CheckListing the Natural Language Inference Task.,2021,0.011849179516861727,2
W3201344816,NOPE: A Corpus of Naturally-Occurring Presuppositions in English,2021,0.011848777209129286,2
W3088056511,What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams,2020,0.011842522642212645,2
W4385570895,RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank,2023,0.011840939032799397,2
W4382202848,LIQUID: A Framework for List Question Answering Dataset Generation,2023,0.011840250904866042,2
W4285162225,Your fairness may vary: Pretrained language model fairness in toxic text classification,2022,0.01183784762452903,2
W4389519954,KEPLET: Knowledge-Enhanced Pretrained Language Model with Topic Entity Awareness,2023,0.01183781678278291,2
W4205758343,Towards Improving Adversarial Training of NLP Models,2021,0.01183277590890166,2
W4386566702,Evaluating the Robustness of Discrete Prompts,2023,0.011831771200874006,2
W3169920290,Towards Few-shot Fact-Checking via Perplexity,2021,0.011828102279460587,2
W4387143043,Zero-shot Learning for Named Entity Recognition in Software Specification Documents,2023,0.011826939708878001,2
W3186727973,"AStarTwice at SemEval-2021 Task 5: Toxic Span Detection Using RoBERTa-CRF, Domain Specific Pre-Training and Self-Training",2021,0.011825978368247713,2
W3118781290,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,2021,0.01182268031068185,2
W3104423855,BAE: BERT-based Adversarial Examples for Text Classification,2020,0.01182102656463777,2
W3037530970,A Cross-Task Analysis of Text Span Representations,2020,0.011819162302993695,2
W4393147201,ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference,2024,0.011817091497849675,2
W2975185270,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,2019,0.011815998055543023,2
W3104220682,Long Document Ranking with Query-Directed Sparse Transformer,2020,0.011807563235151062,2
W3180037928,Learned Token Pruning for Transformers,2022,0.011774003817383454,2
W2970928735,Adversarial Domain Adaptation for Machine Reading Comprehension,2019,0.011773883525167136,2
W4385572334,A Survey for Efficient Open Domain Question Answering,2023,0.011771259968604115,2
W3200704197,Incorporating Residual and Normalization Layers into Analysis of Masked Language Models,2021,0.011767644152956837,2
W3034475796,Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions,2020,0.011767462258740001,2
W4385570744,Direct Fact Retrieval from Knowledge Graphs without Entity Linking,2023,0.011767306284057907,2
W3201254286,Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration,2021,0.011766193547723347,2
W3164540570,Self-Alignment Pretraining for Biomedical Entity Representations,2021,0.011762213492891553,2
W3034624105,DoQA - Accessing Domain-Specific FAQs via Conversational QA,2020,0.011760622064942126,2
W3127622310,Better Fine-Tuning by Reducing Representational Collapse,2021,0.011759112103204909,2
W4385574097,Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision,2022,0.011757976056004897,2
W2987215000,On Making Reading Comprehension More Comprehensive,2019,0.011753783368240896,2
W3174738170,Learning Contextualized Knowledge Structures for Commonsense Reasoning,2021,0.011750632813805229,2
W3099143320,Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,2020,0.011750603852574055,2
W3034831508,Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance,2020,0.011750540174892996,2
W2970886003,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,2019,0.011750224037269432,2
W3130880317,Evolution of Semantic Similarity—A Survey,2021,0.01174844856961042,2
W3199638386,Transformers: “The End of History” for Natural Language Processing?,2021,0.011741917120056925,2
W4387103825,Contrastive learning for unsupervised sentence embeddings using negative samples with diminished semantics,2023,0.011739426920493234,2
W4377969288,ELICE: Embedding Language through Informative Contrastive-Encoder,2023,0.01173362219556369,2
W3034467872,Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension,2020,0.01173082958790827,2
W4385573185,ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,2022,0.011721845425150932,2
W4389747880,Zero-Shot Learners for Natural Language Understanding via a Unified Multiple-Choice Perspective,2023,0.011717161067367685,2
W3137353858,Hidden Biases in Unreliable News Detection Datasets,2021,0.011714988352872522,2
W3034584102,Towards Robustifying NLI Models Against Lexical Dataset Biases,2020,0.011710693383165991,2
W3004153848,ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation,2020,0.011705979922541639,2
W4385571915,SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval,2023,0.011702131902898863,2
W2995040292,A Mutual Information Maximization Perspective of Language Representation Learning,2020,0.011701880964947906,2
W3176640961,COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic,2021,0.011699760343392943,2
W4385567015,AdaPrompt: Adaptive Model Training for Prompt-based NLP,2022,0.011697245434312968,2
W4317898419,Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering,2023,0.011686957638046478,2
W3205816523,Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm,2022,0.011685665614960499,2
W3117738520,Syntactic Structure Distillation Pretraining for Bidirectional Encoders,2020,0.01168431751100932,2
W3172931322,TuringAdvice: A Generative and Dynamic Evaluation of Language Use,2021,0.011682789464946088,2
W3103410128,"What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation",2020,0.011678949124518402,2
W3114544027,CosMo: Conditional Seq2Seq-based Mixture Model for Zero-Shot Commonsense Question Answering,2020,0.011678655384236264,2
W4236738970,Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access,2021,0.01167265480534047,2
W3174660442,Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,2021,0.01167246258194098,2
W3035733645,WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema Challenge,2020,0.011670880853717864,2
W4385567008,"ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-Shot Generalization",2022,0.011665537696860162,2
W3196731672,Cross-Task Generalization via Natural Language Crowdsourcing Instructions,2022,0.011664229172671134,2
W2942128719,Understanding Dataset Design Choices for Multi-hop Reasoning,2019,0.011663343159902584,2
W3187071356,Improved Text Classification via Contrastive Adversarial Training,2022,0.011661051702283052,2
W4385572608,Plug-and-Play Knowledge Injection for Pre-trained Language Models,2023,0.011656175041930242,2
W3168125510,Rethinking Network Pruning – under the Pre-train and Fine-tune Paradigm,2021,0.011654550709605882,2
W3083182073,KILT: a Benchmark for Knowledge Intensive Language Tasks,2020,0.011653260255461453,2
W2985482647,Commonsense Inference in Natural Language Processing (COIN) - Shared Task Report,2019,0.01164824876603605,2
W3014521650,A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation,2020,0.011646447487928832,2
W2953365054,"Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension",2019,0.011645995346305936,2
W4385570973,Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning,2023,0.011640475817088509,2
W2998733856,JEC-QA: A Legal-Domain Question Answering Dataset,2020,0.011635376246736037,2
W3041843309,A Survey on Transfer Learning in Natural Language Processing,2020,0.011633377311340268,2
W3207553988,Generated Knowledge Prompting for Commonsense Reasoning,2022,0.011632404595924786,2
W4385572960,Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling,2022,0.011628835882628245,2
W3175349176,A Unified Pretraining Framework for Passage Ranking and Expansion,2021,0.011624403981617201,2
W3156216837,Documenting the English Colossal Clean Crawled Corpus.,2021,0.011618931949342248,2
W2982944182,"When Choosing Plausible Alternatives, Clever Hans can be Clever",2019,0.011618698334505696,2
W3177308635,xMoCo: Cross Momentum Contrastive Learning for Open-Domain Question Answering,2021,0.011617170481206546,2
W4389523675,Context-faithful Prompting for Large Language Models,2023,0.011608203811362407,2
W3152740956,Case-based Reasoning for Natural Language Queries over Knowledge Bases,2021,0.011605583252770925,2
W3162022378,The text-package: An R-package for Analyzing and Visualizing Human Language Using Natural Language Processing and Deep Learning,2021,0.011600066883899442,2
W4285174271,Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models,2022,0.011581928223907639,2
W3199885074,Benchmarking Commonsense Knowledge Base Population with an Effective Evaluation Dataset,2021,0.011579714782728926,2
W4206199121,TAG: Gradient Attack on Transformer-based Language Models,2021,0.011579101743926566,2
W3209030604,Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable,2021,0.011579028242160961,2
W3156476125,Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models,2021,0.011577470621847245,2
W4230908641,Exploiting Reasoning Chains for Multi-hop Science Question Answering,2021,0.011573413374819753,2
W3213159541,Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization,2021,0.011558834613221962,2
W3195153845,Uniqorn: Unified question answering over RDF knowledge graphs and natural language text,2024,0.011555712660876712,2
W3213059131,Effective Convolutional Attention Network for Multi-label Clinical Document Classification,2021,0.011551814773940744,2
W4399130767,Experimental study on short-text clustering using transformer-based semantic similarity measure,2024,0.011551377442423814,2
W4399020384,Experimental Design of Extractive Question-Answering Systems: Influence of Error Scores and Answer Length,2024,0.011548758841779007,2
W4389524150,Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings,2023,0.011548021044075292,2
W2911966030,Improving Question Answering with External Knowledge,2019,0.011546930197794195,2
W4386576638,Enhancing Information Retrieval in Fact Extraction and Verification,2023,0.011546455087520677,2
W3175621180,Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene,2021,0.011546232201303625,2
W3016828850,B<scp>reak</scp> It Down: A Question Understanding Benchmark,2020,0.011535363699789285,2
W3159588055,"Transformers: ""The End of History"" for NLP?",2021,0.01153473563013169,2
W2998557616,Inducing Relational Knowledge from BERT,2020,0.01153458283876507,2
W4385570124,"On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning",2023,0.011533007782187825,2
W3115772171,Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification,2020,0.011526704933225551,2
W4225380759,Entity-aware Transformers for Entity Search,2022,0.011525510407271296,2
W4385574115,Acceptability Judgements via Examining the Topology of Attention Maps,2022,0.011522754227385672,2
W3212849024,Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering,2021,0.011521555323067529,2
W4382561244,ELECTRA-based graph network model for multi-hop question answering,2023,0.011520772646762857,2
W2794557536,Universal Sentence Encoder,2018,0.01151910789888231,2
W3176197839,Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis,2021,0.011517878462887219,2
W3021677691,Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering,2020,0.011512281206010446,2
W3033188311,Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing,2020,0.01150983008552562,2
W4389891246,E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation,2023,0.011502341517992832,2
W3165416482,PTR: Prompt Tuning with Rules for Text Classification,2021,0.011499881576308267,2
W3105661746,"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA",2020,0.01149363844930831,2
W3217387898,A Review on Fact Extraction and Verification,2021,0.011492203241194128,2
W2890961898,Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text,2018,0.011486357207363372,2
W2964044490,Don’t Take the Premise for Granted: Mitigating Artifacts in Natural Language Inference,2019,0.011481494767207076,2
W4399353257,Pre-trained language models in medicine: A survey,2024,0.011475765564497946,2
W3197907569,Contrastive Domain Adaptation for Question Answering using Limited Text Corpora,2021,0.011473549546437646,2
W3175557894,Robust Transfer Learning with Pretrained Language Models through Adapters,2021,0.011469426762052155,2
W4385571840,Robust Natural Language Understanding with Residual Attention Debiasing,2023,0.011463416000430021,2
W4295308427,A Survey of Text Representation Methods and Their Genealogy,2022,0.011463313814804138,2
W4205870266,Explaining Answers with Entailment Trees,2021,0.011443248684059769,2
W4206634569,Block Pruning For Faster Transformers,2021,0.011442187062411945,2
W3200033622,The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders,2021,0.011440968098517029,2
W3118184560,Learning Dense Representations of Phrases at Scale,2020,0.011433253973590862,2
W3166920165,Refining Targeted Syntactic Evaluation of Language Models,2021,0.011426472823042873,2
W4223956331,Label Semantic Aware Pre-training for Few-shot Text Classification,2022,0.011421474148136166,2
W2978124139,MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,2019,0.011420194643245068,2
W4285200483,Word Order Does Matter and Shuffled Language Models Know It,2022,0.011418880906612278,2
W4287887967,ConfliBERT: A Pre-trained Language Model for Political Conflict and Violence,2022,0.011416530662058907,2
W3091432621,Autoregressive Entity Retrieval,2020,0.011415627197497741,2
W4226142803,SimKGC: Simple Contrastive Knowledge Graph Completion with Pre-trained Language Models,2022,0.011408183441104145,2
W3026350704,Aggregating Customer Review Attributes for Online Reputation Generation,2020,0.011407554814400691,2
W4283796192,LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification,2022,0.011405647049760591,2
W3160596727,Med7: A transferable clinical natural language processing model for electronic health records,2021,0.01140499036343406,2
W3173854146,Syntax-Enhanced Pre-trained Model,2021,0.011401924567644158,2
W3175236579,Dynamic Contextualized Word Embeddings,2021,0.011395829605073467,2
W4385573562,Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference,2022,0.011387681306032524,2
W3173765306,DuReader_robust: A Chinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications,2021,0.011387442473855378,2
W4385572907,DRLK: Dynamic Hierarchical Reasoning with Language Model and Knowledge Graph for Question Answering,2022,0.011382062575511956,2
W3174264667,Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence,2021,0.011381192917339909,2
W4401852331,Enhancing performance of transformer-based models in natural language understanding through word importance embedding,2024,0.01137844408986684,2
W3152936485,Does Putting a Linguist in the Loop Improve NLU Data Collection?,2021,0.011373925923179125,2
W3176757281,Question Answering Over Temporal Knowledge Graphs,2021,0.011373687768846565,2
W4295885374,SpaDE,2022,0.011369336160869645,2
W3103838460,Investigating Transferability in Pretrained Language Models,2020,0.0113688184577822,2
W3169602049,Static Embeddings as Efficient Knowledge Bases?,2021,0.0113665914250687,2
W3191230274,APER: AdaPtive Evidence-driven Reasoning Network for machine reading comprehension with unanswerable questions,2021,0.011365744783447929,2
W3105643199,An Empirical Investigation of Contextualized Number Prediction,2020,0.011365485535258452,2
W3032960712,An Overview of Neural Network Compression,2020,0.011364428223031658,2
W4385574304,Retrofitting Multilingual Sentence Embeddings with Abstract Meaning Representation,2022,0.011361731371008384,2
W3014961314,A Question Answering-Based Framework for One-Step Event Argument Extraction,2020,0.011359981919474323,2
W3095992020,An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models,2020,0.011359375506031142,2
W4385574243,Grape: Knowledge Graph Enhanced Passage Reader for Open-domain Question Answering,2022,0.011356773926236791,2
W3176751053,Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models,2021,0.011353832592185809,2
W3214374241,The Power of Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,2022,0.01135357324178293,2
W3108757211,Transformer-Based Models for Automatic Identification of Argument Relations: A Cross-Domain Evaluation,2021,0.011347754519477367,2
W4226101361,Read before Generate! Faithful Long Form Question Answering with Machine Reading,2022,0.01134761853274047,2
W2972987451,Testing the Generalization Power of Neural Network Models across NLI Benchmarks,2019,0.011327845906235728,2
W2976833415,Reweighted Proximal Pruning for Large-Scale Language Representation,2019,0.011318915443197066,2
W2946345909,ERNIE: Enhanced Language Representation with Informative Entities,2019,0.011311694676871929,2
W3035305735,Information-Theoretic Probing for Linguistic Structure,2020,0.011309110153427449,2
W3092185277,A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks,2020,0.011298552918796499,2
W3101381894,BiTeM at WNUT 2020 Shared Task-1: Named Entity Recognition over Wet Lab Protocols using an Ensemble of Contextual Language Models,2020,0.011298458864164591,2
W3188759137,Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning,2022,0.011296987287196544,2
W3103855731,Interpretation of NLP models through input marginalization,2020,0.011295547742192812,2
W3166417463,Capturing Row and Column Semantics in Transformer Based Question Answering over Tables,2021,0.011283450597014915,2
W3099954076,Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning,2020,0.011278773008150425,2
W3044749682,The Lottery Ticket Hypothesis for Pre-trained BERT Networks,2020,0.01126823115752785,2
W3098469895,Learning from Unlabelled Data for Clinical Semantic Textual Similarity,2020,0.011265932102152884,2
W3048018176,Does BERT Solve Commonsense Task via Commonsense Knowledge,2020,0.011260819873247245,2
W4280649755,AFS Graph: Multidimensional Axiomatic Fuzzy Set Knowledge Graph for Open-Domain Question Answering,2022,0.011259951897198205,2
W3094292782,"Retrieve, Rerank, Read, then Iterate: Answering Open-Domain Questions of Arbitrary Complexity from Text.",2020,0.011258621011248523,2
W3094815596,Dynamic Neuro-Symbolic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,2019,0.011256918912956932,2
W4312443713,Automated Question Answering for Improved Understanding of Compliance Requirements: A Multi-Document Study,2022,0.011246856437173871,2
W4287887100,"Re2G: Retrieve, Rerank, Generate",2022,0.011244578903571237,2
W3190730109,"An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools",2021,0.011241241084750482,2
W4287887161,Quantifying Adaptability in Pre-trained Language Models with 500 Tasks,2022,0.011234520570220832,2
W3020482686,Self-Attention Attribution: Interpreting Information Interactions Inside Transformer,2020,0.011230688750952598,2
W3109919947,Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning,2020,0.01122821948843469,2
W4385570119,WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings,2023,0.011219141477534363,2
W2995643077,Abductive Commonsense Reasoning,2020,0.011218628884487587,2
W3171397222,"Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",2021,0.011217840239479804,2
W3119438769,Prefix-Tuning: Optimizing Continuous Prompts for Generation,2021,0.011217544109658728,2
W3201193395,MirrorWiC: On Eliciting Word-in-Context Representations from Pretrained Language Models,2021,0.011217273519608344,2
W3175987672,Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level,2021,0.011205946076544113,2
W2950784811,A Multiscale Visualization of Attention in the Transformer Model,2019,0.011201160520301404,2
W4200631806,From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression,2022,0.011199541220691073,2
W3167364369,MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories,2021,0.01119784483498514,2
W3099246072,TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions,2020,0.01119763089154023,2
W3154165903,On the evolution of syntactic information encoded by BERT’s contextualized representations,2021,0.011196508907867941,2
W4287887521,"Yes, No or IDK: The Challenge of Unanswerable Yes/No Questions",2022,0.0111961634651917,2
W3190876505,Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing,2021,0.011195724828862217,2
W3169113923,WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models,2021,0.011194903214227399,2
W2785611959,T-Rex : A Large Scale Alignment of Natural Language with Knowledge Base Triples,2017,0.0111863687869234,2
W4385573063,Tiny-NewsRec: Effective and Efficient PLM-based News Recommendation,2022,0.011182026719204943,2
W3131259441,Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder.,2021,0.01117776063970864,2
W4287889465,EASE: Entity-Aware Contrastive Learning of Sentence Embedding,2022,0.011175823647636238,2
W4389519532,NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports,2023,0.011174486955168874,2
W4285601776,DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning,2022,0.011165000448489656,2
W4402352511,Option-Differentiated Clue Augmentation for Commonsense Question Answering,2024,0.0111633357517273,2
W3159119521,Human-Model Divergence in the Handling of Vagueness,2021,0.011152774871432515,2
W4221165884,Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Learning,2022,0.011152378327306702,2
W3152884768,Knowledge Neurons in Pretrained Transformers,2022,0.011151488800929038,2
W3018642446,Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,2020,0.011149878066832817,2
W3103975738,A little goes a long way: Improving toxic language classification despite data scarcity,2020,0.011149849082218373,2
W4407264142,KG-prompt: Interpretable knowledge graph prompt for pre-trained language models,2025,0.011149438691496068,2
W2982111970,HUBERT Untangles BERT to Improve Transfer across NLP Tasks,2019,0.011149046273528846,2
W4391855109,"A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",2024,0.01114785302455574,2
W3186492090,When does pretraining help?,2021,0.011146037509072376,2
W4385570306,Ranking-Enhanced Unsupervised Sentence Representation Learning,2023,0.011145187956811636,2
W3099976981,Don’t Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering,2020,0.01113847278582136,2
W3185051273,Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings,2021,0.011138162170273086,2
W3174461835,Accelerating BERT Inference for Sequence Labeling via Early-Exit,2021,0.011136934603697203,2
W3128654100,InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective,2020,0.0111346415938911,2
W3160883893,Few-Shot Conversational Dense Retrieval,2021,0.011129083067371846,2
W4385572867,Iteratively Prompt Pre-trained Language Models for Chain of Thought,2022,0.011115816513232929,2
W3104613320,TopicBERT for Energy Efficient Document Classification,2020,0.011112423326150088,2
W3170369042,"Generate, Annotate, and Learn: Generative Models Advance Self-Training and Knowledge Distillation.",2021,0.01110850172417097,2
W3196790170,ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding,2021,0.011107713691263862,2
W4287888691,Efficient Hierarchical Domain Adaptation for Pretrained Language Models,2022,0.011107654638985194,2
W4285254250,Zero- and Few-Shot NLP with Pretrained Language Models,2022,0.011104826633406296,2
W4385488817,Defending Machine Reading Comprehension against Question-Targeted Attacks,2023,0.011104335768303894,2
W3021649351,Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering,2020,0.011103256894394441,2
W3173588320,Improving Document Representations by Generating Pseudo Query Embeddings for Dense Retrieval,2021,0.011103032772360685,2
W4388778348,In-Context Retrieval-Augmented Language Models,2023,0.011099956337863384,2
W2963126845,Adversarial Example Generation with Syntactically Controlled Paraphrase Networks,2018,0.011095970803247684,2
W2806055002,SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge,2018,0.011085782531450334,2
W3117841010,KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation,2020,0.011079121304982646,2
W4385571564,Patton: Language Model Pretraining on Text-Rich Networks,2023,0.011076927436943242,2
W3022518881,Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?,2020,0.011076424290181634,2
W3096403953,CODER: Knowledge-infused cross-lingual medical term embedding for term normalization,2022,0.011075469332297484,2
W2970820321,The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives,2019,0.01107506620763827,2
W2971042182,Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension,2019,0.011073927194027569,2
W3113280695,KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning,2020,0.011071820081862766,2
W3172364764,Explainable Multi-hop Verbal Reasoning Through Internal Monologue,2021,0.011065983315669391,2
W2963967365,Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension,2018,0.01106226479170263,2
W4287120901,"Generate, Annotate, and Learn: NLP with Synthetic Text",2022,0.011061491733624551,2
W2988787701,Team DOMLIN: Exploiting Evidence Enhancement for the FEVER Shared Task,2019,0.011059844631917925,2
W3047185145,Aligning AI With Shared Human Values,2020,0.0110554858915305,2
W3094476119,Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,2020,0.011055340036209577,2
W3159959439,<i>Did Aristotle Use a Laptop?</i>A Question Answering Benchmark with Implicit Reasoning Strategies,2021,0.011055097606022832,2
W2892085193,I Know What You Want: Semantic Learning for Text Comprehension,2018,0.011054903790934082,2
W3194310511,Pre-training for Ad-hoc Retrieval: Hyperlink is Also You Need,2021,0.01105115531817416,2
W4221154651,MERIt: Meta-Path Guided Contrastive Learning for Logical Reasoning,2022,0.011049444758686702,2
W3202026671,LexGLUE: A Benchmark Dataset for Legal Language Understanding in English,2021,0.011042196503148746,2
W3104235802,Entity Enhanced BERT Pre-training for Chinese NER,2020,0.011036560861830273,2
W4385572063,FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models,2023,0.011036133585538377,2
W2968289784,SenseBERT: Driving Some Sense into BERT,2019,0.011035616935654509,2
W2970764230,Movie Plot Analysis via Turning Point Identification,2019,0.011035051709322103,2
W4287854822,Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning,2022,0.01103394278524313,2
W3083978629,Task-specific Objectives of Pre-trained Language Models for Dialogue Adaptation,2020,0.01103277805657454,2
W3121076170,FiD-Ex: Improving Sequence-to-Sequence Models for Extractive Rationale Generation,2021,0.011023022971931955,2
W4385571411,Is GPT-3 a Good Data Annotator?,2023,0.011021624100459978,2
W3173482217,LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding,2021,0.011018290927042517,2
W2911529999,Dual Co-Matching Network for Multi-choice Reading Comprehension,2019,0.011013650924388996,2
W4225808286,"Adapting GPT, GPT-2 and BERT Language Models for Speech Recognition",2021,0.011012874490228116,2
W3159727696,SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning,2021,0.011012085938980927,2
W4224865658,Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction,2022,0.011010418497892659,2
W4385573179,Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning,2022,0.011010211941301126,2
W3158360872,Gradient-based Adversarial Attacks against Text Transformers,2021,0.011008329932947456,2
W3115231918,Cross-lingual Machine Reading Comprehension with Language Branch Knowledge Distillation,2020,0.011006924323260444,2
W3154376977,Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models,2021,0.011002424535478969,2
W2964242047,Probing Biomedical Embeddings from Language Models,2019,0.010996480600052157,2
W3175515348,Reader-Guided Passage Reranking for Open-Domain Question Answering,2021,0.010995437774794614,2
W4287854320,KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation,2022,0.010994934699809665,2
W4205131770,Generate &amp; Rank: A Multi-task Framework for Math Word Problems,2021,0.01099437041779406,2
W3037636427,What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?,2020,0.010992182431880652,2
W2947415936,Combating Adversarial Misspellings with Robust Word Recognition,2019,0.010978150463912602,2
W3197780238,KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs,2021,0.010977782603612637,2
W4385571498,Generating Better Items for Cognitive Assessments Using Large Language Models,2023,0.010977227179909238,2
W3177101259,Empowering Language Understanding with Counterfactual Reasoning,2021,0.010971355230283024,2
W4388936696,Prompt-Learning for Short Text Classification,2023,0.010969334297355512,2
W4387789854,Unifying Structure Reasoning and Language Pre-Training for Complex Reasoning Tasks,2023,0.010967016408141921,2
W3183822815,"SemEval 2021 Task 7: HaHackathon, Detecting and Rating Humor and Offense",2021,0.010966455384958674,2
W2995998574,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,2019,0.010964170332834842,2
W3177468934,Evaluation Examples are not Equally Informative: How should that change NLP Leaderboards?,2021,0.010962284199984688,2
W4385718075,Language models are not naysayers: an analysis of language models on negation benchmarks,2023,0.010958316676826493,2
W4385567093,Efficient Large Scale Language Modeling with Mixtures of Experts,2022,0.010950116728067649,2
W3101777798,“I’d rather just go to bed”: Understanding Indirect Answers,2020,0.01093756549817434,2
W4319793479,Training-free Lexical Backdoor Attacks on Language Models,2023,0.01093161826977911,2
W3173256823,Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains,2021,0.010928813960877628,2
W3106388706,Interpreting Attention Models with Human Visual Attention in Machine Reading Comprehension,2020,0.010924288479344667,2
W2793978524,AllenNLP: A Deep Semantic Natural Language Processing Platform,2018,0.010923312591556942,2
W2948771346,Visualizing and Measuring the Geometry of BERT,2019,0.010922415443117782,2
W3199350934,NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases,2021,0.010920257405690132,2
W4315778472,ShortcutLens: A Visual Analytics Approach for Exploring Shortcuts in Natural Language Understanding Dataset,2023,0.01091433342570462,2
W3099299360,When Do You Need Billions of Words of Pretraining Data?,2020,0.010913974265699466,2
W3034830866,Transformers as Soft Reasoners over Language,2020,0.010913925442708515,2
W3205696278,Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer,2021,0.010913189193513249,2
W3175049034,A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space,2021,0.010905234549994189,2
W3213458975,<scp>ParsiNLU</scp>: A Suite of Language Understanding Challenges for Persian,2021,0.010903848703582948,2
W3160847843,Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters,2022,0.010892476440055575,2
W2909672886,Multi-style Generative Reading Comprehension,2019,0.010888579057568045,2
W3035064231,Learning to Faithfully Rationalize by Construction,2020,0.010880116331986402,2
W4386798146,ActiveGLAE: A Benchmark for Deep Active Learning with Transformers,2023,0.010870016566078205,2
W2963383094,Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets,2019,0.01086804901849514,2
W4385002643,Exploring the Landscape of Natural Language Processing Research,2023,0.010864917856842779,2
W4285190530,TimeLMs: Diachronic Language Models from Twitter,2022,0.010858905931319397,2
W3021191241,E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT,2019,0.010856316120091742,2
W4389519063,FedID: Federated Interactive Distillation for Large-Scale Pretraining Language Models,2023,0.010853931614705091,2
W4385573090,Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection,2022,0.010853196675382737,2
W3019882988,Collecting Entailment Data for Pretraining: New Protocols and Negative Results.,2020,0.010851875774245309,2
W3079786700,"Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries",2020,0.010849597082199354,2
W4396506940,Challenges and opportunities of using transformer-based multi-task learning in NLP through ML lifecycle: A position paper,2024,0.010847872110622813,2
W4385574191,Event-Centric Question Answering via Contrastive Learning and Invertible Event Transformation,2022,0.010842636227372011,2
W4389519029,Universal Self-Adaptive Prompting,2023,0.010839554324204834,2
W3121064530,Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain Detection,2021,0.010838076097235997,2
W4285149128,A Sentence is Worth 128 Pseudo Tokens: A Semantic-Aware Contrastive Learning Framework for Sentence Embeddings,2022,0.010832918646365711,2
W3208393080,EasyTransfer,2021,0.01083282637509318,2
W2971380169,Semantics-aware BERT for Language Understanding,2019,0.01083010828287303,2
W4409404441,Medical short text classification via Soft Prompt-tuning,2025,0.010827795836916147,2
W4225295188,Detecting Textual Adversarial Examples Based on Distributional Characteristics of Data Representations,2022,0.010825460731017733,2
W3035099133,Contextualized Sparse Representations for Real-Time Open-Domain Question Answering,2020,0.010824351199164493,2
W4385571807,Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification,2023,0.010818811585541885,2
W4385570691,NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models,2023,0.01081528739418466,2
W4386566639,Enriching Biomedical Knowledge for Low-resource Language Through Large-scale Translation,2023,0.010814531768801714,2
W4283691240,Analyzing Encoded Concepts in Transformer Language Models,2022,0.010812743752311494,2
W4381686872,Questions Are All You Need to Train a Dense Passage Retriever,2023,0.010804008045861103,2
W3202627914,Towards Interpretable and Reliable Reading Comprehension: A Pipeline Model with Unanswerability Prediction,2021,0.010796826375480838,2
W4385574162,Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations,2022,0.010796801537030205,2
W3106278732,ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention,2020,0.010789754567826135,2
W3033077667,Classification aware neural topic model for COVID-19 disinformation categorisation,2021,0.010782901726213795,2
W3206375861,Prix-LM: Pretraining for Multilingual Knowledge Base Construction,2022,0.010780988595804309,2
W2989033444,A Hybrid Neural Network Model for Commonsense Reasoning,2019,0.010776036321184006,2
W3196340340,Large Biomedical Question Answering Models with ALBERT and ELECTRA.,2021,0.010775574786110301,2
W4386857681,"A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and Challenges",2023,0.010774364940091386,2
W4393146982,DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning,2024,0.010767478517788668,2
W4287854971,PLM-ICD: Automatic ICD Coding with Pretrained Language Models,2022,0.01076182243947006,2
W4386776484,A survey of deep learning techniques for machine reading comprehension,2023,0.010760350099482039,2
W4280546523,Transkimmer: Transformer Learns to Layer-wise Skim,2022,0.010759387830843067,2
W4205523551,Automated Essay Scoring Using Transformer Models,2021,0.010755262749081836,2
W4385764314,Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation,2023,0.01074787543802468,2
W2998536339,ManyModalQA: Modality Disambiguation and QA over Diverse Inputs,2020,0.010746942470267999,2
W4283793025,Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability,2022,0.010745657808634717,2
W4385256174,An Inferential Commonsense-Driven Framework for Predicting Political Bias in News Headlines,2023,0.010741701029888962,2
W4389519806,BERT Has More to Offer: BERT Layers Combination Yields Better Sentence Embeddings,2023,0.010741103087433142,2
W4285127912,Probing for the Usage of Grammatical Number,2022,0.01073989959279847,2
W3120519792,EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets,2021,0.010735967382957286,2
W4205424278,Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations,2021,0.010735065701248483,2
W2521709538,ReasoNet,2017,0.010734577285664426,2
W3196783364,Enhanced Speaker-Aware Multi-Party Multi-Turn Dialogue Comprehension,2023,0.010734177675952132,2
W3214608568,What’s in a Name? Answer Equivalence For Open-Domain Question Answering,2021,0.010733386224828962,2
W3203149535,Towards Efficient Post-training Quantization of Pre-trained Language Models,2021,0.010729489949605877,2
W4225418906,POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection,2022,0.010720990807301182,2
W4389519438,LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain,2023,0.010719917012191408,2
W3035367371,Weight Poisoning Attacks on Pretrained Models,2020,0.010716191114649358,2
W3154186000,ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning,2021,0.010710782539684214,2
W4295807725,Natural language processing in clinical neuroscience and psychiatry: A review,2022,0.010705509235677842,2
W2970728484,Question Answering for Privacy Policies: Combining Computational and Legal Perspectives,2019,0.010705148628910291,2
W3209492300,Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework,2023,0.010702057814461064,2
W3034602344,Temporal Common Sense Acquisition with Minimal Supervision,2020,0.010698852121132415,2
W4294044662,Artificial intelligence for topic modelling in Hindu philosophy: Mapping themes between the Upanishads and the Bhagavad Gita,2022,0.010697614275295983,2
W2952354317,XQA: A Cross-lingual Open-domain Question Answering Dataset,2019,0.010696132885840074,2
W4385570914,Modeling Adversarial Attack on Pre-trained Language Models as Sequential Decision Making,2023,0.01069612486985765,2
W3206843525,Can Explanations Be Useful for Calibrating Black Box Models?,2022,0.010694952814605087,2
W3046209160,DeepMet: A Reading Comprehension Paradigm for Token-level Metaphor Detection,2020,0.010694243570864721,2
W2986380433,Generation-Distillation for Efficient Natural Language Understanding in Low-Data Settings,2019,0.010694052389034127,2
W4381803920,Transparency Helps Reveal When Language Models Learn Meaning,2023,0.0106932787487459,2
W4213122582,Identifying Machine-Paraphrased Plagiarism,2022,0.010688045210597272,2
W2963958374,pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference,2019,0.010683446314663662,2
W4385571153,"Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering",2023,0.010683058970427872,2
W3118796926,"Polyjuice: Automated, General-purpose Counterfactual Generation.",2021,0.010682370535155573,2
W4389520409,TaskWeb: Selecting Better Source Tasks for Multi-task NLP,2023,0.0106746094803017,2
W3156170450,Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models,2021,0.010667502258859386,2
W2913352150,A question-entailment approach to question answering,2019,0.01066376279443812,2
W3034779619,How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope,2020,0.010662638119291447,2
W3210528623,Fake News Detection on Social Media Using A Natural Language Inference Approach,2020,0.010656576663157438,2
W3014328670,Investigating Prior Knowledge for Challenging Chinese Machine Reading Comprehension,2020,0.01065422292506659,2
W3168355451,Modeling Fine-Grained Entity Types with Box Embeddings,2021,0.010651588571518664,2
W3179534853,Tailor: Generating and Perturbing Text with Semantic Controls,2022,0.010645756684224909,2
W3023160663,Connecting the Dots: A Knowledgeable Path Generator for Commonsense Question Answering,2020,0.010632521893485609,2
W3024131638,Enabling Language Models to Fill in the Blanks,2020,0.010627909890734257,2
W4310130650,Answering Count Questions with Structured Answers from Text,2022,0.010626971882610506,2
W3131755153,A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks,2021,0.010624426458475892,2
W4226455589,How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis,2022,0.010622337613948735,2
W3015612646,Diagnosing BERT with Retrieval Heuristics,2020,0.010621643403907774,2
W3089271032,TernaryBERT: Distillation-aware Ultra-low Bit BERT,2020,0.010621046636680632,2
W3021524072,Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models,2020,0.010620665111887435,2
W2963344337,Gated-Attention Readers for Text Comprehension,2017,0.010619896975639373,2
W4287854424,SemEval-2022 Task 9: R2VQ – Competence-based Multimodal Question Answering,2022,0.010618976971635052,2
W2970648593,Linking artificial and human neural representations of language,2019,0.010617317753578957,2
W3196923642,Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer,2020,0.01061226797931942,2
W3205718179,Retrieval-guided Counterfactual Generation for QA,2022,0.010611439711989813,2
W3198005027,Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations,2021,0.010609969916423496,2
W4213156872,Interpretable modular knowledge reasoning for machine reading comprehension,2022,0.010609657517116169,2
W3102400851,WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols,2020,0.010609463138165437,2
W4385573497,Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference,2022,0.010597627681899204,2
W4313596849,"Beyond Rating Scales: With Targeted Evaluation, Language Models are Poised for Psychological Assessment",2023,0.010592897064916881,2
W4385574253,ComFact: A Benchmark for Linking Contextual Commonsense Knowledge,2022,0.010590403636810693,2
W4385571457,Generating Deep Questions with Commonsense Reasoning Ability from the Text by Disentangled Adversarial Inference,2023,0.01058837842929986,2
W3034257506,An Iterative Multi-Source Mutual Knowledge Transfer Framework for Machine Reading Comprehension,2020,0.010579782962821159,2
W2963506049,CliCR: a Dataset of Clinical Case Reports for Machine Reading Comprehension,2018,0.01057952773439466,2
W2998579922,DCMN+: Dual Co-Matching Network for Multi-Choice Reading Comprehension,2020,0.010578829074174715,2
W3161024824,"Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System",2021,0.01057588839588307,2
W3170101424,Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling,2021,0.01056827513339306,2
W3129415623,SparseBERT: Rethinking the Importance Analysis in Self-attention,2021,0.01056796366690416,2
W3202408265,Parallel Refinements for Lexically Constrained Text Generation with BART,2021,0.010562739144101949,2
W3197499505,CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge,2021,0.010562060555503487,2
W4385573552,PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings,2022,0.0105530588855705,2
W3086456409,Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue,2021,0.010551269885604656,2
W3173586048,KACE: Generating Knowledge Aware Contrastive Explanations for Natural Language Inference,2021,0.010551252717118546,2
W3016309009,Entities as Experts: Sparse Memory Access with Entity Supervision,2020,0.010546513588860081,2
W4220998098,Multilingual multi-aspect explainability analyses on machine reading comprehension models,2022,0.01054617517781168,2
W4385571760,RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question,2023,0.010540510144122996,2
W3103054319,Pareto Probing: Trading Off Accuracy for Complexity,2020,0.010530165741614847,2
W4399971973,Differentially Private Fine-tuning of Language Models,2024,0.010528614185157721,2
W4401507672,Prompt Tuning on Graph-Augmented Low-Resource Text Classification,2024,0.010524881755198865,2
W4252760000,Proceedings of the 1st Workshop on NLP for Positive Impact,2021,0.010513781547567594,2
W3118017018,Linguistic Profiling of a Neural Language Model,2020,0.010505900998611551,2
W3181361218,Neural Natural Language Processing for unstructured data in electronic health records: A review,2022,0.010503360851587505,2
W4389520361,PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training,2023,0.010500029902081973,2
W2973123473,A Discrete Hard EM Approach for Weakly Supervised Question Answering,2019,0.01049995676722105,2
W3007257988,DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding,2020,0.010486806197908623,2
W2806120502,Neural Network Acceptability Judgments,2018,0.010485180700876758,2
W2977745385,Improving Question Answering by Commonsense-Based Pre-training,2019,0.01048003661039856,2
W3120772090,Multi-task Retrieval for Knowledge-Intensive Tasks,2021,0.010472106614436658,2
W2970344931,Sieg at MEDIQA 2019: Multi-task Neural Ensemble for Biomedical Inference and Entailment,2019,0.010470345690500696,2
W4285206744,Cluster &amp; Tune: Boost Cold Start Performance in Text Classification,2022,0.010468237930912781,2
W4385574263,Measuring the Mixing of Contextual Information in the Transformer,2022,0.010453496412313048,2
W3174881085,Structural Pre-training for Dialogue Comprehension,2021,0.010450474329322458,2
W4385572734,GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models,2022,0.010447847008166694,2
W3104748221,Scalable Zero-shot Entity Linking with Dense Entity Retrieval,2020,0.010445121305096417,2
W3105313172,Adapting Open Domain Fact Extraction and Verification to COVID-FACT through In-Domain Language Modeling,2020,0.010442960421243479,2
W4389520386,SimCSE++: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives,2023,0.010441238198860099,2
W3205305305,Contrastive Document Representation Learning with Graph Attention Networks,2021,0.010440609108793152,2
W4386576685,MTEB: Massive Text Embedding Benchmark,2023,0.01043482250113234,2
W3016512233,PALM: Pre-training an Autoencoding&amp;Autoregressive Language Model for Context-conditioned Generation,2020,0.010430015273029667,2
W3108936148,Answering Questions on COVID-19 in Real-Time,2020,0.010421389093663386,2
W4385573637,ClinicalT5: A Generative Language Model for Clinical Text,2022,0.010419881658007333,2
W2995744795,Are Transformers universal approximators of sequence-to-sequence functions?,2019,0.01041073260346832,2
W2971277088,Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling,2019,0.010409179749089502,2
W3031001133,When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions,2020,0.010406915909096969,2
W4407025433,A prompt tuning method based on relation graphs for few-shot relation extraction,2025,0.010401405647822418,2
W3200655919,Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning,2021,0.010396633408444737,2
W3116231956,Hitachi at SemEval-2020 Task 11: An Empirical Study of Pre-Trained Transformer Family for Propaganda Detection,2020,0.010395275141946707,2
W4385570872,SemEval-2023 Task 6: LegalEval - Understanding Legal Texts,2023,0.010390095528685283,2
W4221141197,A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models,2022,0.010383423040031644,2
W4311457721,Demystifying BERT: System Design Implications,2022,0.010375084963054407,2
W4385570815,Parameter-Efficient Fine-Tuning without Introducing New Latency,2023,0.010374301987516014,2
W4399920053,LMCK: pre-trained language models enhanced with contextual knowledge for Vietnamese natural language inference,2024,0.010372861231101347,2
W3116968030,Sentence Matching with Syntax- and Semantics-Aware BERT,2020,0.01037193276639731,2
W4385570657,Prompt-based Zero-shot Text Classification with Conceptual Knowledge,2023,0.010369070924763416,2
W2970169521,Learning with Limited Data for Multilingual Reading Comprehension,2019,0.010367392807041766,2
W4385572323,Do PLMs Know and Understand Ontological Knowledge?,2023,0.010359086958435265,2
W2966491090,Trick Me If You Can: Human-in-the-Loop Generation of Adversarial Examples for Question Answering,2019,0.01035879736398021,2
W3102749280,Thinking Like a Skeptic: Defeasible Inference in Natural Language,2020,0.010357338112236507,2
W2610858497,Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning,2017,0.010355981702386038,2
W4395680635,Semantics of Multiword Expressions in Transformer-Based Models: A Survey,2024,0.010349290659849675,2
W2949428332,Multi-step Retriever-Reader Interaction for Scalable Open-domain Question Answering,2019,0.010349068402670976,2
W4384211302,Efficient Methods for Natural Language Processing: A Survey,2023,0.010347981679878224,2
W3098415518,Interpreting Predictions of NLP Models,2020,0.010345384154215554,2
W3037458976,Contextual and Non-Contextual Word Embeddings: an in-depth Linguistic Investigation,2020,0.01034411488573123,2
W4385573795,Knowledge Stimulated Contrastive Prompting for Low-Resource Stance Detection,2022,0.010341519068813114,2
W4390805651,Hevenli: A Large Dataset for English-Vietnamese Cross-Lingual Natural Language Inference in Healthcare,2024,0.010332858811298385,2
W2798416089,Neural Natural Language Inference Models Enhanced with External Knowledge,2018,0.010332417304000002,2
W4206808166,The World of an Octopus: How Reporting Bias Influences a Language Model’s Perception of Color,2021,0.010328303776117998,2
W3100262863,Neural Deepfake Detection with Factual Structure of Text,2020,0.010322854953773641,2
W4382465573,Parameter-Efficient Model Adaptation for Vision Transformers,2023,0.010320911559081625,2
W3153543512,Do Syntax Trees Help Pre-trained Transformers Extract Information?,2021,0.010319216390078472,2
W4200635123,GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval,2022,0.010318704063160924,2
W4286905174,Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?,2022,0.010311248456390382,2
W3201090304,Towards Zero-Label Language Learning,2021,0.010308898396892226,2
W4385573055,Robustness of Demonstration-based Learning Under Limited Data Scenario,2022,0.01030563803363315,2
W4389523929,Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements,2023,0.0102977909804826,2
W4385572928,Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space,2022,0.010290041718533949,2
W3148437589,From natural language processing to neural databases,2021,0.010282507372520861,2
W3204650139,GNN is a Counter? Revisiting GNN for Question Answering,2021,0.010280887455651862,2
W4292215729,CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks,2022,0.010275304515818899,2
W4287890137,Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora,2022,0.010265882394716751,2
W2905266130,Conditional BERT Contextual Augmentation,2019,0.01026265003698083,2
W3014564055,Evaluating NLP Models via Contrast Sets,2020,0.010255617446886838,2
W4385573514,Natural Language Deduction with Incomplete Information,2022,0.010251706219046178,2
W3098266846,BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models,2020,0.010248752295358503,2
W3034655581,DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking,2020,0.010247138519173141,2
W3115913034,Task-Aware Representation of Sentences for Generic Text Classification,2020,0.010243197948462958,2
W2971016963,Quantity doesn’t buy quality syntax with neural language models,2019,0.010236061864862419,2
W2799081691,Denoising Distantly Supervised Open-Domain Question Answering,2018,0.01023393973241379,2
W4390490761,Explainability for Large Language Models: A Survey,2024,0.010232400391977132,2
W4385570289,Boosting Text Augmentation via Hybrid Instance Filtering Framework,2023,0.010224320138905878,2
W4310231311,Fusing external knowledge resources for natural language understanding techniques: A survey,2022,0.010219522428312306,2
W3174828871,A Survey of Data Augmentation Approaches for NLP,2021,0.01021852666583589,2
W3158631574,PanGu-$α$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation,2021,0.010211278535613768,2
W3089659770,A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation,2020,0.01020930890042417,2
W4319934130,Memory-Aware Attentive Control for Community Question Answering With Knowledge-Based Dual Refinement,2023,0.01020816184861949,2
W3139002032,SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers,2021,0.010198452518806305,2
W3177191283,The Curse of Dense Low-Dimensional Information Retrieval for Large Index Sizes,2021,0.010197834247295296,2
W3101345273,Cold-start Active Learning through Self-supervised Language Modeling,2020,0.010196601236775547,2
W3206996280,CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training,2022,0.010192002806969072,2
W3199051766,A Simple Approach to Jointly Rank Passages and Select Relevant Sentences in the OBQA Context,2022,0.010188875485506545,2
W3152497014,Factual Probing Is [MASK]: Learning vs. Learning to Recall,2021,0.010172988337223604,2
W4385893872,ScoNe: Benchmarking Negation Reasoning in Language Models With Fine-Tuning and In-Context Learning,2023,0.01017075593257019,2
W4385573504,Active Example Selection for In-Context Learning,2022,0.01016623581874296,2
W3137214022,All NLP Tasks Are Generation Tasks: A General Pretraining Framework,2021,0.010163061621302505,2
W3133101440,PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains.,2021,0.010157987555353753,2
W3017541546,A Simple and Effective Model for Answering Multi-span Questions,2019,0.010157547078216419,2
W3171010408,A Proposed Chatbot Framework for COVID-19,2021,0.010150977108724717,2
W3175234986,Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data,2021,0.010149482978083832,2
W3103436911,Towards Interpretable Natural Language Understanding with Explanations as Latent Variables,2020,0.01014878635924382,2
W4385574176,DuQM: A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models,2022,0.010148180285187465,2
W3194983542,Validation on machine reading comprehension software without annotated labels: a property-based method,2021,0.010143871809937886,2
W4224950137,MCSE: Multimodal Contrastive Learning of Sentence Embeddings,2022,0.010140004823683373,2
W4287271794,Cooperative Self-training of Machine Reading Comprehension,2022,0.010138585205123568,2
W4385572697,When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain,2022,0.010138461349186787,2
W3177382889,Lawformer: A pre-trained language model for Chinese legal long documents,2021,0.010129225958512554,2
W3155001903,Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion,2021,0.010128713429657895,2
W4406863965,KRongBERT: Enhanced factorization-based morphological approach for the Korean pretrained language model,2025,0.010127295409808461,2
W2970501962,Machine Reading Comprehension Using Structural Knowledge Graph-aware Network,2019,0.010126280007970001,2
W3103934057,T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack,2020,0.010124876183785743,2
W4386794616,DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective,2023,0.010122830707551591,2
W3111425437,Distilling Knowledge from Reader to Retriever for Question Answering,2020,0.010119304293292587,2
W3127787589,Extremely Small BERT Models from Mixed-Vocabulary Training,2019,0.01011682377677775,2
W3212191244,Measuring Association Between Labels and Free-Text Rationales,2021,0.010115414073346315,2
W3206816211,UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning,2022,0.010112503180612643,2
W3199359833,Efficient Domain Adaptation of Language Models via Adaptive Tokenization,2021,0.010111208135276325,2
W4392901911,TOCOL: improving contextual representation of pre-trained language models via token-level contrastive learning,2024,0.010110548626732073,2
W4385574183,Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence,2022,0.010105724472338519,2
W3174234060,Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning,2021,0.010105713289108515,2
W4405695223,A comprehensive survey of large language models and multimodal large language models in medicine,2024,0.010099191192707378,2
W2998231212,Infusing Knowledge into the Textual Entailment Task Using Graph Convolutional Networks,2020,0.010095073806084511,2
W3214523360,CATE: A Contrastive Pre-trained Model for Metaphor Detection with Semi-supervised Learning,2021,0.01008981368228865,2
W4224903949,Imagination-Augmented Natural Language Understanding,2022,0.010088279902760684,2
W3172871932,Multi-Grained Knowledge Distillation for Named Entity Recognition,2021,0.01008751520688758,2
W4393853326,Generative AI-Based Text Generation Methods Using Pre-Trained GPT 2 Model,2024,0.01008650912228996,2
W2997924070,ORB: An Open Reading Benchmark for Comprehensive Evaluation of Machine Reading Comprehension.,2019,0.010086281835031328,2
W3041304706,ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing.,2021,0.010080642556284948,2
W4389520407,Explicit Planning Helps Language Models in Logical Reasoning,2023,0.010077106119232947,2
W3172045361,Fool Me Twice: Entailment from Wikipedia Gamification,2021,0.01007670561954034,2
W3156000544,Sequence tagging for biomedical extractive question answering,2022,0.010074254682241835,2
W4385574074,Debiasing Masks: A New Framework for Shortcut Mitigation in NLU,2022,0.010069080895412077,2
W3029927342,Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task,2020,0.010068178589450505,2
W4385571334,SemEval-2023 Task 5: Clickbait Spoiling,2023,0.010067409361336878,2
W3196813608,All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality,2021,0.010066322930181937,2
W3191330753,Distinct but correct: generating diversified and entity-revised medical response,2024,0.010064940720531152,2
W3088652013,Embedding-based Zero-shot Retrieval through Query Generation,2020,0.010060203998391819,2
W3170925726,On the Transformer Growth for Progressive BERT Training,2021,0.010042815972415776,2
W4390754775,Input-oriented demonstration learning for hybrid evidence fact verification,2024,0.010042358881279155,2
W3166720688,UDALM: Unsupervised Domain Adaptation through Language Modeling,2021,0.01004019336106137,2
W2970609357,Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension,2019,0.010037710153522365,2
W4380086855,Generating Better Items for Cognitive Assessments Using Large Language Models,2023,0.010035928973467487,2
W4287854995,Aligning Generative Language Models with Human Values,2022,0.010031486563753281,2
W4385570670,Clustering-Aware Negative Sampling for Unsupervised Sentence Representation,2023,0.010030430968823502,2
W2972381305,A Simple but Effective Method to Incorporate Multi-turn Context with BERT for Conversational Machine Comprehension,2019,0.010030024402057868,2
W2980418356,Answering Complex Open-domain Questions Through Iterative Query Generation,2019,0.010028086156779527,2
W4385569785,Pre-Training to Learn in Context,2023,0.010027358976475706,2
W3117660267,Picking BERT’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis,2020,0.010027256803277628,2
W3170962005,Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models,2021,0.010027248863211163,2
W2888120268,Lessons from Natural Language Inference in the Clinical Domain,2018,0.010020796495839061,2
W3169737649,Clusformer: A Transformer based Clustering Approach to Unsupervised Large-scale Face and Visual Landmark Recognition,2021,0.010016207724917504,2
W4396873766,A head-to-head attention with prompt text augmentation for text classification,2024,0.010014548996475562,2
W3116510459,SG-Net: Syntax Guided Transformer for Language Representation,2020,0.010014096308747023,2
W3203869270,Narrative Question Answering with Cutting-Edge Open-Domain QA Techniques: A Comprehensive Study,2021,0.010010142956242972,2
W4229031767,Unified Semantic Typing with Meaningful Label Inference,2022,0.010005775659207544,2
W3213014097,BabyBERTa: Learning More Grammar With Small-Scale Child-Directed Language,2021,0.01000299146789877,2
W3117433489,Generating Natural Language Attacks in a Hard Label Black Box Setting,2021,0.009997301474014912,2
W3176609328,AUBER: Automated BERT regularization,2021,0.00999592333412762,2
W4380729715,PharmBERT: a domain-specific BERT model for drug labels,2023,0.009995465559180046,2
W2937036051,Document Expansion by Query Prediction,2019,0.009990784968815971,2
W3166298099,Incorporating External Knowledge to Enhance Tabular Reasoning,2021,0.009988265007281503,2
W4385574039,"The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative",2022,0.00998439788857729,2
W4382202569,Feature-Level Debiased Natural Language Understanding,2023,0.009981903673039737,2
W4281806276,THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption,2022,0.0099757279136571,2
W3173937547,Modeling Event-Pair Relations in External Knowledge Graphs for Script Reasoning,2021,0.009973933837167222,2
W3101033885,Adversarial Semantic Collisions,2020,0.00996283764804102,2
W4385573998,MICO: A Multi-alternative Contrastive Learning Framework for Commonsense Knowledge Representation,2022,0.009959117853678808,2
W3037620288,BERTology Meets Biology: Interpreting Attention in Protein Language Models,2020,0.009957926257189097,2
W3142036593,Transformers aftermath,2021,0.009957136375300208,2
W3023124213,SegaBERT: Pre-training of Segment-aware BERT for Language Understanding,2020,0.009956582104059464,2
W3023553115,It’s Morphin’ Time! Combating Linguistic Discrimination with Inflectional Perturbations,2020,0.00995414902614772,2
W3168921656,MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding,2021,0.009949121197857822,2
W3153414861,TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning,2021,0.009947921193291683,2
W3099658661,An information theoretic view on selecting linguistic probes,2020,0.009938356215002038,2
W4229022551,SemAttack: Natural Textual Attacks via Different Semantic Spaces,2022,0.009937297981182033,2
W4312220150,A large language model for electronic health records,2022,0.009934657970810742,2
W4386566482,"RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media",2023,0.009933292392690314,2
W2516930406,Machine Comprehension Using Match-LSTM and Answer Pointer,2016,0.009931769119549405,2
W4385573900,monoQA: Multi-Task Learning of Reranking and Answer Extraction for Open-Retrieval Conversational Question Answering,2022,0.009919587583035074,2
W4385570980,CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,2023,0.009913527572559576,2
W4287854450,Reframing Human-AI Collaboration for Generating Free-Text Explanations,2022,0.009912760818191802,2
W3174413662,Improving BERT with Syntax-aware Local Attention,2021,0.00990972979439919,2
W2563574619,Does String-Based Neural MT Learn Source Syntax?,2016,0.009908237718515005,2
W4302343710,Learned in Translation: Contextualized Word Vectors,2017,0.009904481789374421,2
W4385573966,TweetNLP: Cutting-Edge Natural Language Processing for Social Media,2022,0.009902316523694886,2
W4386566605,AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models,2023,0.009901931703499449,2
W3199712787,Semantic Networks for Engineering Design: State of the Art and Future Directions,2021,0.00989955665535185,2
W3093844431,RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering,2020,0.009894909838349623,2
W3209619108,PathReasoner: Explainable reasoning paths for commonsense question answering,2021,0.00989105075416077,2
W3132736064,Calibrate Before Use: Improving Few-Shot Performance of Language Models,2021,0.009887616969104182,2
W3171450754,Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation,2021,0.009885889843443308,2
W2948036864,Explain Yourself! Leveraging Language Models for Commonsense Reasoning,2019,0.009882281920615466,2
W4385573936,Measurement Extraction with Natural Language Processing: A Review,2022,0.009881927738577963,2
W3097861677,Quantifying the Contextualization of Word Representations with Semantic Class Probing,2020,0.009880786110012432,2
W3142869857,Answer Sentence Selection Using Local and Global Context in Transformer Models,2021,0.009879414573853355,2
W4205857304,Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right,2021,0.009874609585066993,2
W4389520670,Enabling Large Language Models to Generate Text with Citations,2023,0.00986819949536378,2
W3211152275,AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain,2021,0.009866668246240128,2
W2552027021,Dynamic Coattention Networks For Question Answering,2016,0.009866030004767552,2
W4386365737,LMGFuse: Language Models and Graph reasoning Fuse deeply for question answering,2023,0.009861795323489634,2
W4400869738,A Survey of LLM Datasets: From Autoregressive Model to AI Chatbot,2024,0.009852832620191515,2
W2984673553,Higher-order Comparisons of Sentence Encoder Representations,2019,0.009852032435934875,2
W3213844504,Neural Media Bias Detection Using Distant Supervision With BABE - Bias Annotations By Experts,2021,0.009850835721322058,2
W2921890305,Neural language models as psycholinguistic subjects: Representations of syntactic state,2019,0.00985051495626807,2
W3165244476,Unsupervised multi-sense language models for natural language processing tasks,2021,0.009848657713771322,2
W3213921583,Incorporating medical knowledge in BERT for clinical relation extraction,2021,0.009845738572068205,2
W4286988498,So Cloze Yet So Far: N400 Amplitude Is Better Predicted by Distributional Information Than Human Predictability Judgements,2022,0.009842885746732203,2
W3105192994,Social Commonsense Reasoning with Multi-Head Knowledge Attention,2020,0.009842327856639384,2
W4385572766,Sentence Representation Learning with Generative Objective rather than Contrastive Objective,2022,0.009842304881650889,2
W3175160511,SKR-QA: Semantic ranking and knowledge revise for multi-choice question answering,2021,0.009839621665256166,2
W3100714086,MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models,2020,0.009830842343002235,2
W4226163593,WikiContradiction: Detecting Self-Contradiction Articles on Wikipedia,2021,0.009830437911679011,2
W3173787059,Self-Attention Attribution: Interpreting Information Interactions Inside Transformer,2021,0.009829917220858263,2
W4312687935,Knowledge Distillation of Russian Language Models with Reduction of Vocabulary,2022,0.009829836930455525,2
W4285151662,Open Domain Question Answering with A Unified Knowledge Interface,2022,0.009816275867645654,2
W2964150944,The Effect of Different Writing Tasks on Linguistic Style: A Case Study of the ROC Story Cloze Task,2017,0.00981393043857455,2
W3173813266,Explaining NLP Models via Minimal Contrastive Editing (MiCE),2021,0.009811479673208324,2
W4285294837,Emergent Structures and Training Dynamics in Large Language Models,2022,0.00980499916606818,2
W3045683288,Language Models as Fact Checkers?,2020,0.009801051444694703,2
W3171324702,Assertion Detection in Clinical Notes: Medical Language Models to the Rescue?,2021,0.009798402373983654,2
W3034842695,What Question Answering can Learn from Trivia Nerds,2020,0.009797596063974037,2
W3166593409,Improving Biomedical Pretrained Language Models with Knowledge,2021,0.009795764522259205,2
W3176750236,KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning,2021,0.009792569412993392,2
W4367281801,A Multi-Level Supervised Contrastive Learning Framework for Low-Resource Natural Language Inference,2023,0.00978840773204948,2
W3175175610,Alignment Rationale for Natural Language Inference,2021,0.009788265501146585,2
W4385570095,What’s the Meaning of Superhuman Performance in Today’s NLU?,2023,0.009784370933444805,2
W4389520743,DSI++: Updating Transformer Memory with New Documents,2023,0.009778121851950962,2
W3198192082,Debiasing Methods in Natural Language Understanding Make Bias More Accessible,2021,0.009774569493576314,2
W3099590177,doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset,2020,0.009773655860608216,2
W4226095990,Things not Written in Text: Exploring Spatial Commonsense from Visual Signals,2022,0.009773346784629986,2
W4385569782,Language model acceptability judgements are not always robust to context,2023,0.00977194547161439,2
W2952826391,Zero-Shot Entity Linking by Reading Entity Descriptions,2019,0.00976936155005263,2
W4386566764,Augmenting Pre-trained Language Models with QA-Memory for Open-Domain Question Answering,2023,0.009764733768854516,2
W3174318939,Joint Verification and Reranking for Open Fact Checking Over Tables,2021,0.009764458858188987,2
W2963096510,Hierarchical Neural Story Generation,2018,0.009758883151990493,2
W4386566577,Context Generation Improves Open Domain Question Answering,2023,0.00975796768125584,2
W4285603001,PPT: Backdoor Attacks on Pre-trained Models via Poisoned Prompt Tuning,2022,0.009753478289345146,2
W2963015836,The LAMBADA dataset: Word prediction requiring a broad discourse context,2016,0.009749885483630113,2
W3153055969,B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval,2021,0.009749248027193099,2
W3200368352,Connecting Attributions and QA Model Behavior on Realistic Counterfactuals,2021,0.009741563329939331,2
W4391019401,Short text classification with Soft Knowledgeable Prompt-tuning,2024,0.00973937586662,2
W3174099131,Learning to Generate Questions by Learning to Recover Answer-containing Sentences,2021,0.009736115267390273,2
W3212246833,T3-Vis: visual analytic for Training and fine-Tuning Transformers in NLP,2021,0.009731838037057808,2
W4385573517,Collateral facilitation in humans and language models,2022,0.009729622937569914,2
W4385571813,Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models,2023,0.009726923231519535,2
W2981828710,Thieves on Sesame Street! Model Extraction of BERT-based APIs,2019,0.009725868831789847,2
W2891304738,Interpretation of Natural Language Rules in Conversational Machine Reading,2018,0.00972556096659312,2
W2963898730,Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,2018,0.009724842620429108,2
W4385570112,Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model,2023,0.009724506353208581,2
W3207715300,Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora,2022,0.00972363912294871,2
W3103702973,Constrained Fact Verification for FEVER,2020,0.009718454060031998,2
W3034876042,Harvesting and Refining Question-Answer Pairs for Unsupervised QA,2020,0.009717542107448164,2
W3007306981,Training Question Answering Models From Synthetic Data,2020,0.009716719704507824,2
W4225294552,Comparison of Transfer Learning and Traditional Machine Learning Approach for Text Classification,2022,0.009710055943307599,2
W4327737695,A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning,2023,0.009707248764457268,2
W3035317050,Generating Fact Checking Explanations,2020,0.009698495414206202,2
W4226208236,Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents,2022,0.009695841139610787,2
W4281706244,Ad astra or astray: Exploring linguistic knowledge of multilingual BERT through NLI task,2022,0.009695241027515436,2
W3017701505,Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias,2020,0.009694199387308668,2
W3100949878,What Are You Trying to Do? Semantic Typing of Event Processes,2020,0.009694146465582006,2
W4226059645,TopiOCQA: Open-domain Conversational Question Answering with Topic Switching,2022,0.009691595957686865,2
W3117339789,Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models,2020,0.00969123201286665,2
W4312314464,Compositional Evaluation on Japanese Textual Entailment and Similarity,2022,0.009687699887392886,2
W3203380178,Probing Language Models for Understanding of Temporal Expressions,2021,0.009673272974346517,2
W4394725593,Adaptive Gradient-based Word Saliency for adversarial text attacks,2024,0.009671122271764657,2
W2973061659,Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT,2019,0.009670420151304135,2
W4385573597,Textual Manifold-based Defense Against Natural Language Adversarial Examples,2022,0.009664301892832424,2
W4287854750,AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models,2022,0.00966326998220793,2
W4385570304,Causality-aware Concept Extraction based on Knowledge-guided Prompting,2023,0.009660986389165762,2
W3201266133,Improving Unsupervised Question Answering via Summarization-Informed Question Generation,2021,0.009653884003760963,2
W3175930218,"On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation",2021,0.009653128539090469,2
W4389520749,SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models,2023,0.009651566129930704,2
W2949276121,Augmenting Neural Networks with First-order Logic,2019,0.009651427396003233,2
W3116459227,Few-Shot Text Generation with Pattern-Exploiting Training,2020,0.009651286037871179,2
W4285164445,Robust Lottery Tickets for Pre-trained Language Models,2022,0.009647697268651525,2
W3108672584,Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines,2020,0.009646753379482419,2
W4385572148,Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction,2023,0.009645406038434754,2
W2952750383,DisSent: Learning Sentence Representations from Explicit Discourse Relations,2019,0.009643971741071922,2
W4385570481,"RARR: Researching and Revising What Language Models Say, Using Language Models",2023,0.00962897403042633,2
W2991382858,How Can We Know What Language Models Know,2019,0.009625838303342318,2
W4399848758,Defsent+: Improving Sentence Embeddings of Language Models by Projecting Definition Sentences into a Quasi-Isotropic or Isotropic Vector Space of Unlimited Dictionary Entries,2024,0.009619385265187153,2
W4385573071,Balanced Adversarial Training: Balancing Tradeoffs between Fickleness and Obstinacy in NLP Models,2022,0.009618251075173273,2
W4393159548,Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model,2024,0.009614195481426634,2
W4287111051,Time-Aware Language Models as Temporal Knowledge Bases,2022,0.009614075374873762,2
W3186858063,Survey of BERT (Bidirectional Encoder Representation Transformer) types,2021,0.009611532701279511,2
W2970688856,Incorporating Domain Knowledge into Medical NLI using Knowledge Graphs,2019,0.009605905760957163,2
W3087623576,BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition,2020,0.009604827845640072,2
W2963120843,Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment,2018,0.009604065406398708,2
W3010293452,Data Augmentation using Pre-trained Transformer Models,2020,0.00960013893714275,2
W3172794097,Robustness Gym: Unifying the NLP Evaluation Landscape,2021,0.009594616920932778,2
W3162542754,Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning,2021,0.009592441823241421,2
W4323896847,Prompt Consistency for Multi-Label Textual Emotion Detection,2023,0.009589682043881345,2
W2997868155,Attending to Entities for Better Text Understanding,2020,0.009589123841208193,2
W3208397797,Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning,2021,0.009579079268789674,2
W3175534941,Unsupervised Out-of-Domain Detection via Pre-trained Transformers,2021,0.00957441665922207,2
W2985964562,Bend but Don’t Break? Multi-Challenge Stress Test for QA Models,2019,0.00957156172589192,2
W3105817677,Efficient Document Re-Ranking for Transformers by Precomputing Term Representations,2020,0.009569590458949768,2
W3176776997,Learning Event Graph Knowledge for Abductive Reasoning,2021,0.009569365546169831,2
W3174870841,Zero-shot Event Extraction via Transfer Learning: Challenges and Insights,2021,0.009568062580127757,2
W3179867916,Answering Any-hop Open-domain Questions with Iterative Document Reranking,2021,0.009568053219739301,2
W2952603081,BAM! Born-Again Multi-Task Networks for Natural Language Understanding,2019,0.00955984708677668,2
W3021282678,"Sparse, Dense, and Attentional Representations for Text Retrieval",2020,0.00955449890135153,2
W3156830884,COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List,2021,0.009544535524895112,2
W4385571791,Analyzing Transformers in Embedding Space,2023,0.00954401365973017,2
W3118027562,A Memory Efficient Baseline for Open Domain Question Answering,2020,0.009540931413773936,2
W4385572887,Prompt-Based Meta-Learning For Few-shot Text Classification,2022,0.009538858192297028,2
W4400259231,Unlocking the Potential: A Comprehensive Systematic Review of ChatGPT in Natural Language Processing Tasks,2024,0.00953322704716748,2
W4385570698,RMLM: A Flexible Defense Framework for Proactively Mitigating Word-level Adversarial Attacks,2023,0.009528692480726271,2
W2947469743,On the Robustness of Self-Attentive Models,2019,0.00952599647242677,2
W3049254243,A Survey of Active Learning for Text Classification using Deep Neural Networks,2020,0.00952579254634193,2
W4385572482,Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers,2023,0.009525476459986786,2
W4388594219,Retrieval for Extremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-Ranker,2023,0.009525011446646452,2
W2963595025,Text Understanding with the Attention Sum Reader Network,2016,0.009524759202186484,2
W3167002899,Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models,2021,0.009524052227252777,2
W3087273879,Generation-Augmented Retrieval for Open-domain Question Answering,2020,0.009521913291338889,2
W3190936700,Asynchronous Multi-grained Graph Network For Interpretable Multi-hop Reading Comprehension,2021,0.009521661766748162,2
W4385570658,Exploring the Effectiveness of Prompt Engineering for Legal Reasoning Tasks,2023,0.009518951527756477,2
W4386576819,Interventional Probing in High Dimensions: An NLI Case Study,2023,0.00951441732472754,2
W3213049935,Semi-supervised Intent Discovery with Contrastive Learning,2021,0.009506374226827506,2
W2950501607,RACE: Large-scale ReAding Comprehension Dataset From Examinations,2017,0.00950210604525574,2
W4384697713,"Lexical-Semantic Content, Not Syntactic Structure, Is the Main Contributor to ANN-Brain Similarity of fMRI Responses in the Language Network",2023,0.009499958639213227,2
W2951025380,Attention is not Explanation,2019,0.009499869570266217,2
W3083410900,Measuring Massive Multitask Language Understanding,2020,0.009496962615934149,2
W4226263264,VALUE: Understanding Dialect Disparity in NLU,2022,0.009493385776662122,2
W4388288227,Comparing neural sentence encoders for topic segmentation across domains: not your typical text similarity task,2023,0.009480195021104374,2
W4295857769,Pre-Trained Language Models and Their Applications,2022,0.0094794679059141,2
W4389519126,OssCSE: Overcoming Surface Structure Bias in Contrastive Learning for Unsupervised Sentence Embedding,2023,0.009474482539300243,2
W4205508242,FinQA: A Dataset of Numerical Reasoning over Financial Data,2021,0.009470230873675869,2
W4385574011,Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer,2022,0.009468574190228334,2
W3103671331,DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,2020,0.00946621123608852,2
W3196437953,LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER,2021,0.009465470375687686,2
W3105949871,Modularized Transfomer-based Ranking Framework,2020,0.009465424944121384,2
W3089988449,Paragraph-level Commonsense Transformers with Recurrent Memory,2021,0.009464233170010631,2
W4287888135,SKILL: Structured Knowledge Infusion for Large Language Models,2022,0.009462291155439805,2
W2974273066,Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings,2019,0.009461059044044297,2
W4390590855,A Survey of Text Classification With Transformers: How Wide? How Large? How Long? How Accurate? How Expensive? How Safe?,2024,0.00946063365611739,2
W4229673855,Does Vision-and-Language Pretraining Improve Lexical Grounding?,2021,0.009460469160714368,2
W4385573127,NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge,2022,0.009458816182706711,2
W4285243012,Factual Consistency of Multilingual Pretrained Language Models,2022,0.0094497136584712,2
W2986495993,Identifying Supporting Facts for Multi-hop Question Answering with Document Graph Networks,2019,0.009446003295545707,2
W3107969673,Modifying Memories in Transformer Models,2020,0.00944571480625562,2
W3105601216,Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA,2020,0.009445168083982266,2
W3105295588,Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,2020,0.009442530277520646,2
W4387575740,Meaning Modulations and Stability in Large Language Models: An Analysis of BERT Embeddings for Psycholinguistic Research,2023,0.00944202704269803,2
W4287854777,Meta Learning for Natural Language Processing: A Survey,2022,0.009440862901671555,2
W4280583915,e-CARE: a New Dataset for Exploring Explainable Causal Reasoning,2022,0.00944047872439541,2
W3117281880,A Graph Representation of Semi-structured Data for Web Question Answering,2020,0.009433598587681336,2
W3173566921,Prompting Contrastive Explanations for Commonsense Reasoning Tasks,2021,0.009432704214228423,2
W4385573361,PACIFIC: Towards Proactive Conversational Question Answering over Tabular and Textual Data in Finance,2022,0.009430843787611873,2
W2971871542,NEZHA: Neural Contextualized Representation for Chinese Language Understanding,2019,0.009426791379262405,2
W4385572711,Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination,2022,0.009420384657593907,2
W4398152371,"Comparing Fine-Tuning, Zero and Few-Shot Strategies with Large Language Models in Hate Speech Detection in English",2024,0.00941880565102364,2
W3099342932,WNUT-2020 Task 2: Identification of Informative COVID-19 English Tweets,2020,0.009417362423220632,2
W3189383166,Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models,2021,0.00941692132588717,2
W2951568144,Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets,2019,0.00941570115030178,2
W3199807247,Numerical reasoning in machine reading comprehension tasks: are we there yet?,2021,0.009406701616643148,2
W2963259903,WinoGrande: An Adversarial Winograd Schema Challenge at Scale,2019,0.009404828933487654,2
W4399107405,On knowing a gene: A distributional hypothesis of gene function,2024,0.009403074784779744,2
W3198480521,Table-based Fact Verification With Salience-aware Learning,2021,0.009398631042688666,2
W4384932493,Lexicon-enhanced Pre-trained Language Models for Chinese Ethics-related Tasks,2023,0.00939844792282977,2
W4384930037,Lexicon-enhanced Pre-trained Language Models for Chinese Ethics-related Tasks,2023,0.00939844792282977,2
W4287646439,Pareto Probing: Trading Off Accuracy for Complexity,2020,0.00939524307905338,2
W4385567371,Open-domain Question Answering via Chain of Reasoning over Heterogeneous Knowledge,2022,0.009394914689547378,2
W3158303960,TABBIE: Pretrained Representations of Tabular Data,2021,0.00938852985036243,2
W3201174429,TruthfulQA: Measuring How Models Mimic Human Falsehoods,2022,0.009388492199390436,2
W3092806700,CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding,2020,0.009387053634677773,2
W4385569796,Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications,2023,0.009385971240934567,2
W3111412136,Text mining approaches for dealing with the rapidly expanding literature on COVID-19,2020,0.009385058625052617,2
W4389518776,A Cheaper and Better Diffusion Language Model with Soft-Masked Noise,2023,0.00938313638404504,2
W3197746952,Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories,2021,0.009381087900509512,2
W4385569949,Multilingual Conceptual Coverage in Text-to-Image Models,2023,0.009374708045932744,2
W2984208213,Fine-Grained Propaganda Detection with Fine-Tuned BERT,2019,0.009372813723849913,2
W4385572714,"Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation",2022,0.009372214892195783,2
W3170967211,Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks,2021,0.009370143832216137,2
W4389518895,Self-Knowledge Guided Retrieval Augmentation for Large Language Models,2023,0.009366594393940225,2
W3199225381,Explainable Identification of Dementia From Transcripts Using Transformer Networks,2022,0.009360513874877926,2
W4309267672,Deep lexical hypothesis: Identifying personality structure in natural language.,2022,0.009360223128041413,2
W4366779715,Using Bidirectional Encoder Representations from Transformers (BERT) to classify traffic crash severity types,2023,0.00935972048449466,2
W3037234229,"Evidence Inference 2.0: More Data, Better Models",2020,0.00935771768704348,2
W4285300583,Modular and Parameter-Efficient Multimodal Fusion with Prompting,2022,0.009357209451435347,2
W4387951242,Tale of Two Cs: Computation vs. Communication Scaling for Future Transformers on Future Hardware,2023,0.009353932408136402,2
W4293248701,The Role of Complex NLP in Transformers for Text Ranking,2022,0.009353722893124573,2
W3163313089,The Low-Dimensional Linear Geometry of Contextualized Word Representations,2021,0.00935347517998962,2
W4385572953,UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models,2022,0.009330957194871153,2
W4385574272,IDK-MRC: Unanswerable Questions for Indonesian Machine Reading Comprehension,2022,0.009330549178073811,2
W4206727248,Knowledge-Guided Paraphrase Identification,2021,0.009327176172264758,2
W3104982372,A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support,2020,0.009326703191122306,2
W3175802991,Dialogue Graph Modeling for Conversational Machine Reading,2021,0.009324494573560648,2
W4385488503,HSimCSE: Improving Contrastive Learning of Unsupervised Sentence Representation with Adversarial Hard Positives and Dual Hard Negatives,2023,0.009322546378943867,2
W3157746834,GraphFormers: GNN-nested Language Models for Linked Text Representation.,2021,0.0093223442216839,2
W4382202633,Converge to the Truth: Factual Error Correction via Iterative Constrained Editing,2023,0.009320035567115038,2
W4385573075,You Only Need One Model for Open-domain Question Answering,2022,0.009313799857761535,2
W4285200900,Predicate-Argument Based Bi-Encoder for Paraphrase Identification,2022,0.009313612790748594,2
W2949128310,Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency,2019,0.009312413941940758,2
W4389524573,Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers,2023,0.009312368837636661,2
W3173556213,Style is NOT a single variable: Case Studies for Cross-Stylistic Language Understanding,2021,0.009299639131895339,2
W3015297483,DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement,2020,0.009296529267075646,2
W4372342402,From Easy to Hard: Two-Stage Selector and Reader for Multi-Hop Question Answering,2023,0.009295961437310978,2
W2950308662,Mining Discourse Markers for Unsupervised Sentence Representation Learning,2019,0.009295789210981741,2
W3213325056,Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation,2021,0.009292875562118528,2
W3183820005,BERT Goes Shopping: Comparing Distributional Models for Product Representations,2021,0.009290713053206612,2
W4386566613,Fine-Tuning Deteriorates General Textual Out-of-Distribution Detection by Distorting Task-Agnostic Features,2023,0.009288595781882744,2
W3035407756,Contextual Embeddings: When Are They Worth It?,2020,0.009288477793489623,2
W4385571531,How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases,2023,0.009288074318702535,2
W3144600905,ASER: Towards large-scale commonsense knowledge acquisition via higher-order selectional preference over eventualities,2022,0.00928014418508848,2
W3026992186,Explicit Contextual Semantics for Text Comprehension,2018,0.009279616356241239,2
W4285251104,How does the pre-training objective affect what large language models learn about linguistic properties?,2022,0.009277872759429998,2
W3088599783,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,2020,0.009277364540160692,2
W3179219974,FaVIQ: FAct Verification from Information-seeking Questions,2022,0.009271055921831979,2
W4221148719,FaiRR: Faithful and Robust Deductive Reasoning over Natural Language,2022,0.009271028546922511,2
W3034912286,Probing Neural Language Models for Human Tacit Assumptions,2020,0.009269379214231321,2
W4388645436,pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks,2023,0.0092670564146492,2
W3104591237,Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,2020,0.009261993493292157,2
W2995856824,Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction,2020,0.00925725151861411,2
W3126553126,Pitfalls of Static Language Modelling.,2021,0.009255956933646468,2
W4372262731,Pyramid Dynamic Inference: Encouraging Faster Inference Via Early Exit Boosting,2023,0.009254611760446138,2
W3105886116,HoVer: A Dataset for Many-Hop Fact Extraction And Claim Verification,2020,0.00925451341653031,2
W4410201740,A novel data extraction framework using Natural Language Processing (DEFNLP) techniques,2025,0.009237757312916654,2
W4385572441,Text Augmented Open Knowledge Graph Completion via Pre-Trained Language Models,2023,0.009237698045464218,2
W4285278721,Structural Characterization for Dialogue Disentanglement,2022,0.009234785077470805,2
W4229005744,Learning to Transfer Prompts for Text Generation,2022,0.009233061701852677,2
W3216626696,A Survey on Event Extraction for Natural Language Understanding: Riding the Biomedical Literature Wave,2021,0.009232814898918612,2
W3008424731,KEML: A Knowledge-Enriched Meta-Learning Framework for Lexical Relation Classification,2021,0.009228875671816447,2
W4385571920,FactKG: Fact Verification via Reasoning on Knowledge Graphs,2023,0.009225922130223845,2
W3020987135,Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning,2020,0.00922301398246011,2
W2971209824,Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval,2019,0.00921644207714846,2
W4321770433,EduQG: A Multi-Format Multiple-Choice Dataset for the Educational Domain,2023,0.009209598879859242,2
W4396667547,Enhancing Machine-Generated Text Detection: Adversarial Fine-Tuning of Pre-Trained Language Models,2024,0.009209065023183367,2
W3160648428,Making Punctuation Restoration Robust and Fast with Multi-Task Learning and Knowledge Distillation,2021,0.009209004930175742,2
W3176390686,UnitedQA: A Hybrid Approach for Open Domain Question Answering,2021,0.009203531699236166,2
W4388778142,How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure,2023,0.009202772340124576,2
W4224866872,The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems,2022,0.009202608944452304,2
W2947813521,Defending Against Neural Fake News,2019,0.009194274096130271,2
W3126934640,Comparative Analysis of Different Transformer Based Architectures Used in Sentiment Analysis,2020,0.009180335390308547,2
W4377235148,A Composable Generative Framework Based on Prompt Learning for Various Information Extraction Tasks,2023,0.009174099683330236,2
W3167748596,Open Domain Question Answering over Tables via Dense Retrieval,2021,0.009173737826201577,2
W4313887409,cpgQA: A Benchmark Dataset for Machine Reading Comprehension Tasks on Clinical Practice Guidelines and a Case Study Using Transfer Learning,2023,0.00917159505205675,2
W3175304423,Retrieval Enhanced Model for Commonsense Generation,2021,0.009167330992187053,2
W3167361718,Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study,2021,0.009167170924217417,2
W3173699110,Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering,2021,0.009163038947044977,2
W4283790835,Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs,2022,0.009161733051235202,2
W3104486441,Attention-over-Attention Neural Networks for Reading Comprehension,2017,0.009160049923760143,2
W3172427031,UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus,2021,0.009159297936128045,2
W4382239620,ConTextual Masked Auto-Encoder for Dense Passage Retrieval,2023,0.00915235475346523,2
W3114142294,Molweni: A Challenge Multiparty Dialogues-based Machine Reading Comprehension Dataset with Discourse Structure,2020,0.009152303166183255,2
W3132259035,Limitations of Transformers on Clinical Text Classification,2021,0.009149297692920562,2
W4285254489,Memorisation versus Generalisation in Pre-trained Language Models,2022,0.009147650032967579,2
W3116527904,Investigating Learning Dynamics of BERT Fine-Tuning,2020,0.00914722338867268,2
W4393147154,LLM vs Small Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model,2024,0.009146130097454044,2
W3159622706,ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders,2021,0.009145487828504885,2
W3169993339,Temporal Reasoning on Implicit Events from Distant Supervision,2021,0.009141295695932243,2
W3104992282,Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,2020,0.009141055167044998,2
W4406370386,GeAR: Generation Augmented Retrieval,2025,0.009140190628693416,2
W2984060780,Contextual Text Denoising with Masked Language Model,2019,0.00914006328520981,2
W3205352824,Schrödinger's tree—On syntax and neural language models,2022,0.009138605985335193,2
W4385570296,RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models,2023,0.009138378830646976,2
W3103491646,Robustness to Modification with Shared Words in Paraphrase Identification,2020,0.009136371604498258,2
W4287854917,MultiVerS: Improving scientific claim verification with weak supervision and full-document context,2022,0.009135925195615527,2
W4385570396,Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?,2023,0.00913230108427704,2
W2938205538,Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering,2019,0.009131952738545565,2
W3160137267,Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction,2021,0.00913178855832219,2
W3037888463,BERTology Meets Biology: Interpreting Attention in Protein Language Models,2020,0.009128910357455486,2
W4389518684,LLMaAA: Making Large Language Models as Active Annotators,2023,0.00912875235084811,2
W3167227435,Discourse Probing of Pretrained Language Models,2021,0.009125375650517726,2
W4388921019,A T5-based interpretable reading comprehension model with more accurate evidence training,2023,0.00912354267844848,2
W3173134000,Reconstructing Implicit Knowledge with Language Models,2021,0.009123524686380773,2
W2951855948,Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension,2019,0.009122646698847827,2
W3198593990,NumGPT: Improving Numeracy Ability of Generative Pre-trained Models,2021,0.009121731079495531,2
W4230636291,NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application,2021,0.009121587147400446,2
W4389519914,How to Determine the Most Powerful Pre-trained Language Model without Brute Force Fine-tuning? An Empirical Survey,2023,0.009121371818816658,2
W4368346119,Paraphrase Detection: Human vs. Machine Content,2023,0.009121157214452703,2
W4285163271,Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?,2022,0.009121145871160344,2
W2949803292,RankQA: Neural Question Answering with Answer Re-Ranking,2019,0.009120806144114205,2
W3017024317,A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation,2020,0.009118057575212741,2
W3105302490,Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training,2020,0.009111961020941636,2
W4385574217,A Fine-grained Interpretability Evaluation Benchmark for Neural NLP,2022,0.009107772243180524,2
W4385573862,Language Models that Seek for Knowledge: Modular Search &amp; Generation for Dialogue and Prompt Completion,2022,0.00910570619145938,2
W3171494313,ReadTwice: Reading Very Large Documents with Memories,2021,0.009100783985053366,2
W3195296860,A Statutory Article Retrieval Dataset in French,2022,0.009099738690306265,2
W3097547734,Differentiable Open-Ended Commonsense Reasoning,2020,0.009099541196902986,2
W2913946806,Parameter-Efficient Transfer Learning for NLP,2019,0.009096921633716294,2
W4320024180,Large-Scale Knowledge Synthesis and Complex Information Retrieval from Biomedical Documents,2022,0.009094854752791322,2
W4389070894,Event Knowledge in Large Language Models: The Gap Between the Impossible and the Unlikely,2023,0.009091300953505629,2
W3200253633,Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification,2021,0.009090775924116,2
W3217177838,DiLBERT: Cheap Embeddings for Disease Related Medical NLP,2021,0.009090220991695824,2
W2979300423,Real Life Application of a Question Answering System Using BERT Language Model,2019,0.009088819195969109,2
W3160858468,Assessing BERT’s ability to learn Italian syntax: a study on null-subject and agreement phenomena,2021,0.00908826745407997,2
W3194005300,Chatbot Interaction with Artificial Intelligence: human data augmentation with T5 and language transformer ensemble for text classification,2021,0.00908707535556502,2
W4224321652,"Text Adversarial Attacks and Defenses: Issues, Taxonomy, and Perspectives",2022,0.009082225375350724,2
W3173647480,Making the Relation Matters: Relation of Relation Learning Network for Sentence Semantic Matching,2021,0.009079982664791594,2
W4393407448,Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks,2024,0.009079244580594337,2
W3105868192,An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction,2020,0.009078345004099646,2
W4285297805,RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining,2022,0.009076106085493248,2
W4389520739,QA-NatVer: Question Answering for Natural Logic-based Fact Verification,2023,0.009075782875449924,2
W2995446988,On Identifiability in Transformers,2020,0.009070439504153477,2
W3023690688,WT5?! Training Text-to-Text Models to Explain their Predictions,2020,0.009065652411466774,2
W3013838212,Pre-trained Language Model for Biomedical Question Answering,2020,0.009060291199786312,2
W3184477871,SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS),2021,0.009058492531744391,2
W4389524398,Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models,2023,0.009051728083946366,2
W2806081754,DRCD: a Chinese Machine Reading Comprehension Dataset,2018,0.009046541904255077,2
W4389520378,FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning,2023,0.00904585039343569,2
W3153594491,A Linguistic Study on Relevance Modeling in Information Retrieval,2021,0.009043878606405563,2
W4386576886,Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering,2023,0.00904266797589429,2
W4285254998,Generating Scientific Claims for Zero-Shot Scientific Fact Checking,2022,0.009038048970122075,2
W3119017286,Improving question answering for event-focused questions in temporal collections of news articles,2021,0.009035831552665626,2
W3089314434,Unsupervised Pre-training for Biomedical Question Answering,2020,0.009034244931063301,2
W3163504421,Certified Robustness to Text Adversarial Attacks by Randomized [MASK],2023,0.009032541602726228,2
W4385573636,Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning,2022,0.009031574568027946,2
W4385572965,Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations,2022,0.009030718496042082,2
W4283790681,The King Is Naked: On the Notion of Robustness for Natural Language Processing,2022,0.009029622093054952,2
W3099381148,Attentive History Selection for Conversational Question Answering,2019,0.009029055018855165,2
W4388184878,LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts,2023,0.009023758761838717,2
W3199748991,Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color,2021,0.009020055945096857,2
W4210451781,FeTaQA: Free-form Table Question Answering,2022,0.009019450365099185,2
W3123330721,BERTology Meets Biology: Interpreting Attention in Protein Language Models,2021,0.009017531309644989,2
W4386566474,A Survey on Dynamic Neural Networks for Natural Language Processing,2023,0.009013684263977487,2
W3204993626,Automated quality assessment of cognitive behavioral therapy sessions through highly contextualized language representations,2021,0.009011367522746061,2
W4225707828,Neural reality of argument structure constructions,2022,0.00900967476156799,2
W4410384179,RelBERT: Embedding Relations with Language Models,2025,0.009008463249647262,2
W3115049126,Intermediate Self-supervised Learning for Machine Translation Quality Estimation,2020,0.009001392114126691,2
W4285149549,Adversarial Soft Prompt Tuning for Cross-Domain Sentiment Analysis,2022,0.00900072774214535,2
W4389518671,Query Rewriting in Retrieval-Augmented Large Language Models,2023,0.008998395724224862,2
W3174090807,Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?,2021,0.008996744039040992,2
W3102970018,How does BERT capture semantics? A closer look at polysemous words,2020,0.00899262991400108,2
W4205725534,Types of Out-of-Distribution Texts and How to Detect Them,2021,0.00898849803109548,2
W4205460703,Transformer Feed-Forward Layers Are Key-Value Memories,2021,0.008988147809066744,2
W2761988601,DisSent: Sentence Representation Learning from Explicit Discourse Relations,2017,0.008981527938996795,2
W2983772459,SentiLR: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis.,2019,0.008979377594149544,2
W3199493219,Relevance-guided Supervision for OpenQA with ColBERT,2021,0.008979096845696914,2
W3031914912,Emergent linguistic structure in artificial neural networks trained by self-supervision,2020,0.008974730192611092,2
W3212786531,A Model of Cross-Lingual Knowledge-Grounded Response Generation for Open-Domain Dialogue Systems,2021,0.008969565939043633,2
W3207429447,Large Language Models Can Be Strong Differentially Private Learners,2021,0.008963379784878331,2
W4385573938,Two is Better than Many? Binary Classification as an Effective Approach to Multi-Choice Question Answering,2022,0.00896174265579274,2
W3090260555,How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?.,2020,0.00895554103113197,2
W3196194504,Semantic Answer Similarity for Evaluating Question Answering Models,2021,0.008955110235990835,2
W3156031972,Distributed NLI: Learning to Predict Human Opinion Distributions for Language Reasoning,2022,0.008951114862729505,2
W4401357421,"An Empirical Evaluation of the Zero-Shot, Few-Shot, and Traditional Fine-Tuning Based Pretrained Language Models for Sentiment Analysis in Software Engineering",2024,0.008947713043537217,2
W3064953855,PARADE: Passage Representation Aggregation forDocument Reranking,2023,0.008946416237556952,2
W3185326432,SemEval-2021 Task 11: NLPContributionGraph - Structuring Scholarly NLP Contributions for a Research Knowledge Graph,2021,0.00894232027666125,2
W4380875839,Description-Enhanced Label Embedding Contrastive Learning for Text Classification,2023,0.008941867723817474,2
W4409875692,Detection of Patient Metadata in Published Articles for Genomic Epidemiology Using Machine Learning and Large Language Models,2025,0.00894083322128891,2
W3034654962,Language (Re)modelling: Towards Embodied Language Understanding,2020,0.008933854762195987,2
W4385572602,ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning,2023,0.00892906765047435,2
W2970102799,Improving Neural Story Generation by Targeted Common Sense Grounding,2019,0.00892900674783048,2
W4200629408,JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering,2022,0.008922574921303418,2
W4306873598,Pre-trained Language Model-based Retrieval and Ranking for Web Search,2022,0.008922211184481978,2
W3174848559,Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice,2021,0.008921143977917883,2
W3196841052,Learning with Different Amounts of Annotation: From Zero to Many Labels,2021,0.008920138920831926,2
W4385573677,An Efficient Active Learning Pipeline for Legal Text Classification,2022,0.008919927505020527,2
W2987878148,Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks,2019,0.008914633050471727,2
W2970645034,"PANLP at MEDIQA 2019: Pre-trained Language Models, Transfer Learning and Knowledge Distillation",2019,0.00891158022655979,2
W4384345672,Log Parsing with Prompt-based Few-shot Learning,2023,0.008903057544282705,2
W3168663263,FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information,2021,0.008899554873477654,2
W3034717099,Math-word embedding in math search and semantic extraction,2020,0.008898439754456207,2
W4392932503,A simple and efficient dialogue generation model incorporating commonsense knowledge,2024,0.0088855564965454,2
W4313315719,Deep learning-based question answering: a survey,2022,0.008878929503167033,2
W3155936402,Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples,2021,0.008875345534577963,2
W3173197792,DoT: An efficient Double Transformer for NLP tasks with tables,2021,0.008868020732579312,2
W3035490055,Neural Graph Matching Networks for Chinese Short Text Matching,2020,0.008865003957721837,2
W3183866186,A Collaborative AI-Enabled Pretrained Language Model for AIoT Domain Question Answering,2021,0.008862549584798493,2
W3045620180,Distilling the Evidence to Augment Fact Verification Models,2020,0.008862497802384408,2
W2997521893,What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge,2019,0.008847979353073732,2
W4406924410,Construction Grammar and Language Models,2025,0.008843315260758319,2
W4385571145,LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development,2023,0.00884298574703565,2
W3136888420,Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases,2021,0.008840094686420445,2
W3213468647,A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs,2021,0.008838422762763679,2
W4387171611,Preserving Semantics in Textual Adversarial Attacks,2023,0.008835805458242743,2
W3214576388,SeqAttack: On Adversarial Attacks for Named Entity Recognition,2021,0.008829022843435131,2
W4389520124,Consistency Analysis of ChatGPT,2023,0.008827703541703967,2
W3118846367,HopRetriever: Retrieve Hops over Wikipedia to Answer Complex Questions,2021,0.008825052397664294,2
W3170826848,Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing,2021,0.008819921897464224,2
W2891113091,emrQA: A Large Corpus for Question Answering on Electronic Medical Records,2018,0.008814864775418338,2
W3103536442,Syntactic Structure from Deep Learning,2020,0.008802098059038666,2
W4312408631,An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding,2022,0.008798697188859393,2
W4285288514,Can Pre-trained Language Models Interpret Similes as Smart as Human?,2022,0.008797951040556156,2
W4367165010,On the Robustness of Dialogue History Representation in Conversational Question Answering: A Comprehensive Study and a New Prompt-based Method,2023,0.008797500919057705,2
W2786464815,An efficient framework for learning sentence representations,2018,0.008796528365979666,2
W4377100957,KPT++: Refined knowledgeable prompt tuning for few-shot text classification,2023,0.008793684421138662,2
W3174231090,Semi-Supervised Text Classification with Balanced Deep Representation Distributions,2021,0.008790545126504507,2
W4386566901,Understanding Transformer Memorization Recall Through Idioms,2023,0.008790063464463068,2
W3103873238,More Bang for Your Buck: Natural Perturbation for Robust Question Answering,2020,0.00878909988601334,2
W3200809495,Frequency Effects on Syntactic Rule Learning in Transformers,2021,0.008788748650779457,2
W3113833985,Applying Distilled BERT for Question Answering on ASRS Reports,2020,0.00878648141892915,2
W2962854673,Neural Legal Judgment Prediction in English,2019,0.008778935933096792,2
W3176229980,Database reasoning over text,2021,0.008778265954682181,2
W3094306499,CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters,2020,0.008775272523684624,2
W2970453125,Counterfactual Story Reasoning and Generation,2019,0.00877268122447692,2
W4385573096,Optimizing text representations to capture (dis)similarity between political parties,2022,0.008770531993693443,2
W4389519206,SeqXGPT: Sentence-Level AI-Generated Text Detection,2023,0.008768807777186085,2
W2967659330,Visualizing and Understanding the Effectiveness of BERT,2019,0.008768361074491386,2
W4389524402,A Thorough Examination on Zero-shot Dense Retrieval,2023,0.008765278132756535,2
W2985671014,Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering,2019,0.008761970209265854,2
W4385569780,Evaluating Open-Domain Question Answering in the Era of Large Language Models,2023,0.00876038994957293,2
W4320018401,TaughtNet: Learning Multi-Task Biomedical Named Entity Recognition From Single-Task Teachers,2023,0.008759901795690932,2
W4313182274,Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation,2022,0.00875911021317565,2
W4386212383,Do Pretrained Language Models Indeed Understand Software Engineering Tasks?,2023,0.008757555678362177,2
W4285298468,Interpreting the Robustness of Neural NLP Models to Textual Perturbations,2022,0.00875578402516004,2
W4385574113,Calibrating Factual Knowledge in Pretrained Language Models,2022,0.008755658183499068,2
W3198052459,Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast,2021,0.008751377104744266,2
W4225783786,Counterfactual Representation Augmentation for Cross-Domain Sentiment Analysis,2022,0.008745615163447512,2
W4385572752,Generating Literal and Implied Subquestions to Fact-check Complex Claims,2022,0.008743546334128943,2
W2988245244,A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking,2019,0.00873992701839327,2
W3173277859,Joint Models for Answer Verification in Question Answering Systems,2021,0.00872708558585692,2
W3017344694,BERT for Evidence Retrieval and Claim Verification,2020,0.008724112111117007,2
W3010108619,CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model,2020,0.0087233907562724,2
W3099550034,SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search,2020,0.008719310518010413,2
W4221066116,NewsPod: Automatic and Interactive News Podcasts,2022,0.008717823767975237,2
W3099219382,MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale,2020,0.008714021103234569,2
W4385573607,Transformer Language Models without Positional Encodings Still Learn Positional Information,2022,0.008704135677090753,2
W4310568840,A comparative study of pretrained language models for long clinical text,2022,0.008691947088793232,2
W4385573042,Prompting for Multimodal Hateful Meme Classification,2022,0.008680670312792424,2
W3120014137,Reader-Guided Passage Reranking for Open-Domain Question Answering,2021,0.008680624726287367,2
W3049039618,Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition,2020,0.008679070911420889,2
W4283795504,C2L: Causally Contrastive Learning for Robust Text Classification,2022,0.008678503182489597,2
W3102783139,Noisy Text Data: Achilles’ Heel of BERT,2020,0.008676072141581675,2
W4385569968,Making Language Models Better Reasoners with Step-Aware Verifier,2023,0.008674369032133459,2
W3104213339,Retrofitting Structure-aware Transformer Language Model for End Tasks,2020,0.008674135396783893,2
W4388486466,BioPRO: Context-Infused Prompt Learning for Biomedical Entity Linking,2023,0.008666598348858233,2
W3135190223,CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review,2021,0.008666105931117172,2
W3174173633,ROSITA: Refined BERT cOmpreSsion with InTegrAted techniques,2021,0.008661620955220122,2
W3129155125,FAD-BERT: Improved prediction of FAD binding sites using pre-training of deep bidirectional transformers,2021,0.00865910635950039,2
W3034906811,Uncertain Natural Language Inference,2020,0.008655574090245869,2
W3120793869,CoDA: Contrast-enhanced and Diversity-promoting Data Augmentation for Natural Language Understanding,2021,0.008653701594057595,2
W2995636882,Differentiable Reasoning over a Virtual Knowledge Base,2020,0.008650495970294485,2
W2971350781,Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension,2019,0.00864624335417704,2
W3080739421,Taming Pretrained Transformers for Extreme Multi-label Text Classification,2019,0.008644256544623835,2
W4389523773,C-STS: Conditional Semantic Textual Similarity,2023,0.008641204824268556,2
W4285302754,Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages,2022,0.008639261100426397,2
W3211024016,CLIMATEBERT: A Pretrained Language Model for Climate-Related Text,2022,0.008639191258330008,2
W3211525823,Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers,2021,0.00863346550223359,2
W3035083185,Knowledge-Aided Open-Domain Question Answering,2020,0.008633076413042352,2
W3102812725,Intrinsic Probing through Dimension Selection,2020,0.008630586061337103,2
W4327709862,Importance-aware contrastive learning via semantically augmented instances for unsupervised sentence embeddings,2023,0.008627742464255307,2
W4382202605,Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding,2023,0.008627604407731958,2
W4385734222,Distinguishing Fact from Fiction: A Benchmark Dataset for Identifying Machine-Generated Scientific Papers in the LLM Era.,2023,0.008620456311597857,2
W4385569706,miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings,2023,0.008619717037672787,2
W4285168837,Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model,2022,0.008618547935852849,2
W4385571789,Instruction Induction: From Few Examples to Natural Language Task Descriptions,2023,0.008613984561130605,2
W2951732656,Empirical Linguistic Study of Sentence Embeddings,2019,0.008608716934713878,2
W2982596739,Multi-Stage Document Ranking with BERT,2019,0.00860727412192477,2
W2946379006,Cognitive Graph for Multi-Hop Reading Comprehension at Scale,2019,0.008603127425056718,2
W4386826409,A survey of techniques for optimizing transformer inference,2023,0.008602399026249798,2
W3116152597,Classifier Probes May Just Learn from Linear Context Features,2020,0.008598000142910293,2
W2605717780,What do Neural Machine Translation Models Learn about Morphology?,2017,0.008596248286253378,2
W2965210982,ERNIE 2.0: A Continual Pre-training Framework for Language Understanding,2019,0.008593018015678865,2
W3193084543,How to Query Language Models?,2021,0.008587128406141026,2
W4385570599,BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models,2023,0.0085823665138379,2
W3176393001,Bad Characters: Imperceptible NLP Attacks,2022,0.008581913901071499,2
W3022402349,A Review of Winograd Schema Challenge Datasets and Approaches,2020,0.008580074392792219,2
W2963895422,Improving Natural Language Inference Using External Knowledge in the Science Questions Domain,2019,0.008579596774224495,2
W2984775947,Do Multi-hop Readers Dream of Reasoning Chains?,2019,0.008579581561477787,2
W4385573115,PASTA: Table-Operations Aware Fact Verification via Sentence-Table Cloze Pre-training,2022,0.008575591271219669,2
W3153719108,MultiReQA: A Cross-Domain Evaluation for Retrieval Question Answering Models,2020,0.008574270173541024,2
W4402034408,Attention-Driven Dropout: A Simple Method to Improve Self-supervised Contrastive Sentence Embeddings,2024,0.008573057120019758,2
W2947497897,Latent Retrieval for Weakly Supervised Open Domain Question Answering,2019,0.008571672559081823,2
W4372348172,Coarse-To-Fine Knowledge Selection for Document Grounded Dialogs,2023,0.00856812012837599,2
W2969624041,Revealing the Dark Secrets of BERT,2019,0.008560125850788055,2
W3199938859,Transforming Fake News: Robust Generalisable News Classification Using Transformers,2021,0.00855755937847584,2
W2970912185,WikiCREM: A Large Unsupervised Corpus for Coreference Resolution,2019,0.008556120650982511,2
W4406262262,Enhancing semantical text understanding with fine-tuned large language models: A case study on Quora Question Pair duplicate identification,2025,0.008556039792386491,2
W3170137791,Posthoc Verification and the Fallibility of the Ground Truth,2022,0.008554013203958221,2
W4287888693,BLINK with Elasticsearch for Efficient Entity Linking in Business Conversations,2022,0.008544039233828524,2
W3155396033,"MultiModalQA: Complex Question Answering over Text, Tables and Images",2021,0.008539245864141613,2
W3038445625,Representation Learning for Natural Language Processing,2023,0.008530277553860677,2
W4388802931,AI direct tests: LNE and NIST evaluations,2023,0.008528760815319019,2
W4200237457,Novelty Detection: A Perspective from Natural Language Processing,2021,0.00852861690911516,2
W3167262725,Pre-trained Language Model for Web-scale Retrieval in Baidu Search,2021,0.00852603272067045,2
W3174785103,Combining Feature and Instance Attribution to Detect Artifacts,2022,0.00852592877022003,2
W3090196146,Self-training Improves Pre-training for Natural Language Understanding,2020,0.008523752743673719,2
W4223543859,An Extensive Study on Pretrained Models for Natural Language Processing Based on Transformers,2022,0.008516222459633913,2
W3214020110,NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework,2021,0.008508990131078132,2
W3176175390,What if This Modified That? Syntactic Interventions with Counterfactual Embeddings,2021,0.008508831898054759,2
W4385573010,An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks,2022,0.008506655033745274,2
W3100879603,Do Language Embeddings capture Scales?,2020,0.008501397701161673,2
W3197004538,Multi-level retrieval with semantic Axiomatic Fuzzy Set clustering for question answering,2021,0.008499422216886125,2
W4389520075,From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification,2023,0.008497900210252813,2
W4385574202,SciFact-Open: Towards open-domain scientific claim verification,2022,0.00849585144241576,2
W4385571490,Are Synonym Substitution Attacks Really Synonym Substitution Attacks?,2023,0.008491290392986073,2
W3088577908,Analyzing ELMo and DistilBERT on Socio-political News Classification,2020,0.008489542341915337,2
W4205952419,Compression of Deep Learning Models for Text: A Survey,2022,0.008487665897986664,2
W4409384710,Transfer learning for software vulnerability prediction using Transformer models,2025,0.00848554420665275,2
W3116103312,Explaining NLP Models via Minimal Contrastive Editing (MiCE),2020,0.008484605482557249,2
W4287889353,METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation,2022,0.008480201224216366,2
W3097132740,Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping,2020,0.00847997643727582,2
W4388709718,WebUltron: An Ultimate Retriever on Webpages Under the Model-Centric Paradigm,2023,0.008479563712290632,2
W4389524278,CASE: Commonsense-Augmented Score with an Expanded Answer Space,2023,0.008473170324322194,2
W4399422471,Lv-Adapter: Adapting Vision Transformers for Visual Classification with Linear-layers and Vectors,2024,0.008471340087676964,2
W3098214632,BERT for Monolingual and Cross-Lingual Reverse Dictionary,2020,0.008464943597050764,2
W4366378733,A Prompt-Based Topic-Modeling Method for Depression Detection on Low-Resource Data,2023,0.008464178279926744,2
W4405127520,Enhanced question understanding for multi-type legal question answering,2024,0.008458454375770608,2
W4403968649,Developing healthcare language model embedding spaces,2024,0.008453291566615758,2
W3103543556,"Reasoning about Goals, Steps, and Temporal Ordering with WikiHow",2020,0.008453216637553877,2
W2988346595,ERASER: A Benchmark to Evaluate Rationalized NLP Models,2019,0.008451697846800688,2
W3034229721,Biomedical Entity Representations with Synonym Marginalization,2020,0.00844597241012832,2
W3176703834,Reliability Testing for Natural Language Processing Systems,2021,0.008441694097735956,2
W2889729765,Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering,2018,0.008439980783989167,2
W3156194904,Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT,2021,0.008437962848345606,2
W4389524393,Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling,2023,0.008433157831970147,2
W3106954555,How Can We Know When Language Models Know,2020,0.008425594094372995,2
W3098771568,Constrained BERT BiLSTM CRF for understanding multi-sentence entity-seeking questions,2020,0.00841476726855014,2
W4285116174,Pretrained Biomedical Language Models for Clinical NLP in Spanish,2022,0.00841142680733773,2
W4389523706,Large Language Models Can Self-Improve,2023,0.008405229566030636,2
W4385567084,Continual Training of Language Models for Few-Shot Learning,2022,0.008403862544340552,2
W3213719910,Learning to Rank in the Age of Muppets: Effectiveness–Efficiency Tradeoffs in Multi-Stage Ranking,2021,0.008403031612369377,2
W4389519070,Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study,2023,0.008398362998772422,2
W4313567241,Material transformers: deep learning language models for generative materials design,2022,0.008393530292669888,2
W4404112581,Open-Ethical AI: Advancements in Open-Source Human-Centric Neural Language Models,2024,0.008393155254488673,2
W2963997607,Variational Pretraining for Semi-supervised Text Classification,2019,0.008391538685816618,2
W4394951779,"Optimclm: Optimizing Clinical Language Models for Predicting Patient Outcomes Via Knowledge Distillation, Pruning and Quantization",2024,0.00838987681510579,2
W2784121710,Fine-tuned Language Models for Text Classification.,2018,0.008387374879474295,2
W3105604018,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",2020,0.008384500322383104,2
W4353004366,Efficient Long-Text Understanding with Short-Text Models,2023,0.00838144260959652,2
W4283815817,Go Wider Instead of Deeper,2022,0.008381411921959797,2
W3101673779,Methods for Numeracy-Preserving Word Embeddings,2020,0.008380487895689029,2
W4385573755,Prompt-based Connective Prediction Method for Fine-grained Implicit Discourse Relation Recognition,2022,0.008378443318124816,2
W3096331697,AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts,2020,0.008373626236019272,2
W4206208161,Rethinking Why Intermediate-Task Fine-Tuning Works,2021,0.008372692353991951,2
W4391402460,An Analysis on Matching Mechanisms and Token Pruning for Late-interaction Models,2024,0.008372200304907934,2
W4221159394,PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks,2022,0.008370447873776266,2
W580074167,Large-scale Simple Question Answering with Memory Networks,2015,0.008366344823158345,2
W4385572879,How Large Language Models are Transforming Machine-Paraphrase Plagiarism,2022,0.00836183087011988,2
W4385573098,Word Order Matters When You Increase Masking,2022,0.00836069497080322,2
W3034985160,Contrastive Self-Supervised Learning for Commonsense Reasoning,2020,0.008357337222090092,2
W4225881819,UNIREX: A Unified Learning Framework for Language Model Rationale Extraction,2022,0.008357096296506536,2
W4283789538,Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers,2022,0.00835669759704434,2
W4226086464,TA-SBERT: Token Attention Sentence-BERT for Improving Sentence Representation,2022,0.008354714977858196,2
W4385570151,CodePrompt: Task-Agnostic Prefix Tuning for Program and Language Generation,2023,0.008354681395272933,2
W3112689365,Extracting Training Data from Large Language Models,2020,0.008348457590366396,2
W4223549958,The HoPE Model Architecture: a Novel Approach to Pregnancy Information Retrieval Based on Conversational Agents,2022,0.008341455176903942,2
W2932376173,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Auto-Encoders,2019,0.00834140912103376,2
W2974593375,"Unifying Question Answering, Text Classification, and Regression via Span Extraction",2019,0.008341238934475264,2
W4389524114,CarExpert: Leveraging Large Language Models for In-Car Conversational Question Answering,2023,0.008333315869265933,2
W3211142893,Question Answering for the Curated Web: Tasks and Methods in QA over Knowledge Bases and Text Collections,2021,0.008332426830162788,2
W3199258975,RNG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering,2022,0.008328705585613772,2
W3143557598,Automatic question-answer pairs generation and question similarity mechanism in question answering system,2021,0.008328672211160657,2
W2970927596,Specializing Word Embeddings (for Parsing) by Information Bottleneck,2019,0.008325266590774403,2
W3212990991,"BERT, XLNet or RoBERTa: The Best Transfer Learning Model to Detect Clickbaits",2021,0.008324205096678519,2
W3106092787,An Empirical Study on Large-Scale Multi-Label Text Classification Including Few and Zero-Shot Labels,2020,0.008321548540677123,2
W4385573222,"Generate, Discriminate and Contrast: A Semi-Supervised Sentence Representation Learning Framework",2022,0.008321175600052423,2
W4285308960,DuReadervis: A : A Chinese Dataset for Open-domain Document Visual Question Answering,2022,0.008318734499883424,2
W4385572862,Finding Dataset Shortcuts with Grammar Induction,2022,0.008318578885792604,2
W4281740565,Pre-trained transformers: an empirical comparison,2022,0.008317495964605928,2
W2994896922,Thieves on Sesame Street! Model Extraction of BERT-based APIs,2020,0.008317247982856858,2
W3175655865,Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking,2021,0.008316559972357328,2
W3173644040,Psycholinguistic Tripartite Graph Network for Personality Detection,2021,0.008308354443130637,2
W3209409148,MentalBERT: Publicly Available Pretrained Language Models for Mental Healthcare,2021,0.00830787286097485,2
W3098284381,Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders,2020,0.008304207849516411,2
W3019779721,AmbigQA: Answering Ambiguous Open-domain Questions,2020,0.008304010706658434,2
W4396523379,Core-View Contrastive Learning Network for Building Lightweight Cross-Domain Consultation System,2024,0.008293893646687017,2
W3205835614,Småprat: DialoGPT for Natural Language Generation of Swedish Dialogue by Transfer Learning,2022,0.008291877054784205,2
W4389519095,SpEL: Structured Prediction for Entity Linking,2023,0.00829073001814184,2
W4285252640,On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency,2022,0.008289082597202558,2
W3034555021,ExpBERT: Representation Engineering with Natural Language Explanations,2020,0.008286861006874513,2
W2948629866,KERMIT: Generative Insertion-Based Modeling for Sequences,2019,0.008281483041026228,2
W2972312591,How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer Representations,2019,0.0082799737108982,2
W3105184673,This is a BERT. Now there are several of them. Can they generalize to novel words?,2020,0.00826926142130725,2
W4389524456,NORMSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly,2023,0.008268709361653035,2
W4389520362,Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU,2023,0.008266993878706283,2
W4312634749,Complex Knowledge Base Question Answering: A Survey,2022,0.008259810128469776,2
W3034256339,Roles and Utilization of Attention Heads in Transformer-based Neural Language Models,2020,0.008258493710743385,2
W3129831491,ZeRO: Memory optimizations Toward Training Trillion Parameter Models,2020,0.008255224184694822,2
W2741263286,Coarse-to-Fine Question Answering for Long Documents,2017,0.008251476798476643,2
W4296564631,Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations,2022,0.008250186711529262,2
W2798858969,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,2018,0.008250059624204898,2
W2995289474,Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring,2020,0.008248309161004748,2
W3174691662,A DQN-based Approach to Finding Precise Evidences for Fact Verification,2021,0.008246816398312014,2
W3126974869,Combining pre-trained language models and structured knowledge.,2021,0.008244197282165086,2
W4389524397,Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs,2023,0.008236501949377223,2
W4386566866,Prompt Tuning with Contradictory Intentions for Sarcasm Recognition,2023,0.00823537459464095,2
W3172097791,Coarse-grained decomposition and fine-grained interaction for multi-hop question answering,2021,0.008233242351039904,2
W4385572951,Efficient Nearest Neighbor Emotion Classification with BERT-whitening,2022,0.008231096969243816,2
W4360993130,Enhancing Biomedical ReQA With Adversarial Hard In-Batch Negative Samples,2023,0.008225582082824806,2
W3203135439,Towards a Robust Deep Neural Network against Adversarial Texts: A Survey,2021,0.008225317093982085,2
W4308366892,Improving Biomedical ReQA With Consistent NLI-Transfer and Post-Whitening,2022,0.008223245963758686,2
W3035718362,Probing Linguistic Systematicity,2020,0.008222731947919748,2
W3035091181,Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge.,2020,0.008217631736708995,2
W4389520682,DUMB: A Dutch Model Benchmark,2023,0.008215444906731763,2
W4385571597,Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text,2023,0.008210314468248875,2
W2917001081,R-Trans: RNN Transformer Network for Chinese Machine Reading Comprehension,2019,0.008202870639547943,2
W3005187732,Description Based Text Classification with Reinforcement Learning,2020,0.008201934026567905,2
W3153672153,Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration,2021,0.00820076689848144,2
W2955041501,SemEval-2019 Task 4: Hyperpartisan News Detection,2019,0.0081999606375894,2
W4377820934,Knowledge Acquired by Foundation Models,2023,0.008193554448231416,2
W4406472189,Large language models for newspaper sentiment analysis during COVID-19: The Guardian,2025,0.008190141302745393,2
W4392637206,Investigating Zero- and Few-shot Generalization in Fact Verification,2023,0.008189855690283597,2
W3182696977,HTLM: Hyper-Text Pre-Training and Prompting of Language Models,2021,0.008189650871692652,2
W3010763821,Enhanced attentive convolutional neural networks for sentence pair modeling,2020,0.008182102244624628,2
W2888491130,Adversarially Regularising Neural,2018,0.008181824389884317,2
W2994846609,Curriculum Learning Strategies for IR,2020,0.008179521939285408,2
W2939380783,Repurposing Entailment for Multi-Hop Question Answering Tasks,2019,0.008178915258017321,2
W3198510419,Diagnostics-Guided Explanation Generation,2022,0.008171761811365478,2
W4385571310,DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction,2023,0.008170543657285917,2
W4285239949,Multi-Stage Prompting for Knowledgeable Dialogue Generation,2022,0.008168483026042648,2
W3034928924,Bridging Anaphora Resolution as Question Answering,2020,0.008168166640413164,2
W3034395406,Fluent Response Generation for Conversational Question Answering,2020,0.008165222210536642,2
W3173825754,Evidence-based Factual Error Correction,2021,0.008165132186542193,2
W3115037692,Question Rewriting for Conversational Question Answering,2021,0.0081622205183311,2
W3154065069,"COVID-19 information retrieval with deep-learning based semantic search, question answering, and abstractive summarization",2021,0.008162207540961423,2
W4287887107,"When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it",2022,0.008161591887723144,2
W4284668485,PTAU,2022,0.008151371770874364,2
W3199004261,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021,0.008150503289791739,2
W4378009677,TextCNN-based ensemble learning model for Japanese Text Multi-classification,2023,0.00815000158237758,2
W2950768109,Is Attention Interpretable?,2019,0.008148417931681962,2
W4366439832,A novel self-attention enriching mechanism for biomedical question answering,2023,0.008148185463661113,2
W3104657626,Beyond [CLS] through Ranking by Generation,2020,0.008144916038936287,2
W4313276100,Coding energy knowledge in constructed responses with explainable<scp>NLP</scp>models,2022,0.008143995681715196,2
W3197868468,Searching for an Effective Defender: Benchmarking Defense against Adversarial Word Substitution,2021,0.00814381896419079,2
W4385565351,Precise Zero-Shot Dense Retrieval without Relevance Labels,2023,0.008143768155904705,2
W4406714995,Enhancing knowledge retrieval with in-context learning and semantic search through generative AI,2025,0.008142788774722971,2
W4385574083,Expose Backdoors on the Way: A Feature-Based Efficient Defense against Textual Backdoor Attacks,2022,0.008142765560949687,2
W3102009955,Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading,2020,0.008133651506349805,2
W4396214772,Learning to Improve Out-of-Distribution Generalization via Self-adaptive Language Masking,2024,0.00813069691624739,2
W3089102176,What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge,2020,0.008129388120247242,2
W3037063616,An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining,2020,0.00812238719508666,2
W3176617251,CPM-2: Large-scale cost-effective pre-trained language models,2021,0.008122099550986887,2
W4285116311,On the Effectiveness of Pre-Trained Language Models for Legal Natural Language Processing: An Empirical Study,2022,0.008121115090620641,2
W4395689476,Synergizing machine learning &amp; symbolic methods: A survey on hybrid approaches to natural language processing,2024,0.008119661059631725,2
W3213394935,Beyond Reptile: Meta-Learned Dot-Product Maximization between Gradients for Improved Single-Task Regularization,2021,0.008119488593244214,2
W4385572478,The Diminishing Returns of Masked Language Models to Science,2023,0.00811804636433348,2
W4410380157,HMI: hierarchical knowledge management for efficient multi-tenant inference in pretrained language models,2025,0.008117456331861393,2
W2969307504,Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering,2019,0.008116829304091714,2
W3165015862,InferBERT: A Transformer-Based Causal Inference Framework for Enhancing Pharmacovigilance,2021,0.00811550303132072,2
W3172706523,Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking,2021,0.008114938639998962,2
W4389822970,Incivility detection in open source code review and issue discussions,2023,0.008109055206967197,2
W3202794254,Inductive Learning on Commonsense Knowledge Graph Completion,2021,0.008104086941789138,2
W3214536449,KLMo: Knowledge Graph Enhanced Pretrained Language Model with Fine-Grained Relationships,2021,0.008101473238481082,2
W4386566659,DyLoRA: Parameter-Efficient Tuning of Pre-trained Models using Dynamic Search-Free Low-Rank Adaptation,2023,0.008097353727981307,2
W3210694367,Pragmatic competence of pre-trained language models through the lens of discourse connectives,2021,0.00809721717472605,2
W3158724005,Decontextualization: Making Sentences Stand-Alone,2021,0.008095800853556115,2
W3117034213,Scientific Keyphrase Identification and Classification by Pre-Trained Language Models Intermediate Task Transfer Learning,2020,0.008090082948710399,2
W4385573139,SimANS: Simple Ambiguous Negatives Sampling for Dense Text Retrieval,2022,0.008088532392730649,2
W4390755743,RecBERT: Semantic recommendation engine with large language model enhanced query segmentation for k-nearest neighbors ranking retrieval,2024,0.00808748984454895,2
W2587528408,Question Answering through Transfer Learning from Large Fine-grained Supervision Data,2017,0.00808633062302487,2
W4380591192,Clinical concept and relation extraction using prompt-based machine reading comprehension,2023,0.008079918081898455,2
W4317520062,Row-based hierarchical graph network for multi-hop question answering over textual and tabular data,2023,0.008078302760082161,2
W4389520742,Model-tuning Via Prompts Makes NLP Models Adversarially Robust,2023,0.008075636017835314,2
W4385574274,Generating Natural Language Proofs with Verifier-Guided Search,2022,0.008073253554261275,2
W4385571204,FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,2023,0.008070458492202502,2
W2982054702,Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification,2019,0.00806782358848216,2
W4385569969,Modeling Cross-Cultural Pragmatic Inference with Codenames Duet,2023,0.008067597479102918,2
W4403679878,Efficiency at scale: Investigating the performance of diminutive language models in clinical tasks,2024,0.008067258519120957,2
W3152670075,Improving Biomedical Pretrained Language Models with Knowledge,2021,0.00806493388931571,2
W3163314873,Evaluating Pretrained Transformer-based Models for COVID-19 Fake News Detection,2021,0.008062158943439588,2
W3202780679,Pushing on Text Readability Assessment: A Transformer Meets Handcrafted Linguistic Features,2021,0.008059221200900147,2
W2963082277,Contextualized Word Representations for Reading Comprehension,2018,0.008059174871962602,2
W3184454880,Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing,2021,0.00805849273050449,2
W4389520296,Improving Sequential Model Editing with Fact Retrieval,2023,0.008057456984178172,2
W4226003933,Better Language Model with Hypernym Class Prediction,2022,0.008047314582211541,2
W3198963570,Adaptive Information Seeking for Open-Domain Question Answering,2021,0.008046678194412538,2
W3115195983,PROP: Pre-training with Representative Words Prediction for Ad-hoc Retrieval,2021,0.008034924174286667,2
W4365420739,Event Extraction With Dynamic Prefix Tuning and Relevance Retrieval,2023,0.008034089980992016,2
W2907822478,Coarse-grain Fine-grain Coattention Network for Multi-evidence Question Answering,2019,0.008030201191275333,2
W4286530321,Aspect-Based API Review Classification: How Far Can Pre-Trained Transformer Model Go?,2022,0.008027727735858383,2
W2963025830,Assessing Composition in Sentence Vector Representations.,2018,0.008013020352524729,2
W3125591643,Learning to Deceive Knowledge Graph Augmented Models via Targeted Perturbation,2021,0.008011914449920905,2
W3098371839,SubjQA: A Dataset for Subjectivity and Review Comprehension,2020,0.008011102825558366,2
W4386566684,AutoTriggER: Label-Efficient and Robust Named Entity Recognition with Auxiliary Trigger Extraction,2023,0.008009562554788525,2
W3206435361,Automated fact‐checking: A survey,2021,0.008003934952076177,2
W4297347707,SupMPN: Supervised Multiple Positives and Negatives Contrastive Learning Model for Semantic Textual Similarity,2022,0.008002203243252334,2
W4385571479,SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data,2023,0.008001838245879269,2
W3153289922,Attention Can Reflect Syntactic Structure (If You Let It),2021,0.007991090411233939,2
W4372259936,Exploiting Prompt Learning with Pre-Trained Language Models for Alzheimer’s Disease Detection,2023,0.007990531510440227,2
W3037273551,The neural architecture of language: Integrative modeling converges on predictive processing,2020,0.00798889076627659,2
W3091101427,A Deep Transfer Learning Approach for Fake News Detection,2020,0.007985660992761674,2
W4396768561,CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts,2024,0.007983659500572431,2
W2612431505,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,2017,0.00798256627586718,2
W4400322765,Text Matching Model Combining Ranking Information and Negative Example Smoothing Strategies,2024,0.00798207919309604,2
W4385571096,Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments,2023,0.007980375236297911,2
W4367396999,DictPrompt: Comprehensive dictionary-integrated prompt tuning for pre-trained language model,2023,0.007971549031132251,2
W4385574319,How Long Is Enough? Exploring the Optimal Intervals of Long-Range Clinical Note Language Modeling,2022,0.00797032518994336,2
W2971455676,Reasoning Over Semantic-Level Graph for Fact Checking,2019,0.0079697683574249,2
W4385571232,Can Large Language Models Be an Alternative to Human Evaluations?,2023,0.007968785902099847,2
W3098981070,A building regulation question answering system: A deep learning methodology,2020,0.007965625191660266,2
W2977162702,Attention Interpretability Across NLP Tasks.,2019,0.007962783362935115,2
W3197002404,Mixed Attention Transformer for Leveraging Word-Level Knowledge to Neural Cross-Lingual Information Retrieval,2021,0.007960281495972886,2
W4382202590,FiTs: Fine-Grained Two-Stage Training for Knowledge-Aware Question Answering,2023,0.007959223486356881,2
W3100742171,GenAug: Data Augmentation for Finetuning Text Generators,2020,0.007958470535821615,2
W3176270593,Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger,2021,0.007952043550800078,2
W4389520299,Orthogonal Subspace Learning for Language Model Continual Learning,2023,0.007950639890871006,2
W3111099392,Label Confusion Learning to Enhance Text Classification Models,2021,0.007949301419093545,2
W3105935311,Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia,2020,0.007944871099598686,2
W4406917337,CoT-driven framework for short text classification: Enhancing and transferring capabilities from large to smaller model,2025,0.0079438020157046,2
W4389524274,Symbol tuning improves in-context learning in language models,2023,0.007942669106343584,2
W2972342261,From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions,2019,0.007942606830004754,2
W3170883728,Can BERT Dig It? Named Entity Recognition for Information Retrieval in the Archaeology Domain,2022,0.00794030497941919,2
W3034760557,Probing Linguistic Features of Sentence-Level Representations in Relation Extraction,2020,0.007937724396935908,2
W4389612030,KI-MAG: A knowledge-infused abstractive question answering system in medical domain,2023,0.007933985397711056,2
W3035183674,Reranking for Efficient Transformer-based Answer Selection,2020,0.007928800320981009,2
W3081505754,Conceptualized Representation Learning for Chinese Biomedical Text Mining,2020,0.007927150128436098,2
W3084470717,Critical Thinking for Language Models,2020,0.007922233242826683,2
W3091311542,Biomedical Named-Entity Recognition by Hierarchically Fusing BioBERT Representations and Deep Contextual-Level Word-Embedding,2020,0.007920929183973382,2
W4319782877,Compression Methods for Transformers in Multidomain Sentiment Analysis,2022,0.007916733074541502,2
W4385571219,Large Language Models Are Reasoning Teachers,2023,0.007912213568704824,2
W4285217916,Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning,2022,0.007907095774036761,2
W3176398225,From Discourse to Narrative: Knowledge Projection for Event Relation Extraction,2021,0.007906199077530032,2
W4205450747,Can Language Models be Biomedical Knowledge Bases?,2021,0.00790544352194575,2
W4307817449,A Siamese Neural Network for Learning Semantically-Informed Sentence Embeddings,2022,0.007900795365434386,2
W4389518789,A Causal View of Entity Bias in (Large) Language Models,2023,0.007896413969567553,2
W4389520486,Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking,2023,0.007895757297394381,2
W3196986263,Neuron-level Interpretation of Deep NLP Models: A Survey,2022,0.007886372278504914,2
W4310390625,Transformers for Tabular Data Representation: A Survey of Models and Applications,2023,0.007878901185708968,2
W4281656839,MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data,2022,0.007876690764559525,2
W4385574068,Facilitating Contrastive Learning of Discourse Relational Senses by Exploiting the Hierarchy of Sense Relations,2022,0.00787291096122357,2
W4225392244,Deep learning-based approach for Arabic open domain question answering,2022,0.007863818824309633,2
W4224950688,Can Rationalization Improve Robustness?,2022,0.00786196634423365,2
W3211582110,Retrieval Augmentation Reduces Hallucination in Conversation,2021,0.007859201908525315,2
W2734823783,Quasar: Datasets for Question Answering by Search and Reading,2017,0.007849909689315166,2
W3212993480,Generate & Rank: A Multi-task Framework for Math Word Problems,2021,0.007847970854094732,2
W4399205243,TeC: A Novel Method for Text Clustering with Large Language Models Guidance and Weakly-Supervised Contrastive Learning,2024,0.007847673503065975,2
W3088059392,,2019,0.007847019643019036,2
W3161320446,SILT: Efficient transformer training for inter-lingual inference,2022,0.007843560718001609,2
W4385570624,Content Moderation for Evolving Policies using Binary Question Answering,2023,0.007843202161847735,2
W3153051631,Representations for Question Answering from Documents with Tables and Text,2021,0.007839980290262944,2
W4287888046,Enhancing Self-Attention with Knowledge-Assisted Attention Maps,2022,0.007837427893173826,2
W3207604732,DialFact: A Benchmark for Fact-Checking in Dialogue,2022,0.007831591470659377,2
W4387005290,Learning Representations on Logs for AIOps,2023,0.007829917998276089,2
W2964047576,Effective Subword Segmentation for Text Comprehension,2019,0.00782514116479715,2
W3203595782,Encoder Adaptation of Dense Passage Retrieval for Open-Domain Question Answering,2021,0.007824831104685173,2
W2889048825,Large-scale Cloze Test Dataset Created by Teachers,2018,0.00782134567978267,2
W2952744660,Theoretical Limitations of Self-Attention in Neural Sequence Models,2020,0.007818594984892497,2
W4393147971,UniGen: A Unified Generative Framework for Retrieval and Question Answering with Large Language Models,2024,0.007818524753157668,2
W3101786835,A Technical Question Answering System with Transfer Learning,2020,0.007815559001873536,2
W4317795002,Mask and Cloze: Automatic Open Cloze Question Generation Using a Masked Language Model,2023,0.007808689286642536,2
W4283796213,Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning,2022,0.007805180523283299,2
W4225090118,Testing the Ability of Language Models to Interpret Figurative Language,2022,0.007804773182581142,2
W3026805444,Fluent Response Generation for Conversational Question Answering,2020,0.007798148916521844,2
W3199241049,Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers,2021,0.007792902985209876,2
W3172267148,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,2021,0.007787827325666205,2
W3093681547,XOR QA: Cross-lingual Open-Retrieval Question Answering,2020,0.007786147948098336,2
W4200635688,ValueNet: A New Dataset for Human Value Driven Dialogue System,2022,0.007783921057512221,2
W3114740437,Learn to Combine Linguistic and Symbolic Information for Table-based Fact Verification,2020,0.007781788981909274,2
W3034588688,Contextualized Weak Supervision for Text Classification,2020,0.007781116538686665,2
W4288057740,Piccolo: Exposing Complex Backdoors in NLP Transformer Models,2022,0.007779577162298142,2
W3110453189,Cross-Lingual Passage Re-Ranking With Alignment Augmented Multilingual BERT,2020,0.007775995468475775,2
W4385573926,Mitigating Spurious Correlation in Natural Language Understanding with Counterfactual Inference,2022,0.007772506078012673,2
W2963448850,Key-Value Memory Networks for Directly Reading Documents,2016,0.007769882295486034,2
W3010714856,TRANS-BLSTM: Transformer with Bidirectional LSTM for Language Understanding,2020,0.007767912738667579,2
W3103923992,Claim Check-Worthiness Detection as Positive Unlabelled Learning,2020,0.007765987479475711,2
W4400199239,Shared functional specialization in transformer-based language models and the human brain,2024,0.0077657476546615645,2
W4389518958,On the Dimensionality of Sentence Embeddings,2023,0.007761010257887181,2
W3134100302,Contrastive Explanations for Model Interpretability,2021,0.007758997834948448,2
W4387185357,TrojBits: A Hardware Aware Inference-Time Attack on Transformer-Based Language Models,2023,0.007758069284125125,2
W4385570326,Large Language Models with Controllable Working Memory,2023,0.00775740423460008,2
W3215670835,Triggerless Backdoor Attack for NLP Tasks with Clean Labels,2022,0.00775545854141859,2
W4315630900,Textual Pre-Trained Models for Gender Identification Across Community Question-Answering Members,2023,0.0077532936664928154,2
W3171636201,DATE: Detecting Anomalies in Text via Self-Supervision of Transformers,2021,0.007750298721318128,2
W3170666909,Learning to Recognize Dialect Features,2021,0.0077467146443200696,2
W4385570578,DiscoPrompt: Path Prediction Prompt Tuning for Implicit Discourse Relation Recognition,2023,0.007744693811845298,2
W3036818742,Review of Deep Learning Techniques for Improving the Performance of Machine Reading Comprehension Problem,2020,0.007741603346836869,2
W2973047874,Blackbox Meets Blackbox: Representational Similarity &amp; Stability Analysis of Neural Language Models and Brains,2019,0.007740915572828471,2
W3088342637,Multilingual Probing of Deep Pre-Trained Contextual Encoders,2019,0.007740011424184636,2
W2952570576,Attention Is (not) All You Need for Commonsense Reasoning,2019,0.0077381224102722026,2
W3125812125,A Text Mining Approach to Discovering COVID-19 Relevant Factors,2020,0.007737681148718622,2
W4377091689,KEBLM: Knowledge-Enhanced Biomedical Language Models,2023,0.007736541847173504,2
W4385570579,Entity Tracking in Language Models,2023,0.007735089257452242,2
W4224903411,Exploring the Universal Vulnerability of Prompt-based Learning Paradigm,2022,0.00773053877209983,2
W3171155106,Joint structured pruning and dense knowledge distillation for efficient transformer model compression,2021,0.007726685979111159,2
W2953039212,Are Red Roses Red? Evaluating Consistency of Question-Answering Models,2019,0.007726284452533169,2
W3126259453,Aligning AI With Shared Human Values,2020,0.0077259655472825695,2
W2889283903,Question Answering by Reasoning Across Documents with Graph Convolutional Networks,2018,0.00772401712976544,2
W4390590487,DeBERTa-BiLSTM: A multi-label classification model of arabic medical questions using pre-trained models and deep learning,2024,0.007723097631662714,2
W3174481949,AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry,2022,0.007722043492785162,2
W4385572492,FLamE: Few-shot Learning from Natural Language Explanations,2023,0.007717159047786885,2
W3183138634,NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning,2021,0.0077129474337085425,2
W3155632693,Through the Looking Glass: Learning to Attribute Synthetic Text Generated by Language Models,2021,0.007706390662408082,2
W3157651462,"Neural, symbolic and neural-symbolic reasoning on knowledge graphs",2021,0.007705294105615328,2
W4291012579,A comprehensive overview of knowledge graph completion,2022,0.007705146560885974,2
W4225141790,HybriDialogue: An Information-Seeking Dialogue Dataset Grounded on Tabular and Textual Data,2022,0.007704568328006906,2
W4385567522,Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings,2022,0.007701211883235085,2
W3176380929,Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence,2021,0.007700705563782758,2
W4385573668,Towards Tracing Knowledge in Language Models Back to the Training Data,2022,0.007699841090399884,2
W4229868159,Think about it! Improving defeasible reasoning by first modeling the question scenario.,2021,0.007699316869676477,2
W3177005832,RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English),2021,0.007697795473353057,2
W3086963535,Can Machines Tell Stories? A Comparative Study of Deep Neural Language Models and Metrics,2020,0.007695201027568652,2
W2996507500,Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models,2019,0.00768851578038025,2
W3126763054,Challenges in Automated Debiasing for Toxic Language Detection,2021,0.007687129012554093,2
W4385570569,Task-aware Retrieval with Instructions,2023,0.007684759816178966,2
W3171589458,Disentangling Syntax and Semantics in the Brain with Deep Networks,2021,0.00768422476170049,2
W4385567351,Is anisotropy really the cause of BERT embeddings not being semantic?,2022,0.007682747931653263,2
W4385572905,Zero-Shot Text Classification with Self-Training,2022,0.007681262370597529,2
W2971569798,"How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings",2019,0.007680098300500195,2
W4310424784,Pre-training language model incorporating domain-specific heterogeneous knowledge into a unified representation,2022,0.007677745644126779,2
W4385572043,Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity,2023,0.007675911077920815,2
W3115149992,Graph-Based Knowledge Integration for Question Answering over Dialogue,2020,0.0076723521119618195,2
W3166933661,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,2021,0.007670815519949176,2
W2996251235,Neural Module Networks for Reasoning over Text,2019,0.0076703129855704225,2
W3020152921,Rapidly Bootstrapping a Question Answering Dataset for COVID-19,2020,0.007669948681648605,2
W4283798762,Flexible Instance-Specific Rationalization of NLP Models,2022,0.0076652682767712945,2
W4393039272,Automatic Coding of Contingency in Child-Caregiver Conversations,2024,0.007663452858144411,2
W4385572290,Chain of Thought Prompting Elicits Knowledge Augmentation,2023,0.0076631856131652454,2
W3114204117,Connecting the Dots Between Fact Verification and Fake News Detection,2020,0.0076624451259826205,2
W4389518213,Explaining Data Patterns in Natural Language with Language Models,2023,0.007655160934082125,2
W4367298452,Decomposed Two-Stage Prompt Learning for Few-Shot Named Entity Recognition,2023,0.007649814133938735,2
W3021037761,Understanding and Improving Information Transfer in Multi-Task Learning,2020,0.007642641138720345,2
W3115540322,A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification,2020,0.0076420996039066585,2
W3085329811,DDRQA: Dynamic Document Reranking for Open-domain Multi-hop Question Answering.,2020,0.007640852652619088,2
W3138967041,Open Domain Question Answering over Tables via Dense Retrieval,2021,0.007633351109940548,2
W4400517951,TurkishBERTweet: Fast and reliable large language model for social media analysis,2024,0.007630490408009385,2
W4389104898,Performance Analysis of Federated Learning Algorithms for Multilingual Protest News Detection Using Pre-Trained DistilBERT and BERT,2023,0.007628867484270055,2
W4385570688,Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks,2023,0.007622340872341137,2
W4206633687,Flexible Generation of Natural Language Deductions,2021,0.007621826385944299,2
W4285798540,On the Explainability of Natural Language Processing Deep Models,2022,0.007620189461935032,2
W2950819771,Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation,2018,0.007619230800297237,2
W4385571638,Exploring Anisotropy and Outliers in Multilingual Language Models for Cross-Lingual Semantic Sentence Similarity,2023,0.0076177792766688865,2
W4389524581,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,2023,0.007616791904201828,2
W4409235391,Application of large language models in medicine,2025,0.007615830287916658,2
W4283314135,An Automatic and Efficient BERT Pruning for Edge AI Systems,2022,0.007613942348774991,2
W4401536723,FLAT: Fusing layer representations for more efficient transfer learning in NLP,2024,0.007612160676086905,2
W4385573594,Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation,2022,0.0076049998443262325,2
W3211653317,Putting Words in BERT’s Mouth: Navigating Contextualized Vector Spaces with Pseudowords,2021,0.007604640233973788,2
W3099864716,A survey on Recognizing Textual Entailment as an NLP Evaluation,2020,0.007603480007810997,2
W3088199239,Chart-based Zero-shot Constituency Parsing on Multiple Languages,2020,0.007600801597221563,2
W3200936406,Pre-train or Annotate? Domain Adaptation with a Constrained Budget,2021,0.00759985563582078,2
W4290994954,Reducing Conversational Agents’ Overconfidence Through Linguistic Calibration,2022,0.007597461750333333,2
W4393864958,Evaluation and Analysis of Large Language Models for Clinical Text Augmentation and Generation,2024,0.007596113551430438,2
W4385571643,FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning,2023,0.007590002542217785,2
W4389520133,Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing,2023,0.00758990698816452,2
W4390579019,Entity recognition from colloquial text,2024,0.0075852340512485685,2
W2970243238,WIQA: A dataset for “What if...” reasoning over procedural text,2019,0.007579358626336656,2
W4400927874,DPAL-BERT: A Faster and Lighter Question Answering Model,2024,0.007574836869784775,2
W3027639267,Query Resolution for Conversational Search with Limited Supervision,2020,0.0075737661847024886,2
W4285240641,OPI@LT-EDI-ACL2022: Detecting Signs of Depression from Social Media Text using RoBERTa Pre-trained Language Models,2022,0.007568532415940238,2
W2982096936,Context-Aware Sentence/Passage Term Importance Estimation For First Stage Retrieval,2019,0.00756428002573399,2
W3174281149,Structural Guidance for Transformer Language Models,2021,0.007551681796139385,2
W2994636820,Cross-Lingual Machine Reading Comprehension,2019,0.007542321404859073,2
W3156366114,Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases,2021,0.007541053480445912,2
W3210948652,EviDR: Evidence-Emphasized Discrete Reasoning for Reasoning Machine Reading Comprehension,2021,0.007539855660216926,2
W3154707883,Unification-based Reconstruction of Multi-hop Explanations for Science Questions,2021,0.007537074121097628,2
W3173784240,Rethinking Stealthiness of Backdoor Attack against NLP Models,2021,0.007527675498390559,2
W3199301749,ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning,2021,0.007527545385771423,2
W3119164154,Autoregressive Entity Retrieval,2020,0.007517892737076342,2
W4385574084,InfoCSE: Information-aggregated Contrastive Learning of Sentence Embeddings,2022,0.007515270079980274,2
W3165066581,CSKG: The CommonSense Knowledge Graph,2021,0.007514817428048114,2
W2952087461,Classification and Clustering of Arguments with Contextualized Word Embeddings,2019,0.0075145243325644975,2
W3038831159,Relevance-guided Supervision for OpenQA with ColBERT,2020,0.007509922713139292,2
W4223651117,KCD: Knowledge Walks and Textual Cues Enhanced Political Perspective Detection in News Media,2022,0.0075091360925378485,2
W3126639583,GT-Finder: Classify the family of glucose transporters with pre-trained BERT language models,2021,0.007507964163549191,2
W3035454789,GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples,2020,0.0075066480660557905,2
W4385573301,Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives,2022,0.007505000039311659,2
W2962910668,Large-Scale Multi-Label Text Classification on EU Legislation,2019,0.007502804347245808,2
W3103573410,What Can We Do to Improve Peer Review in NLP?,2020,0.007500384594784255,2
W4400426842,Intent aware data augmentation by leveraging generative AI for stress detection in social media texts,2024,0.007497032711260221,2
W4205434581,AnomalyAdapters: Parameter-Efficient Multi-Anomaly Task Detection,2022,0.007496722176981624,2
W3167021021,KPQA: A Metric for Generative Question Answering Using Keyphrase Weights,2021,0.00748072418977983,2
W3167126457,Constructing Taxonomies from Pretrained Language Models,2021,0.007479070587175286,2
W4388329494,Embracing ambiguity: Improving similarity-oriented tasks with contextual synonym knowledge,2023,0.007477270621728189,2
W4385570047,Is Anisotropy Truly Harmful? A Case Study on Text Clustering,2023,0.007476744269658446,2
W2949858875,"Errudite: Scalable, Reproducible, and Testable Error Analysis",2019,0.00747489513039569,2
W4385571810,"Unified Language Representation for Question Answering over Text, Tables, and Images",2023,0.007472295783880474,2
W3205058818,Meta-learning via Language Model In-context Tuning,2022,0.007472042543178647,2
W3098198541,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,2020,0.0074704051524942545,2
W4327656064,An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2023,0.007468326711204526,2
W4385572475,Alleviating Over-smoothing for Unsupervised Sentence Representation,2023,0.007464915143792094,2
W4289868412,Improving the robustness of machine reading comprehension via contrastive learning,2022,0.007462649216647075,2
W4385572153,Viettel-AI at SemEval-2023 Task 6: Legal Document Understanding with Longformer for Court Judgment Prediction with Explanation,2023,0.007460648686181911,2
W3174863418,Answer Generation for Retrieval-based Question Answering Systems,2021,0.007460045336494573,2
W2963983586,Tracking State Changes in Procedural Text: a Challenge Dataset and Models for Process Paragraph Comprehension,2018,0.007451881123326248,2
W3175002116,"Not Far Away, Not So Close: Sample Efficient Nearest Neighbour Data Augmentation via MiniMax",2021,0.00744759228297087,2
W3176589722,Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model,2021,0.007446495631335981,2
W3145630588,Complement Lexical Retrieval Model with Semantic Residual Embeddings,2021,0.007443297007744166,2
W3039578880,Facts as Experts: Adaptable and Interpretable Neural Memory over Symbolic Knowledge,2020,0.007442391184615507,2
W4406304217,Learning Shortcuts: On the Misleading Promise of NLU in Language Models,2025,0.007437655840250167,2
W4409648970,Detecting sarcasm in user-generated content integrating transformers and gated graph neural networks,2025,0.0074375120395538535,2
W4384643961,AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity Using Contrastive Learning and Structured Knowledge,2023,0.0074352169026154795,2
W3035620114,LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network,2020,0.007432222852104507,2
W4385571401,BIC: Twitter Bot Detection with Text-Graph Interaction and Semantic Consistency,2023,0.007432003365246488,2
W3035287090,Harnessing the linguistic signal to predict scalar inferences,2020,0.007428796527895253,2
W4226198567,DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation,2022,0.007428296970547047,2
W3158990693,Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review,2022,0.007427231806819956,2
W4390873328,Introducing Language Guidance in Prompt-based Continual Learning,2023,0.007425577458400849,2
W4389269373,Towards Effective and Efficient Sparse Neural Information Retrieval,2023,0.007424252199042225,2
W3202390784,MultiDoc2Dial: Modeling Dialogues Grounded in Multiple Documents,2021,0.007424188776833068,2
W4385319136,What does Chinese BERT learn about syntactic knowledge?,2023,0.007424028940710148,2
W2975208319,Question Answering is a Format; When is it Useful?,2019,0.007419669391921657,2
W3102259594,Learning to Model and Ignore Dataset Bias with Mixed Capacity Ensembles,2020,0.0074176624380921955,2
W4392357044,A Survey of Knowledge Enhanced Pre-trained Language Models,2024,0.007415726657264959,2
W2986138048,CogniVal: A Framework for Cognitive Word Embedding Evaluation,2019,0.007413289484337358,2
W4385573298,Revisiting Transformer-based Models for Long Document Classification,2022,0.007410816564390542,2
W4313459382,Investigating Reasons for Disagreement in Natural Language Inference,2022,0.007408971389997477,2
W4386494407,Surprisal From Language Models Can Predict ERPs in Processing Predicate-Argument Structures Only if Enriched by an Agent Preference Principle,2023,0.007408586341167247,2
W4287891033,Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts,2022,0.0074031868352519075,2
W4385571097,YNU-HPCC at SemEval-2023 Task 6: LEGAL-BERT Based Hierarchical BiLSTM with CRF for Rhetorical Roles Prediction,2023,0.007403028737841456,2
W2983962589,Do Massively Pretrained Language Models Make Better Storytellers?,2019,0.007401814383014563,2
W3099873751,Table Fact Verification with Structure-Aware Transformer,2020,0.007401673570234426,2
W4402352741,Subgraph-Based Attention Network for Multi-Hop Question Answering,2024,0.007397804916385571,2
W3115271704,Towards Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning,2020,0.007397383409552095,2
W2799187742,A Co-Matching Model for Multi-choice Reading Comprehension,2018,0.007396895484857968,2
W3011279327,Adv-BERT: BERT is not robust on misspellings! Generating nature adversarial samples on BERT,2020,0.007394226762777378,2
W4389520367,Crystal: Introspective Reasoners Reinforced with Self-Feedback,2023,0.007390890329797368,2
W3155487259,NPE: An FPGA-based Overlay Processor for Natural Language Processing,2021,0.0073904208783773006,2
W2593833795,Bilateral Multi-Perspective Matching for Natural Language Sentences,2017,0.007390184426549273,2
W3175042000,Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation,2021,0.007387939887515515,2
W4286252375,A review on Natural Language Processing Models for COVID-19 research,2022,0.007387872492460531,2
W4285113702,Adaptive Testing and Debugging of NLP Models,2022,0.007386584497420937,2
W4368408045,CsFEVER and CTKFacts: acquiring Czech data for fact verification,2023,0.007386082649669977,2
W4389523748,A surprisal oracle for active curriculum language modeling,2023,0.007385367365524395,2
W3199212893,Relation-Guided Pre-Training for Open-Domain Question Answering,2021,0.007385268125685435,2
W4385573674,Automatic Rule Induction for Efficient Semi-Supervised Learning,2022,0.007383198545999702,2
W4385572582,FMI-SU at SemEval-2023 Task 7: Two-level Entailment Classification of Clinical Trials Enhanced by Contextual Data Augmentation,2023,0.00738298877016361,2
W4385572901,TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models,2022,0.007378309295268704,2
W3173798466,Implicit Representations of Meaning in Neural Language Models,2021,0.007375933070702873,2
W2963559137,Exploiting Explicit Paths for Multi-hop Reading Comprehension,2019,0.007373500379944351,2
W3154732937,CHOLAN: A Modular Approach for Neural Entity Linking on Wikipedia and Wikidata,2021,0.007371968444781174,2
W2962979564,MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge,2018,0.007368651517266834,2
W4210900105,Fine-Grained Entity Typing with a Type Taxonomy: a Systematic Review,2022,0.007364600688203401,2
W3166143997,Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning,2021,0.0073596808267666096,2
W4405107145,Audio-based Music Retrieval,2024,0.007357660915621948,2
W4405107149,Conversational Search,2024,0.007357660915621948,2
W4405107234,Knowledge Graphs and Search,2024,0.007357660915621948,2
W4405107143,Cross-language Retrieval,2024,0.007357660915621948,2
W2945329331,Alignment over Heterogeneous Embeddings for Question Answering,2019,0.007356051342640502,2
W4281742449,A COVID-19 Search Engine (CO-SE) with Transformer-based architecture,2022,0.007345725996860222,2
W2951048068,Barack’s Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling,2019,0.007343188438318399,2
W4408799105,A survey on moral foundation theory and pre-trained language models: current advances and challenges,2025,0.007342707036550478,2
W3034350582,Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection,2020,0.007341743419582252,2
W3171355829,Multilingual Language Models Predict Human Reading Behavior,2021,0.007338690223092041,2
W4322766882,Parameter-efficient fine-tuning of large-scale pre-trained language models,2023,0.007337595706040849,2
W4385570856,UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language,2023,0.007334030012797923,2
W4287888019,Probing via Prompting,2022,0.007331848727028149,2
W4389403907,Legalbench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models,2023,0.007330002506722481,2
W2931212643,A Multi-Task Approach for Disentangling Syntax and Semantics in Sentence Representations,2019,0.007329774329448992,2
W3181113307,LexSubCon: Integrating Knowledge from Lexical Resources into Contextual Embeddings for Lexical Substitution,2022,0.00732687446519598,2
W4389519059,Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations,2023,0.0073206484344826295,2
W3155979791,Modeling Context in Answer Sentence Selection Systems on a Latency Budget,2021,0.007312307818214244,2
W3130395060,TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models,2021,0.007309750741482651,2
W4385572845,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,2022,0.007309421043032761,2
W3012625840,The value of text for small business default prediction: A Deep Learning approach,2021,0.0073088134953293304,2
W4390679741,Software Vulnerability Detection with GPT and In-Context Learning,2023,0.007308731450766684,2
W3008811720,Towards End-to-End Multilingual Question Answering,2020,0.007307810006928348,2
W3139427994,A Deep Learning Approach for Robust Detection of Bots in Twitter Using Transformers,2021,0.007307584616541695,2
W2951815760,Bidirectional Attention Flow for Machine Comprehension,2016,0.00730742872841873,2
W2949181687,"Identification of Tasks, Datasets, Evaluation Metrics, and Numeric Scores for Scientific Leaderboards Construction",2019,0.00730571424531952,2
W4385478147,Exploring Large Language Models’ Emotion Detection Abilities: Use Cases From the Middle East,2023,0.007303392260636849,2
W3103901889,BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining,2020,0.007296918369932259,2
W3177415603,Efficient Passage Retrieval with Hashing for Open-domain Question Answering,2021,0.007292909938433405,2
W2945127593,Deeper Text Understanding for IR with Contextual Neural Language Modeling,2019,0.007291930753873459,2
W4385718077,Limits for learning with language models,2023,0.007284667109507672,2
W2953010614,Real-Time Open-Domain Question Answering with Dense-Sparse Phrase Index,2019,0.007283819155318981,2
W4385572447,NormMark: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery,2023,0.007282209809637431,2
W3152562554,B-PROP,2021,0.007276922268321566,2
W3099475240,Combining BERT with Static Word Embeddings for Categorizing Social Media,2020,0.00727450798078507,2
W4385265580,Grouped Contrastive Learning of Self-supervised Sentence Representation,2023,0.00727295837377239,2
W3200245893,Efficient Test Time Adapter Ensembling for Low-resource Language Varieties,2021,0.007271719128956894,2
W3213549365,Theme Transformer: Symbolic Music Generation With Theme-Conditioned Transformer,2022,0.007268885886365668,2
W3207867270,Embedding Electronic Health Records to Learn BERT-based Models for Diagnostic Decision Support,2021,0.007263594201722544,2
W4385571278,Legal_try at SemEval-2023 Task 6: Voting Heterogeneous Models for Entities identification in Legal Documents,2023,0.007263588585467715,2
W4385571523,PoliToHFI at SemEval-2023 Task 6: Leveraging Entity-Aware and Hierarchical Transformers For Legal Entity Recognition and Court Judgment Prediction,2023,0.007257831783216805,2
W4385570734,uOttawa at SemEval-2023 Task 6: Deep Learning for Legal Text Understanding,2023,0.0072558458389131225,2
W4385324506,Deciphering “the language of nature”: A transformer-based language model for deleterious mutations in proteins,2023,0.007254152887874884,2
W4243640523,"Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey",2020,0.007248488302825488,2
W4393161188,Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models,2024,0.007247106042486242,2
W3210432446,E.T.,2021,0.007247097543728672,2
W4406172063,Adaptive Prompt Learning with Distilled Connective Knowledge for Implicit Discourse Relation Recognition,2025,0.007244930897412598,2
W4389518706,Let’s Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models,2023,0.007244737450264977,2
W4220913575,Negation and uncertainty detection in clinical texts written in Spanish: a deep learning-based approach,2022,0.007243704881884808,2
W4287887244,Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants,2022,0.007241086872822726,2
W3046527848,Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning,2020,0.007240619270305968,2
W4206557209,Am I Being Bullied on Social Media? An Ensemble Approach to Categorize Cyberbullying,2021,0.007237356753958197,2
W3202442632,Narrative event segmentation in the cortical reservoir,2021,0.007234708705033015,2
W4385573481,Enhancing Out-of-Distribution Detection in Natural Language Understanding via Implicit Layer Ensemble,2022,0.0072334260246576165,2
W3134665270,Pretrained Transformers for Text Ranking: BERT and Beyond,2021,0.007225410914498662,2
W4283835457,MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,2022,0.007224715135336684,2
W2949433733,Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books,2015,0.007223577644409056,2
W2970723181,Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base,2019,0.0072218783831806055,2
W4389520408,What do Deck Chairs and Sun Hats Have in Common? Uncovering Shared Properties in Large Concept Vocabularies,2023,0.007221387667346068,2
W3171218748,End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents,2021,0.007217659504409083,2
W3042795397,Can neural networks acquire a structural bias from raw linguistic data?,2020,0.007217320790541228,2
W3169976744,How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds,2021,0.007211138911963733,2
W4221151543,Improving Biomedical Information Retrieval with Neural Retrievers,2022,0.007210218347402549,2
W3099206682,Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models,2020,0.007206577993337723,2
W4409747196,Enhancing pre-trained language model by answering natural questions for event extraction,2025,0.007202232621034744,2
W2988092105,The FEVER2.0 Shared Task,2019,0.007197374142844199,2
W4293565917,Improving text classification with transformers and layer normalization,2022,0.007196441704417861,2
W3194589868,Verification mechanism to obtain an elaborate answer span in machine reading comprehension,2021,0.00719566250295246,2
W4385571733,ResearchTeam_HCN at SemEval-2023 Task 6: A knowledge enhanced transformers based legal NLP system,2023,0.007190682318854409,2
W3104655669,CHARM: Inferring Personal Attributes from Conversations,2020,0.0071892704358145635,2
W4309685935,Prompt text classifications with transformer models! An exemplary introduction to prompt-based learning with large language models,2022,0.007186045765047716,2
W4401656847,AgXQA: A benchmark for advanced Agricultural Extension question answering,2024,0.007186037884008402,2
W4385155440,UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition,2023,0.007183730681611039,2
W2946545670,Story Ending Prediction by Transferable BERT,2019,0.007181761301532805,2
W2949845972,Robust Representation Learning of Biomedical Names,2019,0.00718101857770311,2
W3107377711,Self-Explaining Structures Improve NLP Models,2020,0.007176747074663471,2
W3157428592,Rethinking Search: Making Experts out of Dilettantes,2021,0.007174544487105799,2
W2964060837,The Fact Extraction and VERification (FEVER) Shared Task,2018,0.007174313119528594,2
W3206066344,Differentially Private Fine-tuning of Language Models,2021,0.007174238386390198,2
W3034763191,A Tale of a Probe and a Parser,2020,0.007169819950787913,2
W4395101324,GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models,2023,0.007168007651030492,2
W4389664922,PromptCast: A New Prompt-Based Learning Paradigm for Time Series Forecasting,2023,0.00716733179279902,2
W4385572836,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,2022,0.007167135392647996,2
W3092462689,FIND: Human-in-the-Loop Debugging Deep Text Classifiers,2020,0.007167099857787885,2
W4389518800,Ultra-Fine Entity Typing with Prior Knowledge about Labels: A Simple Clustering Based Strategy,2023,0.007164870110874318,2
W4385570732,Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming,2023,0.0071622179945244014,2
W2605035112,Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features,2018,0.007156836620135295,2
W3105987139,NLP North at WNUT-2020 Task 2: Pre-training versus Ensembling for Detection of Informative COVID-19 English Tweets,2020,0.007150150850353436,2
W3169404852,Factorization-Aware Training of Transformers for Natural Language Understanding on the Edge,2021,0.007149847553111467,2
W3196692796,Open Aspect Target Sentiment Classification with Natural Language Prompts,2021,0.007149451702194876,2
W4394932682,Knowledge-Enhanced Prompt Learning for Few-Shot Text Classification,2024,0.007149131580805324,2
W3034693764,Text Classification with Negative Supervision,2020,0.007146554023556754,2
W2963368301,QUAREL: A Dataset and Models for Answering Questions about Qualitative Relationships,2019,0.007141173371045767,2
W3092932788,Neural Databases,2020,0.007137046643426581,2
W4401943697,Missed Connections: Lateral Thinking Puzzles for Large Language Models,2024,0.007135138444799671,2
W3201915713,Data augmentation approaches in natural language processing: A survey,2022,0.007135102079427236,2
W4391109462,Extracting Patient Lifestyle Characteristics from Dutch Clinical Text with BERT Models,2024,0.007131425383776738,2
W4385571246,Multi-granularity Temporal Question Answering over Knowledge Graphs,2023,0.007127459866396533,2
W4221152448,“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction,2022,0.007125915945703846,2
W4285206788,Knowledge Distillation Meets Few-Shot Learning: An Approach for Few-Shot Intent Classification Within and Across Domains,2022,0.007121601980371793,2
W3177233335,Claim Matching Beyond English to Scale Global Fact-Checking,2021,0.007115945471391576,2
W4200632828,Reasoning over Hybrid Chain for Table-and-Text Open Domain Question Answering,2022,0.007113154287853102,2
W3206387060,Deep Transfer Learning &amp; Beyond: Transformer Language Models in Information Systems Research,2022,0.007108913464816913,2
W4312516176,"Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond",2022,0.007106885687767471,2
W4386566654,A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods,2023,0.007105197693347144,2
W3037128914,RepBERT: Contextualized Text Embeddings for First-Stage Retrieval,2020,0.007093407521644272,2
W3034862985,Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings,2020,0.007089355656321797,2
W4385570645,A Length-Extrapolatable Transformer,2023,0.007082881578158492,2
W3103111734,A Deep Cascade Model for Multi-Document Reading Comprehension,2019,0.007077751008270114,2
W4386576804,Improving Numeracy by Input Reframing and Quantitative Pre-Finetuning Task,2023,0.007076429836889951,2
W4385570984,Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor,2023,0.007075373344064216,2
W3204619801,BadNL: Backdoor Attacks against NLP Models with Semantic-preserving Improvements,2021,0.007073757271962938,2
W4223569930,Using Interactive Feedback to Improve the Accuracy and Explainability of Question Answering Systems Post-Deployment,2022,0.007072133439684239,2
W4391100869,Artificial Neural Network Language Models Predict Human Brain Responses to Language Even After a Developmentally Realistic Amount of Training,2024,0.007070870943284803,2
W4391473819,Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces,2024,0.007064486403849809,2
W3167303745,Progressive Generation of Long Text with Pretrained Language Models,2021,0.007064344206393375,2
W4408853582,RECoT: Relation-enhanced Chains-of-Thoughts for knowledge-intensive multi-hop questions answering,2025,0.007062947206251308,2
W4385573394,Detecting Euphemisms with Literal Descriptions and Visual Imagery,2022,0.007058507219062034,2
W4220997345,FedQAS: Privacy-Aware Machine Reading Comprehension with Federated Learning,2022,0.007057444912963424,2
W4399205875,SLaNT: A Semi-supervised Label Noise-Tolerant Framework for Text Sentiment Analysis,2024,0.007057346426676898,2
W2965826089,AmazonQA: A Review-Based Question Answering Task,2019,0.007047491421542036,2
W4318245360,When Transformer models are more compositional than humans: The case of the depth charge illusion,2023,0.007045950576686685,2
W3153069173,A Graph-guided Multi-round Retrieval Method for Conversational Open-domain Question Answering,2021,0.007042201067028456,2
W2987972786,Biomedical Named Entity Recognition with Multilingual BERT,2019,0.007039410221540697,2
W4389520046,Elevating Code-mixed Text Handling through Auditory Information of Words,2023,0.007038699562643833,2
W4393902605,Question-answering system extracts information on injection drug use from clinical notes,2024,0.007030868180884685,2
W4383198091,Multi-class categorization of reasons behind mental disturbance in long texts,2023,0.007029597218353173,2
W3185011771,TAPAS at SemEval-2021 Task 9: Reasoning over tables with intermediate pre-training,2021,0.007029329871477161,2
W2927746189,An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models,2019,0.0070291862771492365,2
W4225716497,Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference,2022,0.007026761245177913,2
W4389523974,Tokenization Consistency Matters for Generative Models on Extractive NLP Tasks,2023,0.007023887417917776,2
W3203587881,Improving Inductive Link Prediction Using Hyper-relational Facts,2021,0.0070155245793293896,2
W2963804400,Integrating Semantic Knowledge to Tackle Zero-shot Text Classification,2019,0.007013318429562368,2
W4385893893,A Better Way to Do Masked Language Model Scoring,2023,0.007010422840134487,2
W4389524352,COMET-M: Reasoning about Multiple Events in Complex Sentences,2023,0.007009474099378713,2
W4311448205,PADL: Language-Directed Physics-Based Character Control,2022,0.0070091262605358395,2
W3162404768,Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction,2021,0.007007639316838146,2
W3101295217,We Can Detect Your Bias: Predicting the Political Ideology of News Articles,2020,0.007007549855917056,2
W4391122821,What is the Consumer Attitude toward Healthcare Services? A Transfer Learning Approach for Detecting Emotions from Consumer Feedback,2024,0.007002635206919886,2
W4366975604,A social media event detection framework based on transformers and swarm optimization for public notification of crises and emergency management,2023,0.006999999813961463,2
W2970789589,Distributionally Robust Language Modeling,2019,0.006995681758431029,2
W3212606841,The Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) Shared Task,2021,0.006994774523868719,2
W4386847948,Ask to Understand: Question Generation for Multi-hop Question Answering,2023,0.006988082413227419,2
W4220902914,Towards Textual Out-of-Domain Detection Without In-Domain Labels,2022,0.006985966054145775,2
W4402353529,QLSC: A Query Latent Semantic Calibrator for Robust Extractive Question Answering,2024,0.006985778936512308,2
W3176245452,Language Models Use Monotonicity to Assess NPI Licensing,2021,0.006980096487908679,2
W3153269634,Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media,2021,0.006977248422319662,2
W2922565841,Linguistic Knowledge and Transferability of Contextual Representations,2019,0.006975467621504297,2
W4375928954,AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference With Transformers,2023,0.006975025925983986,2
W4399577880,"Natural language processing: An overview of models, transformers and applied practices",2024,0.006974530504010585,2
W4385573087,Large language models are few-shot clinical information extractors,2022,0.006971462761073995,2
W2909970382,Grammatical Analysis of Pretrained Sentence Encoders with Acceptability Judgments.,2019,0.00696897729820284,2
W4409202420,The BERT Model Family and Encoder-Only Transformers,2025,0.0069637338094812795,2
W4205927329,What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models,2021,0.006962633776843533,2
W3147292006,Rethink Training of BERT Rerankers in Multi-stage Retrieval Pipeline,2021,0.0069602648840872815,2
W4389523905,How Does Generative Retrieval Scale to Millions of Passages?,2023,0.006958089538432902,2
W4386566925,A Survey of Methods for Addressing Class Imbalance in Deep-Learning Based Natural Language Processing,2023,0.0069531758944107735,2
W4392748121,MedChatZH: A tuning LLM for traditional Chinese medicine consultations,2024,0.00695132820264067,2
W4313555442,Transformer-Based Named Entity Recognition on Drone Flight Logs to Support Forensic Investigation,2023,0.006950473274504034,2
W4401753105,"Detection of Bipolar Disorder on Social Media Data Utilizing Biomedical, Clinical and Mental Health Domain Fine-Tuned Word Embeddings",2024,0.00694533143365827,2
W3120355935,Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval,2021,0.006942382995609519,2
W2952862139,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",2018,0.00694233195793293,2
W4389524005,Improving Bias Mitigation through Bias Experts in Natural Language Understanding,2023,0.006939349315769405,2
W4309532049,Miko Team: Deep Learning Approach for Legal Question Answering in ALQAC 2022,2022,0.00693454758730217,2
W3099548761,Biomedical Event Extraction as Multi-turn Question Answering,2020,0.006932521674600073,2
W2963938442,Semi-Supervised QA with Generative Domain-Adaptive Nets,2017,0.006930887295929696,2
W4407746323,GenKP: generative knowledge prompts for enhancing large language models,2025,0.006926107291039224,2
W3207622241,EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference,2021,0.006925034859623912,2
W4367551721,Mapping Relationship Discovery of Multidimensional Architectures in Autonomous Transportation System Based on Text-Matching Model,2023,0.006921861953430789,2
W2611029872,Efficient Natural Language Response Suggestion for Smart Reply,2017,0.006921257911400017,2
W4385270687,Schema Matching using Pre-Trained Language Models,2023,0.0069187164377524715,2
W3034712001,GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension,2020,0.006913450013147321,2
W4391464262,Large-scale text analysis using generative language models: A case study in discovering public value expressions in AI patents,2024,0.006913018806881561,2
W2921848006,To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks,2019,0.006912566756548605,2
W3084317880,The Graph Reasoning Approach Based on the Dynamic Knowledge Auxiliary for Complex Fact Verification,2020,0.0069066985672419975,2
W3083500347,Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models,2020,0.006904914748245796,2
W3109435212,WabiQA: A Wikipedia-Based Thai Question-Answering System,2020,0.006903764909912076,2
W3208182228,BERT based clinical knowledge extraction for biomedical knowledge graph construction and analysis,2021,0.006901014820139053,2
W2892163801,UKP-Athene: Multi-Sentence Textual Entailment for Claim Verification,2018,0.006900541511628961,2
W2948743095,Episodic Memory in Lifelong Language Learning,2019,0.006899793353003971,2
W4385572451,Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers,2023,0.006895652198004396,2
W4389260541,Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention,2024,0.006891769641259118,2
W2970834904,On the Importance of Delexicalization for Fact Verification,2019,0.006888415893081262,2
W4324098937,Does ChatGPT resemble humans in language use?,2023,0.006885135963532711,2
W3207316101,Identifying Similar Test Cases That Are Specified in Natural Language,2022,0.006883524591105317,2
W4311987334,Refining fine-tuned transformers with hand-crafted features for gender screening on question-answering communities,2022,0.0068827119415132264,2
W4221009220,StruBERT: Structure-aware BERT for Table Search and Matching,2022,0.006881004141654108,2
W3175052694,Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution,2021,0.006875771465706037,2
W4407219056,Comparative analysis of generative LLMs for labeling entities in clinical notes,2025,0.006873714042222924,2
W4287855087,TCU at SemEval-2022 Task 8: A Stacking Ensemble Transformer Model for Multilingual News Article Similarity,2022,0.006873023226969664,2
W4385570415,Counterfactual Active Learning for Out-of-Distribution Generalization,2023,0.00687021259129775,2
W4393032070,SHAPAttack: Shapley-Guided Multigranularity Adversarial Attack Against Text Transformers,2024,0.0068701590881625,2
W4389520333,LM vs LM: Detecting Factual Errors via Cross Examination,2023,0.006868643577630844,2
W3082392125,Reading Comprehension in Czech via Machine Translation and Cross-Lingual Transfer,2020,0.006861870020769358,2
W3035075850,Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification,2020,0.006860310227072511,2
W2963542836,Learning Natural Language Inference with LSTM,2016,0.0068599453147325665,2
W4319025279,SelfCCL: Curriculum Contrastive Learning by Transferring Self-Taught Knowledge for Fine-Tuning BERT,2023,0.006857250004476395,2
W4310331933,LitCovid ensemble learning for COVID-19 multi-label classification,2022,0.006854505885372874,2
W3197309300,Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification,2021,0.006852962981707343,2
W4229029358,XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction,2022,0.006852509264028836,2
W4285188740,Discontinuous Constituency and BERT: A Case Study of Dutch,2022,0.006852069866037505,2
W3145777983,Transformer-Based Approach Towards Music Emotion Recognition from Lyrics,2021,0.0068483851635291245,2
W4385572293,Text Adversarial Purification as Defense against Adversarial Attacks,2023,0.006845324587892591,2
W4393146052,An Enhanced Topic Modeling Method in Educational Domain by Integrating LDA with Semantic,2024,0.0068438159874659715,2
W4385572073,HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks,2023,0.006838238141616108,2
W4327644068,Probing BERT for Ranking Abilities,2023,0.006837996346798529,2
W4386566629,On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex,2023,0.006835777838211448,2
W3153632605,SICK-NL: A Dataset for Dutch Natural Language Inference,2021,0.006830302900847168,2
W4293182797,"Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions",2022,0.006826576018147052,2
W4403195962,Knowledge Editing for Large Language Models: A Survey,2024,0.00682261917732282,2
W4364860029,A Survey on BERT and Its Applications,2023,0.006816982835113963,2
W2940009958,Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey,2019,0.006816650049358275,2
W4383751019,Machine-Generated Text: A Comprehensive Survey of Threat Models and Detection Methods,2023,0.006815663695789728,2
W4385573970,Large Dual Encoders Are Generalizable Retrievers,2022,0.0068100615193009955,2
W4385567227,A Systematic Investigation of Commonsense Knowledge in Large Language Models,2022,0.006809071414979036,2
W2998554035,Rare Words: A Major Problem for Contextualized Embeddings and How to Fix it by Attentive Mimicking,2020,0.006808797899705189,2
W2970078867,Certified Robustness to Adversarial Word Substitutions,2019,0.006806307705588153,2
W4389519585,Sources of Hallucination by Large Language Models on Inference Tasks,2023,0.0068056811855567514,2
W4297459110,Visual Comparison of Language Model Adaptation,2022,0.0068017507363378965,2
W4392849751,<scp>CySecBERT</scp> : A Domain-Adapted Language Model for the Cybersecurity Domain,2024,0.006797236503931154,2
W3197798882,MATE: Multi-view Attention for Table Transformer Efficiency,2021,0.006796776029657631,2
W4383999518,A benchmark for evaluating Arabic contextualized word embedding models,2023,0.006795412002119366,2
W2962891712,Ultra-Fine Entity Typing,2018,0.006793812901674046,2
W4287890938,Pragmatic and Logical Inferences in NLI Systems: The Case of Conjunction Buttressing,2022,0.006792956605149927,2
W3102803571,Improving Neural Topic Models using Knowledge Distillation,2020,0.006791332987971549,2
W3209223032,Large-Scale News Classification using BERT Language Model: Spark NLP Approach,2021,0.006791081627243827,2
W2949959978,Understanding Dataset Design Choices for Multi-hop Reasoning,2019,0.0067908936752278225,2
W3157498557,Let's Play Mono-Poly: BERT Can Reveal Words' Polysemy Level and Partitionability into Senses.,2021,0.006790579088112265,2
W3179963059,Benchmarking for biomedical natural language processing tasks with a domain specific ALBERT,2022,0.006789323631830963,2
W3176178685,Decoding Word Embeddings with Brain-Based Semantic Features,2021,0.006788413647748595,2
W4385574286,Can language models learn from explanations in context?,2022,0.0067871353091277075,2
W3127302468,Memory Augmented Sequential Paragraph Retrieval for Multi-hop Question Answering,2021,0.006786228499740287,2
W4205807230,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,2021,0.00678490720818783,2
W3204121251,ODIST: Open World Classification via Distributionally Shifted Instances,2021,0.006782632699994986,2
W3176001432,Counterfactual Inference for Text Classification Debiasing,2021,0.006774799631220397,2
W3167831019,Probing for Bridging Inference in Transformer Language Models,2021,0.0067725804034873965,2
W4283798680,Hybrid Autoregressive Inference for Scalable Multi-Hop Explanation Regeneration,2022,0.00677200575852524,2
W4385570088,Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models,2023,0.006771880291521916,2
W4391401609,Are my answers medically accurate? Exploiting medical knowledge graphs for medical question answering,2024,0.006768165439087657,2
W4386422441,SiMaLSTM-SNP: novel semantic relatedness learning model preserving both Siamese networks and membrane computing,2023,0.006768038246801144,2
W4385573038,A Framework for Adapting Pre-Trained Language Models to Knowledge Graph Completion,2022,0.0067672200934861294,2
W3148237218,TAPAS at SemEval-2021 Task 9: Reasoning over tables with intermediate pre-training,2021,0.006764696894406428,2
W4376876984,An empirical study of pre-trained language models in simple knowledge graph question answering,2023,0.006762933035015481,2
W3192405822,Noisy Channel Language Model Prompting for Few-Shot Text Classification,2022,0.006760980549745681,2
W4226244192,ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps,2022,0.006757818689549132,2
W2955315229,Deep Learning for Natural Language Inference,2019,0.006756941786056105,2
W4385573102,Training Language Models with Memory Augmentation,2022,0.00675308359016164,2
W4408735404,Estimating the plausibility of commonsense statements by novelly fusing large language model and graph neural network,2025,0.006752935072688518,2
W4385570234,To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering,2023,0.006749508701323437,2
W3159343375,RECAST,2021,0.006746366253036559,2
W3034608141,Zero-shot Text Classification via Reinforced Self-training,2020,0.006746074170514843,2
W4409955947,BERT-Prompt Based Equipment to Support Domain Sentence Vector Training,2025,0.006745416881537047,2
W3122244192,Building Interpretable Interaction Trees for Deep NLP Models,2021,0.006740311851256155,2
W4386607580,Benchmarks for Automated Commonsense Reasoning: A Survey,2023,0.00673841162152983,2
W4385570378,LRL_NC at SemEval-2023 Task 6: Sequential Sentence Classification for Legal Documents Using Topic Modeling Features,2023,0.006733314237010518,2
W4398774699,Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation,2024,0.006731675556348841,2
W4385570484,Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation,2023,0.006731675556348841,2
W4402352514,Defect Correction Method for Software Requirements Text Using Large Language Models,2024,0.006731504429843155,2
W2972969579,Knowledge Enhanced Contextual Word Representations,2019,0.006731011160082476,2
W4225454120,URLTran: Improving Phishing URL Detection Using Transformers,2021,0.006730510214950051,2
W4389519078,Linking Surface Facts to Large-Scale Knowledge Graphs,2023,0.006728049680516955,2
W3212511129,"Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",2022,0.00672774368553118,2
W3136991969,Tiny Transformers for Environmental Sound Classification at the Edge,2021,0.006725660038351872,2
W4308939312,Is neuro-symbolic AI meeting its promises in natural language processing? A structured review,2022,0.006724980080375637,2
W4221052873,ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling,2022,0.006722003478001158,2
W4287887934,Boosted Dense Retriever,2022,0.006720513464068761,2
W2789078277,Hierarchical Attention Flow for Multiple-Choice Reading Comprehension,2018,0.0067181415354617075,2
W3177156688,Leveraging Type Descriptions for Zero-shot Named Entity Recognition and Classification,2021,0.006714157400568694,2
W3201503287,MEM-KGC: Masked Entity Model for Knowledge Graph Completion With Pre-Trained Language Model,2021,0.006711134903452358,2
W2945878859,Learning to Denoise Distantly-Labeled Data for Entity Typing,2019,0.006709894211264335,2
W3102466593,Word Rotator’s Distance,2020,0.006709710490694656,2
W4410548482,AKER: Arabic Knowledge-Enriched Reader for Machine Reading Comprehension,2025,0.006708518386775819,2
W3212166010,PalmTree: Learning an Assembly Language Model for Instruction Embedding,2021,0.006708394868751306,2
W3171953676,Human Sentence Processing: Recurrence or Attention?,2021,0.006706786760966276,2
W4287855004,Fine-tuning Transformers with Additional Context to Classify Discursive Moves in Mathematics Classrooms,2022,0.006703200986174533,2
W3206379359,Improving Multi-Party Dialogue Discourse Parsing via Domain Integration,2021,0.0066985284499579325,2
W3045600569,Stance Prediction and Claim Verification: An Arabic Perspective,2020,0.006698010130900911,2
W4285223485,"E8-IJS@LT-EDI-ACL2022 - BERT, AutoML and Knowledge-graph backed Detection of Depression",2022,0.006697630571584331,2
W3035064549,Cross-Linguistic Syntactic Evaluation of Word Prediction Models,2020,0.0066940998064295935,2
W2798459010,Stochastic Answer Networks for Natural Language Inference,2018,0.006690089886092627,2
W4385574224,BioSimCSE: BioMedical Sentence Embeddings using Contrastive learning,2022,0.006687842528804421,2
W3162734203,A Survey of Data Augmentation Approaches for NLP,2021,0.006686224104812177,2
W4285241989,Exploring the Impact of Negative Samples of Contrastive Learning: A Case Study of Sentence Embedding,2022,0.006683181987479753,2
W3034998021,"Rˆ3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge",2020,0.006679450222056093,2
W3109752844,SLICE: Supersense-based Lightweight Interpretable Contextual Embeddings,2020,0.00667874047412964,2
W4319662676,Unified benchmark for zero-shot Turkish text classification,2023,0.0066780611481299535,2
W3103163889,Detection of Mental Health from Reddit via Deep Contextualized Representations,2020,0.006676954753588783,2
W4323065879,Knowledge Graph-Based Reinforcement Federated Learning for Chinese Question and Answering,2023,0.006673495247494798,2
W2973722444,Teaching Pretrained Models with Commonsense Reasoning: A Preliminary KB-Based Approach,2019,0.0066722163746605,2
W2889646190,Exploring Graph-structured Passage Representation for Multi-hop Reading Comprehension with Graph Neural Networks,2018,0.006668493071784364,2
W4285022096,Correspondence between the layered structure of deep language models and temporal structure of natural language processing in the human brain,2022,0.006665867942437329,2
W4205266770,MFAQ: a Multilingual FAQ Dataset,2021,0.006665255465260203,2
W4385573021,Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer,2022,0.006663756134819984,2
W3134642945,Measuring Mathematical Problem Solving With the MATH Dataset,2021,0.006656158980779132,2
W4200630739,Open Vocabulary Electroencephalography-to-Text Decoding and Zero-Shot Sentiment Classification,2022,0.006655220376253572,2
W2562979205,Understanding Neural Networks through Representation Erasure,2016,0.006653820798005758,2
W4406276517,Knowledge Enhanced Language Model for Biomedical Natural Language Processing: Introducing a New Language Model for BioNLP,2025,0.006650123303304343,2
W4385571011,Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes,2023,0.006643609454872398,2
W3087714553,Multi-turn intent determination and slot filling with neural networks and regular expressions,2020,0.0066417715688085474,2
W4287889356,Cross-Domain Detection of GPT-2-Generated Technical Text,2022,0.006640672538808993,2
W4385573205,Missing Counter-Evidence Renders NLP Fact-Checking Unrealistic for Misinformation,2022,0.006639273716622402,2
W2966176804,Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition,2019,0.0066392574561579655,2
W3152559633,Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval,2021,0.006635764835849671,2
W4287887311,SemEval-2022 Task 3: PreTENS-Evaluating Neural Networks on Presuppositional Semantic Knowledge,2022,0.0066297921956569635,2
W4289865932,R&lt;sup&gt;3&lt;/sup&gt;: Reinforced Ranker-Reader for Open-Domain Question Answering,2018,0.0066279791799146535,2
W4399426475,Scope Ambiguities in Large Language Models,2024,0.0066266245669958125,2
W4287890652,Natural Language Inference with Self-Attention for Veracity Assessment of Pandemic Claims,2022,0.006626212922336191,2
W4400926865,Matching tasks to objectives: Fine-tuning and prompt-tuning strategies for encoder-decoder pre-trained language models,2024,0.006625755111165885,2
W3184599456,A Differentiable Language Model Adversarial Attack on Text Classifiers,2022,0.0066242112334757635,2
W4385572819,SafeText: A Benchmark for Exploring Physical Safety in Language Models,2022,0.006623692136427038,2
W4285161221,Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?,2022,0.006623399247034486,2
W3098323839,GLUCOSE: GeneraLized and COntextualized Story Explanations,2020,0.006623381023618607,2
W4389519322,Connecting Symbolic Statutory Reasoning with Legal Information Extraction,2023,0.006621982099197563,2
W4375869395,NCL: Textual Backdoor Defense Using Noise-Augmented Contrastive Learning,2023,0.00661801284896971,2
W4321242712,Evaluating Deep Learning Techniques for Natural Language Inference,2023,0.006617118988981687,2
W4385567121,Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering,2022,0.006614507361065977,2
W4283793506,TempoQR: Temporal Question Reasoning over Knowledge Graphs,2022,0.0066140946448753025,2
W2998811572,CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark for Chinese,2020,0.006613999133396666,2
W2969574947,Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring,2019,0.006613470481086094,2
W4398757454,Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering,2024,0.0066129028558497924,2
W3203502727,A proposed conceptual framework for a representational approach to information retrieval,2021,0.006611961729336348,2
W3155381456,Scientific Claim Verification with VERT5ERINI,2020,0.0066117790011756454,2
W4206236515,Explanation-Based Human Debugging of NLP Models: A Survey,2021,0.006610321202705634,2
W2970103342,Applying BERT to Document Retrieval with Birch,2019,0.006609042288241371,2
W4288368497,Shared functional specialization in transformer-based language models and the human brain,2022,0.006607830906368363,2
W3175291199,Robustifying Multi-hop QA through Pseudo-Evidentiality Training,2021,0.006607537160452236,2
W4321229682,CLSEP: Contrastive learning of sentence embedding with prompt,2023,0.006604153507536131,2
W2753329127,R$^3$: Reinforced Reader-Ranker for Open-Domain Question Answering,2017,0.006604140858947406,2
W4385571522,CSECU-DSG at SemEval-2023 Task 6: Segmenting Legal Documents into Rhetorical Roles via Fine-tuned Transformer Architecture,2023,0.0066029330527380995,2
W4390884707,A Knowledge-enhanced Two-stage Generative Framework for Medical Dialogue Information Extraction,2024,0.006597559821187549,2
W4385571276,Sequential Integrated Gradients: a simple but effective method for explaining language models,2023,0.006596601221966346,2
W3037116584,Interpretability and Analysis in Neural NLP,2020,0.006596270226755216,2
W3141235929,Natural language processing,2021,0.006593770200762592,2
W3012070096,Supervised and unsupervised language modelling in Chest X-Ray radiological reports,2020,0.006592197237436808,2
W4408864216,Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline,2025,0.006591233088405087,2
W3199207650,Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning,2021,0.006589385671759385,2
W4225624948,LPViT: A Transformer Based Model for PCB Image Classification and Defect Detection,2022,0.006586894948526122,2
W4307821210,Exploring Dimensionality Reduction Techniques in Multilingual Transformers,2022,0.0065812135228277896,2
W3023293091,Temporal Common Sense Acquisition with Minimal Supervision,2020,0.0065795293361164305,2
W3195376057,A Dataset for Answering Time-Sensitive Questions,2021,0.006575647205798731,2
W4383368985,Enhancing multiple-choice question answering through sequential fine-tuning and Curriculum Learning strategies,2023,0.006574506533243729,2
W4385567096,Does Your Model Classify Entities Reasonably? Diagnosing and Mitigating Spurious Correlations in Entity Typing,2022,0.006573483027677698,2
W3205301323,Misinfo Reaction Frames: Reasoning about Readers’ Reactions to News Headlines,2022,0.0065723103954090385,2
W4385572738,Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations,2022,0.006572094664630639,2
W3016339201,The Cost of Training NLP Models: A Concise Overview,2020,0.006571601487924357,2
W2891308403,Transforming Question Answering Datasets Into Natural Language Inference Datasets,2018,0.006569589283640649,2
W3135542296,"Automated Coding of Under-Studied Medical Concept Domains: Linking Physical Activity Reports to the International Classification of Functioning, Disability, and Health",2021,0.00656933117994632,2
W3120860016,"Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences",2021,0.006568920860963868,2
W4391651108,Tabular reasoning via two-stage knowledge injection,2024,0.006567764730120236,2
W4384705353,ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs,2023,0.006565994569255103,2
W2986191653,Cost-Sensitive BERT for Generalisable Sentence Classification on Imbalanced Data,2019,0.006565814823053563,2
W4385570982,Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions,2023,0.0065596361473881735,2
W4389519118,Active Retrieval Augmented Generation,2023,0.006556657773271125,2
W4392669904,Joint Representations of Text and Knowledge Graphs for Retrieval and Evaluation,2023,0.0065531057706220835,2
W3167841296,Case Study: Deontological Ethics in NLP,2021,0.006552926847279689,2
W3101479482,Generating Label Cohesive and Well-Formed Adversarial Claims,2020,0.006552405723149089,2
W3148364056,Classifying Scientific Publications with BERT - Is Self-attention a Feature Selection Method?,2021,0.006549634971934393,2
W3015202090,Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence,2020,0.0065495737335616085,2
W2913129712,"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",2019,0.006549206155590329,2
W3097239661,Is Graph Structure Necessary for Multi-hop Question Answering?,2020,0.006548755228323388,2
W3174781928,Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks,2021,0.006541028537340388,2
W2756386045,Natural Language Inference over Interaction Space,2017,0.006540172597480317,2
W4390750480,Fraud's Bargain Attack: Generating Adversarial Text Samples via Word Manipulation Process,2024,0.006538367695540871,2
W2949818215,HEAD-QA: A Healthcare Dataset for Complex Reasoning,2019,0.0065360474446416635,2
W4383678258,Efficient Domain Adaptation of Sentence Embeddings Using Adapters,2023,0.006532373247696746,2
W4388676985,Enhancing Medical Text Representation for Lung Diagnosis Prediction Via Knowledge Infusion,2023,0.006532240888966749,2
W3161694370,Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking,2021,0.006526767941145336,2
W4285226597,WatClaimCheck: A new Dataset for Claim Entailment and Inference,2022,0.0065180637090819895,2
W3213645763,“Will You Find These Shortcuts?” A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification,2022,0.0065178913675818925,2
W3015347994,The Russian Drug Reaction Corpus and neural models for drug reactions and effectiveness detection in user reviews,2020,0.006516647455974259,2
W4385571150,Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts,2023,0.0065155740107879405,2
W3104104399,Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers,2020,0.006514690776098615,2
W3173109101,On learning and representing social meaning in NLP: a sociolinguistic perspective,2021,0.0065059571267265725,2
W4393147193,Editing Language Model-Based Knowledge Graph Embeddings,2024,0.006504641211229387,2
W2964212550,ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations,2018,0.006501093456830508,2
W3175910938,Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision,2021,0.006497367623849382,2
W2964082993,Semantic Sentence Matching with Densely-Connected Recurrent and Co-Attentive Information,2019,0.006494894882461894,2
W3034503357,Code and Named Entity Recognition in StackOverflow,2020,0.006492698861669099,2
W4387521241,BlendCSE: Blend contrastive learnings for sentence embeddings with rich semantics and transferability,2023,0.006489287722858725,2
W4317829471,PLM-AS: Pre-trained Language Models Augmented with Scanpaths for Sentiment Classification,2023,0.006489154133111423,2
W3198963017,Reframing Instructional Prompts to GPTk’s Language,2022,0.006488949190460668,2
W3129160532,A pre-training and self-training approach for biomedical named entity recognition,2021,0.006486378356700843,2
W4365813230,Siamese Interaction and Fine-Tuning Representation of Chinese Semantic Matching Algorithm Based on RoBERTa-wwm-ext,2023,0.0064850797308828815,2
W4385573593,Temporal Word Meaning Disambiguation using TimeLMs,2022,0.006484569324385511,2
W3084489656,Multi-Hop Fact Checking of Political Claims,2020,0.0064809123784710385,2
W4389683743,A survey of inductive knowledge graph completion,2023,0.006478991881589962,2
W4385571561,LTRC at SemEval-2023 Task 6: Experiments with Ensemble Embeddings,2023,0.006476009719918027,2
W2963249435,AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples,2018,0.006473852552164849,2
W4287889344,SemEval-2022 Task 4: Patronizing and Condescending Language Detection,2022,0.006468711118135462,2
W3155762665,EmpathBERT: A BERT-based Framework for Demographic-aware Empathy Prediction,2021,0.00646864490241073,2
W4406615479,Safety analysis in the era of large language models: A case study of STPA using ChatGPT,2025,0.0064667451453957366,2
W3174987059,CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding,2021,0.006464466435901044,2
W4409806408,Addressing Asymmetry in Contrastive Learning: LLM-Driven Sentence Embeddings with Ranking and Label Smoothing,2025,0.006463535613112218,2
W4400142326,Knowledge-injected prompt learning for actionable information extraction from crisis-related tweets,2024,0.006462916447586374,2
W3171416056,TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names,2021,0.006461986106953979,2
W4318391587,DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing,2023,0.006461164868759103,2
W4399530301,Automatic Knowledge Structuration of Automotive User Manual for Question Answering,2023,0.0064583067973207255,2
W4249001067,"Closed and Open Vocabulary Approaches to Text Analysis: A Review, Quantitative Comparison, and Recommendations",2020,0.006457511726921197,2
W3035317046,tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection,2020,0.006456025484802155,2
W4385848332,Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT,2023,0.006455201476399509,2
W4388764805,The unreasonable effectiveness of large language models in zero-shot semantic annotation of legal texts,2023,0.006454907157603761,2
W3133650345,Bidirectional Representation Learning From Transformers Using Multimodal Electronic Health Record Data to Predict Depression,2021,0.0064541833413887485,2
W3043372854,Meta-learning for Few-shot Natural Language Processing: A Survey,2020,0.006451547304041826,2
W3152711910,Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection,2021,0.0064500213959408975,2
W2978832950,Distilling Transformers into Simple Neural Networks with Unlabeled Transfer Data.,2019,0.00644939883292149,2
W2996287690,The Curious Case of Neural Text Degeneration,2020,0.006449047682978291,2
W4389518761,Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation,2023,0.006439779414627812,2
W2964348592,HAS-QA: Hierarchical Answer Spans Model for Open-Domain Question Answering,2019,0.0064384026939860485,2
W4292963003,UnCommonSense: Informative Negative Knowledge about Everyday Concepts,2022,0.006436056090166368,2
W2964222268,Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information,2018,0.006435959951818632,2
W3175795662,ReTraCk: A Flexible and Efficient Framework for Knowledge Base Question Answering,2021,0.006423416661083934,2
W3034156543,Patent classification by fine-tuning BERT language model,2020,0.006422957277780112,2
W4403173949,Knowledge Augmented Intelligence Using Large Language Models for Advanced Data Analytics,2024,0.006422854277704489,2
W4382202847,Contrastive Learning Reduces Hallucination in Conversations,2023,0.006421503271605119,2
W4385573529,TIARA: Multi-grained Retrieval for Robust Question Answering over Large Knowledge Base,2022,0.006418949878980229,2
W3135098861,Does the Magic of BERT Apply to Medical Code Assignment? A Quantitative Study,2021,0.006416227301912898,2
W4385572570,Language Model Analysis for Ontology Subsumption Inference,2023,0.006415283516949885,2
W2912351236,An Analysis of Encoder Representations in Transformer-Based Machine Translation,2018,0.006414604861217679,2
W4317493055,Sentiment enhanced answer generation and information fusing for product-related question answering,2023,0.00641378978750733,2
W4226218072,DREAM: Improving Situational QA by First Elaborating the Situation,2022,0.00641362256143752,2
W2259472270,Exploring the Limits of Language Modeling,2016,0.006409145592592294,2
W4385734176,Training Data Extraction From Pre-trained Language Models: A Survey,2023,0.006408971325476096,2
W4212926012,Effects of Similarity Score Functions in Attention Mechanisms on the Performance of Neural Question Answering Systems,2022,0.006407252517545588,2
W4406998844,TurkMedNLI: a Turkish medical natural language inference dataset through large language model based translation,2025,0.0064048554679120625,2
W3170403598,Are NLP Models really able to Solve Simple Math Word Problems?,2021,0.006402914878556527,2
W4393156193,G-Adapter: Towards Structure-Aware Parameter-Efficient Transfer Learning for Graph Transformer Networks,2024,0.006401666757597274,2
W3175582404,Self-Supervised Pillar Motion Learning for Autonomous Driving,2021,0.006399254419290962,2
W3098405901,Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation,2020,0.006397414077108828,2
W4380136067,Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts,2023,0.006396658533790161,2
W4285123725,On the Importance of Effectively Adapting Pretrained Language Models for Active Learning,2022,0.00639562095415839,2
W4225297420,OPERA: Operation-Pivoted Discrete Reasoning over Text,2022,0.006391821257043611,2
W3209254806,Does the magic of BERT apply to medical code assignment? A quantitative study,2021,0.006390810051119346,2
W3199422761,BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition,2021,0.006383189685038248,2
W4385571301,Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting,2023,0.006377218183645965,2
W4390684394,Topics in the Haystack: Enhancing Topic Quality through Corpus Expansion,2024,0.00637274610064313,2
W3021813138,Data Augmentation for Spoken Language Understanding via Pretrained Models.,2020,0.006368756579464674,2
W4385573708,Injecting Domain Knowledge in Language Models for Task-oriented Dialogue Systems,2022,0.006366852069431995,2
W2938224028,Understanding the Behaviors of BERT in Ranking,2019,0.006362595766338394,2
W3139632155,Evidence-based Verification for Real World Information Needs.,2021,0.006360162456912084,2
W4409019114,Enhancing Language Models via HTML DOM Tree for Text Structure Understanding,2025,0.006357495126008619,2
W4385565472,DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering,2023,0.006356925441644501,2
W3103940211,AxCell: Automatic Extraction of Results from Machine Learning Papers,2020,0.006354230076790317,2
W4385572615,ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding,2023,0.00634704595781384,2
W4391903691,"Introduction to the Minitrack on Diversity, Equity, and Inclusion in Digital Government: Narrowing the Divides",2023,0.006346690569839242,2
W3091759883,A university map of course knowledge,2020,0.006344808972527879,2
W4386302269,Continual Learning for Generative Retrieval over Dynamic Corpora,2023,0.006342468005130715,2
W3034707327,BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval,2020,0.006334600359863033,2
W4389524463,Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators,2023,0.006334025017302075,2
W2950695840,Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives,2019,0.006330649358146845,2
W3211805281,Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification,2021,0.00632081386319838,2
W4385573451,Sentence-level Media Bias Analysis Informed by Discourse Structures,2022,0.006314699732694666,2
W2970868452,Improving classification of Adverse Drug Reactions through Using Sentiment Analysis and Transfer Learning,2019,0.006310335733914334,2
W3120588017,Reddit entity linking dataset,2021,0.006309536006807135,2
W3034834827,Understanding Attention for Text Classification,2020,0.006306628795552625,2
W3102793471,Document Classification for COVID-19 Literature,2020,0.006306332823791395,2
W4385571098,Data Curation Alone Can Stabilize In-context Learning,2023,0.006304031962883208,2
W3116179800,Incorporating Syntax and Frame Semantics in Neural Network for Machine Reading Comprehension,2020,0.006303845669984529,2
W4385187297,Deepfake Text Detection: Limitations and Opportunities,2023,0.0063037312242412765,2
W4385570380,AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models,2023,0.0063009239986552236,2
W4385573569,Are Large Pre-Trained Language Models Leaking Your Personal Information?,2022,0.006299712098768154,2
W3209096362,Evaluating Unsupervised Text Embeddings on Software User Feedback,2021,0.006297570395216764,2
W2969282568,Visual Interaction with Deep Learning Models through Collaborative Semantic Inference,2019,0.006297542496267734,2
W3164323420,A systematic review of natural language processing applied to radiology reports,2021,0.006297258426725819,2
W2984124903,A Review of Automated Speech and Language Features for Assessment of Cognitive and Thought Disorders,2019,0.006294805763035997,2
W4385573365,PAR: Political Actor Representation Learning with Social Context and Expert Knowledge,2022,0.0062928523208629055,2
W2963859254,TextBugger: Generating Adversarial Text Against Real-world Applications,2019,0.006292760272050593,2
W3176512822,Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification,2021,0.006292516928304589,2
W4380609152,Instance-Aware Prompt Learning for Language Understanding and Generation,2023,0.00629064015864855,2
W4381799451,Politically-oriented information inference from text,2023,0.006283284754375736,2
W3211641885,HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge,2021,0.006283110398178753,2
W4387074826,A Practical Survey on Zero-shot Prompt Design for In-context Learning,2023,0.006281223586526963,2
W4287887329,Contrastive Data and Learning for Natural Language Processing,2022,0.006281202288938903,2
W4285261371,Efficient Classification of Long Documents Using Transformers,2022,0.006279800293578836,2
W3109356881,Second-Order NLP Adversarial Examples,2020,0.00627828940862745,2
W3168552854,Causal Effects of Linguistic Properties,2021,0.006274138416169758,2
W4385282679,Tele-Knowledge Pre-training for Fault Analysis,2023,0.006273357913560407,2
W4392637089,LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings,2023,0.006272977098180623,2
W2998184481,Do Not Have Enough Data? Deep Learning to the Rescue!,2020,0.006268637289498438,2
W4389520553,Adversarial Robustness for Large Language NER models using Disentanglement and Word Attributions,2023,0.006265355541638963,2
W4385572408,MolXPT: Wrapping Molecules with Text for Generative Pre-training,2023,0.006264183545655133,2
W4312605747,Multi-task Active Learning for Pre-trained Transformer-based Models,2022,0.006263123400390337,2
W3035025198,A Frame-based Sentence Representation for Machine Reading Comprehension,2020,0.006262736183468653,2
W4313420283,Generating knowledge aware explanation for natural language inference,2022,0.006262733524897865,2
W3022373106,Complementing Lexical Retrieval with Semantic Residual Embedding,2020,0.006261532896559996,2
W3034287667,Automatic Detection of Generated Text is Easiest when Humans are Fooled,2020,0.006259689920202523,2
W4399116719,An Efficient Corpus Indexer for dynamic corpora retrieval,2024,0.0062588253042878605,2
W4385570476,TeamUnibo at SemEval-2023 Task 6: A transformer based approach to Rhetorical Roles prediction and NER in Legal Texts,2023,0.006258322157108636,2
W4387058885,Rationalization for explainable NLP: a survey,2023,0.006257353697204974,2
W4385569957,Query Structure Modeling for Inductive Logical Reasoning Over Knowledge Graphs,2023,0.006256734107494979,2
W4385572435,Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting,2023,0.0062543996067313794,2
W3171244865,Open-Domain Question Answering Goes Conversational via Question Rewriting,2021,0.0062542237827981,2
W4385572179,nclu_team at SemEval-2023 Task 6: Attention-based Approaches for Large Court Judgement Prediction with Explanation,2023,0.006252718978796708,2
W4313187582,Mr.BiQ: Post-Training Non-Uniform Quantization based on Minimizing the Reconstruction Error,2022,0.0062482329233420585,2
W4389519087,Make Every Example Count: On the Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets,2023,0.006242770266553273,2
W4206962558,Question answering with deep neural networks for semi-structured heterogeneous genealogical knowledge graphs,2022,0.006241081169835055,2
W4401110069,Semantic Textual Similarity Analysis of Clinical Text in the Era of LLM,2024,0.0062353505508279845,2
W2998901379,Learning Cross-Context Entity Representations from Text,2020,0.006232070861290154,2
W3008736151,Hierarchical Transformers for Long Document Classification,2019,0.0062286152125069025,2
W2962829230,ThisIsCompetition at SemEval-2019 Task 9: BERT is unstable for out-of-domain samples,2019,0.006227980931144494,2
W3175866696,LIREx: Augmenting Language Inference with Relevant Explanations,2021,0.006225755828538642,2
W3091091747,A BERT-based Approach with Relation-aware Attention for Knowledge Base Question Answering,2020,0.006225409960312043,2
W3175366715,Bird’s Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach,2021,0.006225205740653392,2
W4385573400,Inferring Implicit Relations in Complex Questions with Language Models,2022,0.006224143811834277,2
W3158086504,GermanQuAD and GermanDPR: Improving Non-English Question Answering and Passage Retrieval,2021,0.006223846609509901,2
W2963149412,Learning Semantic Textual Similarity from Conversations,2018,0.006218513134712405,2
W2995649781,Neural Module Networks for Reasoning over Text,2020,0.006214214206781924,2
W4385768243,Fine-tuned vs. Prompt-tuned Supervised Representations: Which Better Account for Brain Language Representations?,2023,0.006214131740513616,2
W3117163888,UPB at SemEval-2020 Task 11: Propaganda Detection with Domain-Specific Trained BERT,2020,0.0062128611146847245,2
W3155192455,CLiMP: A Benchmark for Chinese Language Model Evaluation,2021,0.006210224780624901,2
W4224215744,SensiMix: Sensitivity-Aware 8-bit index &amp; 1-bit value mixed precision quantization for BERT compression,2022,0.006209928582835431,2
W4388743339,STMAP: A novel semantic text matching model augmented with embedding perturbations,2023,0.006209165012311923,2
W4394730685,Lookahead Bias in Pretrained Language Models,2024,0.006207177605774386,2
W4379986648,"Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI",2023,0.0062051953305662134,2
W2808308446,Multiway Attention Networks for Modeling Sentence Pairs,2018,0.006201013585157381,2
W2963783335,Crowdsourcing Question-Answer Meaning Representations,2018,0.006199390522187414,2
W3130740619,Overview of the TREC 2019 deep learning track,2020,0.006195809286823226,2
W4385573352,Detecting Relevant Differences Between Similar Legal Texts,2022,0.006185357513822706,2
W2771275742,Inference is Everything: Recasting Semantic Resources into a Unified Evaluation Framework,2017,0.006182828654679941,2
W4388924707,Language augmentation approach for code-mixed text classification,2023,0.006181057085390506,2
W2913222130,Generative Question Answering: Learning to Answer the Whole Question.,2018,0.006180502262292724,2
W3156248418,Scientific Discourse Tagging for Evidence Extraction,2021,0.006177504644042616,2
W4401608846,Enhancing Cyberbullying Detection on Social Media Using Transformer Models,2024,0.006174630929835429,2
W4391876565,Red Teaming Language Model Detectors with Language Models,2024,0.006170807612394146,2
W4402701904,Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports,2024,0.006165974804463827,2
W4393146985,Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text Classification via Anchor Generation and Classification Reframing,2024,0.006165525095758499,2
W4390604950,"Completeness, Recall, and Negation in Open-world Knowledge Bases: A Survey",2024,0.006164578128800366,2
W4391277633,Evaluation of Medium-large Language Models at Zero-shot Closed Book Generative Question Answering,2024,0.0061604774410276356,2
W4385572276,Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning,2023,0.00615853948425972,2
W3138392969,Are NLP Models really able to Solve Simple Math Word Problems,2021,0.006155983680393426,2
W3174342531,Unified Dual-view Cognitive Model for Interpretable Claim Verification,2021,0.00615065982444004,2
W3101767350,Sentence embeddings in NLI with iterative refinement encoders,2019,0.006150419136039362,2
W4205377692,Learning with Instance Bundles for Reading Comprehension,2021,0.006150302339470239,2
W4392669760,Retrieval Augmented Generation with Rich Answer Encoding,2023,0.006140774372970021,2
W4283797513,Unsupervised Sentence Representation via Contrastive Learning with Mixing Negatives,2022,0.00613954476264774,2
W4292793781,The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization,2022,0.006133800400676968,2
W2984469754,Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,2019,0.006133713170548663,2
W4319587062,BERT-based Chinese Medicine Named Entity Recognition Model Applied to Medication Reminder Dialogue System,2022,0.006133539534642282,2
W4394805293,Unsupervised Sentence Representation Learning with Frequency-induced Adversarial tuning and Incomplete sentence filtering,2024,0.006132355784140787,2
W4402727837,LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction,2024,0.006131310041820718,2
W4250944796,Proceedings of the 13th Linguistic Annotation Workshop,2019,0.0061281556992249095,2
W2947012833,Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain),2019,0.006124185061202265,2
W2804243436,Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog,2018,0.0061240793554148895,2
W4307367874,Aggregating pairwise semantic differences for few-shot claim verification,2022,0.006122239792534688,2
W2997636389,Unsupervised Domain Adaptation on Reading Comprehension,2020,0.006121243797245261,2
W3174311454,SSMix: Saliency-Based Span Mixup for Text Classification,2021,0.006117902800224815,2
W4385570547,An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text,2023,0.006116731838672567,2
W4410197203,Knowledge-enhanced Parameter-efficient Transfer Learning with METER for medical vision-language tasks,2025,0.006115685823893952,2
W3002226419,Named Entity Recognition Using BERT BiLSTM CRF for Chinese Electronic Health Records,2019,0.006115038151978792,2
W3152979241,DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense Knowledge,2021,0.006113010988942709,2
W3171899052,“I’m Not Mad”: Commonsense Implications of Negation and Contradiction,2021,0.006110463973280883,2
W3085552234,QED: A Framework and Dataset for Explanations in Question Answering,2020,0.006105759602345542,2
W4385571765,Exploring Zero and Few-shot Techniques for Intent Classification,2023,0.006105081994484146,2
W4389524441,Generative Table Pre-training Empowers Models for Tabular Prediction,2023,0.006103833222351207,2
W3206730738,StaResGRU-CNN with CMedLMs: A stacked residual GRU-CNN with pre-trained biomedical language models for predictive intelligence,2021,0.006099511590504326,2
W3034873522,Obtaining Faithful Interpretations from Compositional Neural Networks,2020,0.006095551278371247,2
W4225156065,On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model,2022,0.006094823192180626,2
W2949884065,ChID: A Large-scale Chinese IDiom Dataset for Cloze Test,2019,0.006087990914873899,2
W3212187545,Does BERT Understand Idioms? A Probing-Based Empirical Study of BERT Encodings of Idioms,2021,0.006086545904619472,2
W2955654429,MCScript2.0: A Machine Comprehension Corpus Focused on Script Events and Participants,2019,0.006085681169943231,2
W3157700644,Adaptive Semiparametric Language Models,2021,0.0060808222267392,2
W4285265874,Knowledge-Augmented Language Models for Cause-Effect Relation Classification,2022,0.0060794318318772135,2
W3115511229,LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression,2020,0.006079310042452661,2
W4385573195,RuCoLA: Russian Corpus of Linguistic Acceptability,2022,0.006077801485875336,2
W4389037425,Contextualized Sentiment Analysis using Large Language Models,2023,0.006071171809572944,2
W3035051781,Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention,2020,0.006064167152270206,2
W2964669873,T-CVAE: Transformer-Based Conditioned Variational Autoencoder for Story Completion,2019,0.00606361759479761,2
W4306317476,Cross-Domain Aspect Extraction using Transformers Augmented with Knowledge Graphs,2022,0.006060661778734127,2
W3172076499,Automatic Construction of Evaluation Suites for Natural Language Generation Datasets,2021,0.006060614618671302,2
W4379382656,Zero-Shot and Few-Shot Learning With Knowledge Graphs: A Comprehensive Survey,2023,0.0060577504222737895,2
W4292939148,Data-Centric and Model-Centric Approaches for Biomedical Question Answering,2022,0.006057492000242029,2
W3215317537,Med-BERT: A Pretraining Framework for Medical Records Named Entity Recognition,2021,0.006055404915365096,2
W4287887646,Pretrained Models for Multilingual Federated Learning,2022,0.006053742276645194,2
W4285254085,Improving the Adversarial Robustness of NLP Models by Information Bottleneck,2022,0.006048925721368399,2
W3015569920,Evaluating Machines by their Real-World Language Use,2020,0.006045088094001301,2
W3102373121,Text Segmentation by Cross Segment Attention,2020,0.006040107375914235,2
W3199969499,Coarse2Fine: Fine-grained Text Classification on Coarsely-grained Annotated Data,2021,0.006039251368100734,2
W4394993638,ArEntail: manually-curated Arabic natural language inference dataset from news headlines,2024,0.0060366038717360425,2
W4389519044,"Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4",2023,0.0060364403474669395,2
W3167528419,AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance Detection for Fact Checking,2021,0.006035483296221029,2
W2964159778,Visualizing and Understanding Neural Models in NLP,2016,0.00602878297533656,2
W3037082750,TURL: Table Understanding through Representation Learning,2020,0.006022025505335922,2
W3171388604,An Empirical Survey of Data Augmentation for Limited Data Learning in NLP,2021,0.006015219442169055,2
W4387708435,Multi-task Pre-training Language Model for Semantic Network Completion,2023,0.006009372178329138,2
W3186138538,Internet-Augmented Dialogue Generation,2022,0.006005587084893708,2
W2769395616,Evidence Aggregation for Answer Re-Ranking in Open-Domain Question Answering,2017,0.006005369772568183,2
W4226365369,Relational Memory-Augmented Language Models,2022,0.006004947220061007,2
W2898858752,MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller,2018,0.0060036514434220825,2
W4389518708,"Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",2023,0.006003014066391044,2
W4385392860,SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples,2023,0.00599917909058809,2
W4385573354,Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial NLP,2022,0.005998011545760521,2
W4285146641,Towards Improving Selective Prediction Ability of NLP Systems,2022,0.005995832789367315,2
W4285141429,"When classifying grammatical role, BERT doesn’t care about word order... except when it matters",2022,0.005995237023555119,2
W4385718022,KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction,2023,0.005991537461880735,2
W3199484478,Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT,2021,0.005990552876590907,2
W4385571914,IRIT_IRIS_A at SemEval-2023 Task 6: Legal Rhetorical Role Labeling Supported by Dynamic-Filled Contextualized Sentence Chunks,2023,0.0059890643935368295,2
W4224541024,Hierarchical label-wise attention transformer model for explainable ICD coding,2022,0.005987109229786105,2
W3118280924,An entity-graph based reasoning method for fact verification,2021,0.005985610948671003,2
W4327928001,Deep Learning Mental Health Dialogue System,2023,0.005982259133805229,2
W4385567170,Knowledge Graph Generation From Text,2022,0.005981169566643695,2
W4385573449,NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation,2022,0.00597647881163917,2
W4385574314,Structural Contrastive Representation Learning for Zero-shot Multi-label Text Classification,2022,0.005973526787370398,2
W3177743683,Pretrained Transformers for Text Ranking: BERT and Beyond,2021,0.005972169378616437,2
W2970205254,Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention,2019,0.005968705490817478,2
W3034918576,TransOMCS: From Linguistic Graphs to Commonsense Knowledge,2020,0.00596616792508053,2
W4389921502,Dense Text Retrieval Based on Pretrained Language Models: A Survey,2023,0.005964708640304887,2
W3036290069,"CO-Search: COVID-19 Information Retrieval with Semantic Search, Question Answering, and Abstractive Summarization",2020,0.00596013679747355,2
W2997915791,Order Matters: Semantic-Aware Neural Networks for Binary Code Similarity Detection,2020,0.00595319175803985,2
W4379740528,Integrating domain knowledge for biomedical text analysis into deep learning: A survey,2023,0.0059512426877952876,2
W4387171303,Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations,2023,0.005950454711808528,2
W4385006064,Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations,2023,0.005950454711808528,2
W2510759893,WikiReading: A Novel Large-scale Language Understanding Task over Wikipedia,2016,0.0059476988073324365,2
W4385570181,Prompt Discriminative Language Models for Domain Adaptation,2023,0.005947546982743054,2
W2963488798,Neural Domain Adaptation for Biomedical Question Answering,2017,0.005946638281246962,2
W3007978994,Differentiable Reasoning over a Virtual Knowledge Base,2020,0.005944768157460621,2
W3215779969,Evaluating Topic Models in Portuguese Political Comments About Bills from Brazil’s Chamber of Deputies,2021,0.005944491029582751,2
W4385893874,Large Language Models as Instructors: A Study on Multilingual Clinical Entity Extraction,2023,0.005932145493693296,2
W3034917890,Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?,2020,0.0059316820123032025,2
W2971600926,Commonsense Knowledge Mining from Pretrained Models,2019,0.005928965193011945,2
W3033346947,Application of Machine Learning and Word Embeddings in the Classification of Cancer Diagnosis Using Patient Anamnesis,2020,0.005926489469945919,2
W3102232851,Multi-label Few/Zero-shot Learning with Knowledge Aggregated from Multiple Label Graphs,2020,0.005925266745834831,2
W3198300102,FinBERT—A Deep Learning Approach to Extracting Textual Information,2020,0.005922090169146377,2
W4223609000,Hyperlink-induced Pre-training for Passage Retrieval in Open-domain Question Answering,2022,0.005920429041106177,2
W4389518647,DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text,2023,0.005915932125192288,2
W3177474387,SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics,2021,0.005914810095984404,2
W3094500738,EIGEN: Event Influence GENeration using Pre-trained Language Models,2020,0.005914635892018868,2
W4283790703,Siamese BERT-Based Model for Web Search Relevance Ranking Evaluated on a New Czech Dataset,2022,0.005914625549311167,2
W4388676590,The Combination of Contextualized Topic Model and MPNet for User Feedback Topic Modeling,2023,0.005913734522216808,2
W4396762144,"Natural Language Reasoning, A Survey",2024,0.005910236653111967,2
W4225590069,PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains,2022,0.005909626926720547,2
W3016263386,LadaBERT: Lightweight Adaptation of BERT through Hybrid Model Compression,2020,0.005902856143729224,2
W4385567137,ClassActionPrediction: A Challenging Benchmark for Legal Judgment Prediction of Class Action Cases in the US,2022,0.0059028227306651745,2
W3204886703,DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models,2022,0.005902670487317927,2
W4327993545,Retrieving false claims on Twitter during the Russia-Ukraine conflict,2023,0.0059000201768933565,2
W4362456756,"Extracting social determinants of health events with transformer-based multitask, multilabel named entity recognition",2023,0.005899285424796082,2
W3176971293,Have We Solved The Hard Problem? It’s Not Easy! Contextual Lexical Contrast as a Means to Probe Neural Coherence,2021,0.005894404494519602,2
W3208602306,WikiCheck,2021,0.00589014724331241,2
W4389520103,Measuring and Narrowing the Compositionality Gap in Language Models,2023,0.00588995321695247,2
W2954447110,Team yeon-zi at SemEval-2019 Task 4: Hyperpartisan News Detection by De-noising Weakly-labeled Data,2019,0.005886265733880138,2
W3113524397,An Empirical Study of Contextual Data Augmentation for Japanese Zero Anaphora Resolution,2020,0.005885767053347529,2
W4392632268,Evaluation of Medium-Sized Language Models in German and English Language,2024,0.005885379401076056,2
W4385570911,Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values,2023,0.0058845567252691754,2
W3034300118,Learning to Extract Attribute Value from Product via Question Answering: A Multi-task Approach,2020,0.005883309582348966,2
W4205933428,Project Debater APIs: Decomposing the AI Grand Challenge,2021,0.005880977609474256,2
W4385605314,LAnoBERT: System log anomaly detection based on BERT masked language model,2023,0.005876695394126378,2
W3015766957,A Systematic Analysis of Morphological Content in BERT Models for Multiple Languages,2020,0.005874818817949398,2
W4408610570,Subset selection for domain adaptive pre-training of language model,2025,0.005873172381272669,2
W2964181805,Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension,2018,0.005870061366515189,2
W3175864309,X-Fact: A New Benchmark Dataset for Multilingual Fact Checking,2021,0.0058680579026206174,2
W4206648492,Active Learning by Acquiring Contrastive Examples,2021,0.005867270418530781,2
W4378635602,Generating risk response measures for subway construction by fusion of knowledge and deep learning,2023,0.005862180522316621,2
W3102933348,What do we expect from Multiple-choice QA Systems?,2020,0.00586152635544495,2
W3174102190,Embracing Ambiguity: Shifting the Training Target of NLI Models,2021,0.005858090625795696,2
W4226325987,Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction,2022,0.005853969543911077,2
W3157029757,Named Entity Recognition Using BERT with Whole World Masking in Cybersecurity Domain,2021,0.005848189163775683,2
W2953307569,BERT Rediscovers the Classical NLP Pipeline.,2019,0.005846360164749285,2
W3174693043,Reasoning over Entity-Action-Location Graph for Procedural Text Understanding,2021,0.005843952896113195,2
W4385574362,Using Roark-Hollingshead Distance to Probe BERT’s Syntactic Competence,2022,0.005839453393486276,2
W4386566572,Double Retrieval and Ranking for Accurate Question Answering,2023,0.005837216869608308,2
W4392956390,Supervised Contrast Learning Text Classification Model Based on Data Quality Augmentation,2024,0.005833496438749991,2
W3096160408,Achieving Reliable Sentiment Analysis in the Software Engineering Domain using BERT,2020,0.00582815029736786,2
W3131335651,"Automatic Exam Correction Framework (AECF) for the MCQs, Essays, and Equations Matching",2021,0.005822179146205929,2
W3034902472,Premise Selection in Natural Language Mathematical Texts,2020,0.005822048005055151,2
W4289323723,Fact Checking with Insufficient Evidence,2022,0.005817349624408228,2
W4407158522,Structured reasoning and answer verification: Enhancing question answering system accuracy and explainability,2025,0.005816964906036086,2
W4385572255,KInITVeraAI at SemEval-2023 Task 3: Simple yet Powerful Multilingual Fine-Tuning for Persuasion Techniques Detection,2023,0.0058115947600855845,2
W4403909032,Quo Vadis ChatGPT? From large language models to Large Knowledge Models,2024,0.005810985366454957,2
W2970648896,AMPERSAND: Argument Mining for PERSuAsive oNline Discussions,2019,0.0058104071757839205,2
W4393111197,Automatic Scoring of Metaphor Creativity with Large Language Models,2024,0.0058095721610507,2
W3202070718,"Probing Classifiers: Promises, Shortcomings, and Advances",2021,0.005809129692047046,2
W4407841165,Sentiment Analysis with Large Language Models Applied to the Federal Reserve Beige Book,2025,0.005809111699271177,2
W3153866062,Self-Training with Weak Supervision,2021,0.005804503806459913,2
W3099384026,Expansion via Prediction of Importance with Contextualization,2020,0.005804378760116798,2
W3169971770,Emotion-Infused Models for Explainable Psychological Stress Detection,2021,0.005804251771909173,2
W3110300144,Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?,2022,0.00580196252939133,2
W3014434762,Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models,2020,0.0057972447637825355,2
W4287890934,In-BoXBART: Get Instructions into Biomedical Multi-Task Learning,2022,0.005795272031852939,2
W3035164567,R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason,2020,0.005795162824310606,2
W4385569771,Reasoning with Language Model Prompting: A Survey,2023,0.005793355501567629,2
W2990591037,ToNy: Contextual embeddings for accurate multilingual discourse segmentation of full documents,2019,0.005791357514299146,2
W3118831779,The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models,2021,0.005787015792120754,2
W4389519941,Comparing Prompt-Based and Standard Fine-Tuning for Urdu Text Classification,2023,0.0057775174931267645,2
W2953130735,Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders,2019,0.005775594340555977,2
W3046747294,A Text Generation and Prediction System: Pre-training on New Corpora Using BERT and GPT-2,2020,0.00577297412428246,2
W4367671721,FeQA: Fusion and enhancement of multi-source knowledge on question answering,2023,0.005768206484594283,2
W4409357852,SemiRALD: A semi-supervised hybrid language model for Robust Anomalous Log Detection,2025,0.005767594507376918,2
W4206294441,Contrastive Explanations for Model Interpretability,2021,0.005765400450830816,2
W3116010752,"Rhetoric, Logic, and Dialectic: Advancing Theory-based Argument Quality Assessment in Natural Language Processing",2020,0.0057627574929991505,2
W3092539409,Review of Natural Language Processing in Radiology,2020,0.005760994659439571,2
W3101493110,Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation,2020,0.005754198826103333,2
W4405384707,GeoGLUE: A Chinese GeoGraphic Language Understanding Evaluation Benchmark,2024,0.005752927458486881,2
W4389009482,The Next Chapter: A Study of Large Language Models in Storytelling,2023,0.00575064218737698,2
W4285291532,FairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing,2022,0.005747222581176237,2
W3177281527,Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder,2021,0.005745197833159911,2
W3187658747,Multi-Hop Fact Checking of Political Claims,2021,0.005745113751749922,2
W4386576718,Causal Reasoning of Entities and Events in Procedural Texts,2023,0.005745021093631315,2
W4322775994,Around the GLOBE: Numerical Aggregation Question-answering on Heterogeneous Genealogical Knowledge Graphs with Deep Neural Networks,2023,0.005741927985502103,2
W4280612849,Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words,2022,0.005734700095142633,2
W4392942881,Measuring and Modifying Factual Knowledge in Large Language Models,2023,0.005731407256194222,2
W4386763311,GPT-4 as a Twitter Data Annotator: Unraveling Its Performance on a Stance Classification Task,2023,0.005729741025701454,2
W3035261420,Do Neural Language Models Show Preferences for Syntactic Formalisms?,2020,0.005728295786192445,2
W4353069824,Knowledge-Guided Prompt Learning for Few-Shot Text Classification,2023,0.0057265954439371936,2
W4386566526,"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",2023,0.005723911829895975,2
W3173247797,Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification,2021,0.00572362410627459,2
W3172365208,Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis,2021,0.005719004446710989,2
W4393156784,STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models,2024,0.005714684728925633,2
W4409166218,Hierarchical Prefixes for Long Document Representations,2025,0.0057133137526901925,2
W4362720788,NL-Augmenter 🦎 → 🐍 A Framework for Task-Sensitive Natural Language Augmentation,2023,0.005711104065220333,2
W4392359953,Can large language models reason about medical questions?,2024,0.005707684503283094,2
W4385565162,WeLT: Improving Biomedical Fine-tuned Pre-trained Language Models with Cost-sensitive Learning,2023,0.005705951142711234,2
W3104597243,Predicting Clinical Diagnosis from Patients Electronic Health Records Using BERT-Based Neural Networks,2020,0.005703325217336384,2
W3035357591,DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding,2020,0.005702065302907123,2
W2953337107,What Does BERT Look At? An Analysis of BERT's Attention,2019,0.005701926918879619,2
W4282597027,Adapting vs. Pre-training Language Models for Historical Languages,2022,0.0056979846216536015,2
W2978170550,Exploiting Structural and Semantic Context for Commonsense Knowledge Base Completion,2019,0.005697725931343584,2
W4387782510,A taxonomy and review of generalization research in NLP,2023,0.005686326307896928,2
W4400970942,Biulkbqa: A Joint Generate and Retrieve Efficient Dual-Stream Framework for Knowledge Base Question Answering,2024,0.005675070678037332,2
W3175729092,Scaling Federated Learning for Fine-Tuning of Large Language Models,2021,0.005673996612386177,2
W3044812140,Conformer-Kernel with Query Term Independence for Document Retrieval.,2020,0.005673637094678754,2
W4375868933,Meta Learning for Domain Agnostic Soft Prompt,2023,0.005670720876968133,2
W3031043369,WorldTree V2: A corpus of science-domain structured explanations and inference patterns supporting multi-hop inference,2020,0.005669219252924973,2
W4385284874,Data Ambiguity Profiling for the Generation of Training Examples,2023,0.005667584872241481,2
W4385570090,Parallel Context Windows for Large Language Models,2023,0.0056659658846094,2
W3100507548,"BERT Knows Punta Cana is not just beautiful, it’s gorgeous: Ranking Scalar Adjectives with Contextualised Representations",2020,0.005664616285384974,2
W2932592714,Probing Biomedical Embeddings from Language Models,2019,0.005664538355719783,2
W2968250601,Few-shot Text Classification with Distributional Signatures,2019,0.005663188823442269,2
W3035990700,"Understanding spatial language in radiology: Representation framework, annotation, and spatial relation extraction from chest X-ray reports using deep learning",2020,0.005661391319451548,2
W3210661875,Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning,2021,0.005659667138007129,2
W2997545008,Commonsense Knowledge Base Completion with Structural and Semantic Context,2020,0.0056592350772040004,2
W4385573855,Successive Prompting for Decomposing Complex Questions,2022,0.0056535947198612935,2
W3198057698,Mitigation of Diachronic Bias in Fake News Detection Dataset,2021,0.005650285432980597,2
W4382318860,An Ensemble Distillation Framework for Sentence Embeddings with Multilingual Round-Trip Translation,2023,0.005648831395635623,2
W4385567201,Memory-assisted prompt editing to improve GPT-3 after deployment,2022,0.005643877433012733,2
W3170644058,"“Call me sexist, but...” : Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples",2021,0.005643641622782421,2
W4385572343,Exploring the Curious Case of Code Prompts,2023,0.005642187336980433,2
W4387846571,"Spans, Not Tokens: A Span-Centric Model for Multi-Span Reading Comprehension",2023,0.005638536910555436,2
W4409957554,AdaFT: An efficient domain-adaptive fine-tuning framework for sentiment analysis in chinese financial texts,2025,0.005636977349652625,2
W4393342165,A Survey on Automatic Generation of Figurative Language: From Rule-based Systems to Large Language Models,2024,0.00563094692018076,2
W4404021177,"A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges",2024,0.005624909872383601,2
W3088418428,Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models,2020,0.005621184385894256,2
W4396216171,Understanding writing style in social media with a supervised contrastively pre-trained transformer,2024,0.005621076703144115,2
W4226094293,A Cross-Lingual Sentence Similarity Calculation Method With Multifeature Fusion,2022,0.0056202384776586345,2
W3114765853,AlexU-AUX-BERT at SemEval-2020 Task 3: Improving BERT Contextual Similarity Using Multiple Auxiliary Contexts,2020,0.005618326209465245,2
W3035009410,Bayesian Hierarchical Words Representation Learning,2020,0.005618326209465245,2
W4392939486,Graph Structure Enhanced Pre-Training Language Model for Knowledge Graph Completion,2024,0.005616169950937397,2
W4407827488,A systematic review of automated hyperpartisan news detection,2025,0.005610642835552806,2
W3155933491,Bi-GRU Urgent Classification for MOOC Discussion Forums Based on BERT,2021,0.005608830681368362,2
W4409166643,Jina Embeddings V3: Multilingual Text Encoder with Low-Rank Adaptations,2025,0.005602679738413439,2
W4375868971,Self-Supervised Adversarial Training for Contrastive Sentence Embedding,2023,0.005595377100888606,2
W4391097066,Efficient Classification of Malicious URLs: M-BERT—A Modified BERT Variant for Enhanced Semantic Understanding,2024,0.005594340065415081,2
W2757177109,Reasoning with Heterogeneous Knowledge for Commonsense Machine Comprehension,2017,0.005593923758007647,2
W3047855687,Empirical evaluation of multi-task learning in deep neural networks for natural language processing,2020,0.005589018909792643,2
W3100363073,DeSMOG: Detecting Stance in Media On Global Warming,2020,0.005584833620723268,2
W4385570359,Towards Robust Ranker for Text Retrieval,2023,0.005583887543490285,2
W4327811582,Review of Natural Language Processing in Pharmacology,2023,0.005581562669192516,2
W3094336525,Neural Passage Retrieval with Improved Negative Contrast,2020,0.005581397188685402,2
W4385570910,Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations,2023,0.00558008279367979,2
W4386566601,Detecting Contextomized Quotes in News Headlines by Contrastive Learning,2023,0.005578517749091113,2
W4313563818,Natural Test Generation for Precise Testing of Question Answering Software,2022,0.00557762603423446,2
W2308720496,A Fast Unified Model for Parsing and Sentence Understanding,2016,0.0055753568068728394,2
W3174859448,Argument Pair Extraction via Attention-guided Multi-Layer Multi-Cross Encoding,2021,0.0055752740178396576,2
W4362723303,A BERT-based deontic logic learner,2023,0.0055702932282596365,2
W3119854206,Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge,2020,0.0055695354827429745,2
W4386566910,CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models,2023,0.0055692750345929035,2
W3173558867,PAWLS: PDF Annotation With Labels and Structure,2021,0.0055670379593578765,2
W2981573048,A Unified MRC Framework for Named Entity Recognition,2019,0.005564576907544691,2
W4385571534,Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe,2023,0.005562017604508177,2
W4285158689,Knowledge Enhanced Reflection Generation for Counseling Dialogues,2022,0.005556896492425521,2
W3170989320,A Package for Learning on Tabular and Text Data with Transformers,2021,0.005556535188530642,2
W3047026265,Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets,2020,0.005555793744738659,2
W3135857201,PharmKE: Knowledge Extraction Platform for Pharmaceutical Texts Using Transfer Learning,2023,0.005554243916544666,2
W3127307050,Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data,2021,0.005552148509748017,2
W4285228888,AMR-DA: Data Augmentation by Abstract Meaning Representation,2022,0.005551035541808384,2
W2964059756,Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks,2017,0.0055506325217222485,2
W4285249364,"MMCoQA: Conversational Question Answering over Text, Tables, and Images",2022,0.00555026418712966,2
W3117435054,Hierarchical Chinese Legal event extraction via Pedal Attention Mechanism,2020,0.005544261221538391,2
W4406010253,Generative Artificial Intelligence-Based Medical Entity Data Extractor Using Large Language Models,2025,0.005544036030603873,2
W4385567248,Visual Named Entity Linking: A New Dataset and A Baseline,2022,0.005539465591830847,2
W3121735664,Detecting Stance in Media on Global Warming,2020,0.005538823415256903,2
W2995322030,Few-shot Text Classification with Distributional Signatures,2020,0.005537896550356043,2
W2995335514,Zero-shot Text Classification With Generative Language Models,2019,0.00553617884481567,2
W4392337553,A comparative analysis of knowledge injection strategies for large language models in the scholarly domain,2024,0.005535353746216256,2
W4401664886,PMANet: Malicious URL detection via post-trained language model guided multi-level feature attention network,2024,0.0055272345812881355,2
W3204414009,Graph Reasoning with Context-Aware Linearization for Interpretable Fact Extraction and Verification,2021,0.005520007709579191,2
W4282813613,Ask to Know More,2022,0.005518844571568262,2
W4385488681,GLQA: A Generation-based Method for Legal Question Answering,2023,0.0055148495662856595,2
W4221147232,elBERto: Self-supervised commonsense learning for question answering,2022,0.005514641904402768,2
W4385572404,Prompting Language Models for Linguistic Structure,2023,0.00551463112833982,2
W4385782731,Exploring the state of the art in legal QA systems,2023,0.0055124294487142525,2
W3169066049,Improving BERT Model Using Contrastive Learning for Biomedical Relation Extraction,2021,0.0055123817430822435,2
W3094804884,LM4KG: Improving Common Sense Knowledge Graphs with Language Models,2020,0.005509791225468917,2
W4410015771,Test-driving information Theory-based Compositional Distributional Semantics: A case study on Spanish song lyrics,2025,0.005506034031271084,2
W2937531012,Exploring Unsupervised Pretraining and Sentence Structure Modelling for Winograd Schema Challenge,2019,0.00550185869279418,2
W3034403033,On the Importance of Word and Sentence Representation Learning in Implicit Discourse Relation Classification,2020,0.005501827132701972,2
W4382203222,Multi-perspective contrastive learning framework guided by sememe knowledge and label information for sarcasm detection,2023,0.005500466631468424,2
W4312349643,Evidence Extraction to Validate Medical Claims in Fake News Detection,2022,0.005499836739744057,2
W4401943272,When Search Engine Services meet Large Language Models: Visions and Challenges,2024,0.005497396493141535,2
W4225580830,Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering,2022,0.005494728085218475,2
W3177920269,A GPT-2 Language Model for Biomedical Texts in Portuguese,2021,0.005494200419972957,2
W4386942346,Retrieving Supporting Evidence for Generative Question Answering,2023,0.00549364341279299,2
W3208243843,Is BERT the New Silver Bullet? - An Empirical Investigation of Requirements Dependency Classification,2021,0.005492522623180402,2
W3004654429,Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking,2020,0.005489795259260934,2
W4385573188,ReSel: N-ary Relation Extraction from Scientific Text and Tables by Learning to Retrieve and Select,2022,0.005489792500468517,2
W2981757109,Depth-Adaptive Transformer,2019,0.005484900601251722,2
W4389519823,Detecting Syntactic Change with Pre-trained Transformer Models,2023,0.005484756550247922,2
W4385567182,Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models,2022,0.005481043777140771,2
W2963651521,Deep RNNs Encode Soft Hierarchical Syntax,2018,0.005474055183999623,2
W4221144388,Continual Sequence Generation with Adaptive Compositional Modules,2022,0.005470171419523189,2
W4382933960,Bengali Text Classification: A New multi-class Dataset and Performance Evaluation of Machine Learning and Deep Learning Models,2023,0.005469497055968958,2
W4226157820,A Label Dependence-Aware Sequence Generation Model for Multi-Level Implicit Discourse Relation Recognition,2022,0.005469032114267954,2
W3109746361,Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking,2020,0.005467310581425879,2
W4200350226,What Does a Language-And-Vision Transformer See: The Impact of Semantic Information on Visual Representations,2021,0.005463555958509738,2
W4387316165,CitePrompt: Using Prompts to Identify Citation Intent in Scientific Papers,2023,0.005462473998581992,2
W4384211326,Chinese Idiom Paraphrasing,2023,0.0054624115717567465,2
W4399803256,Detecting hallucinations in large language models using semantic entropy,2024,0.00545156652588005,2
W3022569409,Establishing Baselines for Text Classification in Low-Resource Languages,2020,0.005450485563411671,2
W3106346472,Language Through a Prism: A Spectral Approach for Multiscale Language Representations,2020,0.005450339597143261,2
W4386740843,Testing the limits of natural language models for predicting human language judgements,2023,0.005442845421801433,2
W4399205996,Theme-Driven Keyphrase Extraction to Analyze Social Media Discourse,2024,0.005441321020035043,2
W4385570233,Dataset Distillation with Attention Labels for Fine-tuning BERT,2023,0.005437574273160261,2
W2949629417,Scalable Syntax-Aware Language Models Using Knowledge Distillation,2019,0.005435486629302343,2
W4401549296,GS-CBR-KBQA: Graph-structured case-based reasoning for knowledge base question answering,2024,0.005433234939043958,2
W4385076927,PLPMpro: Enhancing promoter sequence prediction with prompt-learning based pre-trained language model,2023,0.005423182659553811,2
W4385766704,TopoBERT: a plug and play toponym recognition module harnessing fine-tuned BERT,2023,0.005414658060177582,2
W4321242372,A semantics-aware approach for multilingual natural language inference,2023,0.00541440422918129,2
W4389001184,A survey of the recent trends in deep learning for literature based discovery in the biomedical domain,2023,0.0054128919031866726,2
W4388141694,An effective negative sampling approach for contrastive learning of sentence embedding,2023,0.005409750763091523,2
W4287855194,Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner,2022,0.005408807030057105,2
W4385488736,"AfriWOZ: Corpus for Exploiting Cross-Lingual Transfer for Dialogue Generation in Low-Resource, African Languages",2023,0.0054069712446167765,2
W2970530230,Evaluating adversarial attacks against multiple fact verification systems,2019,0.005406257357736954,2
W4381856660,Tourism Information QA Datasets for Smart Tourism Chatbot,2023,0.005405545273390443,2
W3174202502,"Automated Storytelling via Causal, Commonsense Plot Ordering",2021,0.005405233420834124,2
W4391661692,Explainable Attention Pruning: A Metalearning-Based Approach,2024,0.0054025624156627495,2
W3171057731,Continual Learning for Text Classification with Information Disentanglement Based Regularization,2021,0.00540222752998745,2
W2967269971,QuGAN: Quasi Generative Adversarial Network for Tibetan Question Answering Corpus Generation,2019,0.00540116625115787,2
W4389518441,Qur’an QA 2023 Shared Task: Overview of Passage Retrieval and Reading Comprehension Tasks over the Holy Qur’an,2023,0.005397530123804469,2
W4319918988,Joint reasoning with knowledge subgraphs for Multiple Choice Question Answering,2023,0.005396718018749128,2
W3176142035,Don’t Miss the Labels: Label-semantic Augmented Meta-Learner for Few-Shot Text Classification,2021,0.005396085828097822,2
W2952902402,A Tensorized Transformer for Language Modeling,2019,0.005395955485815879,2
W2962874939,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,2017,0.0053926292579678965,2
W4389518653,Tunable Soft Prompts are Messengers in Federated Learning,2023,0.005391702013892266,2
W4322505467,Automated ICD coding for coronary heart diseases by a deep learning method,2023,0.005391012946641886,2
W3032232719,LEDGAR : a large-scale multi-label corpus for text classification of legal provisions in contracts,2020,0.005388592026578438,2
W3100843744,Entity Linking in 100 Languages,2020,0.005385924306869674,2
W4285310604,Data Augmentation for Intent Classification with Off-the-shelf Large Language Models,2022,0.005384620046788127,2
W4385764034,A Survey on Efficient Training of Transformers,2023,0.00538261995960564,2
W4398150831,ChatGPT Label: Comparing the Quality of Human-Generated and LLM-Generated Annotations in Low-Resource Language NLP Tasks,2024,0.005379940008718218,2
W3035453001,Ensemble Distillation for Robust Model Fusion in Federated Learning,2020,0.0053727659121965594,2
W3140853032,Comparing Score Aggregation Approaches for Document Retrieval with Pretrained Transformers,2021,0.0053692412380358994,2
W3179633195,Contextualized query expansion via unsupervised chunk selection for text retrieval,2021,0.005369133851818559,2
W3154498239,Text-to-Text Multi-view Learning for Passage Re-ranking,2021,0.005367849876345282,2
W4281384899,Label Anchored Contrastive Learning for Language Understanding,2022,0.005366941399590203,2
W4385572282,Impact of Adversarial Training on Robustness and Generalizability of Language Models,2023,0.005366690163252949,2
W4385574265,Control Prefixes for Parameter-Efficient Text Generation,2022,0.0053662262114521535,2
W2607892599,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,2017,0.005361281110499596,2
W4226087293,Compression of Generative Pre-trained Language Models via Quantization,2022,0.005360793769861814,2
W4323519786,Chemical identification and indexing in full-text articles: an overview of the NLM-Chem track at BioCreative VII,2023,0.005360611746328617,2
W4385572004,U-CREAT: Unsupervised Case Retrieval using Events extrAcTion,2023,0.005359592948029145,2
W4406614395,DeBERTA-Att-LMCQA: A hybrid model of DeBERTA and attention for legal multi-choice question answering,2025,0.00535906689578476,2
W4393157285,Some Like It Small: Czech Semantic Embedding Models for Industry Applications,2024,0.005357237003743865,2
W4407268166,K-Bloom: unleashing the power of pre-trained language models in extracting knowledge graph with predefined relations,2025,0.0053544013233453695,2
W4389009551,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,2022,0.0053485886750231715,2
W2741075451,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,2017,0.005346221607039009,2
W4283452667,CHEF: A Pilot Chinese Dataset for Evidence-Based Fact-Checking,2022,0.005344199545172842,2
W3101891351,Authorship Attribution for Neural Text Generation,2020,0.0053437906733647626,2
W3094300879,Customizing Triggers with Concealed Data Poisoning.,2020,0.005342293622018986,2
W4385987638,Adapting an ASR Foundation Model for Spoken Language Assessment,2023,0.005333525448509982,2
W4389262624,WellXplain: Wellness concept extraction and classification in Reddit posts for mental health analysis,2023,0.005331389855621293,2
W4205694376,Efficient Nearest Neighbor Language Models,2021,0.005330496945933392,2
W4383618720,AD-BERT: Using pre-trained language model to predict the progression from mild cognitive impairment to Alzheimer's disease,2023,0.005327906298308544,2
W4391061229,MED-Prompt: A novel prompt engineering framework for medicine prediction on free-text clinical notes,2024,0.005326852049924876,2
W2983771403,Memory Graph Networks for Explainable Memory-grounded Question Answering,2019,0.005320961067163507,2
W3092683697,RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering,2020,0.005319894651739595,2
W3164024373,Embed2Detect: temporally clustered embedded words for event detection in social media,2021,0.005314179879750995,2
W3133029875,Investigating the Limitations of Transformers with Simple Arithmetic Tasks,2021,0.005311952634591569,2
W4377116057,Sub-Character Tokenization for Chinese Pretrained Language Models,2023,0.005311061389900687,2
W4319300084,A URL-Based Social Semantic Attacks Detection With Character-Aware Language Model,2023,0.0053073304094741576,2
W4399036027,Improving Robustness in Language Models for Legal Textual Entailment Through Artifact-Aware Training,2024,0.0053068712150542985,2
W4385572634,Self-Instruct: Aligning Language Models with Self-Generated Instructions,2023,0.005305724162116272,2
W3091973425,NoRBERT: Transfer Learning for Requirements Classification,2020,0.00530459161693448,2
W4385572158,Cognitive Reframing of Negative Thoughts through Human-Language Model Interaction,2023,0.005299654516517298,2
W4385570518,"MaChAmp at SemEval-2023 tasks 2, 3, 4, 5, 7, 8, 9, 10, 11, and 12: On the Effectiveness of Intermediate Training on an Uncurated Collection of Datasets.",2023,0.00529443024948923,2
W4327594639,A survey on legal question–answering systems,2023,0.005294038972350042,2
W4385570123,NITK_LEGAL at SemEval-2023 Task 6: A Hierarchical based system for identification of Rhetorical Roles in legal judgements,2023,0.005293200539485259,2
W3119308520,A GA-Based Approach to Fine-Tuning BERT for Hate Speech Detection,2020,0.005292152043337114,2
W4285292976,CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues,2022,0.0052882255861432075,2
W3139815689,"Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover",2021,0.005283076400693169,2
W3006188107,Transformers as Soft Reasoners over Language,2020,0.005282027902800924,2
W4221151164,IAM: A Comprehensive and Large-Scale Dataset for Integrated Argument Mining Tasks,2022,0.005281907456546577,2
W4396636677,A Dataset for Evaluating Contextualized Representation of Biomedical Concepts in Language Models,2024,0.005281226661846094,2
W3100005111,Incorporating Relation Knowledge into Commonsense Reading Comprehension with Multi-task Learning,2019,0.005275741720266809,2
W4394742621,JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims,2024,0.0052724242405046695,2
W3045441970,An Effective Domain Adaptive Post-Training Method for BERT in Response Selection,2019,0.005270192338187378,2
W3163109966,Conversational Entity Linking: Problem Definition and Datasets,2021,0.005269104044100864,2
W2887557046,A Multi-Stage Memory Augmented Neural Network for Machine Reading Comprehension,2018,0.005268847425292636,2
W4393157091,N-gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding,2024,0.005264251225439108,2
W4385571803,Check-COVID: Fact-Checking COVID-19 News Claims with Scientific Evidence,2023,0.005260100025310819,2
W4376279187,Dynamic Topic Modelling for Exploring the Scientific Literature on Coronavirus: An Unsupervised Labelling Technique,2023,0.00525806224649613,2
W4386796175,Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages,2023,0.0052570717725761735,2
W2923890923,Simple Applications of BERT for Ad Hoc Document Retrieval,2019,0.00525300362893939,2
W4385570866,ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems,2023,0.005252007446734982,2
W4402604910,CoT-BERT: Enhancing Unsupervised Sentence Representation Through Chain-of-Thought,2024,0.005251314642570069,2
W3125292083,Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning,2021,0.0052509073872642606,2
W4386791134,Cell2Sentence: Teaching Large Language Models the Language of Biology,2023,0.005250052962001456,2
W3102532959,Investigating representations of verb bias in neural language models,2020,0.005248487323774292,2
W4408854165,LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models,2025,0.005245755637888937,2
W2788363870,Medical Exam Question Answering with Large-scale Reading Comprehension,2018,0.005245432643125437,2
W4308341600,Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data,2022,0.0052436369045648495,2
W4408228388,NLP modeling recommendations for restricted data availability in clinical settings,2025,0.005239199814577741,2
W3016888066,BanFakeNews: A Dataset for Detecting Fake News in Bangla,2020,0.0052383536812559426,2
W4386065780,Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning,2023,0.005232140060164156,2
W4385571944,Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step,2023,0.005227085269855925,2
W4392745350,Question-Directed Reasoning With Relation-Aware Graph Attention Network for Complex Question Answering Over Knowledge Graph,2024,0.005226378077720576,2
W4386566755,What happens before and after: Multi-Event Commonsense in Event Coreference Resolution,2023,0.005224907928813826,2
W3003289092,Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP,2019,0.005224694334589707,2
W4392904073,Recovering from Privacy-Preserving Masking with Large Language Models,2024,0.005224694084067154,2
W3153184434,KeyBLD: Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval,2021,0.005223640066574794,2
W4385571405,Question-Interlocutor Scope Realized Graph Modeling over Key Utterances for Dialogue Reading Comprehension,2023,0.005223173336975852,2
W4205103289,Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval,2021,0.00521976136003441,2
W4389438760,Toward explainable AI (XAI) for mental health detection based on language behavior,2023,0.005216168679250352,2
W4287888710,Extracting Temporal Event Relation with Syntax-guided Graph Transformer,2022,0.005210073621473846,2
W4365511667,Fine-tuning large neural language models for biomedical natural language processing,2023,0.005206669162846448,2
W4385571951,Distilling Script Knowledge from Large Language Models for Constrained Language Planning,2023,0.005206260400340177,2
W4385573418,Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts,2022,0.005205332787133631,2
W3005650752,Does BERT need domain adaptation for clinical negation detection?,2020,0.005204577774304279,2
W3130583616,A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine,2021,0.005200064883620619,2
W4390571036,Driving and suppressing the human language network using large language models,2024,0.005196201015054535,2
W4385567092,Towards Intention Understanding in Suicidal Risk Assessment with Natural Language Processing,2022,0.005195975823592963,2
W4385571070,Team:PULSAR at ProbSum 2023:PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients’ Problems and Data Augmentation with Black-box Large Language Models,2023,0.005195709424281301,2
W4406609894,Amplifying commonsense knowledge via bi-directional relation integrated graph-based contrastive pre-training from large language models,2025,0.005195075668885495,2
W4410109416,It’s all in the [MASK]: Simple instruction-tuning enables BERT-like masked language models as generative classifiers,2025,0.005192868417442522,2
W4385572768,HPT: Hierarchy-aware Prompt Tuning for Hierarchical Text Classification,2022,0.0051924373960500324,2
W2964151654,Neural Models for Reasoning over Multiple Mentions Using Coreference,2018,0.005183656641893925,2
W4221159558,PTM4Tag,2022,0.005177550008901984,2
W4391765649,Domain-specific language models pre-trained on construction management systems corpora,2024,0.0051748808044600175,2
W4386892339,Exploring new depths: Applying machine learning for the analysis of student argumentation in chemistry,2023,0.005169089793515523,2
W4287075683,Self-Supervised Contrastive Learning with Adversarial Perturbations for Defending Word Substitution-based Attacks,2022,0.0051661805963755456,2
W2950726992,Skip-Thought Vectors,2015,0.005163296256928082,2
W4281884846,Natural Language Processing: from Bedside to Everywhere,2022,0.005160741417871255,2
W4205332217,Joint Passage Ranking for Diverse Multi-Answer Retrieval,2021,0.005156622990144935,2
W4285199989,Probing BERT’s priors with serial reproduction chains,2022,0.005156014741067081,2
W3173841754,"Writing Polishment with Simile: Task, Dataset and A Neural Approach",2021,0.005154376326430849,2
W4385569686,ConvGQR: Generative Query Reformulation for Conversational Search,2023,0.00515326478050467,2
W4317935084,The Choice of Textual Knowledge Base in Automated Claim Checking,2023,0.0051522406009589785,2
W4360995184,Slovak Dataset for Multilingual Question Answering,2023,0.00514795726826023,2
W4385574345,LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning,2022,0.005144606656663392,2
W3177765786,Deduplicating Training Data Makes Language Models Better,2022,0.005143406052994932,2
W3200275576,A simple and efficient text matching model based on deep interaction,2021,0.005142889600846236,2
W4389289000,Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit,2023,0.005139455988105975,2
W4389288957,Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit,2023,0.005139455988105975,2
W4382318023,Help Me Heal: A Reinforced Polite and Empathetic Mental Health and Legal Counseling Dialogue System for Crime Victims,2023,0.005137458455709716,2
W4283734474,Augmenting Textbooks with cQA Question-Answers and Annotated YouTube Videos to Increase Its Relevance,2022,0.005136917348735219,2
W4321002059,The 2022 n2c2/UW shared task on extracting social determinants of health,2023,0.00513666163361202,2
W4386566516,How Many Data Samples is an Additional Instruction Worth?,2023,0.005128683580811068,2
W3170572542,Concealed Data Poisoning Attacks on NLP Models,2021,0.005125067526841358,2
W3119200132,Deep bi-directional interaction network for sentence matching,2021,0.005124963927631334,2
W4399366010,A Survey of Text-Matching Techniques,2024,0.005124766159694871,2
W3105107530,Understanding BERT Rankers Under Distillation,2020,0.005121304378701186,2
W3166913490,X-Class: Text Classification with Extremely Weak Supervision,2021,0.0051193252574603535,2
W4366003941,Driving and suppressing the human language network using large language models,2023,0.00511844321767466,2
W4392747524,Label-aware debiased causal reasoning for Natural Language Inference,2024,0.0051072695318972496,2
W4206217845,Coreference Resolution for the Biomedical Domain: A Survey,2021,0.005102936592116982,2
W4388462011,Analyzing the relation among different factors leading to Ph.D. dropout using numerical association rule mining,2023,0.005102734788675953,2
W4318460697,Multi-stage transfer learning with BERTology-based language models for question answering system in vietnamese,2023,0.005098050906362285,2
W4376464559,Automatic Context Pattern Generation for Entity Set Expansion,2023,0.0050979717239443365,2
W4390222516,Year 2022 in Medical Natural Language Processing: Availability of Language Models as a Step in the Democratization of NLP in the Biomedical Area,2023,0.005097020552378922,2
W4285147034,KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base,2022,0.005096290018418655,2
W4394789579,Surveying biomedical relation extraction: a critical examination of current datasets and the proposal of a new resource,2024,0.005095085440748359,2
W4401971058,The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models,2024,0.005094971991798151,2
W3202082017,Weakly-supervised Text Classification Based on Keyword Graph,2021,0.005093945158409311,2
W3198948523,Exploring Decomposition for Table-based Fact Verification,2021,0.005093372964706488,2
W3212837704,Low-resource Taxonomy Enrichment with Pretrained Language Models,2021,0.005092578750112588,2
W3120706522,Robustness Gym: Unifying the NLP Evaluation Landscape,2021,0.005092386016913602,2
W4386566671,UnifEE: Unified Evidence Extraction for Fact Verification,2023,0.0050857763470143385,2
W4406827500,Identifying protected health information by transformers-based deep learning approach in Chinese medical text,2025,0.005081953711610766,2
W3184553750,Small-Text: Active Learning for Text Classification in Python,2023,0.005079304855831128,2
W4393159599,SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation,2024,0.0050778532060837414,2
W3136215575,Multilingual Autoregressive Entity Linking,2022,0.005074955345044849,2
W4391023916,Layer Configurations of BERT for Multitask Learning and Data Augmentation,2024,0.005073581467788624,2
W2970619710,Shallow Syntax in Deep Water,2019,0.005071129045898251,2
W4386394330,Deep Learning-based Sentence Embeddings using BERT for Textual Entailment,2023,0.0050696884571015165,2
W4391467401,Distractor Generation Through Text-to-Text Transformer Models,2024,0.005069393403685797,2
W2939507640,DocBERT: BERT for Document Classification,2019,0.0050672194272081425,2
W2250635077,Representing Text for Joint Embedding of Text and Knowledge Bases,2015,0.005062093101580545,2
W3136270197,Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models’ Transferability,2021,0.005061317737126941,2
W4283329669,Synwmd: Syntax-Aware Word Mover's Distance for Sentence Similarity Evaluation,2022,0.005061105858689367,2
W4385571309,I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation,2023,0.005061006694249407,2
W3172066753,"Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions",2021,0.005058000699878453,2
W4385734210,Improving Dutch Vaccine Hesitancy Monitoring via Multi-Label Data Augmentation with GPT-3.5,2023,0.005053788034758935,2
W4221157571,Ontology-enhanced Prompt-tuning for Few-shot Learning,2022,0.005051275993784675,2
W2987266335,Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding,2019,0.005047036140002178,2
W4385573387,Fine-Grained Extraction and Classification of Skill Requirements in German-Speaking Job Ads,2022,0.005043695329894088,2
W4318960928,TextGuise: Adaptive adversarial example attacks on text classification model,2023,0.005039749035647683,2
W3201977280,ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts,2021,0.005038951695854634,2
W4388333192,Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?,2023,0.005033434137531557,2
W4291653336,An Algorithm–Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers,2022,0.005028695730262017,2
W4285172793,Sequence-to-Sequence Knowledge Graph Completion and Question Answering,2022,0.0050230296260682745,2
W4408472883,BERT applications in natural language processing: a review,2025,0.0050192872124834986,2
W3169508552,Detecting Multilingual COVID-19 Misinformation on Social Media via Contextualized Embeddings,2021,0.005017540146681688,2
W3177898556,A Systematic Survey of Text Worlds as Embodied Natural Language Environments,2022,0.00501644759666704,2
W3082429057,HittER: Hierarchical Transformers for Knowledge Graph Embeddings,2021,0.005015559750223156,2
W4408925102,Prediction of Chromatographic Retention Time of a Small Molecule from SMILES Representation Using a Hybrid Transformer-LSTM Model,2025,0.005009455818391598,2
W4399857017,Use of Bidirectional Encoder Representations from Transformers (BERT) and Robustly Optimized Bert Pretraining Approach (RoBERTa) for Nepali News Classification,2024,0.005009455818391598,2
W4407425408,A review on persian question answering systems: from traditional to modern approaches,2025,0.00500751541263268,2
W2962685628,Shortcut-Stacked Sentence Encoders for Multi-Domain Inference,2017,0.005006414986097556,2
W3131577358,MTQA: Text‐Based Multitype Question and Answer Reading Comprehension Model,2021,0.005005343192090415,2
W3204342617,EntQA: Entity Linking as Question Answering,2021,0.005005004964938926,2
W4410371768,Continual Learning of Large Language Models: A Comprehensive Survey,2025,0.005002307058268089,2
W4394717739,Interactive Question Answering Systems: Literature Review,2024,0.00500025009618535,2
W4289523473,Performance analysis of transformer-based architectures and their ensembles to detect trait-based cyberbullying,2022,0.004999444003713534,2
W2995983533,Depth-adaptive Transformer,2020,0.004998575374878946,2
W4400230955,Hate Speech Detection using CoT and Post-hoc Explanation through Instruction-based Fine Tuning in Large Language Models,2024,0.004996577803455095,2
W2967690619,BERT-based Ranking for Biomedical Entity Normalization,2019,0.004996204847935111,2
W3034893234,Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks,2020,0.004994601993373064,2
W4409079480,NLP verification: towards a general methodology for certifying robustness,2025,0.004992715502099065,2
W2900874631,Attentive Convolution: Equipping CNNs with RNN-style Attention Mechanisms,2018,0.004989849271222956,2
W3172352177,Extreme Multi-label Learning for Semantic Matching in Product Search,2021,0.004983914330813374,2
W4285135170,Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge,2022,0.004982504469749304,2
W3100778284,Probing for Multilingual Numerical Understanding in Transformer-Based Language Models,2020,0.004979663800333455,2
W3101058639,BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition,2020,0.004979470919863996,2
W3092560821,Does Data Augmentation Improve Generalization in NLP?,2020,0.0049792686721482965,2
W4224313980,EvidenceNet: Evidence Fusion Network for Fact Verification,2022,0.004976811688325855,2
W4389523890,MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter,2023,0.0049702231630472425,2
W4206068491,Numeracy enhances the Literacy of Language Models,2021,0.004968364208151201,2
W4388405827,The Impact of Large Language Modeling on Natural Language Processing in Legal Texts: A Comprehensive Survey,2023,0.004966247411123298,2
W2982424689,A survey of word embeddings for clinical text,2019,0.004963998300984823,2
W4387078920,Incorporating entity-level knowledge in pretrained language model for biomedical dense retrieval,2023,0.004959921710932292,2
W2964172681,,2019,0.004958664046704058,2
W4389520007,Understanding HTML with Large Language Models,2023,0.004957646072927644,2
W3176899693,Effective Batching for Recurrent Neural Network Grammars,2021,0.004955915247390976,2
W3117559572,"Aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF, and Transfer Learning",2020,0.004955338736143916,2
W4406299883,Digital forgetting in large language models: a survey of unlearning methods,2025,0.004946102691515759,2
W2985638129,Book QA: Stories of Challenges and Opportunities,2019,0.004945400289438883,2
W4385573990,ScienceWorld: Is your Agent Smarter than a 5th Grader?,2022,0.004943641409762482,2
W3104313653,The Explanation Game: Towards Prediction Explainability through Sparse Communication,2020,0.00494300062558106,2
W4407761215,Building an intelligent diabetes Q&amp;A system with knowledge graphs and large language models,2025,0.004930286420621987,2
W3200247363,Explainable Natural Language Processing,2021,0.004915950583555008,2
W4385269188,SLR: A million-scale comprehensive crossword dataset for simultaneous learning and reasoning,2023,0.004909826006859092,2
W2963197830,What if We Simply Swap the Two Text Fragments? A Straightforward yet Effective Way to Test the Robustness of Methods to Confounding Signals in Nature Language Inference Tasks,2019,0.004908088619797025,2
W3116847845,Improving Multi-hop Knowledge Base Question Answering by Learning Intermediate Supervision Signals,2021,0.0049045365018192625,2
W2966750840,GraphFlow: Exploiting Conversation Flow with Graph Neural Networks for Conversational Machine Comprehension,2019,0.004903639136793216,2
W4390578107,On-Device Smishing Classifier Resistant to Text Evasion Attack,2024,0.004903466683791536,2
W4390660356,Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering,2024,0.0049007242312575396,2
W4385573616,Natural Language Deduction through Search over Statement Compositions,2022,0.004899552201613298,2
W2962911926,The Importance of Being Recurrent for Modeling Hierarchical Structure,2018,0.004891739413418986,2
W4385566950,Named Entity Recognition in Indian court judgments,2022,0.004889266842648264,2
W4303857107,Sentence-CROBI: A Simple Cross-Bi-Encoder-Based Neural Network Architecture for Paraphrase Identification,2022,0.004888023961059634,2
W4380085693,Retrospective Multi-granularity Fusion Network for Chinese Idiom Cloze-style Reading Comprehension,2023,0.004886403128421668,2
W4389518752,Contrastive Learning of Sentence Embeddings from Scratch,2023,0.004886258368257443,2
W3093553144,UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus,2020,0.004884187383468442,2
W2971339032,Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning,2019,0.004883803550413809,2
W4404688462,"Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",2024,0.004880338355065756,2
W4392637261,"MedRedQA for Medical Consumer Question Answering: Dataset, Tasks, and Neural Baselines",2023,0.004877087285061789,2
W3015339533,Generating Fact Checking Explanations,2020,0.0048767744117428164,2
W4320921249,Transformer-based language models for mental health issues: A survey,2023,0.004875036956652197,2
W2962961857,What do RNN Language Models Learn about Filler–Gap Dependencies?,2018,0.004874588380139431,2
W4407254385,Dynamic Layer-Wise Token Pruning for Sequence-to-Sequence Transformer Inference,2025,0.004873433636361886,2
W4225496162,Toward Practical Usage of the Attention Mechanism as a Tool for Interpretability,2022,0.004873137896299374,2
W4225661174,LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation,2022,0.004866195278602673,2
W3177450194,Language Model Evaluation Beyond Perplexity,2021,0.004864567469085071,2
W3189818462,Information Retrieval in an Infodemic: The Case of COVID-19 Publications,2021,0.004862611585296897,2
W4385973406,Automated reading passage generation with OpenAI's large language model,2023,0.004859776262204237,2
W3017549762,MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask Learning,2020,0.0048585284915071145,2
W4401857375,A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models,2024,0.0048550315731619486,2
W3021393209,Contrastive Self-Supervised Learning for Commonsense Reasoning,2020,0.004853107250453694,2
W4389523912,Evaluating Large Language Models on Controlled Generation Tasks,2023,0.0048513309866242945,2
W4407016464,A soft prompt learning method for medical text classification with simulated human cognitive capabilities,2025,0.00485052846525369,2
W4389520137,Disentangling Structure and Style: Political Bias Detection in News by Inducing Document Hierarchy,2023,0.004848865809887123,2
W4323055192,Controllable Text Generation Using Semantic Control Grammar,2023,0.004848533077310394,2
W3155457426,Cross-lingual Contextualized Topic Models with Zero-shot Learning,2021,0.004846495961488209,2
W4389520360,From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base,2023,0.004841754749660477,2
W4405025923,Investigating OCR-Sensitive Neurons to Improve Entity Recognition in Historical Documents,2024,0.004841683991270938,2
W2951675429,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,2019,0.004840708376044069,2
W4392818979,Contrasting Linguistic Patterns in Human and LLM-Generated News Text,2024,0.004838390934434355,2
W4386761922,CTSARF: A Chinese Text Similarity Analysis Model based on Residual Fusion,2023,0.004838325676694239,2
W4285606364,Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering,2022,0.004831972063797684,2
W3093150592,Drug repurposing for COVID-19 via knowledge graph completion,2021,0.004831937196465849,2
W4392669753,"A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",2023,0.004827594233427148,2
W4394877201,CGKPN: Cross-Graph Knowledge Propagation Network with Adaptive Connection for Reasoning-Based Machine Reading Comprehension,2024,0.004827393793292269,2
W4393161231,CFEVER: A Chinese Fact Extraction and VERification Dataset,2024,0.0048252791012446715,2
W2988152053,Dice Loss for Data-imbalanced NLP Tasks,2019,0.004822679629724958,2
W4287889449,StATIK: Structure and Text for Inductive Knowledge Graph Completion,2022,0.004819778663937624,2
W4200629808,ISEEQ: Information Seeking Question Generation Using Dynamic Meta-Information Retrieval and Knowledge Graphs,2022,0.004813150726122207,2
W4390545867,Contextual Word Embedding for Biomedical Knowledge Extraction: a Rapid Review and Case Study,2024,0.004808543077933629,2
W3204085121,Entity Linking Meets Deep Learning: Techniques and Solutions,2021,0.004807660777173737,2
W4389518745,ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph,2023,0.004806152591501725,2
W4391093854,Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models,2023,0.004803100945397769,2
W3199373335,The Grammar-Learning Trajectories of Neural Language Models,2022,0.004802744767430269,2
W3106224367,ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing,2019,0.004802701885889929,2
W4385901301,Effectively Modeling Sentence Interactions With Factorization Machines for Fact Verification,2023,0.004801194288191214,2
W3199912364,Box Embeddings: An open-source library for representation learning using geometric structures,2021,0.004799300136151307,2
W4324056402,OdeBERT: One-stage Deep-supervised Early-exiting BERT for Fast Inference in User Intent Classification,2023,0.004796695468336467,2
W4389829637,A Relation Embedding Assistance Networks for Multi-hop Question Answering,2023,0.004793887744356238,2
W4385570030,What Do NLP Researchers Believe? Results of the NLP Community Metasurvey,2023,0.004793368135918229,2
W4385573164,Fine-tuned Language Models are Continual Learners,2022,0.004792369656360183,2
W3090721331,SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval,2020,0.004789901700166336,2
W4393160541,Mitigating the Impact of False Negative in Dense Retrieval with Contrastive Confidence Regularization,2024,0.004784221131901249,2
W3174693310,AND does not mean OR: Using Formal Languages to Study Language Models’ Representations,2021,0.0047822910055769765,2
W4385728911,A divide and conquer framework for Knowledge Editing,2023,0.004776586049962955,2
W3206832494,Chimera,2021,0.004774647068914491,2
W3105801792,COMETA: A Corpus for Medical Entity Linking in the Social Media,2020,0.004770428475092982,2
W4402753735,Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding,2024,0.004765214910763362,2
W4287855115,Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context,2022,0.004753159520188723,2
W4386488973,<b>MIRACL</b>: A Multilingual Retrieval Dataset Covering 18 Diverse Languages,2023,0.004752224603290078,2
W3144312515,"Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency Benefits",2021,0.004751018404671149,2
W4224873813,Table-based Fact Verification with Self-adaptive Mixture of Experts,2022,0.004750466153231233,2
W4390098899,Pre-Trained Models for Intent Classification in Chatbot: Comparative Study and Critical Analysis,2023,0.004749125198102966,2
W3088335873,A,2019,0.004747818920888625,2
W4380080770,AI-based novelty detection in crowdsourced idea spaces,2023,0.004745952843395477,2
W4221139076,Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation,2022,0.00474240770014625,2
W4285266914,"Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem",2022,0.004742288961043705,2
W4385573834,E-NER — An Annotated Named Entity Recognition Corpus of Legal Text,2022,0.004740862371159765,2
W4409094323,Lightweight Pre-Trained Korean Language Model Based on Knowledge Distillation and Low-Rank Factorization,2025,0.004740417993222329,2
W4385564928,OpenICL: An Open-Source Framework for In-context Learning,2023,0.00473789854728829,2
W4321485718,Making Pre-trained Language Models End-to-end Few-shot Learners with Contrastive Prompt Tuning,2023,0.004734617769563495,2
W4367185264,Prompting Is Programming: A Query Language for Large Language Models,2023,0.004731526282976642,2
W4385570457,Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion,2023,0.004726480905711704,2
W4400580071,Beyond algorithms: The human touch machine-generated titles for enhancing click-through rates on social media,2024,0.004725947264545484,2
W4385571746,DiSCoMaT: Distantly Supervised Composition Extraction from Tables in Materials Science Articles,2023,0.004724801058571427,2
W4400869639,LLM-Commentator: Novel fine-tuning strategies of large language models for automatic commentary generation using football event data,2024,0.004721659398380484,2
W3196778134,Transformer models for enhancing AttnGAN based text to image generation,2021,0.004717729111537033,2
W4393325714,Leverage NLP Models Against Other NLP Models: Two Invisible Feature Space Backdoor Attacks,2024,0.004715041868845884,2
W4310009523,Stance Detection of Political Tweets with Transformer Architectures,2022,0.004712295159079451,2
W3213407400,How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN,2023,0.004710334865648884,2
W3186065854,Dual‐Channel Reasoning Model for Complex Question Answering,2021,0.004708407965894848,2
W4391286686,Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding,2024,0.004704739836812753,2
W3128288937,Sequential sentence classification in research papers using cross-domain multi-task learning,2024,0.0047007779765817935,2
W2963499246,Towards Universal Paraphrastic Sentence Embeddings,2016,0.004700308753774001,2
W4385568240,WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences,2023,0.004697546107319994,2
W4392207929,Legal Document Similarity Matching Based on Ensemble Learning,2024,0.004696789808745066,2
W4393152682,Knowledge Graph Prompting for Multi-Document Question Answering,2024,0.004694822651623953,2
W4389520219,impact of sample selection on in-context learning for entity extraction from scientific writing,2023,0.0046934218721252745,2
W4399077053,Beyond extraction accuracy: addressing the quality of geographical named entity through advanced recognition and correction models using a modified BERT framework,2024,0.00469053493389152,2
W4380302120,Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter,2023,0.00468616906858876,2
W4316039346,TaxonPrompt: Taxonomy-aware curriculum prompt learning for few-shot event classification,2023,0.004681639652155274,2
W3104196571,Machine Generation and Detection of Arabic Manipulated and Fake News,2020,0.004679529767923785,2
W4409572435,Fact retrieval from knowledge graphs through semantic and contextual attention,2025,0.004676806526330241,2
W4285195854,LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,2022,0.004671244720505907,2
W4406901061,SecLMNER: A framework for enhanced named entity recognition in multi-source cybersecurity data using large language models,2025,0.0046710682057629314,2
W4389524484,CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code,2023,0.004670593394056496,2
W4285286514,ExtEnD: Extractive Entity Disambiguation,2022,0.004668899651838603,2
W4400113179,A Review on Neuro-symbolic AI Improvements to Natural Language Processing,2024,0.004668397151849097,2
W4285192560,Large Language Models are Not Models of Natural Language: They are Corpus Models,2022,0.004666422157661182,2
W3137280539,A Hybrid Siamese Neural Network for Natural Language Inference in Cyber-Physical Systems,2021,0.004665035634020969,2
W4403865056,"The Deep Integration of Knowledge Graphs and Large Language Models: Advancements, Challenges, and Future Directions",2024,0.004661344112104271,2
W2739505524,Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task,2017,0.004660652389827667,2
W3215569511,Lexicon-Based Methods vs. BERT for Text Sentiment Analysis,2022,0.004659573940874244,2
W4393066104,Gs-Cbr-Kbqa: Graph-Structured Case-Based Reasoning for Knowledge Base Question Answering,2024,0.004654683124332399,2
W4389520255,Demystifying Prompts in Language Models via Perplexity Estimation,2023,0.004650480059004677,2
W3169554260,A Comparison of Pre-Trained Language Models for Multi-Class Text Classification in the Financial Domain,2021,0.004650408054664978,2
W4385569919,NLPeer: A Unified Resource for the Computational Study of Peer Review,2023,0.004648768145862798,2
W2963096121,On Adversarial Removal of Hypothesis-only Bias in Natural Language Inference,2019,0.004647295834511707,2
W4224491635,Zero-shot Query Contextualization for Conversational Search,2022,0.004643924595785872,2
W3035413376,Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading,2020,0.004641714959770225,2
W4389519513,DUnE: Dataset for Unified Editing,2023,0.004640455141969602,2
W3109507892,KGTK: A Toolkit for Large Knowledge Graph Manipulation and Analysis,2020,0.0046390805670128476,2
W4387171991,Revision Transformers: Instructing Language Models to Change Their Values,2023,0.004636579873408497,2
W4396927733,Scoring Multi-hop Question Decomposition Using Masked Language Models,2024,0.004625589277580826,2
W4393061348,Unlocking maintenance insights in industrial text through semantic search,2024,0.004625584113594905,2
W4307392750,BioKnowPrompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction,2022,0.004622569186401694,2
W4307935822,KEPT: Knowledge Enhanced Prompt Tuning for event causality identification,2022,0.004613602780798828,2
W4406788559,Automatic instantiation of assurance cases from patterns using large language models,2025,0.0046133798275617525,2
W4401413215,The overview of the BioRED (Biomedical Relation Extraction Dataset) track at BioCreative VIII,2024,0.004608912758017381,2
W4401933528,Contrastive Learning with Transformer initialization and clustering prior for text representation,2024,0.004603058125108808,2
W3011794880,Overview of the TREC 2019 deep learning track.,2020,0.004602461252427655,2
W3195014936,Conversations with Search Engines: SERP-based Conversational Response Generation,2021,0.004599408132712925,2
W3195369073,Data Augmentation for Low-Resource Named Entity Recognition Using Backtranslation,2021,0.004596867051061173,2
W4385571271,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,2023,0.004589960817314708,2
W4221150160,GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models,2022,0.004586015136940097,2
W4225882738,Diaformer: Automatic Diagnosis via Symptoms Sequence Generation,2022,0.004584862971664445,2
W3036879053,Memory-Efficient Pipeline-Parallel DNN Training,2020,0.004584700695930188,2
W4220832158,iSEA: An Interactive Pipeline for Semantic Error Analysis of NLP Models,2022,0.004583411160550138,2
W4389519850,GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence,2023,0.004583252030724229,2
W4389524313,We’re Afraid Language Models Aren’t Modeling Ambiguity,2023,0.004581671209527086,2
W3133665323,Biomedical Named Entity Recognition at Scale,2021,0.004578622789067397,2
W3098794549,Weakly- and Semi-supervised Evidence Extraction,2020,0.00457166363456894,2
W4385270279,Construction and Applications of Billion-Scale Pre-Trained Multimodal Business Knowledge Graph,2023,0.004571034246229831,2
W4296142626,Towards More Generalizable and Accurate Sentence Classification in Medical Abstracts with Less Data,2022,0.004570620303063056,2
W2998112083,Automatic Fact-Guided Sentence Modification,2020,0.004568277076330172,2
W4298111738,Virtual prompt pre-training for prototype-based few-shot relation extraction,2022,0.0045675122930336634,2
W4408156536,Robust Infidelity: When Faithfulness Measures on Masked Language Models Are Misleading,2025,0.004567224000961629,2
W4319042407,A study on surprisal and semantic relatedness for eye-tracking data prediction,2023,0.004567102781292277,2
W3198431451,BERT-based Dense Retrievers Require Interpolation with BM25 for Effective Passage Retrieval,2021,0.004565121897175575,2
W4295936540,Evaluating Attribution Methods for Explainable NLP with Transformers,2022,0.00456078017752634,2
W4400770808,Exploring Universal Intrinsic Task Subspace for Few-Shot Learning via Prompt Tuning,2024,0.004556174034873309,2
W4285744810,A BERT-based ensemble learning approach for the BioCreative VII challenges: full-text chemical identification and multi-label classification in PubMed articles,2022,0.004554062410489066,2
W4295065948,Arabic machine reading comprehension on the Holy Qur’an using CL-AraBERT,2022,0.004554059144444418,2
W4323315336,“Transforming” Personality Scale Development: Illustrating the Potential of State-of-the-Art Natural Language Processing,2023,0.004552614083577827,2
W3036116903,Learning Lexical Subspaces in a Distributional Vector Space,2020,0.0045525009392650174,2
W2973154071,SciBERT: A Pretrained Language Model for Scientific Text,2019,0.00455184998815325,2
W4385188677,"Predict, pretrained, select and answer: Interpretable and scalable complex question answering over knowledge bases",2023,0.004551822668626231,2
W3173423412,"Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking",2021,0.004547284022550384,2
W4367628274,A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning,2023,0.004545287428111711,2
W3037626499,Compositional Explanations of Neurons,2020,0.0045426904666351405,2
W4407943414,Attention-based backdoor attacks against natural language processing models,2025,0.0045419041718576985,2
W2766371743,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,2017,0.004539938128643545,2
W3012038618,Document Ranking with a Pretrained Sequence-to-Sequence Model,2020,0.004538542349533495,2
W3152151101,CEQE: Contextualized Embeddings for Query Expansion,2021,0.004538039876551046,2
W3128169560,"Revealing Opinions for COVID-19 Questions Using a Context Retriever, Opinion Aggregator, and Question-Answering Model: Model Development Study",2021,0.00453654298465263,2
W2962925243,Supervised and Unsupervised Transfer Learning for Question Answering,2018,0.00453599483864547,2
W3161231563,Named Entity Aware Transfer Learning for Biomedical Factoid Question Answering,2021,0.004530673465263088,2
W4386566752,"Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",2023,0.0045296829741039235,2
W4317772742,Handbook of Computational Social Science for Policy,2023,0.004528235209614985,2
W4224134764,WikiDiverse: A Multimodal Entity Linking Dataset with Diversified Contextual Topics and Entity Types,2022,0.004522670166534251,2
W3095211433,KnowlyBERT - Hybrid Query Answering over Language Models and Knowledge Graphs,2020,0.004516276516057925,2
W4287887524,Global Entity Disambiguation with BERT,2022,0.004516249007715867,2
W4406711870,Unlocking wisdom: enhancing biomedical question answering with domain knowledge,2025,0.00451396586017595,2
W4385764073,A Survey on Out-of-Distribution Evaluation of Neural NLP Models,2023,0.004511563655662404,2
W4313563529,QATest: A Uniform Fuzzing Framework for Question Answering Systems,2022,0.004510219533462005,2
W3102401511,Content Planning for Neural Story Generation with Aristotelian Rescoring,2020,0.0045064586375085074,2
W4385573261,Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations,2022,0.004504453880125016,2
W4402352366,Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text Detection,2024,0.004503536285592575,2
W4389166691,GPT4AIGChip: Towards Next-Generation AI Accelerator Design Automation via Large Language Models,2023,0.004502877603230517,2
W2955753753,UCL Machine Reading Group: Four Factor Framework For Fact Finding (HexaF),2018,0.004498266505921557,2
W4385572468,EPIC: Multi-Perspective Annotation of a Corpus of Irony,2023,0.004497419495113523,2
W4390739985,<scp>AmbiFC</scp>: Fact-Checking Ambiguous Claims with Evidence,2024,0.00448399147617392,2
W4404486407,Matching patients to clinical trials with large language models,2024,0.004483247590156152,2
W4385757404,Clinical Prompt Learning With Frozen Language Models,2023,0.004479683902617031,2
W3174432697,Exploring Listwise Evidence Reasoning with T5 for Fact Verification,2021,0.004475260489468653,2
W4389524473,Adapting Language Models to Compress Contexts,2023,0.004471376936449978,2
W4385569882,Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework,2023,0.00447103163987217,2
W4281765107,Cross-domain multi-task learning for sequential sentence classification in research papers,2022,0.0044694552935619775,2
W3135665608,CEQE: Contextualized Embeddings for Query Expansion,2021,0.00446814402059062,2
W4389524379,"The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",2023,0.004468017647528269,2
W2949694638,Improving Question Answering over Incomplete KBs with Knowledge-Aware Reader,2019,0.004466682889406788,2
W4388725043,A study of generative large language model for medical research and healthcare,2023,0.004464080988492022,2
W4391613817,Exploring the performance and explainability of fine-tuned BERT models for neuroradiology protocol assignment,2024,0.004461932368860674,2
W4392202585,Automatic quantitative stroke severity assessment based on Chinese clinical named entity recognition with domain-adaptive pre-trained large language model,2024,0.004458038880351499,2
W3175019732,Adversarial Learning for Discourse Rhetorical Structure Parsing,2021,0.0044569572738148095,2
W3170822064,Clustering-based Inference for Biomedical Entity Linking,2021,0.004456848223018525,2
W4406141444,Large language models as oracles for instantiating ontologies with domain-specific knowledge,2025,0.004456729762772821,2
W4395956893,Studying and recommending information highlighting in Stack Overflow answers,2024,0.004455017931045086,2
W4409291291,ALPET: Active few-shot learning for citation worthiness detection in low-resource Wikipedia languages,2025,0.0044488290233102865,2
W3154271556,Conversational Question Answering over Knowledge Graphs with Transformer and Graph Attention Networks,2021,0.00443595782069667,2
W4309659061,Chemical–protein relation extraction with ensembles of carefully tuned pretrained language models,2022,0.004433746915690055,2
W4230262515,BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief,2021,0.004430876071544116,2
W3115990655,Natural Language Processing-Based Quantication of the Mental State of Psychiatric Patients,2020,0.004428358785265964,2
W4315643106,Lexical knowledge enhanced text matching via distilled word sense disambiguation,2023,0.004427288863570346,2
W4385679821,Analyzing Leakage of Personally Identifiable Information in Language Models,2023,0.004421863187113445,2
W4386374572,Improving task generalization via unified schema prompt,2023,0.00442125945624322,2
W4385571787,Contrastive Learning of Sociopragmatic Meaning in Social Media,2023,0.004419769001163446,2
W2994967700,Pre-Training of Deep Bidirectional Protein Sequence Representations with Structural Information,2019,0.004416186090638263,2
W3194157311,Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval,2021,0.004415886937125967,2
W4394743141,Evaluating the Ripple Effects of Knowledge Editing in Language Models,2024,0.004415391286345874,2
W4322766928,Evidence of a predictive coding hierarchy in the human brain listening to speech,2023,0.004412764148236277,2
W4385572727,Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning,2022,0.00441200530965014,2
W3209527742,"Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification",2021,0.004410593957610745,2
W3098205952,Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning,2020,0.004408460256693203,2
W4391127883,Predictive typing method for Persian office automation,2024,0.004407205479805915,2
W4210634745,Survey on English Entity Linking on Wikidata: Datasets and approaches,2022,0.004406471431026639,2
W4311829616,Large-scale application of named entity recognition to biomedicine and epidemiology,2022,0.004397300810979228,2
W4226400527,An Evaluation of Pretrained BERT Models for Comparing Semantic Similarity Across Unstructured Clinical Trial Texts,2022,0.004397154105941252,2
W4407888437,A Maritime Document Knowledge Graph Construction Method Based on Conceptual Proximity Relations,2025,0.0043965204556965405,2
W4385565374,A Practical Toolkit for Multilingual Question and Answer Generation,2023,0.004391910373707986,2
W4285269500,Parameter-Efficient Abstractive Question Answering over Tables or Text,2022,0.004391133289717504,2
W4385571206,Learning Joint Structural and Temporal Contextualized Knowledge Embeddings for Temporal Knowledge Graph Completion,2023,0.004390749573822426,2
W4392849937,Local Interpretations for Explainable Natural Language Processing: A Survey,2024,0.004390517839879017,2
W4303614602,Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future,2022,0.0043889691077168965,2
W3173805051,"ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",2021,0.004386269147328352,2
W3169965252,Grey-box Adversarial Attack And Defence For Sentiment Classification,2021,0.004383649994398306,2
W4312925950,German Medical Named Entity Recognition Model and Data Set Creation Using Machine Translation and Word Alignment: Algorithm Development and Validation,2022,0.004379572922869744,2
W3013843954,Generating Sentiment-Preserving Fake Online Reviews Using Neural Language Models and Their Human- and Machine-Based Detection,2020,0.0043768019536879485,2
W4407054226,EHR-based prediction modelling meets multimodal deep learning: A systematic review of structured and textual data fusion methods,2025,0.004373529742374749,2
W3153080312,UHD-BERT: Bucketed Ultra-High Dimensional Sparse Representations for Full Ranking.,2021,0.004372823366878315,2
W4400065641,A few-shot word-structure embedded model for bridge inspection reports learning,2024,0.004367050104145128,2
W4388267439,Can Large Language Models Revolutionalize Open Government Data Portals? A Case of Using ChatGPT in statistics.gov.scot,2023,0.004367024535257986,2
W4311173007,Developing a deep learning natural language processing algorithm for automated reporting of adverse drug reactions,2022,0.004366938433608599,2
W2791751435,Do latent tree learning models identify meaningful structure in sentences?,2018,0.004364012724658557,2
W3158856706,Scholarly Text Classification with Sentence BERT and Entity Embeddings,2021,0.004363048431503011,2
W1793121960,End-to-end memory networks,2015,0.0043615434596213,2
W2786472750,Complex Sequential Question Answering: Towards Learning to Converse Over Linked Question Answer Pairs with a Knowledge Graph,2018,0.004361143414029389,2
W4409385489,A comprehensive survey on integrating large language models with knowledge-based methods,2025,0.00436019538482432,2
W4385570071,Better Zero-Shot Reasoning with Self-Adaptive Prompting,2023,0.004359620768248217,2
W4385573264,Natural Logic-guided Autoregressive Multi-hop Document Retrieval for Fact Verification,2022,0.0043573023579884275,2
W3157876196,Biomedical named entity recognition using BERT in the machine reading comprehension framework,2021,0.004356589975673483,2
W4408221692,Denoising Implicit Feedback for Extractive Question Answering,2025,0.004356101268370366,2
W4410068129,SensorQA: A Question Answering Benchmark for Daily-Life Monitoring,2025,0.004356030289428929,2
W4407542088,Towards Lifelong Learning of Large Language Models: A Survey,2025,0.004350053168019961,2
W2528904340,Embracing data abundance: BookTest Dataset for Reading Comprehension,2016,0.004344745114089933,2
W3028004046,Recent advances in biomedical literature mining,2020,0.0043430754209345035,2
W4283204541,Lexical semantics enhanced neural word embeddings,2022,0.00433699690450575,2
W3119652442,Machine Generation and Detection of Arabic Manipulated and Fake News,2020,0.0043331094947359925,2
W2984354699,Zero-shot Entity Linking with Dense Entity Retrieval.,2019,0.004331138653005624,2
W4399372400,COA-GPT: Generative Pre-Trained Transformers for Accelerated Course of Action Development in Military Operations,2024,0.004330884615447089,2
W4392435065,PROSAIL-Net: A transfer learning-based dual stream neural network to estimate leaf chlorophyll and leaf angle of crops from UAV hyperspectral images,2024,0.004330884615447089,2
W3104007871,Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph,2020,0.004330865395674097,2
W4221099399,Construction and Evaluation of a High-Quality Corpus for Legal Intelligence Using Semiautomated Approaches,2022,0.0043286271781949525,2
W2889002152,Story Ending Generation with Incremental Encoding and Commonsense Knowledge,2019,0.00432755867409567,2
W4389520411,Does the English Matter? Elicit Cross-lingual Abilities of Large Language Models,2023,0.0043274053116093975,2
W4368616878,Ontology-driven and weakly supervised rare disease identification from clinical notes,2023,0.004327132317008371,2
W4389519496,Unlearn What You Want to Forget: Efficient Unlearning for LLMs,2023,0.004327018627199132,2
W4410570038,Large Language Models in Portuguese for Healthcare: A Systematic Review,2025,0.004325646005930192,2
W4393156919,Combining Multiple Supervision for Robust Zero-Shot Dense Retrieval,2024,0.004322090467281429,2
W2914924671,Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis,2018,0.004320395298587645,2
W3089073032,Exploring the relationship between social presence and learners’ prestige in MOOC discussion forums using automated content analysis and social network analysis,2020,0.004318419665419754,2
W4385567030,Probing Structural Knowledge from Pre-trained Language Model for Argumentation Relation Classification,2022,0.0043156877108918605,2
W4286226650,A sequence labeling framework for extracting drug–protein relations from biomedical literature,2022,0.004313515320806009,2
W3134691471,Explainable automated coding of clinical notes using hierarchical label-wise attention networks and label embedding initialisation,2021,0.004312330603311323,2
W2964302308,Subword-augmented Embedding for Cloze Reading Comprehension.,2018,0.004310947050590763,2
W3174745742,Using Social and Linguistic Information to Adapt Pretrained Representations for Political Perspective Identification,2021,0.00430791639650265,2
W4385574242,UniRPG: Unified Discrete Reasoning over Table and Text as Program Generation,2022,0.004302943896341592,2
W3214530841,FaBULOUS: Fact-checking Based on Understanding of Language Over Unstructured and Structured information,2021,0.004299457722480473,2
W4393158050,Verbal Reports as Data Revisited: Using Natural Language Models to Validate Cognitive Models,2024,0.004295717862652917,2
W4409787112,Benchmarking large language models for automated labeling: The case of issue report classification,2025,0.004292005033695653,2
W2966610483,Representation Degeneration Problem in Training Natural Language Generation Models,2019,0.004291816372686389,2
W4372311765,GS-InGAT: An interaction graph attention network with global semantic for knowledge graph completion,2023,0.004291499477053089,2
W4312091558,Predicting dementia from spontaneous speech using large language models,2022,0.004287299682975149,2
W4284705591,On the Role of Relevance in Natural Language Processing Tasks,2022,0.004285398534405738,2
W1951216520,Visualizing and Understanding Recurrent Networks,2015,0.004281425467907911,2
W4391697043,The MorPhEMe Machine: An Addressable Neural Memory for Learning Knowledge-Regularized Deep Contextualized Chinese Embedding,2024,0.004277650057532876,2
W4385830460,FNCSE: contrastive learning for unsupervised sentence embedding with false negative samples,2023,0.004275315783127103,2
W4385571505,Complementary Explanations for Effective In-Context Learning,2023,0.004273160860189712,2
W3022006665,GenericsKB: A Knowledge Base of Generic Statements,2020,0.004272754973535148,2
W4405195133,Recent Advances of Foundation Language Models-based Continual Learning: A Survey,2024,0.004265562837541829,2
W4389520468,Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy,2023,0.004265291230872328,2
W4385571379,Infusing Hierarchical Guidance into Prompt Tuning: A Parameter-Efficient Framework for Multi-level Implicit Discourse Relation Recognition,2023,0.004264687893919729,2
W3206945533,The Irrationality of Neural Rationale Models,2022,0.004264479223749035,2
W2996125274,Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues Dataset,2019,0.004259748265408534,2
W4387966251,AttentionViz: A Global View of Transformer Attention,2023,0.004259016689675935,2
W3083835029,Generative Language Modeling for Automated Theorem Proving.,2020,0.0042572123873314905,2
W4389519117,Don’t Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs,2023,0.004253924388214771,2
W3106231035,Validation of deep learning natural language processing algorithm for keyword extraction from pathology reports in electronic health records,2020,0.004252640863453605,2
W3188660305,Let’s Play<tt>Mono</tt>-<tt>Poly</tt>: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses,2021,0.004250573007547527,2
W4392712776,DAEPK:Domain-Adaptive Text Feature Enhancement Technology Integrating Prior Knowledge Domain In Text Classification,2024,0.004248797639197244,2
W4399632250,CAREER: Context-Aware API Recognition with Data Augmentation for API Knowledge Extraction,2024,0.004243808318366046,2
W4385574308,Semantic Segmentation of Legal Documents via Rhetorical Roles,2022,0.004240295527475696,2
W4292387193,Query Path Generation via Bidirectional Reasoning for Multihop Question Answering From Knowledge Bases,2022,0.004239206071161879,2
W2566011400,Multi-Perspective Context Matching for Machine Comprehension,2016,0.004239010084868399,2
W4386566918,"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility",2023,0.004238239437309644,2
W2619818172,Jointly learning sentence embeddings and syntax with unsupervised Tree-LSTMs,2019,0.004234936115846562,2
W4408095213,FedHLT: Efficient Federated Low-Rank Adaption with Hierarchical Language Tree for Multilingual Modeling,2025,0.004232577527833809,2
W4385768252,A Multi-Modal Neural Geometric Solver with Textual Clauses Parsed from Diagram,2023,0.004231639884978085,2
W4385572505,Commonsense Knowledge Graph Completion Via Contrastive Pretraining and Node Clustering,2023,0.004228252799465894,2
W4390872859,Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models,2023,0.004227689421425218,2
W4285301802,ProtoTEx: Explaining Model Decisions with Prototype Tensors,2022,0.004223024191282815,2
W3204793433,Question Answering Chatbot for Troubleshooting Queries based on Transfer Learning,2021,0.004220200855360288,2
W4387004608,Combining prompt learning with contextual semantics for inductive relation prediction,2023,0.004219939994579406,2
W4366769254,Surprisal does not explain syntactic disambiguation difficulty: evidence from a large-scale benchmark,2023,0.004218179935390675,2
W4285222754,Tree-KGQA: An Unsupervised Approach for Question Answering Over Knowledge Graphs,2022,0.004217271500775016,2
W4385572096,BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases,2023,0.0042163300348864625,2
W4224315024,LitMC-BERT: Transformer-Based Multi-Label Classification of Biomedical Literature With An Application on COVID-19 Literature Curation,2022,0.004214290679061073,2
W4400491826,A survey on short text similarity measurement methods,2023,0.004206907482371394,2
W2546950329,Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision,2017,0.004205625058111362,2
W4385565015,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,2023,0.004203467240416213,2
W4389520705,Copyright Violations and Large Language Models,2023,0.004201697744516891,2
W4385572389,TüReuth Legal at SemEval-2023 Task 6: Modelling Local and Global Structure of Judgements for Rhetorical Role Prediction,2023,0.004199009489249158,2
W3161381829,"Text Analysis for Psychology: Methods, Principles, and Practices",2021,0.004197309088755854,2
W2211192759,ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs,2016,0.004196234129389058,2
W4285305556,Learning to Generate Programs for Table Fact Verification via Structure-Aware Semantic Parsing,2022,0.004192823720103402,2
W4287887263,Aligning to Social Norms and Values in Interactive Narratives,2022,0.004190131653974934,2
W2949202705,A Multiscale Visualization of Attention in the Transformer Model,2019,0.004187492096698802,2
W3173209146,PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support,2021,0.004182913990643848,2
W4375870244,Programming-by-Demonstration for Long-Horizon Robot Tasks,2024,0.00417713836512748,2
W4379379858,Post Hoc Explanations of Language Models Can Improve Language Models,2023,0.004176294864889997,2
W4389520779,StructGPT: A General Framework for Large Language Model to Reason over Structured Data,2023,0.004176290864661103,2
W4389565330,A hybrid style transfer with whale optimization algorithm model for textual adversarial attack,2023,0.004169649966131378,2
W3216037316,"Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs",2021,0.004165286964770666,2
W4398182257,Towards Controllable Generative Design: A Conceptual Design Generation Approach Leveraging the FBS Ontology and Large Language Models,2024,0.004164563190703048,2
W4408164385,Sentential Cross-lingual Paraphrase Detection for English-Urdu Language Pair,2025,0.004162135333747278,2
W4362679551,Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification,2023,0.004157474084306029,2
W4281641068,The state of the art in open domain complex question answering: a survey,2022,0.004155091072356475,2
W2963951265,"Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",2018,0.004154954631234293,2
W3174687042,Automatic Fake News Detection: Are Models Learning to Reason?,2021,0.0041545438450822135,2
W4385571325,A fine-grained comparison of pragmatic language understanding in humans and language models,2023,0.004154171687798289,2
W4403864719,Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks,2024,0.00415387701427711,2
W4407354885,The foundational capabilities of large language models in predicting postoperative risks using clinical notes,2025,0.004151292917569815,2
W4389524330,DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models,2023,0.004144973150644174,2
W2998386992,Parsing as Pretraining,2020,0.004143089562293197,2
W4407230512,Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations,2025,0.0041426537532502925,2
W4409581404,Handling Causal Tasks by Large Language Models: From Discovery to Reasoning,2025,0.004136469018478482,2
W4378364109,FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction,2023,0.004134197494993372,2
W3030030185,Comparing BERT against traditional machine learning text classification,2020,0.004131805988400227,2
W3173526187,Knowledge-Enriched Event Causality Identification via Latent Structure Induction Networks,2021,0.004131658364201076,2
W4387171262,GENEMASK: Fast Pretraining of Gene Sequences to Enable Few-Shot Learning,2023,0.004129516848866627,2
W3112891652,Causal BERT: Language Models for Causality Detection Between Events Expressed in Text,2021,0.004129056978411413,2
W4393308651,Prefix Data Augmentation for Contrastive Learning of Unsupervised Sentence Embedding,2024,0.004128051317087116,2
W4313492484,Comparison of BERT implementations for natural language processing of narrative medical documents,2022,0.004127387882868938,2
W4385570504,Counterfactual Debiasing for Fact Verification,2023,0.0041246286450968075,2
W4409892328,Assessing and Understanding Creativity in Large Language Models,2025,0.004124175500200769,2
W4389524345,FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions,2023,0.004119983370301642,2
W4285279063,CAISA at WASSA 2022: Adapter-Tuning for Empathy Prediction,2022,0.004116536417997655,2
W4409649497,Leveraging encoder-only large language models for mobile app review feature extraction,2025,0.004115505245756583,2
W4409186139,Annotating scientific uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches,2025,0.004112840756678026,2
W4403943146,Multi-hop Reading Comprehension Model Based on Abstract Meaning Representation and Multi-task Joint Learning,2024,0.004112256821950528,2
W4225632115,Language Models as Knowledge Embeddings,2022,0.004112175174824726,2
W4392594189,Contribution Analysis of Large Language Models and Data Augmentations for Person Names in Solving Legal Bar Examination at COLIEE 2023,2024,0.0041119432798687894,2
W3139266014,Global citation recommendation employing generative adversarial network,2021,0.004110582086402918,2
W3152268000,Data augmentation in natural language processing: a novel text generation approach for long and short text classifiers,2022,0.0041105658898661435,2
W4387221452,Question answering over knowledge graphs using BERT based relation mapping,2023,0.0041086716117396125,2
W4372219079,Sentiment spin: Attacking financial sentiment with GPT-3,2023,0.004107242969990444,2
W4385570695,PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives,2023,0.0041066724686001504,2
W3037979089,Enriching contextualized language model from knowledge graph for biomedical information extraction,2020,0.004106426753014234,2
W3104602136,AnswerFact: Fact Checking in Product Question Answering,2020,0.004103351765437825,2
W4389519877,NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval,2023,0.00410098071964882,2
W4385718101,Can ChatGPT Understand Causal Language in Science Claims?,2023,0.004100118292346466,2
W4386566893,Combining Parameter-efficient Modules for Task-level Generalisation,2023,0.004099609551949341,2
W4385572411,Learning by Analogy: Diverse Questions Generation in Math Word Problem,2023,0.004093521558465255,2
W4386361581,Detection of Fake Generated Scientific Abstracts,2023,0.004093167808438316,2
W2963077723,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,2018,0.004091626087953736,2
W4389518888,"Toward Human Readable Prompt Tuning: Kubrick’s The Shining is a good movie, and a good prompt too?",2023,0.0040822487687466776,2
W2971531230,The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives,2019,0.004081825233628134,2
W3194676777,WinoGrande,2021,0.004081536695603385,2
W4200444669,Learning from Disagreement: A Survey,2021,0.004080358903526319,2
W2963491027,The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations,2017,0.0040783849688776595,2
W3034649382,Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,2020,0.0040775467898007366,2
W4225929875,Leashing the Inner Demons: Self-Detoxification for Language Models,2022,0.0040700041122114766,2
W4402270921,Language Model Crossover: Variation through Few-Shot Prompting,2024,0.004067985329401513,2
W3213748474,DeepRhole: deep learning for rhetorical role labeling of sentences in legal case documents,2021,0.004067343484004598,2
W2609368435,Deep Text Classification Can be Fooled,2018,0.004066626882970627,2
W4401343632,IQAGPT: computed tomography image quality assessment with vision-language and ChatGPT models,2024,0.004066257234237768,2
W4408362299,AI Computing Systems for Large Language Models Training,2025,0.004064316713893425,2
W4226334043,ARL: An adaptive reinforcement learning framework for complex question answering over knowledge base,2022,0.004063164449550098,2
W3163457243,Developing a BERT based triple classification model using knowledge graph embedding for question answering system,2021,0.004054853008193486,2
W4408176728,Development of a natural language processing algorithm to extract social determinants of health from clinician notes,2025,0.0040499304039384116,2
W4367186096,Synwmd: Syntax-aware word Mover’s distance for sentence similarity evaluation,2023,0.00404872322467242,2
W4385574137,Knowledge informed sustainability detection from short financial texts,2022,0.004048005275888696,2
W4308083513,DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation,2022,0.004045835892893226,2
W4389519596,KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection,2023,0.004040198205870486,2
W4372347502,Quotation Recommendation for Multi-party Online Conversations Based on Semantic and Topic Fusion,2023,0.004039913771362911,2
W3037337776,Progressive Generation of Long Text.,2020,0.004036997470080763,2
W3015001695,Adversarial Attacks on Deep-learning Models in Natural Language Processing,2020,0.004035708963475677,2
W3015667612,"Annotating social determinants of health using active learning, and characterizing determinants using neural event extraction",2020,0.00403390442115802,2
W4312791030,VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers,2022,0.004032979227897839,2
W2991316439,CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain,2019,0.004028238281557985,2
W4385572222,Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health,2023,0.004027876997773254,2
W4401226718,Academic expert finding using BERT pre-trained language model,2024,0.004026166491521608,2
W3035850279,Learning Dynamic Belief Graphs to Generalize on Text-Based Games,2020,0.004023309083885636,2
W4406483874,Sentence-graph-level knowledge injection with multi-task learning,2025,0.004015893790814975,2
W4393156804,FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering,2024,0.004015771905206882,2
W4391476916,Unveiling the New Frontier: ChatGPT-3 Powered Translation for Arabic-English Language Pairs,2024,0.004012683158018879,2
W4297924045,S-Net: From Answer Extraction to Answer Synthesis for Machine Reading Comprehension,2018,0.004011893873551139,2
W2953413383,Explorations into the Use of Word Embedding in Math Search and Math Semantics,2019,0.0040089320819948525,2
W3112116031,Language models are an effective representation learning technique for electronic health record data,2020,0.004007069537698898,2
W4307217099,Transformer-based models for ICD-10 coding of death certificates with Portuguese text,2022,0.004005047106122793,2
W3173220653,LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching,2021,0.004004219784156895,2
W4366090965,Transformers in the Real World: A Survey on NLP Applications,2023,0.004002046256839112,2
W4312905556,Transformers for tabular data representation,2022,0.00400204532076821,2
W4399073778,A Review of State of the Art Deep Learning Models for Ontology Construction,2024,0.004001413271817545,2
W2997522493,Clinical Concept Embeddings Learned from Massive Sources of Multimodal Medical Data,2019,0.003995045681051884,2
W4386566852,CoTEVer: Chain of Thought Prompting Annotation Toolkit for Explanation Verification,2023,0.003991681572063098,2
W4389520485,NEWTON: Are Large Language Models Capable of Physical Reasoning?,2023,0.003987872111965822,2
W4408159234,Management of psychological emergency cases on social media: A hybrid approach combining knowledge graphs and graph neural networks,2025,0.003985022507388782,2
W4398247641,Exposing the Achilles’ heel of textual hate speech classifiers using indistinguishable adversarial examples,2024,0.003980082781056271,2
W4385571986,Faithfulness Tests for Natural Language Explanations,2023,0.0039794976645703885,2
W4385570437,IRIT_IRIS_C at SemEval-2023 Task 6: A Multi-level Encoder-based Architecture for Judgement Prediction of Legal Cases and their Explanation,2023,0.00397936031849329,2
W4385571329,Language acquisition: do children and language models follow similar learning stages?,2023,0.0039755027299825385,2
W4399972440,End-to-end pseudonymization of fine-tuned clinical BERT models,2024,0.00397265546018066,2
W4386566774,A Psycholinguistic Analysis of BERT’s Representations of Compounds,2023,0.003970760269343006,2
W3027982260,Extracting drug-drug interactions from texts with BioBERT and multiple entity-aware attentions,2020,0.0039680306511837155,2
W3207699717,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,2021,0.003966312275263387,2
W3090306696,Mining product innovation ideas from online reviews,2020,0.003959707687331286,2
W4200186624,Hierarchical BERT with an adaptive fine-tuning strategy for document classification,2021,0.003954264712990974,2
W4393131999,Privacy-preserving data integration and sharing in multi-party IoT environments: An entity embedding perspective,2024,0.0039533216201402205,2
W3169461013,Claim Detection in Biomedical Twitter Posts,2021,0.003948092635563009,2
W4385231746,Truth-O-Meter: Collaborating with LLM in Fighting its Hallucinations,2023,0.003947833742851419,2
W3118527394,Fake news detection for the Russian language,2020,0.003947593584485258,2
W4386132120,Deepfake Detection on Social Media: Leveraging Deep Learning and FastText Embeddings for Identifying Machine-Generated Tweets,2023,0.003945285964737912,2
W4367672504,Almanac: Retrieval-Augmented Language Models for Clinical Medicine,2023,0.00394459120260562,2
W4401251180,A shared model-based linguistic space for transmitting our thoughts from brain to brain in natural conversations,2024,0.003944197650607222,2
W3203138619,Exploring Data and Model Poisoning Attacks to Deep Learning-Based NLP Systems,2021,0.003944174441797349,2
W4384071683,Large language models encode clinical knowledge,2023,0.003938146374007452,2
W4212865381,Do Language Embeddings capture Scales?,2020,0.003938017543970817,2
W3021606318,Towards the necessity for debiasing natural language inference datasets,2020,0.003932953188670752,2
W4385569955,UO-LouTAL at SemEval-2023 Task 6: Lightweight Systems for Legal Processing,2023,0.003931285434277978,2
W4403582642,Not All Negatives are Equally Negative: Soft Contrastive Learning for Unsupervised Sentence Representations,2024,0.003927761219731854,2
W4392567331,Spectrum-BERT: Pretraining of Deep Bidirectional Transformers for Spectral Classification of Chinese Liquors,2024,0.0039267293782389075,2
W4231844697,Learning and Evaluating a Differentially Private Pre-trained Language Model,2021,0.003924866798544689,2
W3165229798,Evaluation of Federated Learning in Phishing Email Detection,2023,0.0039208845237002675,2
W4384918632,Understanding Multi-Turn Toxic Behaviors in Open-Domain Chatbots,2023,0.0039204199143197125,2
W4392760662,Argumentation effect of a chatbot for ethical discussions about autonomous AI scenarios,2024,0.003917504271691982,2
W3142516437,Extending Multi-Sense Word Embedding to Phrases and Sentences for Unsupervised Semantic Applications,2021,0.0039173183608733776,2
W4362499835,Transformers in Natural Language Processing,2023,0.003915758314080339,2
W4221143523,Code Synonyms Do Matter: Multiple Synonyms Matching Network for Automatic ICD Coding,2022,0.0039151048528513486,2
W4376956580,Check-worthy claim detection across topics for automated fact-checking,2023,0.0039137842146460666,2
W3030772833,Exploring Transformer Text Generation for Medical Dataset Augmentation.,2020,0.0039135941199414095,2
W3172883962,Extracting a Knowledge Base of Mechanisms from COVID-19 Papers,2021,0.003912757929834816,2
W4221166604,UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining,2022,0.003907278865283597,2
W2798237655,QA4IE: A Question Answering Based Framework for Information Extraction,2018,0.003906339264437958,2
W3194769714,ProoFVer: Natural Logic Theorem Proving for Fact Verification,2022,0.0039043872527021197,2
W4393160979,RoPDA: Robust Prompt-Based Data Augmentation for Low-Resource Named Entity Recognition,2024,0.003904256098491247,2
W3176540316,Multi-Document Transformer for Personality Detection,2021,0.00389819929206496,2
W2922293812,On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models,2019,0.003894783075394425,2
W4388973540,Graph reasoning over explicit semantic relation,2023,0.0038925558500746992,2
W4390745240,Legal Information Retrieval and Entailment Using Transformer-based Approaches,2024,0.0038899649273266583,2
W4384298419,A transformer framework for generating context-aware knowledge graph paths,2023,0.0038891277141477145,2
W4287887895,Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback,2022,0.003885017249463221,2
W4401748419,A Proposed Model for Distinguishing Between Human-Based and ChatGPT Content in Scientific Articles,2024,0.0038849981922512255,2
W3159838520,Learning Passage Impacts for Inverted Indexes.,2021,0.003876685798986319,2
W4407045844,Analysis of argument structure constructions in the large language model BERT,2025,0.0038759344917388664,2
W4389519959,HiCL: Hierarchical Contrastive Learning of Unsupervised Sentence Embeddings,2023,0.0038758790141061902,2
W2739749670,MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension,2017,0.0038747838745605383,2
W4319079731,SecureBERT: A Domain-Specific Language Model for Cybersecurity,2023,0.0038738313296115445,2
W4389520008,Detecting Propaganda Techniques in Code-Switched Social Media Text,2023,0.0038735071128856617,2
W4399512647,Text-enhanced knowledge graph representation learning with local structure,2024,0.0038724207894582554,2
W4378515217,Biomedical Relation Extraction Using Dependency Graph and Decoder-Enhanced Transformer Model,2023,0.0038701584128345607,2
W4392154900,Interpretability of deep learning models in analysis of Spanish financial text,2024,0.0038682743137488347,2
W4394911776,Fine-Tuning GPT on Biomedical NLP Tasks: An Empirical Evaluation,2024,0.0038668739317955325,2
W4206958985,Improving complex knowledge base question answering via structural information learning,2022,0.0038597577831081366,2
W4396736493,Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion,2024,0.0038549487567597217,2
W4392951803,Chinese Diabetes Question Classification Using Large Language Models and Transfer Learning,2024,0.003854047210379406,2
W4409768303,Pre- Trained Language Models for Mental Health: An Empirical Study on Arabic Q&amp;A Classification,2025,0.0038523228906355185,2
W4283798026,Weakly Supervised Neuro-Symbolic Module Networks for Numerical Reasoning over Text,2022,0.003849554958428608,2
W3156413894,Intra-Document Cascading,2021,0.0038418348599814013,2
W3214715529,AdapterDrop: On the Efficiency of Adapters in Transformers,2021,0.003841684413510327,2
W4388825438,Reinforcement learning from constraints and focal entity shifting in conversational KGQA,2023,0.0038414241739977402,2
W4385567216,Language Models of Code are Few-Shot Commonsense Learners,2022,0.0038413706816671457,2
W3156831596,NeurJudge: A Circumstance-aware Neural Framework for Legal Judgment Prediction,2021,0.0038411377746828844,2
W4386276891,Dynamic prompt-based virtual assistant framework for BIM information search,2023,0.003840870616355704,2
W4385572854,Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs,2022,0.003837644692921534,2
W4394574605,Discovering Personally Identifiable Information in Textual Data - A Case Study with Automated Concatenation of Embeddings,2024,0.0038368304054253797,2
W4327503230,Improving Log-Based Anomaly Detection by Pre-Training Hierarchical Transformers,2023,0.0038335402771070494,2
W4385573153,"MAVEN-ERE: A Unified Large-scale Dataset for Event Coreference, Temporal, Causal, and Subevent Relation Extraction",2022,0.0038310068679276318,2
W4296783454,Transformers and the Representation of Biomedical Background Knowledge,2022,0.00383025923953689,2
W4385565143,XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models,2023,0.003830167082668339,2
W4386967736,Asking Questions about Scientific Articles—Identifying Large N Studies with LLMs,2023,0.003828730895745698,2
W4385059308,Medical Reports Summarization Using Text-To-Text Transformer,2023,0.0038286225341359263,2
W4382204351,AI assistant for document management Using Lang Chain and Pinecone,2023,0.003828543098366073,2
W4408309178,DE-ESD: Dual encoder-based entity synonym discovery using pre-trained contextual embeddings,2025,0.0038258435293576647,2
W3111514949,Traditional IR rivals neural models on the MS~MARCO Document Ranking Leaderboard.,2020,0.0038226396845486986,2
W4285121883,FORTAP: Using Formulas for Numerical-Reasoning-Aware Table Pretraining,2022,0.0038199988359113777,2
W3184074798,Fine-grained Classification of Political Bias in German News: A Data Set and Initial Experiments,2021,0.0038188333514931278,2
W2961394393,Let's measure run time! Extending the IR replicability infrastructure to include performance aspects.,2019,0.0038138173351554457,2
W2612228435,Search-based Neural Structured Learning for Sequential Question Answering,2017,0.0038123213980413503,2
W2872710616,A Review on Deep Learning Techniques Applied to Answer Selection,2018,0.003804589873932007,2
W3173681001,What Context Features Can Transformer Language Models Use?,2021,0.00380116647905033,2
W4361766487,Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?,2023,0.0037993737156277537,2
W3212197935,Benchmarking knowledge-driven zero-shot learning,2022,0.003797979584200183,2
W3094245163,Causal Effects of Linguistic Properties,2020,0.003795948821481804,2
W4407394771,Text Classification by CEFR Levels Using Machine Learning Methods and the BERT Language Model,2024,0.003795948821481804,2
W3203259592,Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?,2021,0.00379522940287015,2
W4410344873,AEKG4APT: An AI-Enhanced Knowledge Graph for Advanced Persistent Threats with Large Language Model Analysis,2025,0.0037950535276445345,2
W3212804087,BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks,2021,0.0037939608518154076,2
W3158665049,Multi-domain clinical natural language processing with MedCAT: The Medical Concept Annotation Toolkit,2021,0.0037928212072687907,2
W2885396331,Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference,2018,0.003792388474933158,2
W4390098547,Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction,2023,0.003789956130031555,2
W4287887915,stce at SemEval-2022 Task 6: Sarcasm Detection in English Tweets,2022,0.003789828775896013,2
W4386566488,Large Language Models are few(1)-shot Table Reasoners,2023,0.0037891492615809578,2
W4221058809,Towards Analyzing the Bias of News Recommender Systems Using Sentiment and Stance Detection,2022,0.0037823692584483502,2
W3212354382,Patterns of Polysemy and Homonymy in Contextualised Language Models,2021,0.003779083242348086,2
W4406695259,Subreddit to Symptomatology: A Lexicon-based Approach to Extract Symptoms of Complex Conditions from Online Discourse (Preprint),2025,0.003778581306886295,2
W4405412791,Document embeddings for long texts from Transformers and Autoencoders,2024,0.003776035857001737,2
W4319753750,A data-centric way to improve entity linking in knowledge-based question answering,2023,0.0037740972760770715,2
W2808571346,Attention-Fused Deep Matching Network for Natural Language Inference,2018,0.0037706622166527215,2
W3051333395,Hierarchical fusion of common sense knowledge and classifier decisions for answer selection in community question answering,2020,0.003769691743233874,2
W3091406553,Leveraging Semantic and Lexical Matching to Improve the Recall of Document Retrieval Systems: A Hybrid Approach.,2020,0.0037689203778639127,2
W3090732401,Long-Tail Zero and Few-Shot Learning via Contrastive Pretraining on and for Small Data,2022,0.0037674911908748107,2
W2752194699,LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks,2017,0.0037665357275710625,2
W4380267320,Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT,2023,0.0037628975555871126,2
W4408614598,Conversational Explanations: Discussing Explainable AI with Non-AI Experts,2025,0.0037594377974009305,2
W2905016804,Gaussian Transformer: A Lightweight Approach for Natural Language Inference,2019,0.0037587103196009115,2
W4401440681,Mapping vaccine names in clinical trials to vaccine ontology using cascaded fine-tuned domain-specific language models,2024,0.0037578255816862927,2
W4385312233,Biglog: Unsupervised Large-scale Pre-training for a Unified Log Representation,2023,0.00375683483217235,2
W4390789641,Lightweight transformers for clinical natural language processing,2024,0.003755146900259002,2
W4389514989,"Beyond rating scales: With targeted evaluation, large language models are poised for psychological assessment",2023,0.0037545541512333286,2
W4313327571,"Short Text Clustering Algorithms, Application and Challenges: A Survey",2022,0.0037536681184029102,2
W3168830259,On the Impact of Random Seeds on the Fairness of Clinical Classifiers,2021,0.0037502296962975456,2
W3163189794,Word-level human interpretable scoring mechanism for novel text detection using Tsetlin Machines,2022,0.0037499859523236674,2
W4408564445,Evaluating knowledge fusion models on detecting adverse drug events in text,2025,0.003749480793929715,2
W4382202519,Orders Are Unwanted: Dynamic Deep Graph Convolutional Network for Personality Detection,2023,0.0037468548580076753,2
W4389520111,Distilling ChatGPT for Explainable Automated Student Answer Assessment,2023,0.003746527263372387,2
W4221021831,From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer,2022,0.003743097257432611,2
W2980649541,ClaimsKG: A Knowledge Graph of Fact-Checked Claims,2019,0.0037414473308909533,2
W4410506857,Aligning Emotions: A Comparative Analysis of Text and Imagery by European Party Leaders on Instagram,2025,0.0037412751425356817,2
W4404349757,Testing AI on language comprehension tasks reveals insensitivity to underlying meaning,2024,0.003740447647219226,2
W4399800790,Exploring T5 and RGAN for Enhanced Sarcasm Generation in NLP,2024,0.003740115617091556,2
W2970449623,Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation,2019,0.0037377868356791863,2
W3021934057,VisBERT: Hidden-State Visualizations for Transformers,2020,0.00373746341876498,2
W3199386065,Augmenting Open-Domain Event Detection with Synthetic Data from GPT-2,2021,0.003736328399366768,2
W2980808611,Automatic Judgment Prediction via Legal Reading Comprehension,2019,0.003733895845424065,2
W3190540921,DEMix Layers: Disentangling Domains for Modular Language Modeling,2022,0.0037321523700536334,2
W4389518953,NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark,2023,0.003731671789688708,2
W3037115370,SyntaxGym: An Online Platform for Targeted Evaluation of Language Models,2020,0.0037312962377989804,2
W2755637027,Variational Reasoning for Question Answering With Knowledge Graph,2018,0.0037235401610652536,2
W2798727047,"LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better",2018,0.003718963236098323,2
W3104570641,Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models,2020,0.003715610853666572,2
W3203937348,The “Narratives” fMRI dataset for evaluating models of naturalistic language comprehension,2021,0.0037138739086806346,2
W4409716856,Hallucination‐Free? Assessing the Reliability of Leading <scp>AI</scp> Legal Research Tools,2025,0.003711086308832187,2
W3209273204,Complex Temporal Question Answering on Knowledge Graphs,2021,0.003711069567370733,2
W4402263660,"BOLT: Privacy-Preserving, Accurate and Efficient Inference for Transformers",2024,0.0037104803142503583,2
W4287855052,Cross-Domain Classification of Moral Values,2022,0.0037081513956612943,2
W4407096805,Generalist Large Language Models in a Specialized World: Evidence from the Italian National Medical Education Pathway,2025,0.003705550064186671,2
W3080951367,Top2Vec: Distributed Representations of Topics,2020,0.0037024088685113413,2
W4392669928,ConDA: Contrastive Domain Adaptation for AI-generated Text Detection,2023,0.003700801527832367,2
W2963973721,Neural Semantic Encoders,2017,0.003695335528306027,2
W4403945759,Clinical information extraction for lower-resource languages and domains with few-shot learning using pretrained language models and prompting,2024,0.0036936055086192704,2
W4385569933,Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge,2023,0.0036919086323595426,2
W4410356809,A Comparison of the Effects of Model Adaptation Techniques on Large Language Models for Non-Linguistic and Linguistic Tasks,2025,0.0036902884911093983,2
W3154200459,Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions,2021,0.003690099559884774,2
W4386022344,Dataset versus reality: Understanding model performance from the perspective of information need,2023,0.003688158030822614,2
W4312659766,Answer Fast: Accelerating BERT on the Tensor Streaming Processor,2022,0.0036877545470084864,2
W4409351910,STAF-LLM: A scalable and task-adaptive fine-tuning framework for large language models in medical domain,2025,0.003686989129040114,2
W4394579953,Language Models in the Loop: Incorporating Prompting into Weak Supervision,2024,0.003685717609244029,2
W4394769991,A Heterogeneous Directed Graph Attention Network for inductive text classification using multilevel semantic embeddings,2024,0.0036838795250927012,2
W2963545917,Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations,2018,0.0036823171615381132,2
W4389520346,Text Embeddings Reveal (Almost) As Much As Text,2023,0.0036809784127130717,2
W4313591157,LiDA: Language-Independent Data Augmentation for Text Classification,2023,0.0036732228601416067,2
W4392904068,NWS: Natural Textual Backdoor Attacks Via Word Substitution,2024,0.003673186935336162,2
W4389518637,Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data,2023,0.0036720440926833408,2
W4287854500,A Dog Is Passing Over The Jet? A Text-Generation Dataset for Korean Commonsense Reasoning and Evaluation,2022,0.003669843189018447,2
W2996857645,Fine-Grained Entity Typing for Domain Independent Entity Linking,2020,0.0036659042708461573,2
W3111165175,LIREx: Augmenting Language Inference with Relevant Explanation,2020,0.00366403805400998,2
W3129896640,A Definition and a Test for Human-Level Artificial Intelligence,2023,0.0036627736982676137,2
W4402980267,MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models,2024,0.003660628236853154,2
W4224736673,Persona-Guided Planning for Controlling the Protagonist’s Persona in Story Generation,2022,0.0036605841954119524,2
W4384824496,A Short-Text Similarity Model Combining Semantic and Syntactic Information,2023,0.0036600516037493367,2
W4382322807,Constraint-aware and Ranking-distilled Token Pruning for Efficient Transformer Inference,2023,0.0036590699798866432,2
W3205586399,"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems",2022,0.0036589531382836063,2
W4391758695,Do AIs know what the most important issue is? Using language models to code open-text social survey responses at scale,2024,0.0036561959808959505,2
W4410356867,GANDALF: A LLM-based approach to map bark beetle outbreaks in semantic stories of Sentinel-2 images,2025,0.003651257335227589,2
W3048179169,Clinical concept extraction: A methodology review,2020,0.0036504543849517185,2
W3197388494,Lightweight URL-based phishing detection using natural language processing transformers for mobile devices,2021,0.003648509849916382,2
W2970636124,FINBERT: FINANCIAL SENTIMENT ANALYSIS WITH PRE-TRAINED LANGUAGE MODELS,2019,0.003645587703214112,2
W4389105137,Overview of DrugProt task at BioCreative VII: data and methods for large-scale text mining and knowledge graph generation of heterogenous chemical–protein relations,2023,0.003644077604816874,2
W4400156412,Contrastive learning based on linguistic knowledge and adaptive augmentation for text classification,2024,0.003644040314438872,2
W4386324533,Grouped Contrastive Learning of Self-Supervised Sentence Representation,2023,0.0036425118418100034,2
W4403462579,Bridge to better understanding: Syntax extension with virtual linking-phrase for natural language inference,2024,0.003640292131712054,2
W4385734153,Expanding Scope: Adapting English Adversarial Attacks to Chinese,2023,0.003636523340381984,2
W4378227416,Morphosyntactic probing of multilingual BERT models,2023,0.003632025899625591,2
W2977720775,ZeRO: Memory Optimization Towards Training A Trillion Parameter Models.,2019,0.003631484407027926,2
W4396677700,‘cito': an R package for training neural networks using ‘torch',2024,0.00363094065903114,2
W4229440380,Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection,2022,0.0036308790842539233,2
W4406705870,Zero-shot reranking with dense encoder models for news background linking,2025,0.003630186074294821,2
W4321483605,ESG information extraction with cross-sectoral and multi-source adaptation based on domain-tuned language models,2023,0.003629782680714032,2
W4377197291,Counterfactual can be strong in medical question and answering,2023,0.003627234851826941,2
W4393021028,Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study,2024,0.0036258128906399003,2
W4385570369,Large Language Models are Built-in Autoregressive Search Engines,2023,0.0036247898212036444,2
W4385572476,Steno AI at SemEval-2023 Task 6: Rhetorical Role Labelling of Legal Documents using Transformers and Graph Neural Networks,2023,0.0036208949042205716,2
W4372259980,ESCL: Equivariant Self-Contrastive Learning for Sentence Representations,2023,0.00361994809668354,2
W4385570778,FEDLEGAL: The First Real-World Federated Learning Benchmark for Legal NLP,2023,0.003619178466256474,2
W3093767665,NumClaim,2020,0.0036130024417303373,2
W4405031232,MELTing Point: Mobile Evaluation of Language Transformers,2024,0.0036086082987249477,2
W4402423338,Extractive Question Answering with Contrastive Puzzles and Reweighted Clues,2024,0.003607393104804133,2
W3132730420,Decoding EEG Brain Activity for Multi-Modal Natural Language Processing,2021,0.003607112742332602,2
W4281673407,TM-BERT: A Twitter Modified BERT for Sentiment Analysis on Covid-19 Vaccination Tweets,2022,0.0036054342705990157,2
W4402577336,Case-Based Deduction for Entailment Tree Generation,2024,0.0035990108201384824,2
W3186948566,BERT-based ensemble methods with data augmentation for legal textual entailment in COLIEE statute law task,2021,0.0035950054273841637,2
W2808681756,Evaluation of sentence embeddings in downstream and linguistic probing tasks.,2018,0.003594476927380053,2
W4385572563,Jus Mundi at SemEval-2023 Task 6: Using a Frustratingly Easy Domain Adaption for a Legal Named Entity Recognition System,2023,0.003591830919251477,2
W4399914009,Fine-grained and coarse-grained contrastive learning for text classification,2024,0.003591032783685724,2
W2889317091,Reasoning about Actions and State Changes by Injecting Commonsense Knowledge,2018,0.0035904953794440387,2
W3106229690,F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,2020,0.0035901565106119505,2
W4389665359,Task and Motion Planning with Large Language Models for Object Rearrangement,2023,0.0035885235254070023,2
W3151620310,PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer,2021,0.0035883894332017397,2
W3098873988,Fully Quantized Transformer for Machine Translation,2020,0.0035878508846853454,2
W2950031296,Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks,2019,0.0035863391851670605,2
W4386898865,Does ChatGPT have semantic understanding? A problem with the statistics-of-occurrence strategy,2023,0.0035861530299200295,2
W4391876619,Lost in the Middle: How Language Models Use Long Contexts,2024,0.0035833081216716267,2
W4400046281,Injecting the score of the first-stage retriever as text improves BERT-based re-rankers,2024,0.003583295601710212,2
W2997937415,SPARQA: Skeleton-Based Semantic Parsing for Complex Questions over Knowledge Bases,2020,0.003579873157539263,2
W4283819412,BERTMap: A BERT-Based Ontology Alignment System,2022,0.003577050254569822,2
W4404634776,Soft prompt tuning for augmenting dense retrieval with large language models,2024,0.0035766167966265992,2
W4408997033,Fields of the Future: Digital Transformation in Smart Agriculture with Large Language Models and Generative AI,2025,0.003572840566912485,2
W4392909875,BDMMT: Backdoor Sample Detection for Language Models Through Model Mutation Testing,2024,0.0035695055223412555,2
W4386702854,TeaBERT: An Efficient Knowledge Infused Cross-Lingual Language Model for Mapping Chinese Medical Entities to the Unified Medical Language System,2023,0.003568794154993216,2
W3161987470,Artificial Intelligence in Action: Addressing the COVID-19 Pandemic with Natural Language Processing,2021,0.003568202480960406,2
W4394009970,SEBGM: Sentence Embedding Based on Generation Model with multi-task learning,2024,0.0035642326964656157,2
W4407316294,Linguistic changes in spontaneous speech for detecting Parkinson’s disease using large language models,2025,0.003561963801754647,2
W4317651364,aeroBERT-NER: Named-Entity Recognition for Aerospace Requirements Engineering using BERT,2023,0.0035600799802197364,2
W4401722583,Lessons from the Use of Natural Language Inference (NLI) in Requirements Engineering Tasks,2024,0.0035600163116821784,2
W4285219308,SciDeBERTa: Learning DeBERTa for Science Technology Documents and Fine-Tuning Information Extraction Tasks,2022,0.0035568998523714595,2
W4389518992,How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench,2023,0.003554693431404208,2
W4407100023,LLM-mediated domain-specific voice agents: <i>the case of TextileBot</i>,2025,0.0035531941785349316,2
W4409657110,Mitigating Forgetting in Adapting Pre-trained Language Models to Text Processing Tasks via Consistency Alignment,2025,0.003553176420420216,2
W4384828687,SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval,2023,0.0035528920363520713,2
W4365802824,L2QA: Long Legal Article Question Answering with Cascaded Key Segment Learning,2023,0.003552561941244169,2
W4375819616,Generative LLMs and Textual Analysis in Accounting: (Chat) GPT as Research Assistant?,2023,0.003551124141804505,2
W3212213895,Hidden Backdoors in Human-Centric Language Models,2021,0.003549225813773384,2
W4393346561,Meta-learning framework with updating information flow for enhancing inductive prediction,2024,0.0035488559825101428,2
W4287890478,Residue-Based Natural Language Adversarial Attack Detection,2022,0.0035464330949918636,2
W4285141652,KNN-Contrastive Learning for Out-of-Domain Intent Classification,2022,0.0035448758763236493,2
W4408905991,PFDP: privacy-preserving federated distillation method for pretrained language models,2025,0.0035443821699994776,2
W4389520380,Dissecting Recall of Factual Associations in Auto-Regressive Language Models,2023,0.0035406074231152784,2
W4386566916,ferret: a Framework for Benchmarking Explainers on Transformers,2023,0.0035403656005593078,2
W4312602419,Value-Wise ConvNet for Transformer Models: An Infinite Time-Aware Recommender System,2022,0.0035387903114900234,2
W2963614567,Unsupervised Hierarchical Story Infilling,2019,0.0035365334830963525,2
W4224325974,Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations,2022,0.0035353200176658963,2
W4385565217,Targeted Data Generation: Finding and Fixing Model Weaknesses,2023,0.003534977023906254,2
W3024922541,SECNLP: A survey of embeddings in clinical natural language processing,2019,0.0035322030519384577,2
W4385573874,Help me write a Poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing,2022,0.00353103547795387,2
W4385571633,LMentry: A Language Model Benchmark of Elementary Language Tasks,2023,0.0035291421537591294,2
W2971134012,Improving Answer Selection and Answer Triggering using Hard Negatives,2019,0.0035275650614979043,2
W2964025273,Learning Generic Sentence Representations Using Convolutional Neural Networks,2017,0.003527435412753425,2
W2810519866,One-shot Learning for Question-Answering in Gaokao History Challenge,2018,0.003525772357225514,2
W3186681099,UIUC_BioNLP at SemEval-2021 Task 11: A Cascade of Neural Models for Structuring Scholarly NLP Contributions,2021,0.0035226288508129326,2
W4389524305,Selectively Answering Ambiguous Questions,2023,0.003522280614807392,2
W3096912371,To BERT or not to BERT: Comparing Speech and Language-Based Approaches for Alzheimer’s Disease Detection,2020,0.003517640354987783,2
W4387816220,Chinese legal judgment prediction via knowledgeable prompt learning,2023,0.003513524631023828,2
W3136665819,Knowledge Graph Question Answering with semantic oriented fusion model,2021,0.00351113634606022,2
W2890152674,Multilingual Extractive Reading Comprehension by Runtime Machine Translation,2018,0.0035099291285470395,2
W4389518901,Search Augmented Instruction Learning,2023,0.003507877303925337,2
W4384155443,Temporal word embedding with predictive capability,2023,0.0035071393959323137,2
W4385894687,Red Teaming Language Models with Language Models,2022,0.0035056180602094813,2
W3171639130,TellMeWhy: A Dataset for Answering Why-Questions in Narratives,2021,0.0035047611248692516,2
W4409972601,A Few‐Shot Learning Approach for a Multilingual Agro‐Information Question Answering System,2025,0.0035003995558188384,2
W4389519291,Enhancing Chat Language Models by Scaling High-quality Instructional Conversations,2023,0.0034986427035929427,2
W4404918643,A Survey on Model Compression for Large Language Models,2024,0.0034960638614220563,2
W4226346873,CyBERT: Contextualized Embeddings for the Cybersecurity Domain,2021,0.003495008731310551,2
W4385572722,RankGen: Improving Text Generation with Large Ranking Models,2022,0.003492811614778808,2
W4386576644,Using Punctuation as an Adversarial Attack on Deep Learning-Based NLP Systems: An Empirical Study,2023,0.0034889470885127046,2
W4406320500,BiomedRAG: A retrieval augmented large language model for biomedicine,2025,0.003488053365540056,2
W4385163750,"Few-shot learning for medical text: A review of advances, trends, and opportunities",2023,0.0034870392020957637,2
W4391558228,RIXA - Explaining Artificial Intelligence in Natural Language,2023,0.0034866080051863574,2
W4410441532,Intelligent Question-Answering on Geomorphology Knowledge Based on Knowledge Graph Retrieval-Augmented Generation Technology,2025,0.0034833355855791947,2
W4386076454,Exploring Structured Semantic Prior for Multi Label Recognition with Incomplete Labels,2023,0.003482584377905919,2
W4386566794,Quantifying Context Mixing in Transformers,2023,0.0034809309342716897,2
W4318147473,Continuous Prompt Tuning Based Textual Entailment Model for E-commerce Entity Typing,2022,0.003479942527949061,2
W4313215605,Constructing and analyzing domain-specific language model for financial text mining,2022,0.0034787879074925374,2
W3036699898,Lack of selectivity for syntax relative to word meanings throughout the language network,2020,0.0034776725031907324,2
W4385571289,Detecting Edit Failures In Large Language Models: An Improved Specificity Benchmark,2023,0.0034767011121010147,2
W3035487250,"“Who said it, and Why?” Provenance for Natural Language Claims",2020,0.003467859577654577,2
W2572185161,Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity,2016,0.003467426918170936,2
W3035179945,Overestimation of Syntactic Representation in Neural Language Models,2020,0.0034670112687343337,2
W4313590997,Biologically Inspired Design Concept Generation Using Generative Pre-Trained Transformers,2023,0.003463893710947196,2
W4409167408,Enhancing FEVER-Style Claim Fact-Checking Against Wikipedia: A Diagnostic Taxonomy and a Generative Framework,2025,0.0034624820983588465,2
W4307498511,Multimodal Data Matters: Language Model Pre-Training Over Structured and Unstructured Electronic Health Records,2022,0.003462358224858945,2
W2997429021,Iteratively Questioning and Answering for Interpretable Legal Judgment Prediction,2020,0.003461521232549172,2
W3039556919,Language processing in brains and deep neural networks: computational convergence and its limits,2020,0.0034568193144608596,2
W4400587428,CBAs: Character-level Backdoor Attacks against Chinese Pre-trained Language Models,2024,0.003455848929087364,2
W4385573952,Improving HowNet-Based Chinese Word Sense Disambiguation with Translations,2022,0.0034474096002112442,2
W4317879851,Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement,2023,0.003444162967705241,2
W3198196226,A Review of Recent Work in Transfer Learning and Domain Adaptation for Natural Language Processing of Electronic Health Records,2021,0.0034437197160221296,2
W4389520192,Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning,2023,0.0034411540700250124,2
W3168506297,Telling Stories through Multi-User Dialogue by Modeling Character Relations,2021,0.0034407217138837706,2
W4385573004,An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models,2022,0.0034361492499806853,2
W4410555860,Innovative Techniques for Compressing Large Language Models without Performance Loss,2025,0.0034347254025867886,2
W4322760726,Transformers for cardiac patient mortality risk prediction from heterogeneous electronic health records,2023,0.00343429068394351,2
W4401386758,Large Language Models in Healthcare and Medical Domain: A Review,2024,0.003433164132173277,2
W4286512643,Multiview Incomplete Knowledge Graph Integration with application to cross-institutional EHR data harmonization,2022,0.003432715626009428,2
W3094024085,Bootleg: Chasing the Tail with Self-Supervised Named Entity Disambiguation,2020,0.0034326248925609297,2
W4408094780,Contextual information contributes to biomedical named entity normalization,2025,0.003431408785038162,2
W4407843019,Effectiveness of Transformer-Based Large Language Models in Identifying Adverse Drug Reaction Relations from Unstructured Discharge Summaries in Singapore,2025,0.003429448254507836,2
W4400526199,Fine-Tuning LLaMA for Multi-Stage Text Retrieval,2024,0.003429067372645717,2
W4378648500,"An intent classification method for questions in ""Treatise on Febrile diseases"" based on TinyBERT-CNN fusion model",2023,0.0034262468122626694,2
W4362559126,COCO: an annotated Twitter dataset of COVID-19 conspiracy theories,2023,0.0034230178260287476,2
W2946582982,On the Importance of Distinguishing Word Meaning Representations: A Case Study on Reverse Dictionary Mapping,2019,0.0034120148879149812,2
W4389520038,InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations,2023,0.003411440898410849,2
W4391826804,SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization,2024,0.0034088615272992043,2
W2539671052,Learning to Match using Local and Distributed Representations of Text for Web Search,2017,0.003407344998696486,2
W4315481736,Pseudo Relevance Feedback with Deep Language Models and Dense Retrievers: Successes and Pitfalls,2023,0.00340540840883076,2
W4386566756,Multimodal Event Transformer for Image-guided Story Ending Generation,2023,0.003401337435669784,2
W3005237274,Multiple features for clinical relation extraction: A machine learning approach,2020,0.0034002041572416546,2
W4388094126,Intelligent detection on construction project contract missing clauses based on deep learning and NLP,2023,0.0033995748177366903,2
W2970155250,Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control,2019,0.0033981402062491324,2
W4225639074,A joint FrameNet and element focusing Sentence-BERT method of sentence similarity computation,2022,0.0033980387233572503,2
W4387489715,"Information-Restricted Neural Language Models Reveal Different Brain Regions’ Sensitivity to Semantics, Syntax, and Context",2023,0.003395365461829805,2
W4388680526,Diagnosing AI Explanation Methods with Folk Concepts of Behavior,2023,0.003394916007214789,2
W4402351271,Unlocking Chain of Thought in Base Language Models by Heuristic Instruction,2024,0.0033897579568396455,2
W4387942588,New Siamese Neural Networks for Text Classification and Ontologies Alignment,2023,0.0033892921201038287,2
W4400399317,OQA : A question-answering dataset on orthodontic literature,2024,0.0033849057845023608,2
W3211961299,BERT-SMAP: Paying attention to Essential Terms in passage ranking beyond BERT,2021,0.0033848257170171884,2
W4389518681,Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation,2023,0.0033819925467650377,2
W4389946433,Advancing Mental Health Diagnostics: GPT-Based Method for Depression Detection,2023,0.0033804228673304927,2
W4406751855,Evaluating the effectiveness of XAI techniques for encoder-based language models,2025,0.003379359578354142,2
W4285114657,HLDC: Hindi Legal Documents Corpus,2022,0.0033785736840914573,2
W4287887293,Political Ideology and Polarization: A Multi-dimensional Approach,2022,0.0033779211941807703,2
W4406427478,Implicit knowledge-augmented prompting for commonsense explanation generation,2025,0.0033770636335179976,2
W4385570483,Petals: Collaborative Inference and Fine-tuning of Large Models,2023,0.0033769790335766974,2
W3047988254,The Chess Transformer: Mastering Play using Generative Language Models,2020,0.0033769087440361388,2
W4212946592,Positional SHAP (PoSHAP) for Interpretation of machine learning models trained from biological sequences,2022,0.003376669641313425,2
W4362640873,SEARCHFORMER: Semantic patent embeddings by siamese transformers for prior art search,2023,0.0033748872888023367,2
W3175962280,Word Embedding-Based Topic Similarity Measures,2021,0.003373526658058515,2
W4408400712,Dynamically Contrastive Clustering For Sentence Embedding,2024,0.0033724047199171995,2
W4386566646,Investigating Multi-source Active Learning for Natural Language Inference,2023,0.0033685741740386147,2
W4392904185,One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models,2024,0.003366886295878891,2
W4328098479,Regret and Hope on Transformers: An Analysis of Transformers on Regret and Hope Speech Detection Datasets,2023,0.0033624775285637496,2
W3217005823,GIS-KG: building a large-scale hierarchical knowledge graph for geographic information science,2021,0.003360049262913739,2
W4318464561,LAD: Layer-Wise Adaptive Distillation for BERT Model Compression,2023,0.0033589654789886463,2
W2963029083,Inferring Which Medical Treatments Work from Reports of Clinical Trials,2019,0.0033575377481003307,2
W4409676988,Transformer-Based Approach for Nuclear Reactor Event Sequence Forecasting and Trip Detection,2025,0.003355902541330316,2
W4385574127,SEAL: Interactive Tool for Systematic Error Analysis and Labeling,2022,0.0033543410248004463,2
W4385574006,FLUTE: Figurative Language Understanding through Textual Explanations,2022,0.003353575733905623,2
W4380358240,"""This Is Fake News"": Characterizing the Spontaneous Debunking from Twitter Users to COVID-19 False Information",2023,0.003353335305180471,2
W4406513948,Incremental accumulation of linguistic context in artificial and biological neural networks,2025,0.0033532573596495555,2
W4404555905,AECR: Automatic attack technique intelligence extraction based on fine-tuned large language model,2024,0.0033494369924221353,2
W4393160078,Large Language Models Are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales,2024,0.00334747305241964,2
W4405025882,A BERT-Based Method of Named Entity Recognition for Ukiyo-e Titles,2024,0.0033462025589134797,2
W4408466306,Estimating the Confidence of Language Model Generation using Training Data,2025,0.003341081267774861,2
W4389524585,In-Context Learning for Text Classification with Many Labels,2023,0.0033409191962352753,2
W4304192668,Knowledge Injected Prompt Based Fine-tuning for Multi-label Few-shot ICD Coding,2022,0.0033401983855044413,2
W3122031519,Geospatial and Semantic Mapping Platform for Massive COVID-19 Scientific Publication Search,2021,0.003339719514462294,2
W4407162607,Leveraging LLMs for action item identification in Urdu meetings: Dataset creation and comparative analysis,2025,0.003338855709830671,2
W3035187263,Contextual Re-Ranking with Behavior Aware Transformers,2020,0.003338020235349957,2
W4206105765,"Automated recognition of functioning, activity and participation in COVID-19 from electronic patient records by natural language processing: a proof- of- concept",2022,0.003336903763336576,2
W2907833097,"Learning to Transform, Combine, and Reason in Open-Domain Question Answering",2019,0.0033352205374100096,2
W4391579685,KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding,2024,0.003332420330160674,2
W2963947304,Learning short-text semantic similarity with word embeddings and external knowledge sources,2019,0.0033311500797415213,2
W4385570290,Retrieval-based Language Models and Applications,2023,0.003329331165096662,2
W3107366692,Portuguese word embeddings for the oil and gas industry: Development and evaluation,2020,0.0033289143937371584,2
W4410478508,Saliency Attention and Semantic Similarity-Driven Adversarial Perturbation,2025,0.003327469865816008,2
W3017374003,Compositionality Decomposed: How do Neural Networks Generalise?,2020,0.00332672471619208,2
W4406941547,GovBERT-BR: A BERT-Based Language Model for Brazilian Portuguese Governmental Data,2025,0.0033256644137556095,2
W4320913850,Biomedical Text Classification Using Augmented Word Representation Based on Distributional and Relational Contexts,2023,0.0033238491817011735,2
W4317892547,Computational Language Modeling and the Promise of In Silico Experimentation,2023,0.0033223570134918823,2
W4386163596,Large-Scale Biomedical Relation Extraction Across Diverse Relation Types: Model Development and Usability Study on COVID-19,2023,0.0033208153810097385,2
W4408569861,Contrastive learning with large language models for medical code prediction,2025,0.003319626150366068,2
W4327644588,Visconde: Multi-document QA with GPT-3 and Neural Reranking,2023,0.0033172141352077866,2
W2887005207,Unsupervised Random Walk Sentence Embeddings: A Strong but Simple Baseline,2018,0.0033172022766071283,2
W4379033124,Relphormer: Relational Graph Transformer for Knowledge Graph Representations,2023,0.003314901874198411,2
W4382406466,A Contrastive Self-distillation BERT with Kernel Alignment-Based Inference,2023,0.0033144482148932067,2
W4382239991,Multi-Label Few-Shot ICD Coding as Autoregressive Generation with Prompt,2023,0.0033140338402326917,2
W4287887942,"Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data",2022,0.0033121906851672485,2
W2965570621,Pre-training of Graph Augmented Transformers for Medication Recommendation,2019,0.0033118508292678175,2
W2971279492,Short Text Understanding Combining Text Conceptualization and Transformer Embedding,2019,0.003311354441674878,2
W2963190210,Integrating Stance Detection and Fact Checking in a Unified Corpus,2018,0.0033098915486988374,2
W4362650350,HUKB at the COLIEE 2022 Statute Law Task,2023,0.003305766974491156,2
W4391474918,Using Artificial Intelligence to Label Free-Text Operative and Ultrasound Reports for Grading Pediatric Appendicitis,2024,0.00330464649052829,2
W2953075226,Simple and Effective Text Matching with Richer Alignment Features,2019,0.0033041977212782533,2
W3173505468,TexSmart: A System for Enhanced Natural Language Understanding,2021,0.003303275761308014,2
W4400118952,When large language models meet personalization: perspectives of challenges and opportunities,2024,0.003301475744972004,2
W2922523190,The emergence of number and syntax units in,2019,0.0032981279997459645,2
W4409880148,Natural Language Processing Methods for Assessing Social Determinants of Health in the Electronic Health Records: A Narrative Review,2025,0.0032976065825135537,2
W2890194160,Listening Comprehension over Argumentative Content,2018,0.0032948915505418446,2
W4409668151,Words That Matter: Analyzing the Causal Effect of Words,2025,0.0032943545577366834,2
W2929581986,Analyzing and interpreting neural networks for NLP: A report on the first BlackboxNLP workshop,2019,0.003291572164620411,2
W4400904993,One Subgraph for All: Efficient Reasoning on Opening Subgraphs for Inductive Knowledge Graph Completion,2024,0.0032913732465420804,2
W4400489793,RSTIE-KGC: A Relation Sensitive Textual Information Enhanced Knowledge Graph Completion Model,2024,0.003290888504613305,2
W3120075432,TexSmart: A Text Understanding System for Fine-Grained NER and Enhanced Semantic Analysis,2020,0.003290675937000033,2
W3016364508,Advantages and Constraints of a Hybrid Model K-12 E-Learning Assistant Chatbot,2020,0.003288555261151207,2
W4392351097,WaveFormer: transformer-based denoising method for gravitational-wave data,2024,0.0032839131872452456,2
W4407633513,Analysis of social media language reveals the psychological interaction of three successive upheavals,2025,0.0032839131872452456,2
W4382396063,Enhancing text representations separately with entity descriptions,2023,0.0032823810373422842,2
W3175360850,Rationale-Inspired Natural Language Explanations with Commonsense.,2021,0.0032732728549841636,2
W4287887983,A Shoulder to Cry on: Towards A Motivational Virtual Assistant for Assuaging Mental Agony,2022,0.00327106818051819,2
W4407059178,Opportunities to Use Arabic YouTube Comments in Text Mining and Masked Language Modeling,2025,0.003270330017682161,2
W4380434506,A Few-Shot Approach to Resume Information Extraction via Prompts,2023,0.003267733545114137,2
W4386302001,Identifying and Mitigating the Security Risks of Generative AI,2023,0.0032665733519656927,2
W2997050424,ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional Neural Network,2020,0.003265073111823999,2
W4385565160,Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks,2023,0.0032649708580894214,2
W2972353900,"Decomposing Generalization: Models of Generic, Habitual, and Episodic Statements",2019,0.003262155463775883,2
W3121419306,Toward Using Twitter for Tracking COVID-19: A Natural Language Processing Pipeline and Exploratory Data Set,2021,0.003261495546129769,2
W4294875919,"A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned, and Perspectives",2022,0.0032608078791480904,2
W4309190714,Technology identification from patent texts: A novel named entity recognition method,2022,0.003260402917816245,2
W4400132641,FLMatchQA: a recursive neural network-based question answering with customized federated learning model,2024,0.0032590352502748095,2
W4327571437,Topic-aware multi-hop machine reading comprehension using weighted graphs,2023,0.003258085558207429,2
W3004304303,A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP,2020,0.0032571864933403486,2
W4390511900,PTCAS: Prompt tuning with continuous answer search for relation extraction,2024,0.0032518354857047475,2
W4406941683,Using Complex Networks to Improve Legal Text Hierarchical Classification,2025,0.00324858686918073,2
W4388937418,Transformer-Based Models for Named Entity Recognition: A Comparative Study,2023,0.0032480958387970074,2
W4395117503,How the Listener’s Attention Dynamically Switches Between Different Speakers During a Natural Conversation,2024,0.0032480958387970074,2
W4409785640,"Improving Text Classification on Small-Sized, Imbalanced Datasets with Selective Few-Random-Shot Augmentation",2025,0.0032439038918808246,2
W4390970344,FDHFUI: Fusing Deep Representation and Hand-Crafted Features for User Identification,2024,0.0032437701455259537,2
W4400006571,User identification across online social networks based on gated multi-feature extraction,2024,0.0032437701455259533,2
W2964027319,PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution,2018,0.0032418849964776196,2
W4408117980,Enhancing Domain-Specific Knowledge Graph Reasoning via Metapath-Based Large Model Prompt Learning,2025,0.0032409365099716494,2
W2964142373,Explainable Prediction of Medical Codes from Clinical Text,2018,0.0032406128842209564,2
W4205105574,Distilling Knowledge for Empathy Detection,2021,0.0032362162176129,2
W3100659335,Exploring the Role of Argument Structure in Online Debate Persuasion,2020,0.0032354524813689436,2
W2292919134,Representation of Linguistic Form and Function in Recurrent Neural Networks,2017,0.0032341959995256025,2
W4405208940,SPCSE: Soft Positive Enhanced Contrastive Learning for Sentence Embeddings,2024,0.003232697886873414,2
W4401684295,Symbol ungrounding: what the successes (and failures) of large language models reveal about human cognition,2024,0.003231711250041833,2
W4313479714,Extraction of knowledge graph of Covid-19 through mining of unstructured biomedical corpora,2023,0.0032297456145877626,2
W3215639742,Learning Fine-Grained Fact-Article Correspondence in Legal Cases,2021,0.0032272987044429913,2
W4287889735,"Great Power, Great Responsibility: Recommendations for Reducing Energy for Training Language Models",2022,0.003223497950400081,2
W4407157388,SAFE-NLP: How Accurate and Robust is a Text Classification Model?,2025,0.003223184679027046,2
W4387627515,EHR-KnowGen: Knowledge-enhanced multimodal learning for disease diagnosis generation,2023,0.003216891546033046,2
W4389520069,"Values, Ethics, Morals? On the Use of Moral Concepts in NLP Research",2023,0.0032114706545709835,2
W2476140796,A Neural Knowledge Language Model,2016,0.0032104484099727405,2
W3184074368,SemEval-2021 Task 12: Learning with Disagreements,2021,0.003209878325806861,2
W4360611202,A review and comparative study of cancer detection using machine learning: SBERT and SimCSE application,2023,0.003208800603525671,2
W3173436762,Learning from the Best: Rationalizing Predictions by Adversarial Information Calibration,2021,0.0032037124487776926,2
W3209080096,MOOCCubeX: A Large Knowledge-centered Repository for Adaptive Learning in MOOCs,2021,0.0032034147479075596,2
W4220808153,MKGN: A Multi-Dimensional Knowledge Enhanced Graph Network for Multi-Hop Question and Answering,2022,0.0031995005675402944,2
W4297347595,PEINet: Joint Prompt and Evidence Inference Network via Language Family Policy for Zero-Shot Multilingual Fact Checking,2022,0.0031942222183634333,2
W4393147284,OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models,2024,0.003192684900776831,2
W4399449628,Measuring and Improving the Energy Efficiency of Large Language Models Inference,2024,0.0031883794921591458,2
W3191132518,Gender Bias and Under-Representation in Natural Language Processing Across Human Languages,2021,0.003187353167089403,2
W2523467643,Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference.,2016,0.0031857103107505416,2
W4385572831,What Makes Instruction Learning Hard? An Investigation and a New Challenge in a Synthetic Environment,2022,0.0031855674875697353,2
W2963899155,Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings,2017,0.0031787927118812178,2
W4391678886,Federated Freeze BERT for text classification,2024,0.0031767524418481927,2
W4365514966,A distributable German clinical corpus containing cardiovascular clinical routine doctor’s letters,2023,0.0031747820925993892,2
W3159323659,Arabic Question Answering Systems: Gap Analysis,2021,0.0031741338948609865,2
W4407315605,Metacognitive symbolic distillation framework for multi-choice machine reading comprehension,2025,0.0031734780924834753,2
W4388022708,Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts,2023,0.0031705949478064276,2
W4386265233,Towards Improving the Reliability and Transparency of ChatGPT for Educational Question Answering,2023,0.0031685957607812845,2
W4407559544,Machine learning tools match physician accuracy in multilingual text annotation,2025,0.0031668589931510183,2
W4385573422,BERT for Long Documents: A Case Study of Automated ICD Coding,2022,0.0031659646536204202,2
W3129965309,Deep Learning-Based Knowledge Graph Generation for COVID-19,2021,0.0031651438736859973,2
W3034685497,Probing for Referential Information in Language Models,2020,0.003160999486361151,2
W4312385228,Tiny RNN Model with Certified Robustness for Text Classification,2022,0.0031559946756501214,2
W2760753016,Inter-Weighted Alignment Network for Sentence Pair Modeling,2017,0.0031532239879357355,2
W4389518962,Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models,2023,0.003150667547686069,2
W4283768917,CBR-Based Decision Support System for Maintenance Text Using NLP for an Aviation Case Study,2022,0.0031505182109447844,2
W4322008875,Contextualized Construct Representation: Leveraging Psychometric Scales to Advance Theory-Driven Text Analysis,2023,0.0031503980861628585,2
W4404719247,Contextual feature extraction hierarchies converge in large language models and the brain,2024,0.003146549538032718,2
W4406477144,Enhancing textual textbook question answering with large language models and retrieval augmented generation,2025,0.0031459608995092684,2
W4392875681,Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation,2024,0.0031459608995092684,2
W4285165270,"SURREY-CTS-NLP at WASSA2022: An Experiment of Discourse and Sentiment Analysis for the Prediction of Empathy, Distress and Emotion",2022,0.0031456351549114694,2
W3160359732,Improving clinical outcome predictions using convolution over medical entities with multimodal learning,2021,0.0031455700001365734,2
W4366308387,Analyzing Deceptive Opinion Spam Patterns: the Topic Modeling Approach,2022,0.003142091020888074,2
W4210277770,Comparison of Neural Language Modeling Pipelines for Outcome Prediction From Unstructured Medical Text Notes,2022,0.003138369470238382,2
W4393147036,CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models,2024,0.003137907656946096,2
W2945972899,Fixed That for You: Generating Contrastive Claims with Semantic Edits,2019,0.0031374726362793717,2
W4312398513,"Adversarial NLP for Social Network Applications: Attacks, Defenses, and Research Directions",2022,0.0031354638302582625,2
W4283219089,Pretrained domain-specific language model for natural language processing tasks in the AEC domain,2022,0.0031344451969975273,2
W4386566672,Unified Neural Topic Model via Contrastive Learning and Term Weighting,2023,0.0031302790395381867,2
W4385574077,Inferring the Reader: Guiding Automated Story Generation with Commonsense Reasoning,2022,0.003129163760753982,2
W4367679848,TopoBERT: Exploring the topology of fine-tuned word representations,2023,0.003123055510830787,2
W4388455852,Improving First-stage Retrieval of Point-of-interest Search by Pre-training Models,2023,0.0031205288621647762,2
W4392292205,Commonsense knowledge in cognitive robotics: a systematic literature review,2024,0.0031189877413414953,2
W4311903220,An overview of biomedical entity linking throughout the years,2022,0.0031189471585490926,2
W4389493320,Application and evaluation of sentence embedding and clustering methods in the context of concept hierarchy construction,2023,0.0031176652280323646,2
W4407009343,Personality in just a few words: Assessment using natural language processing,2025,0.0031166818167140255,2
W2918996109,Structural Supervision Improves Learning of Non-Local Grammatical Dependencies,2019,0.0031139912075851014,2
W3017490534,Automatic Annotation Service APPI: Named Entity Linking in Legal Domain,2020,0.003112914222188622,2
W4392913295,AI-Generated Text Detector for Arabic Language Using Encoder-Based Transformer Architecture,2024,0.0031121024390028847,2
W4393147000,TaskLAMA: Probing the Complex Task Understanding of Language Models,2024,0.0031119421550954104,2
W3175102705,COLIEE 2020: Methods for Legal Document Retrieval and Entailment,2021,0.0031079034333487145,2
W3038147222,Deep Knowledge Tracing with Transformers,2020,0.003106627424106762,2
W3049275545,Narrative Interpolation for Generating and Understanding Stories,2020,0.0031065481215700554,2
W4401751312,LERCause: Deep learning approaches for causal sentence identification from nuclear safety reports,2024,0.0031057232298305733,2
W2963299810,Deep Enhanced Representation for Implicit Discourse Relation Recognition,2018,0.003105018075094403,2
W4385210537,Emotion-Semantic-Aware Dual Contrastive Learning for Epistemic Emotion Identification of Learner-Generated Reviews in MOOCs,2023,0.003102568009350712,2
W4382133580,Deep learning prediction models based on EHR trajectories: A systematic review,2023,0.0030998980401562527,2
W4321458671,Improving text mining in plant health domain with GAN and/or pre-trained language model,2023,0.003094974785202073,2
W4389519826,FinePrompt: Unveiling the Role of Finetuned Inductive Bias on Compositional Reasoning in GPT-4,2023,0.0030937860853713192,2
W2966584111,Incorporating Structured Commonsense Knowledge in Story Completion,2019,0.003093171780669348,2
W3201427244,Unsupervised law article mining based on deep pre-trained language representation models with application to the Italian civil code,2021,0.003091217826883042,2
W4392934733,A Knowledge-Injected Curriculum Pretraining Framework for Question Answering,2024,0.0030889340918027366,2
W2904664992,DRr-Net: Dynamic Re-Read Network for Sentence Semantic Matching,2019,0.0030841181478613645,2
W4304185307,ASRS-CMFS vs. RoBERTa: Comparing Two Pre-Trained Language Models to Predict Anomalies in Aviation Occurrence Reports with a Low Volume of In-Domain Data Available,2022,0.0030824251513593254,2
W4385984841,A Purely Entity-Based Semantic Search Approach for Document Retrieval,2023,0.003078253141624429,2
W4391352835,Bidirectional Encoder Representations from Transformers in Radiology: A Systematic Review of Natural Language Processing Applications,2024,0.003077561187694157,2
W4390043316,Shortcut Learning of Large Language Models in Natural Language Understanding,2023,0.0030771984495677833,2
W4313003468,Automated Detection of Typed Links in Issue Trackers,2022,0.0030761855870900775,2
W4401123261,Can language models handle recursively nested grammatical structures? A case study on comparing models and humans,2024,0.0030754466169517405,2
W4385571260,Teaching Small Language Models to Reason,2023,0.0030744665549858165,2
W4387924881,Enforcing legal information extraction through context-aware techniques: The ASKE approach,2023,0.003069528160493236,2
W4360989137,NeRBERT- A Biomedical Named Entity Recognition Tagger,2023,0.0030671875726489745,2
W4386566979,MAUPQA: Massive Automatically-created Polish Question Answering Dataset,2023,0.0030669925957335278,2
W4385572464,WebCPM: Interactive Web Search for Chinese Long-form Question Answering,2023,0.003066069318460844,2
W4401197683,Natural language processing in the intensive care unit: A scoping review,2024,0.003062440305776431,2
W4313655989,Knowledge graph extension with a pre-trained language model via unified learning method,2023,0.0030588788949088698,2
W4385572138,"Ginn-Khamov at SemEval-2023 Task 6, Subtask B: Legal Named Entities Extraction for Heterogenous Documents",2023,0.003057181028067087,2
W3011414630,Extracting medical entities from social media,2020,0.0030559319352271236,2
W4391582407,Revolutionizing Cyber Threat Detection With Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices,2024,0.0030548056293772884,2
W4224330577,Where is your app frustrating users?,2022,0.0030513198838059874,2
W3209214366,Gophormer: Ego-Graph Transformer for Node Classification,2021,0.0030510925033412225,2
W4403768714,Semantic-Driven Topic Modeling Using Transformer-Based Embeddings and Clustering Algorithms,2024,0.0030498665753508655,2
W4408743297,Positionally Restricted Masked Knowledge Graph Completion via Multi-Head Mutual Attention,2025,0.003047646792615726,2
W3165195746,A deep database of medical abbreviations and acronyms for natural language processing,2021,0.003047273092011675,2
W4391899877,Few-shot code translation via task-adapted prompt learning,2024,0.003047021159990495,2
W3201339301,Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model,2021,0.003047015109109633,2
W4393147129,Benchmarking Large Language Models in Retrieval-Augmented Generation,2024,0.003045413052960158,2
W4385567664,UnifieR: A Unified Retriever for Large-Scale Retrieval,2023,0.003044222036480352,2
W4393159880,Knowledge Graph Error Detection with Contrastive Confidence Adaption,2024,0.00304093054269533,2
W4393379915,Detection of AI-Generated Text Using Large Language Model,2024,0.003038007806012361,2
W2811010710,On Adversarial Examples for Character-Level Neural Machine Translation,2018,0.0030354805128732644,2
W4385270681,Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics,2023,0.0030346811528863264,2
W4403770996,A Study of the State of the Art Approaches and Datasets for Multilingual Natural Language Inference,2024,0.003033411568486554,2
W4389518740,Can You Follow Me? Testing Situational Understanding for ChatGPT,2023,0.0030314519258189043,2
W4385567900,RecruitPro: A Pretrained Language Model with Skill-Aware Prompt Learning for Intelligent Recruitment,2023,0.003026387855361855,2
W4385571997,Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling,2023,0.003026161017617136,2
W4294955582,How to Dissect a Muppet: The Structure of Transformer Embedding Spaces,2022,0.0030242916204881714,2
W4393146330,Thai-language chatbot security: Detecting instruction attacks with XLM-RoBERTa and Bi-GRU,2024,0.0030206325671955528,2
W4389520114,Exploring Distributional Shifts in Large Language Models for Code Analysis,2023,0.003019012611396097,2
W4410571380,ExDoRA: enhancing the transferability of large language models for depression detection using free-text explanations,2025,0.003018983935915167,2
W4376485639,CoSBERT: A Cosine-Based Siamese BERT-Networks Using for Semantic Textual Similarity,2023,0.0030189684193418476,2
W3205616434,LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5,2021,0.0030136147297504055,2
W4380686953,DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling,2023,0.003008321730417881,2
W4402812485,Is Text Normalization Relevant for Classifying Medieval Charters?,2024,0.0030080774774037585,2
W4409907388,Learn to explain transformer via interpretation path by reinforcement learning,2025,0.003007921499420433,2
W3165941178,Evaluating Saliency Methods for Neural Language Models,2021,0.003006493165603769,2
W3214405796,Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity,2022,0.003006338575102112,2
W4401433330,LitGene: a transformer-based model that uses contrastive learning to integrate textual information into gene representations,2024,0.003004439055329555,2
W4385828568,Unsupervised techniques for generating a standard sample self-explanation answer with knowledge components in a math quiz,2023,0.0030006226061978273,2
W4393950444,The Explainability of Transformers: Current Status and Directions,2024,0.003000073939644116,2
W4392794210,ProMvSD: Towards unsupervised knowledge graph anomaly detection via prior knowledge integration and multi-view semantic-driven estimation,2024,0.0029941144069056563,2
W2948140294,Is Attention Interpretable,2019,0.0029939103318799044,2
W2952230306,FlowQA: Grasping Flow in History for Conversational Machine Comprehension,2018,0.0029908600567937955,2
W4378375354,Examining transaction-specific satisfaction and trust in Airbnb and hotels. An application of BERTopic and Zero-shot text classification,2023,0.0029893195514420195,2
W4318147141,Analysing Longitudinal Social Science Questionnaires: Topic modelling with BERT-based Embeddings,2022,0.0029886878144370762,2
W3155151075,Legal Judgment Prediction with Multi-Stage Case Representation Learning in the Real Court Setting,2021,0.0029866910761809245,2
W3157950068,Do Feature Attribution Methods Correctly Attribute Features?,2022,0.002984684354272594,2
W4225858632,A Survey of Adversarial Defenses and Robustness in NLP,2023,0.0029747521732734765,2
W4389520334,Learning from Mistakes via Cooperative Study Assistant for Large Language Models,2023,0.0029727426782574454,2
W4206104244,Dynamic Graph Reasoning for Conversational Open-Domain Question Answering,2022,0.0029727325720994746,2
W4376122615,Knowledge-enhanced Agents for Interactive Text Games,2023,0.002969469884127766,2
W4394719810,Knowledge Graph Multi-Hop Question Answering Based on Dependent Syntactic Semantic Augmented Graph Networks,2024,0.002966391855997998,2
W4377101293,Retrieve and rerank for automated ICD coding via Contrastive Learning,2023,0.0029650356788150406,2
W4379260118,Identifying Risk Factors Associated With Lower Back Pain in Electronic Medical Record Free Text: Deep Learning Approach Using Clinical Note Annotations,2023,0.002964570190954089,2
W2508865106,Siamese Recurrent Architectures for Learning Sentence Similarity,2016,0.0029642348351363794,2
W4408688975,DAFE: LLM-Based Evaluation Through Dynamic Arbitration for Free-Form Question-Answering,2025,0.002963062926448584,2
W4296878971,Kformer: Knowledge Injection in Transformer Feed-Forward Layers,2022,0.0029604244033252123,2
W4385572433,Metaphor Detection via Explicit Basic Meanings Modelling,2023,0.0029592785133906855,2
W4399500861,INTEGRATING IMAGE FEATURES WITH CONVOLUTIONAL SEQUENCE-TO-SEQUENCE NETWORK FOR MULTILINGUAL VISUAL QUESTION ANSWERING,2024,0.0029579568571401374,2
W2985283356,GF-Net: Improving machine reading comprehension with feature gates,2019,0.002952529764416325,2
W4312480353,Clickbait Headline Detection in Indonesian News Sites using Robustly Optimized BERT Pre-training Approach (RoBERTa),2022,0.0029487685736866664,2
W4385567134,Language Models as Agent Models,2022,0.0029458387882186827,2
W4379599778,Automatic knowledge extraction from Chinese electronic medical records and rheumatoid arthritis knowledge graph construction,2023,0.00294442418344329,2
W3190271517,"A Survey on Complex Knowledge Base Question Answering: Methods, Challenges and Solutions",2021,0.002938283708334472,2
W2769934148,DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications,2017,0.0029337505282776366,2
W3024368629,Using case-level context to classify cancer pathology reports,2020,0.0029335708672166855,2
W2415204069,Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention,2016,0.0029324325748926973,2
W2963073938,Recurrent Neural Network Grammars,2016,0.002931943722357915,2
W4385572133,Trigger Warning Assignment as a Multi-Label Document Classification Problem,2023,0.0029275212928874827,2
W4389520499,"Unifying Text, Tables, and Images for Multimodal Question Answering",2023,0.0029272551453553932,2
W4401798709,Contrasting Linguistic Patterns in Human and LLM-Generated News Text,2024,0.0029229593408506764,2
W4312805275,OERL: Enhanced Representation Learning via Open Knowledge Graphs,2022,0.0029215916126478095,2
W4379206827,Multimodal learning on graphs for disease relation extraction,2023,0.002920610708361544,2
W4389520758,Query2doc: Query Expansion with Large Language Models,2023,0.0029188870613911073,2
W4383199647,Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective,2023,0.0029188777634343724,2
W3196359847,Application of natural language processing in HAZOP reports,2021,0.002917111863396955,2
W4323656451,The Emotional Impact of COVID-19 News Reporting: A Longitudinal Study Using Natural Language Processing,2023,0.002916533610639982,2
W3169306793,Universal Adversarial Attacks with Natural Triggers for Text Classification,2021,0.0029157626261893345,2
W2738015883,Adversarial Examples for Evaluating Reading Comprehension Systems,2017,0.0029149308020171624,2
W4393148139,KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding,2024,0.0029062961392801352,2
W4386826892,Semantic-enhanced Contrastive Learning for Session-based Recommendation,2023,0.0029030188105275017,2
W4285178177,JGLUE: Japanese General Language Understanding Evaluation,2022,0.002902855539499266,2
W4392202731,Applying large language models and chain-of-thought for automatic scoring,2024,0.002902027592713048,2
W4389519535,SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization,2023,0.0029012611793739467,2
W4410065220,Crisis impact extraction language model using transformer-based synergy via transfer learning,2025,0.002897735753362938,2
W4404723500,Can Language Models Trained on Written Monologue Learn to Predict Spoken Dialogue?,2024,0.002895141380310233,2
W4377695252,"Unveiling the inventive process from patents by extracting problems, solutions and advantages with natural language processing",2023,0.0028922968313724136,2
W4400681340,"Language models, like humans, show content effects on reasoning tasks",2024,0.0028916985492960256,2
W4381827011,Identifying and Extracting Rare Disease Phenotypes with Large Language Models,2023,0.0028914725797883393,2
W3161222848,An NLP-Based Architecture for the Autocompletion of Partial Domain Models,2021,0.0028898198769526226,2
W4389109049,Knowledge-enhanced Agents for Interactive Text Games,2023,0.002889276443254799,2
W4327644048,An Experimental Study on Pretraining Transformers from Scratch for IR,2023,0.00288379422527688,2
W4226403411,Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks,2022,0.002881900507856076,2
W3211384372,The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation,2021,0.0028816906189757297,2
W4385570354,Inseq: An Interpretability Toolkit for Sequence Generation Models,2023,0.0028784340233623225,2
W4389520744,Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text,2023,0.0028783152532447388,2
W4206281850,Differentiable Subset Pruning of Transformer Heads,2021,0.002876400319587557,2
W4391070609,Semantics-enabled biomedical literature analytics,2024,0.0028740326533620705,2
W4389519413,Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search,2023,0.0028714304282539413,2
W3197482624,Explainable Sentiment Analysis: A Hierarchical Transformer-Based Extractive Summarization Approach,2021,0.0028713846328614246,2
W4386569329,Reliability and Performance of the Online Literature Database CAMbase after Changing from a Semantic Search to a Score Ranking Algorithm,2023,0.002870020338281758,2
W4385221022,Enhancing Chinese Address Parsing in Low-Resource Scenarios through In-Context Learning,2023,0.0028694202454504977,2
W4379379356,Clinical named entity recognition and relation extraction using natural language processing of medical free text: A systematic review,2023,0.002865855831865008,2
W4385571671,What In-Context Learning “Learns” In-Context: Disentangling Task Recognition and Task Learning,2023,0.0028656057533877404,2
W4388194748,Joint unsupervised contrastive learning and robust GMM for text clustering,2023,0.0028636578076168106,2
W4385569626,Open-Domain Hierarchical Event Schema Induction by Incremental Prompting and Verification,2023,0.0028626921037239817,2
W3012590175,ASER: A Large-scale Eventuality Knowledge Graph,2020,0.0028609953628937197,2
W4391181913,Beyond Surface Linguistics,2023,0.0028607956072713293,2
W4389523765,Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents,2023,0.0028602604903135978,2
W4388016061,Natural Language Processing using Federated Learning: A Structured Literature Review,2023,0.0028577317912320353,2
W4293212040,Automatic depression score estimation with word embedding models,2022,0.0028575185664643705,2
W4288781262,Adversarial Training for Fake News Classification,2022,0.0028563526712705556,2
W4385571890,What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric,2023,0.0028562422610723145,2
W4403221502,"Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",2024,0.0028557152467755943,2
W2951528484,A Compare-Aggregate Model for Matching Text Sequences,2016,0.002855654605551305,2
W4392812489,UTP: A Unified Term Presentation Tool for Clinical Textual Data Using Pattern-Matching Rules and Dictionary-Based Ontologies,2024,0.002849134150988788,2
W4393932132,Adapting transformer-based language models for heart disease detection and risk factors extraction,2024,0.0028480449382668237,2
W2576562514,Reading and Thinking: Re-read LSTM Unit for Textual Entailment Recognition,2016,0.002845926197144915,2
W2951674308,Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge,2018,0.0028450837455247673,2
W4296306361,Research and Implementation of Text Generation Based on Text Augmentation and Knowledge Understanding,2022,0.0028388336621709385,2
W4387212306,Efficient Federated Learning for Modern NLP,2023,0.0028371274957770614,2
W4386847924,Rethinking Label Smoothing on Multi-Hop Question Answering,2023,0.0028343019270397445,2
W4409445749,MVIFSA: Enhancing relation detection in knowledge base question answering through multi-view information fusion and self-attention,2025,0.002831932105373703,2
W2915240437,SemEval-2017 Task 3: Community Question Answering,2017,0.002830693983685238,2
W4287855005,Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge,2022,0.0028306751204824707,2
W4287854464,TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding,2022,0.002827892880463899,2
W4386507114,Pre-trained Language Models for the Legal Domain,2023,0.002827468492373081,2
W4389519518,Aligning Large Language Models through Synthetic Feedback,2023,0.002827453477639488,2
W4400525421,Reciprocating Encoder Portrayal From Reliable Transformer Dependent Bidirectional Long Short-Term Memory for Question and Answering Text Classification,2024,0.0028271173762545653,2
W2606347107,Learning to Generate Reviews and Discovering Sentiment,2017,0.0028234999320249313,2
W4390636040,Exploring low-resource medical image classification with weakly supervised prompt learning,2024,0.0028224518215314892,2
W4393084433,Ensemble learning with soft-prompted pretrained language models for fact checking,2024,0.0028185334462927816,2
W4390420065,PAL-BERT: An Improved Question Answering Model,2023,0.0028107819117225677,2
W3035449958,Generalized Zero-Shot Text Classification for ICD Coding,2020,0.00280926647800689,2
W4392671466,Event-centric hierarchical hyperbolic graph for multi-hop question answering over knowledge graphs,2024,0.002807640004087978,2
W4385664477,ThoughtSource: A central hub for large language model reasoning data,2023,0.002806333352149274,2
W3101118235,CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,2020,0.002798834833478213,2
W4380302165,Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War on Reddit,2023,0.002798277821730041,2
W4367298114,Multi-MCCR: Multiple models regularization for semi-supervised text classification with few labels,2023,0.0027981298204984273,2
W4381587418,Opportunities and challenges for ChatGPT and large language models in biomedicine and health,2023,0.0027938047302978383,2
W4385562585,PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text,2023,0.0027897277545023687,2
W4280605341,AEON: a method for automatic evaluation of NLP test cases,2022,0.002788839555729729,2
W4407194361,Attention heads of large language models,2025,0.0027879205654471307,2
W3217459502,Pretrained Natural Language Processing Model for Intent Recognition (BERT-IR),2021,0.0027860882660611258,2
W4394565991,NativE: Multi-modal Knowledge Graph Completion in the Wild,2024,0.00278442962207715,2
W4378509386,WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia,2023,0.002778766392796729,2
W4389520047,WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia,2023,0.002778766392796729,2
W4386566424,Crawling The Internal Knowledge-Base of Language Models,2023,0.0027768626509225057,2
W4372260394,A Large-Scale Pretrained Deep Model for Phishing URL Detection,2023,0.0027765642242213494,2
W3122775348,Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval,2016,0.002776410117061403,2
W4213191780,Overview and Discussion of the Competition on Legal Information Extraction/Entailment (COLIEE) 2021,2022,0.0027762953205966025,2
W4366336804,Multi-task learning for few-shot biomedical relation extraction,2023,0.0027750081938814047,2
W2965536863,Answering Binary Causal Questions Through Large-Scale Text Mining: An Evaluation Using Cause-Effect Pairs from Human Experts,2019,0.0027743675729647683,2
W4389518784,HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models,2023,0.0027713478873697957,2
W3004119480,Discriminative Topic Mining via Category-Name Guided Text Embedding,2020,0.0027698196058952565,2
W4300797895,Learning Entity Linking Features for Emerging Entities,2022,0.0027677955694568397,2
W4298110867,<scp>FinBERT</scp>: A Large Language Model for Extracting Information from Financial Text*,2022,0.002766244949465808,2
W4386566493,JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs,2023,0.002764054223187868,2
W4401824351,Zero‐ and few‐shot prompting of generative large language models provides weak assessment of risk of bias in clinical trials,2024,0.002761494752540295,2
W4410151876,Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment,2025,0.0027607790912278786,2
W4386530347,An extensive benchmark study on biomedical text generation and mining with ChatGPT,2023,0.0027593473253218846,2
W4403865042,Detecting Bias in LLMs' Natural Language Inference Using Metamorphic Testing,2024,0.002758748712521292,2
W4286776303,"Towards using visual, semantic and structural features to improve code readability classification",2022,0.0027586109997742103,2
W4306661307,Generating Fluent Fact Checking Explanations with Unsupervised Post-Editing,2022,0.002756650025917856,2
W4407254137,Contrastive Language-Entity Pre-training for Richer Knowledge Graph Embedding,2025,0.0027556300915881375,2
W4407359875,Llmbd: Backdoor Defense Via Large Language Model Paraphrasing and Data Voting in Nlp,2025,0.002755297470541069,2
W3034329603,(Re)construing Meaning in NLP,2020,0.002755170122091864,2
W3207707577,On the Study of Transformers for Query Suggestion,2021,0.002755012690965646,2
W4367318870,Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching,2023,0.002754778213772423,2
W4297995647,Deep language algorithms predict semantic comprehension from brain activity,2022,0.002750275526970777,2
W4402761916,Geometry of Textual Data Augmentation: Insights from Large Language Models,2024,0.0027487955461385116,2
W4386005268,L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking BERT Sentence Representations for Hindi and Marathi,2023,0.002748213315440248,2
W4408037575,Leveraging LLaMA2 for improved document classification in English,2025,0.002748016961960194,2
W3090254696,Dataset for Evaluation of Mathematical Reasoning Abilities in Russian,2020,0.002745359544606896,2
W4285261975,Do Pre-trained Models Benefit Knowledge Graph Completion? A Reliable Evaluation and a Reasonable Approach,2022,0.002744783831066563,2
W4408295901,Leveraging large language models for knowledge-free weak supervision in clinical natural language processing,2025,0.002744400499333824,2
W4398160651,Training-free retrieval-based log anomaly detection with pre-trained language model considering token-level information,2024,0.0027379531993765795,2
W4367555203,"Comparison of pre-trained language models in terms of carbon emissions, time and accuracy in multi-label text classification using AutoML",2023,0.0027339497303900747,2
W4390970119,NLP-Based Recommendation Approach for Diverse Service Generation,2024,0.0027276987133816894,2
W4389519287,CLEVA: Chinese Language Models EVAluation Platform,2023,0.002726969785982755,2
W3214133644,"Generalising to German Plural Noun Classes, from the Perspective of a Recurrent Neural Network",2021,0.00272572815660656,2
W4382239863,Visually Grounded Commonsense Knowledge Acquisition,2023,0.0027255042445582466,2
W3172481377,Zero-shot Node Classification with Decomposed Graph Prototype Network,2021,0.0027246504480176285,2
W2903285529,Practical Text Classification With Large Pre-Trained Language Models,2018,0.0027218864690306565,2
W4313563649,PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models,2022,0.0027181460152553897,2
W4221141037,Are You Robert or RoBERTa? Deceiving Online Authorship Attribution Models Using Neural Text Generators,2022,0.0027155210462392375,2
W4407389405,"A Dynamic-Selection-Based, Retrieval-Augmented Generation Framework: Enhancing Multi-Document Question-Answering for Commercial Applications",2025,0.0027133010572367054,2
W4380738738,Contextualized medication event extraction with striding NER and multi-turn QA,2023,0.0027128543190585537,2
W4284689801,Automated handling of anaphoric ambiguity in requirements,2022,0.0027116516406043363,2
W4381436391,DesPrompt: Personality-descriptive prompt tuning for few-shot personality recognition,2023,0.0027057003354160687,2
W4321485427,Knowledge-Augmented Methods for Natural Language Processing,2023,0.0027053450468713506,2
W4214876417,What Can Knowledge Bring to Machine Learning?—A Survey of Low-shot Learning for Structured Data,2022,0.0027043627350240174,2
W4393178543,RetLLM-E: Retrieval-Prompt Strategy for Question-Answering on Student Discussion Forums,2024,0.002701865921497537,2
W3194712525,No Rumours Please! A Multi-Indic-Lingual Approach for COVID Fake-Tweet Detection,2021,0.002698117038307264,2
W4362705335,FinBERT–MRC: Financial Named Entity Recognition Using BERT Under the Machine Reading Comprehension Paradigm,2023,0.0026960923406703146,2
W4400528864,Boosting Conversational Question Answering with Fine-Grained Retrieval-Augmentation and Self-Check,2024,0.002689258204788931,2
W2888236192,End-to-End Neural Entity Linking,2018,0.0026887012172089465,2
W4393990831,Enhancing Early Detection of Cognitive Decline in the Elderly: A Comparative Study Utilizing Large Language Models in Clinical Notes,2024,0.002687385487768854,2
W4381956875,Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-Shot Prompt Learning for Automatic Scoring in Science Education,2023,0.0026872033417621776,2
W4205458180,Recent progress in leveraging deep learning methods for question answering,2022,0.002686060513562738,2
W4405107520,Trustworthy AI: Securing Sensitive Data in Large Language Models,2024,0.0026837143992778974,2
W4389519586,MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions,2023,0.0026806483627709963,2
W4399730120,Building Trust in Conversational AI: A Review and Solution Architecture Using Large Language Models and Knowledge Graphs,2024,0.0026778948638005347,2
W2986128786,Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models,2019,0.0026752357126289075,2
W3036413095,STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths,2020,0.0026743543071143694,2
W4285115684,An Attack Detection Framework Based on BERT and Deep Learning,2022,0.0026738625151954613,2
W4389519937,Are Language Models Worse than Humans at Following Prompts? It’s Complicated,2023,0.0026732249363106628,2
W4393160302,Graph of Thoughts: Solving Elaborate Problems with Large Language Models,2024,0.0026720012619294276,2
W4382318094,NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR,2023,0.00267158767118065,2
W2495998536,Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering,2016,0.0026711138039187503,2
W2944400536,"BioWordVec, improving biomedical word embeddings with subword information and MeSH",2019,0.0026685634538976123,2
W3174995573,COBERT: COVID-19 Question Answering System Using BERT,2021,0.0026618160270615665,2
W4385569886,Explaining How Transformers Use Context to Build Predictions,2023,0.0026594895785924328,2
W4386701652,"GPT-3.5, GPT-4, or BARD? Evaluating LLMs reasoning ability in zero-shot setting and performance boosting through prompts",2023,0.002656339712104528,2
W4367556316,Explainable Natural Language Inference in the Legal Domain via Text Generation,2023,0.0026550837568112104,2
W4388342038,Spatial Commonsense Reasoning for Machine Reading Comprehension,2023,0.0026543168008192628,2
W3175096440,LNN-EL: A Neuro-Symbolic Approach to Short-text Entity Linking,2021,0.0026541076978514573,2
W2998277219,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,2020,0.0026534277169153685,2
W4210489638,Word Acquisition in Neural Language Models,2022,0.002652274595690612,2
W4401076218,Transformer models in biomedicine,2024,0.0026509374165924705,2
W4229036562,RLAS-BIABC: A Reinforcement Learning-Based Answer Selection Using the BERT Model Boosted by an Improved ABC Algorithm,2022,0.002649987623365342,2
W2963580443,Learning to Compose Task-Specific Tree Structures,2018,0.002647310575967037,2
W4287854513,Detecting Suicidality with a Contextual Graph Neural Network,2022,0.002640441806544045,2
W3210489413,WhatTheWikiFact,2021,0.0026395653022241506,2
W4387171578,Transferring Procedural Knowledge Across Commonsense Tasks,2023,0.002633401244943757,2
W2951528897,"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",2019,0.0026332862228786455,2
W4285302878,SGPT: A Generative Approach for SPARQL Query Generation From Natural Language Questions,2022,0.002631212530012172,2
W2997262687,Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs,2020,0.002621863398851109,2
W4220924176,A Deep Fusion Matching Network Semantic Reasoning Model,2022,0.0026186410104151324,2
W4402264369,Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks,2024,0.002617294031148227,2
W4388946674,BIR: Biomedical Information Retrieval System for Cancer Treatment in Electronic Health Record Using Transformers,2023,0.002612308650801943,2
W4223558833,Characterization inference based on joint-optimization of multi-layer semantics and deep fusion matching network,2022,0.0026112284772111523,2
W4393160461,Task Contamination: Language Models May Not Be Few-Shot Anymore,2024,0.002610952953114944,2
W4392688439,Leveraging Text-to-Text Pretrained Language Models for Question Answering in Chemistry,2024,0.0026067784102199506,2
W4372341200,Studying the Influence of Toxicity and Emotion Features for Stress Detection on Social Media,2023,0.0026038139079410504,2
W4409202736,Improving zero-shot cross-domain slot filling via machine reading comprehension prompt template,2025,0.002602675883874497,2
W4210777822,Natural language based analysis of SQuAD: An analytical approach for BERT,2022,0.0026020916540007983,2
W4408542627,Structure-Aware Transformer for hyper-relational knowledge graph completion,2025,0.002598215523909455,2
W3162752841,TabularNet,2021,0.0025973297821867878,2
W4385570587,Client-Customized Adaptation for Parameter-Efficient Federated Learning,2023,0.0025931186706262925,2
W4402605383,Reinforced Multi-teacher Knowledge Distillation for Unsupervised Sentence Representation,2024,0.0025914992315565076,2
W4385571831,Distilling Reasoning Capabilities into Smaller Language Models,2023,0.0025911857335343133,2
W4226252996,Eliciting Knowledge from Pretrained Language Models for Prototypical Prompt Verbalizer,2022,0.0025894101963863246,2
W4402351653,Anchor Your Embeddings Through the Storm: Mitigating Instance-to-Document Semantic Gap,2024,0.0025892789825143567,2
W4409810916,Improving Vietnamese Legal Document Retrieval Using Synthetic Data,2025,0.0025886696532869927,2
W4391164144,Human Versus Machine Intelligence: Assessing Natural Language Generation Models Through Complex Systems Theory,2024,0.0025878416446025116,2
W4389520726,SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables,2023,0.002575662608715976,2
W4318679823,How can voting mechanisms improve the robustness and generalizability of toponym disambiguation?,2023,0.002574375937530941,2
W4388132131,Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family,2023,0.0025724218267416093,2
W4214812012,A comparative evaluation and analysis of three generations of Distributional Semantic Models,2022,0.002569561585837987,2
W4386365475,"Recognizing textual entailment: A review of resources, approaches, applications, and challenges",2023,0.0025683953532587996,2
W4410120115,Cross-lingual prompt and knowledge enhancement-based named entity recognition for low-resource electronic medical records,2025,0.0025666658011402705,2
W4385572248,RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs,2023,0.0025665463267188346,2
W4406900436,Maritime near-miss prediction framework and model interpretation analysis method based on Transformer neural network model with multi-task classification variables,2025,0.0025660975255295507,2
W3094607200,Learning Effective Representations for Person-Job Fit by Feature Fusion,2020,0.0025642293767690347,2
W4389524200,Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations,2023,0.0025636970731191186,2
W3003485993,Multi-hop Knowledge Base Question Answering with an Iterative Sequence Matching Model,2019,0.0025556409135394646,2
W4387781773,Schema matching based on energy domain pre-trained language model,2023,0.002552516174831675,2
W2963863610,On Tree-Based Neural Sentence Modeling,2018,0.0025512649680217644,2
W4393145366,Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling,2023,0.00255067350655981,2
W4391229250,Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach,2024,0.0025497527859198953,2
W4389519950,MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic,2023,0.0025487969678100702,2
W4389523793,Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models,2023,0.002547579898421441,2
W4406940831,Evaluating Large Language Models for Tax Law Reasoning,2025,0.0025459265501399505,2
W4285819142,A Survey of Pretrained Language Models,2022,0.0025419176478683908,2
W4313504207,Context-Based Interpretation of Financial Information,2023,0.00254073657201984,2
W4285183303,RuMedBench: A Russian Medical Language Understanding Benchmark,2022,0.0025371379300143204,2
W3215490307,DATLMedQA: A Data Augmentation and Transfer Learning Based Solution for Medical Question Answering,2021,0.002534466563541239,2
W3118423825,Building Machine Learning Systems for Automated ESG Scoring,2021,0.002530110037745185,2
W4385573684,Why is Winoground Hard? Investigating Failures in Visuolinguistic Compositionality,2022,0.0025289289808443386,2
W4408490751,From corporate earnings calls to social impact: Exploring ESG signals in S&amp;P 500 ESG index companies through transformer-based models,2025,0.0025285628881459767,2
W3186583010,A novel deep learning approach to extract Chinese clinical entities for lung cancer screening and staging,2021,0.00252593009860124,2
W3212805424,Graph-Based Tri-Attention Network for Answer Ranking in CQA,2021,0.002523519038399115,2
W4297903189,ReLMKG: reasoning with pre-trained language models and knowledge graphs for complex question answering,2022,0.0025228426159042733,2
W3034385177,Guided Generation of Cause and Effect,2020,0.002519929074084883,2
W4310910011,Identifying women with postdelivery posttraumatic stress disorder using natural language processing of personal childbirth narratives,2022,0.0025183294255062237,2
W4394796464,A noise audit of human-labeled benchmarks for machine commonsense reasoning,2024,0.0025180090197060565,2
W4319996342,A 95.6-TOPS/W Deep Learning Inference Accelerator With Per-Vector Scaled 4-bit Quantization in 5 nm,2023,0.0025098024221279916,2
W4395112771,Implications of Minimum Description Length for Adversarial Attack in Natural Language Processing,2024,0.00250825574955511,2
W4407134116,A comparative evaluation of the effectiveness of document splitters for large language models in legal contexts,2025,0.0025073235759882585,2
W3175554834,Extracting Semantic Process Information from the Natural Language in Event Logs,2021,0.0025065003817046822,2
W4393155827,Does a language model “understand” high school math? A survey of deep learning based word problem solvers,2024,0.0025023918042547883,2
W4297678984,Towards Complex Document Understanding By Discrete Reasoning,2022,0.0025017720671774725,2
W4213243595,A Multi-task Learning Framework for Product Ranking with BERT,2022,0.002498164593431328,2
W4385768178,Temporal Knowledge Graph Completion: A Survey,2023,0.0024978990807035016,2
W2963001778,Semi-supervised Question Retrieval with Gated Convolutions,2016,0.002496794782215751,2
W4400525136,BERT-based Global Semantic Refinement and Local Semantic Extraction for Distinguishing Urgent Posts in MOOC Forums,2024,0.002493307852044952,2
W4385570771,Lauri Ingman at SemEval-2023 Task 4: A Chain Classifier for Identifying Human Values behind Arguments,2023,0.0024894018749461104,2
W3193521099,Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards,2021,0.0024876830985016136,2
W2963429207,Modeling Naive Psychology of Characters in Simple Commonsense Stories,2018,0.002486331958706593,2
W4385532480,Misinformation Detection Using an Ensemble Method with Emphasis on Sentiment and Emotional Analyses,2023,0.0024862882699285176,2
W2963363070,Learning to Skim Text,2017,0.002486126421660272,2
W4386519508,Legal Holding Extraction from Italian Case Documents using Italian-LEGAL-BERT Text Summarization,2023,0.0024812150707031063,2
W4405576517,"Large language models to process, analyze, and synthesize biomedical texts: a scoping review",2024,0.002481191202166005,2
W4389009541,Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy,2023,0.0024781831882523834,2
W4385571645,Contrastive Decoding: Open-ended Text Generation as Optimization,2023,0.002476386827673035,2
W4285605599,Subgraph Neighboring Relations Infomax for Inductive Link Prediction on Knowledge Graphs,2022,0.0024759605497900196,2
W2955202831,Knowledge Base Question Answering With a Matching-Aggregation Model and Question-Specific Contextual Relations,2019,0.0024733260549158085,2
W4393159433,Prompting Segmentation with Sound Is Generalizable Audio-Visual Source Localizer,2024,0.0024635217013646983,2
W4392938490,Volatility forecasting and assessing risk of financial markets using multi-transformer neural network based architecture,2024,0.0024628498008291426,2
W4389518756,Prompting is not a substitute for probability measurements in large language models,2023,0.002458319062613449,2
W4387846294,Optimizing Upstream Representations for Out-of-Domain Detection with Supervised Contrastive Learning,2023,0.002456277577872737,2
W4390497390,Adversarial machine learning :,2024,0.0024541321148613,2
W4409952321,ICL: In-loop continual learning framework for language model pre-training for E-commerce,2025,0.0024532592263213247,2
W4410373085,Specialized or general AI? a comparative evaluation of LLMs’ performance in legal tasks,2025,0.0024531380714232867,2
W4406240777,What is creative in childhood writing? Computationally measured linguistic characteristics explain much of the variance in subjective human-rated creativity scores,2025,0.0024527971349735424,2
W4393152839,Customizing Language Model Responses with Contrastive In-Context Learning,2024,0.0024519878759188232,2
W4385381606,The shaky foundations of large language models and foundation models for electronic health records,2023,0.002450481811456299,2
W4407405718,Empowering large language models for automated clinical assessment with generation-augmented retrieval and hierarchical chain-of-thought,2025,0.00244940592766474,2
W4392735498,ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models,2024,0.0024470524614628613,2
W2997012196,Latent Relation Language Models,2020,0.00244675355837589,2
W4406107588,M2KGRL: A semantic-matching based framework for multimodal knowledge graph representation learning,2025,0.002445089744248391,2
W4403010755,"ChatGeoAI: Enabling Geospatial Analysis for Public through Natural Language, with Large Language Models",2024,0.0024444845721283276,2
W4409162402,Utilizing Large Language Models to Detect and Decipher Abbreviation in Clinical Notes (Preprint),2025,0.002443321537666622,2
W4283721470,Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation,2022,0.002441953652213838,2
W4395080118,FT-LLM: Development of a Retrieval Augmented Generation based Language Model Framework for Assisting Data Analysis in the Football Industry,2024,0.002441867590233553,2
W4385571248,NLP-Titan at SemEval-2023 Task 6: Identification of Rhetorical Roles Using Sequential Sentence Classification,2023,0.0024367851977661822,2
W4385570040,"Don’t Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments",2023,0.0024352742515555818,2
W4387326738,A Natural Language Processing Model for COVID-19 Detection Based on Dutch General Practice Electronic Health Records by Using Bidirectional Encoder Representations From Transformers: Development and Validation Study,2023,0.0024328716407657942,2
W4225658316,CVSS-BERT: Explainable Natural Language Processing to Determine the Severity of a Computer Security Vulnerability from its Description,2021,0.002427739846241292,2
W2887386650,Read + Verify: Machine Reading Comprehension with Unanswerable Questions,2018,0.0024263716645846456,2
W4385572102,World Models for Math Story Problems,2023,0.0024253479184475927,2
W4380359954,A Data Fusion Framework for Multi-Domain Morality Learning,2023,0.0024232366943917226,2
W3150870318,“Here Are the Rules: Ignore All Rules”: Automatic Contradiction Detection in Spanish,2021,0.0024210147887195775,2
W3100071090,Open Domain Question Answering based on Text Enhanced Knowledge Graph with Hyperedge Infusion,2020,0.002420689163778346,2
W4281632042,Quantum Natural Language Processing: Challenges and Opportunities,2022,0.0024196328661973193,2
W3153389197,Natural Language Processing for Requirements Engineering,2021,0.002417834571478904,2
W3090640022,Explaining Text Matching on Neural Natural Language Inference,2020,0.0024153121126694157,2
W4327644602,Towards Effective Paraphrasing for Information Disguise,2023,0.0024130008014993454,2
W3024226488,Digital begriffsgeschichte: Tracing semantic change using word embeddings,2020,0.002408172981482446,2
W3111005895,Addressing machine learning concept drift reveals declining vaccine sentiment during the COVID-19 pandemic,2020,0.0024062804001224436,2
W4383215352,Refined SBERT: Representing sentence BERT in manifold space,2023,0.0024028170242742617,2
W4395469233,Exploring diverse interests of collaborators in smart cities: A topic analysis using LDA and BERT,2024,0.002396476150733895,2
W4386566719,Question-Answer Sentence Graph for Joint Modeling Answer Selection,2023,0.002395409685026035,2
W4401754921,LLMs for knowledge graph construction and reasoning: recent capabilities and future opportunities,2024,0.0023887647631732193,2
W4404931769,Cluster-Mined Negative Samples for Enhanced Unsupervised Sentence Representation Learning,2024,0.002387586431631477,2
W3195623089,Semantic Approach for Big Five Personality Prediction on Twitter,2021,0.002381652052783591,2
W4392663267,Construction and analysis of uncertainty indices based on multilingual text representations,2024,0.0023804581424558898,2
W4393852514,Detecting the Clinical Features of Difficult-to-Treat Depression Using Synthetic Data from Large Language Models,2024,0.0023713729480171774,2
W4394805543,Research on Long Text Similarity Calculation Method Based on TextRank and BERT,2024,0.002369433295251205,2
W4402727267,Language Models as Black-Box Optimizers for Vision-Language Models,2024,0.002368693322585986,2
W3104059174,BioSentVec: creating sentence embeddings for biomedical texts,2019,0.002365800920629733,2
W4383226706,An evaluation on large language model outputs: Discourse and memorization,2023,0.002365138310465231,2
W4322718777,Precision information extraction for rare disease epidemiology at scale,2023,0.002363777480340353,2
W4406475488,A dataset for evaluating clinical research claims in large language models,2025,0.002360764260982262,2
W4390239756,Exploring the Latest Highlights in Medical Natural Language Processing across Multiple Languages: A Survey,2023,0.002357704129696943,2
W3088572095,Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths,2020,0.002354321711325562,2
W4387809898,Benchmarking the Generation of Fact Checking Explanations,2023,0.002353492151071938,2
W4323341588,Long Short-Term Memory for Non-Factoid Answer Selection in Indonesian Question Answering System for Health Information,2023,0.0023532309298205647,2
W4392384650,Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions,2024,0.0023509735506075033,2
W4388116325,Natural language processing with machine learning methods to analyze unstructured patient-reported outcomes derived from electronic health records: A systematic review,2023,0.0023506180359867956,2
W3214847783,Building a Question Answering System for the Manufacturing Domain,2022,0.002345283159236372,2
W4389524085,Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction,2023,0.0023451474124171457,2
W4406783181,Context Is King: Large Language Models’ Interpretability in Divergent Knowledge Scenarios,2025,0.0023451434911196905,2
W2908230750,Knowledge Graph Embedding Based Question Answering,2019,0.0023439864683453804,2
W4396736142,Improving Retrieval in Theme-specific Applications using a Corpus Topical Taxonomy,2024,0.002342953343464521,2
W4400411628,Unveiling the Impact of Large Language Models on Student Learning: A Comprehensive Case Study,2024,0.002342471904617996,2
W4387968301,SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models,2023,0.002341681200197012,2
W4387171349,Language Models as Controlled Natural Language Semantic Parsers for Knowledge Graph Question Answering,2023,0.0023415152736859924,2
W4327960869,"Short-Text Semantic Similarity (STSS): Techniques, Challenges and Future Perspectives",2023,0.002340754667666398,2
W4386576869,Implicit Temporal Reasoning for Evidence-Based Fact-Checking,2023,0.0023371935071664434,2
W4403063412,Adaptive Bi-Encoder Model Selection and Ensemble for Text Classification,2024,0.0023369210474372586,2
W4389298188,Language inference-based learning for Low-Resource Chinese clinical named entity recognition using language model,2023,0.0023318052927384352,2
W4402352134,SEVEN: Pruning Transformer Model by Reserving Sentinels,2024,0.0023316960694144207,2
W4409094772,Snort Meets Transformers: Accelerating Transformer-Based Network Traffic Classification for Real-Time Performance,2025,0.002331214173302851,2
W4320713530,Urban landscape and climate affect residents’ sentiments based on big data,2023,0.002331214173302851,2
W3044324512,"TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP",2020,0.0023298178540181414,2
W4387394525,Language Model-Based Player Goal Recognition in Open World Digital Games,2023,0.0023278846950821985,2
W4322006551,Large Language Models Demonstrate the Potential of Statistical Learning in Language,2023,0.002326443488806028,2
W4406820596,Semantic Analysis of test items through Large Language Model embeddings predicts a-priori factorial structure of personality tests,2025,0.0023241314989916307,2
W4408079212,A unified prompt-based framework for few-shot multimodal language analysis,2025,0.0023237924792206045,2
W2963058357,Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News,2019,0.002320165969805666,2
W3154280800,SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking,2021,0.0023186018657872028,2
W4400264092,Mlke: A Benchmark for Multilingual Knowledge Editing on Large Language Model,2024,0.002318499516928715,2
W4410247559,"Long Document Classification in the Transformer Era: A Survey on Challenges, Advances, and Open Issues",2025,0.0023177891180325683,2
W4402264152,SneakyPrompt: Jailbreaking Text-to-image Generative Models,2024,0.0023155755586334744,2
W2963019137,Natural Language Comprehension with the EpiReader,2016,0.0023141298838043087,2
W4390916709,BERT-CNN based evidence retrieval and aggregation for Chinese legal multi-choice question answering,2024,0.002301942538476627,2
W4390396506,Evaluating Embeddings from Pre-Trained Language Models and Knowledge Graphs for Educational Content Recommendation,2023,0.002297354101487066,2
W4385571674,Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning,2023,0.002297038628955669,2
W4323782875,A Comparison of SVM Against Pre-trained Language Models (PLMs) for Text Classification Tasks,2023,0.0022964558394053644,2
W4389523828,Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models,2023,0.002295905307083462,2
W4284674178,Explainable Legal Case Matching via Inverse Optimal Transport-based Rationale Extraction,2022,0.002294065961369506,2
W2962998327,Neural Tree Indexers for Text Understanding,2017,0.0022937318326616244,2
W4388144297,LLMs4OL: Large Language Models for Ontology Learning,2023,0.0022910369581003024,2
W3105478389,Recurrent babbling: evaluating the acquisition of grammar from limited input data,2020,0.0022884376468693696,2
W4409085125,Efficient multi-task learning with instance selection for biomedical NLP,2025,0.0022861337969703912,2
W4406738920,Discontinuous named entities in clinical Text: A systematic literature review,2025,0.002282383834773279,2
W2469060249,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,2016,0.0022821281531008545,2
W4367623507,Framing the News: From Human Perception to Large Language Model Inferences,2023,0.0022802393542336696,2
W4385570025,Few-shot In-context Learning on Knowledge Base Question Answering,2023,0.0022746006605796976,2
W4282049100,SsciBERT: a pre-trained language model for social science texts,2022,0.002270447879386024,2
W4410308771,Search method for ship regulations based on semantic similarity using natural language processing,2025,0.002270297590771491,2
W3095642204,Measurement of Semantic Textual Similarity in Clinical Texts: Comparison of Transformer-Based Models,2020,0.0022592910158206816,2
W2963925965,Dynamic Entity Representations in Neural Language Models,2017,0.002257862119037896,2
W2756946152,Deconvolutional Latent-Variable Model for Text Sequence Matching,2018,0.0022568105571099863,2
W4384819947,Leveraging pre-trained language models for mining microbiome-disease relationships,2023,0.0022523668003003117,2
W4389523924,CoLLiE: Collaborative Training of Large Language Models in an Efficient Way,2023,0.0022456473229660262,2
W2548872772,End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension,2016,0.0022418621484429275,2
W4385572399,Knowledge Unlearning for Mitigating Privacy Risks in Language Models,2023,0.0022400280480669287,2
W4393948962,Clustering-Based Joint Topic-Sentiment Modeling of Social Media Data: A Neural Networks Approach,2024,0.00223951243517803,2
W4294791148,Low-Resource Similar Case Matching in Legal Domain,2022,0.0022342997022810666,2
W4389686470,UTDRM: unsupervised method for training debunked-narrative retrieval models,2023,0.002230016210125476,2
W4406844948,AutoML-Guided Fusion of Entity and LLM-Based Representations for Document Classification,2025,0.002227730823401972,2
W4303579584,Using Computational Models to Test Syntactic Learnability,2022,0.0022220921876444405,2
W4393157467,Visual Adversarial Examples Jailbreak Aligned Large Language Models,2024,0.0022208244546430673,2
W2945232141,Submodular Optimization-based Diverse Paraphrasing and its Effectiveness in Data Augmentation,2019,0.0022201091546775785,2
W4410202262,Unleashing the potential of prompt engineering for large language models,2025,0.002218942761241795,2
W4389486944,An in-depth analysis of passage-level label transfer for contextual document ranking,2023,0.002218363290383852,2
W4409019555,Efficient Fine-Tuning of Small-Parameter Large Language Models for Biomedical Bilingual Multi-Task Applications,2025,0.002218036782089529,2
W3168090480,SciFive: a text-to-text transformer model for biomedical literature,2021,0.0022142868117070967,2
W4385330715,Information extraction from weakly structured radiological reports with natural language queries,2023,0.0022136993973186163,2
W2807333695,SemEval-2018 Task 3: Irony Detection in English Tweets,2018,0.002212473444010605,2
W4407207431,Decoding substance use disorder severity from clinical notes using a large language model,2025,0.0022103949682931562,2
W4213059241,Comparative Study of Long Document Classification,2021,0.0022078036784722367,2
W4402169037,A Survey of Text Watermarking in the Era of Large Language Models,2024,0.002207105313843307,2
W4389520494,KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models,2023,0.0022064136892472105,2
W4301594491,Artificial neural network language models predict human brain responses to language even after a developmentally realistic amount of training,2022,0.0022061736082212356,2
W3030406438,Why Attention is Not Explanation: Surgical Intervention and Causal Reasoning about Neural Models.,2020,0.0022052735309155036,2
W4391292768,Improving large language models for clinical named entity recognition via prompt engineering,2024,0.0022027555302128383,2
W4382245789,Retrieval-Based Diagnostic Decision Support: Mixed Methods Study (Preprint),2023,0.0022004111613633157,2
W4320002848,Electric Power Audit Text Classification With Multi-Grained Pre-Trained Language Model,2023,0.0021925775801144086,2
W3013655557,The Limitations of Stylometry for Detecting Machine-Generated Fake News,2020,0.002190117818714021,2
W4405979169,Enhancing Text-to-Image Retrieval by Addressing Parts-of-Speech Imbalance in Vision-Language Models,2025,0.002189992999842429,2
W4385571045,Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,2023,0.0021893359785794407,2
W4324193559,Semantic matching based legal information retrieval system for COVID-19 pandemic,2023,0.0021893003243586154,2
W4377695655,"Towards electronic health record-based medical knowledge graph construction, completion, and applications: A literature study",2023,0.0021883104645141855,2
W4399856164,Evaluation of Language Models for Multilabel Classification of Biomedical Texts,2024,0.0021879948729005513,2
W4404723816,Distributional Semantics: Meaning Through Culture and Interaction,2024,0.0021865999114748912,2
W2890719433,Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models,2018,0.002185379922418434,2
W4398780873,Human-annotated rationales and explainable text classification: a survey,2024,0.0021825023942861174,2
W2926937441,Ranking and Selecting Multi-Hop Knowledge Paths to Better Predict Human Needs,2019,0.002181043278770096,2
W4385825586,Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling,2024,0.0021787206382219396,2
W4389519996,Culturally Aware Natural Language Inference,2023,0.002174682544864412,2
W2963719234,Natural Language Inference over Interaction Space,2017,0.00217453043824398,2
W2782630856,Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs,2018,0.00216749151944375,2
W2173361515,LSTM-based Deep Learning Models for Non-factoid Answer Selection,2015,0.002166846486654107,2
W4409190866,Benchmarking large language models for biomedical natural language processing applications and recommendations,2025,0.002166232338866218,2
W4406502940,Zero-Shot Dense Retrieval Based on Query Expansion,2025,0.002165613923810562,2
W4287854873,Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language,2022,0.0021649652135961746,2
W4409492698,Optimizing Reinforcement Learning with Limited HRI Demonstrations: A Task-Oriented Weight Update Method with Analysis of Multi-Head and Layer Feature Combinations,2025,0.002163843391832375,2
W4409989348,Enhanced Handwritten Text Recognition With Spell Checking by Building a Small Language Model (SLM) With Jaro-Winkler Algorithm,2025,0.002160478047617224,2
W4399465020,Large Language Model and Text Generation,2024,0.0021591666154104116,2
W4392914055,Evaluation of Transfer Learning and Adaptability in Large Language Models with the GLUE Benchmark,2024,0.0021589978122774954,2
W3173970713,"What do end-to-end speech models learn about speaker, language and channel information? A layer-wise and neuron-level analysis",2023,0.0021553929465247226,2
W4407482948,Prompts to Table: Specification and Iterative Refinement for Clinical Information Extraction with Large Language Models,2025,0.002153139967330762,2
W2889272240,MedSTS: a resource for clinical semantic textual similarity,2018,0.002147445445892576,2
W4205788858,SPECTRA: Sparse Structured Text Rationalization,2021,0.0021461890645498587,2
W4283172211,What Does it Mean for a Language Model to Preserve Privacy?,2022,0.0021400002510976934,2
W2950246755,NLProlog: Reasoning with Weak Unification for Question Answering in Natural Language,2019,0.0021386926265231873,2
W3173032467,Unsupervised Document Expansion for Information Retrieval with Stochastic Text Generation,2021,0.0021380343069928977,2
W4406083163,The architecture of language: Understanding the mechanics behind LLMs,2025,0.002133642587121967,2
W4367046683,Structure Pretraining and Prompt Tuning for Knowledge Graph Transfer,2023,0.0021299746078902795,2
W4318464952,Multi-task Learning for Features Extraction in Financial Annual Reports,2023,0.0021287175113444095,2
W3205226109,DiscoDVT: Generating Long Text with Discourse-Aware Discrete Variational Transformer,2021,0.0021276005369989465,2
W4397029721,Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs,2024,0.002127406898916603,2
W4400231005,BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge,2024,0.002127234944674133,2
W4408417561,Conceptual Combination in Large Language Models: Uncovering Implicit Relational Interpretations in Compound Words With Contextualized Word Embeddings,2025,0.002126729157022457,2
W4410461195,Aligning large language models with human preferences using historical text edits,2025,0.0021230288816429955,2
W4388037674,Lost in translation? Not for Large Language Models: Automated divergent thinking scoring performance translates to non-English contexts,2023,0.00212125340381493,2
W2963993699,Plan-and-Write: Towards Better Automatic Storytelling,2019,0.0021206426286711354,2
W2970782003,Incorporating External Knowledge into Machine Reading for Generative Question Answering,2019,0.002120611054310568,2
W4386566751,Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks,2023,0.0021149568025691026,2
W4386566678,Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?,2023,0.0021085930120375625,2
W4299838440,Regularizing and Optimizing LSTM Language Models,2017,0.002105444113728202,2
W4383911975,The Value of Numbers in Clinical Text Classification,2023,0.0021051323144086254,2
W4401847921,A scoping review of large language model based approaches for information extraction from radiology reports,2024,0.002099837227596823,2
W3103939752,Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model,2020,0.002098168297075811,2
W4405075726,A Systematic Comparison Between Open- and Closed-Source Large Language Models in the Context of Generating GDPR-Compliant Data Categories for Processing Activity Records,2024,0.0020926816853094926,2
W3077627272,Clinical trial search: Using biomedical language understanding models for re-ranking,2020,0.00209028302217585,2
W4287887710,Dual-Channel Evidence Fusion for Fact Verification over Texts and Tables,2022,0.002087882074039498,2
W4286544524,Bert-Based Feature Extraction for Long-Lived Bug Prediction in Floss: A Comparative Study,2022,0.002086545432391281,2
W4403051598,Retrieval-style In-context Learning for Few-shot Hierarchical Text Classification,2024,0.00208648188409028,2
W4367670019,Contextual semantic embeddings for ontology subsumption prediction,2023,0.002085976241966079,2
W4402351439,Time-CoT for Enhancing Time Reasoning Factual Question Answering in Large Language Models,2024,0.0020833729727712967,2
W3209946909,Wacky Weights in Learned Sparse Representations and the Revenge of Score-at-a-Time Query Evaluation,2021,0.002082694547238259,2
W3166904074,Adapting natural language processing for technical text,2021,0.0020826857079328776,2
W4392919908,LeanContext: Cost-efficient domain-specific question answering using LLMs,2024,0.002074989369245236,2
W4389519061,Goal-Driven Explainable Clustering via Language Descriptions,2023,0.0020741977849248537,2
W4386566833,A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction,2023,0.0020740344703375677,2
W4405185737,KG-EGV: A Framework for Question Answering with Integrated Knowledge Graphs and Large Language Models,2024,0.002070948709898027,2
W3037252472,FinBERT: A Pretrained Language Model for Financial Communications,2020,0.002068236245263465,2
W4321749402,On the effectiveness of compact biomedical transformers,2023,0.0020674639632115646,2
W3154755316,Learning Passage Impacts for Inverted Indexes,2021,0.0020667553103889363,2
W4385573364,Mask-then-Fill: A Flexible and Effective Data Augmentation Framework for Event Extraction,2022,0.0020637784719156863,2
W4400524784,Generative Retrieval as Multi-Vector Dense Retrieval,2024,0.002054252260481926,2
W2988584128,Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network,2019,0.0020524164819191192,2
W4388820628,FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking,2023,0.002048459302317127,2
W4409897857,LLMs Open-Domain Question Answering Method Based on Retrieval-Augmented Generation and Soft Prompt Optimization,2025,0.002046638699467872,2
W4406772193,CoLE: A collaborative legal expert prompting framework for large language models in law,2025,0.0020466307478691544,2
W4392869958,Translate-Distill: Learning Cross-Language Dense Retrieval by Translation and Distillation,2024,0.0020447963836968197,2
W3210968241,Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback,2021,0.0020442998331534888,2
W4283172096,"Can Machines Help Us Answering Question 16 in Datasheets, and In Turn Reflecting on Inappropriate Content?",2022,0.002043377298379817,2
W4392971467,Enhancing accident cause analysis through text classification and accident causation theory: A case study of coal mine gas explosion accidents,2024,0.002042574843851607,2
W4206221224,Classifying social determinants of health from unstructured electronic health records using deep learning-based natural language processing,2022,0.0020420956397910997,2
W4392935324,Dissociating language and thought in large language models,2024,0.0020407053962344154,2
W2993873509,Deep learning in clinical natural language processing: a methodical review,2019,0.0020405315078745982,2
W3035251378,HAT: Hardware-Aware Transformers for Efficient Natural Language Processing,2020,0.0020376612719191986,2
W4391385091,A knowledge graph-based bio-inspired design approach for knowledge retrieval and reasoning,2024,0.0020348661539341052,2
W4401597819,GPT-4 as an X data annotator: Unraveling its performance on a stance classification task,2024,0.002029915226540476,2
W4221154673,Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification,2022,0.00202950568929833,2
W3012592703,DeepEnroll: Patient-Trial Matching with Deep Embedding and Entailment Prediction,2020,0.0020273696850813877,2
W3100385063,Linguistic generalization and compositionality in modern artificial neural networks,2019,0.0020267130220169936,2
W4327657958,Injecting the BM25 Score as Text Improves BERT-Based Re-rankers,2023,0.00202542220797355,2
W4389518753,Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models,2023,0.002020918414245444,2
W3199848231,Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document,2022,0.0020176636334666227,2
W4220949944,Shared computational principles for language processing in humans and deep language models,2022,0.0020151665790782984,2
W4396735998,RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules,2024,0.0020144472927786003,2
W4382463911,FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer,2023,0.0020122827832009115,2
W4366774505,Enhancing Neural Text Detector Robustness with μAttacking and RR-Training,2023,0.002011130961943128,2
W3201023666,Continual knowledge infusion into pre-trained biomedical language models,2021,0.0020025532380579542,2
W2994803089,Extending Machine Language Models toward Human-Level Language Understanding,2019,0.002002237611597581,2
W4385570558,TeamShakespeare at SemEval-2023 Task 6: Understand Legal Documents with Contextualized Large Language Models,2023,0.001998313461528599,2
W4389606799,Automated Domain Modeling with Large Language Models: A Comparative Study,2023,0.001991571911468058,2
W2411480514,A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task,2016,0.0019896232883055043,2
W4402320017,CoAT: Corpus of artificial texts,2024,0.0019893298056366834,2
W4403051422,Do Vision and Language Models Share Concepts? A Vector Space Alignment Study,2024,0.0019884873297349075,2
W4284698368,Fast changeset-based bug localization with BERT,2022,0.0019884290362087295,2
W4283269716,A Unified Understanding of Deep NLP Models for Text Classification,2022,0.0019867344497776626,2
W4410199817,SynCSE: Syntax Graph-based Contrastive Learning of Sentence Embeddings,2025,0.0019861646131688867,2
W4389519026,GD-COMET: A Geo-Diverse Commonsense Inference Model,2023,0.0019827793485931904,2
W2604368306,Reading Wikipedia to Answer Open-Domain Questions,2017,0.0019801767449990643,2
W2770626128,Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations,2017,0.0019755501333989923,2
W4393147125,PMET: Precise Model Editing in a Transformer,2024,0.001971177813673713,2
W2963351454,Studying the Inductive Biases of,2019,0.001967872232380976,2
W4386541551,A Study of Contrastive Learning Algorithms for Sentence Representation Based on Simple Data Augmentation,2023,0.0019671163945533345,2
W4386566724,Uncovering Implicit Inferences for Improved Relational Argument Mining,2023,0.0019664471973311155,2
W2576754561,Self-Taught convolutional neural networks for short text clustering,2017,0.001956690362308526,2
W4407742619,A survey on learning with noisy labels in Natural Language Processing: How to train models with label noise,2025,0.001953396647129953,2
W2118463056,Reasoning about Entailment with Neural Attention,2015,0.0019530995695473908,2
W4391221150,Almanac — Retrieval-Augmented Language Models for Clinical Medicine,2024,0.0019486388907182955,2
W4398223181,Relation Extraction in Underexplored Biomedical Domains: A Diversity-optimized Sampling and Synthetic Data Generation Approach,2024,0.0019482857690508936,2
W2612867916,Neural Paraphrase Identification of Questions with Noisy Pretraining,2017,0.001945561233834524,2
W3196948150,Overview of the CLEF-2021 CheckThat! Lab Task 2 on detecting previously fact-checked claims in tweets and political debates,2021,0.001943613218718289,2
W4406424553,COMCARE: A Collaborative Ensemble Framework for Context-Aware Medical Named Entity Recognition and Relation Extraction,2025,0.001943106310903631,2
W2739716023,An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge,2017,0.0019355968492161938,2
W4402577557,Cross-biased Contrastive Learning for Answer Selection with Dual-Tower Structure,2024,0.0019349008312091363,2
W2798819017,Exploring Semantic Properties of Sentence Embeddings,2018,0.0019326340092963824,2
W4408420741,A Pilot Study Using Natural Language Processing to Explore Textual Electronic Mental Healthcare Data,2025,0.001931215138724315,2
W4409720324,Quantifying Divergence for Human-AI Collaboration and Cognitive Trust,2025,0.0019291081716406929,2
W4226391416,Squeezing Water from a Stone: A Bag of Tricks for Further Improving Cross-Encoder Effectiveness for Reranking,2022,0.0019213240937445617,2
W4285112556,An Empirical Study on Explanations in Out-of-Domain Settings,2022,0.0019206665432126937,2
W4389518954,Evaluating Verifiability in Generative Search Engines,2023,0.001917094138667617,2
W4409626943,Industrial applications of large language models,2025,0.0019154496214174373,2
W4389949263,BioEGRE: a linguistic topology enhanced method for biomedical relation extraction based on BioELECTRA and graph pointer neural network,2023,0.0019140693295251158,2
W2964230347,A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors,2018,0.0019137402101680613,2
W4410398522,A Symmetric Dual-Drive Text Matching Model Based on Dynamically Gated Sparse Attention Feature Distillation with a Faithful Semantic Preservation Strategy,2025,0.0019091158067580558,2
W4406458013,A Negative Sample Enhancement Strategy to Improve Contrastive Learning for Unsupervised Sentence Representation,2024,0.0019087800112119412,2
W4284682639,Document Expansion Baselines and Learned Sparse Lexical Representations for MS MARCO V1 and V2,2022,0.0019072029829762644,2
W4386284317,Towards More Generalizable and Accurate Sentence Classification in Medical Abstracts with Less Data,2023,0.0019067434463246173,2
W2952841984,A Minimax Game for Instance based Selective Transfer Learning,2019,0.0019036670686745439,2
W3209329320,CyBERT: Cybersecurity Claim Classification by Fine-Tuning the BERT Language Model,2021,0.0019010628828198136,2
W2554915555,What Do Recurrent Neural Network Grammars Learn About Syntax?,2017,0.0018996031255254105,2
W4399891105,Adaption BERT for Medical Information Processing with ChatGPT and Contrastive Learning,2024,0.0018988488342163458,2
W4407414076,Explainable AI for Large Language Models via Context-Aware Word Embeddings,2025,0.001898631665650601,2
W4409478869,Elevating Textual Question Answering with On-Demand Visual Augmentation,2025,0.001897886332422947,2
W4221150629,Hierarchical Interpretation of Neural Text Classification,2022,0.0018947700872151258,2
W4289849144,Multi-label classification of symptom terms from free-text bilingual adverse drug reaction reports using natural language processing,2022,0.00189453155804836,2
W2805003518,Incorporating Context into Language Encoding Models for fMRI,2018,0.0018878248368429044,2
W4403577801,"""Reasoning before Responding"": Towards Legal Long-form Question Answering with Interpretability",2024,0.001884899177587985,2
W2822830299,Modeling Multi-turn Conversation with Deep Utterance Aggregation,2018,0.0018831233002984222,2
W3156891177,Carbon Emissions and Large Neural Network Training.,2021,0.0018824324947254237,2
W4321378748,Explainable clinical coding with in-domain adapted transformers,2023,0.0018768259205096992,2
W4408412978,Towards Effective Time-Aware Language Representation: Exploring Enhanced Temporal Understanding in Language Models,2025,0.0018761520586007391,2
W4284669679,InPars: Unsupervised Dataset Generation for Information Retrieval,2022,0.0018729423485152608,2
W3198536471,Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting,2021,0.0018719944452699016,2
W4385734111,Can we trust the evaluation on ChatGPT?,2023,0.0018688203946033743,2
W4391116601,Exploiting Language Models as a Source of Knowledge for Cognitive Agents,2024,0.0018679469047584156,2
W4408717853,Multi-TuneV: Fine-tuning the fusion of multiple modules for video action recognition,2025,0.0018667697149164363,2
W2971048662,Induction Networks for Few-Shot Text Classification,2019,0.0018638087480382986,2
W4408210925,A Multi-Attribute Mixture Expert Reasoning Approach Based on Large Language Models,2025,0.0018584984045188879,2
W2929198159,Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases,2019,0.0018582929285360014,2
W3105994699,Measuring the Similarity of Sentential Arguments in Dialogue,2016,0.0018558369639786235,2
W3159684727,Mechanisms for handling nested dependencies in neural-network language models and humans,2021,0.0018551169851335045,2
W4385573733,DEER: Descriptive Knowledge Graph for Explaining Entity Relationships,2022,0.0018539581814907072,2
W2963083752,Interpretable Adversarial Perturbation in Input Embedding Space for Text,2018,0.0018524389796588784,2
W4388209043,Advanced sentence-embedding method considering token importance based on explainable artificial intelligence and text summarization model,2023,0.001850688160294862,2
W4367046920,PROD: Progressive Distillation for Dense Retrieval,2023,0.00185053427476153,2
W4407222811,Chinese Medical Spoken Language Understanding Based on Prototypical Modification Network and Contrastive Learning,2025,0.001849259747094207,2
W2148437670,Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),2023,0.0018438016320318884,2
W4385571112,Search-Oriented Conversational Query Editing,2023,0.0018393159082767442,2
W4385574025,The Legal Argument Reasoning Task in Civil Procedure,2022,0.0018392340548448699,2
W4392913756,Natural language instructions induce compositional generalization in networks of neurons,2024,0.0018349439495915546,2
W2962690139,Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples,2018,0.0018312236272728277,2
W4400978391,Empowering Language Model with Guided Knowledge Fusion for Biomedical Document Re-ranking,2024,0.0018282353855695235,2
W4389520282,Chain-of-Thought Reasoning in Tabular Language Models,2023,0.001826842847504313,2
W4389519012,KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion,2023,0.001825695336405846,2
W4394770144,LLaMA-LoRA Neural Prompt Engineering: A Deep Tuning Framework for Automatically Generating Chinese Text Logical Reasoning Thinking Chains,2024,0.0018253050507741567,2
W4382985799,Improving Information Extraction from Pathology Reports using Named Entity Recognition,2023,0.0018239743637171357,2
W4226510082,Human Language Understanding &amp; Reasoning,2022,0.00182231952707115,2
W4392902804,Enabling Device Control Planning Capabilities of Small Language Model,2024,0.001820775613472188,2
W4391520039,Survey of transformers and towards ensemble learning using transformers for natural language processing,2024,0.0018202984100707102,2
W4409137999,Enhancing data quality in medical concept normalization through large language models,2025,0.0018152746815147472,2
W4385565595,How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks,2023,0.001807638199092941,2
W4284664419,From Distillation to Hard Negative Sampling,2022,0.0018068259909128606,2
W4392909639,Exploring Soft Prompt Initialization Strategy for Few-Shot Continual Text Classification,2024,0.0018034322893850704,2
W4386647230,Evading text based emotion detection mechanism via adversarial attacks,2023,0.001802460035351953,2
W4406152959,Hybrid natural language processing tool for semantic annotation of medical texts in Spanish,2025,0.0018015213926855654,2
W2963440143,Recursive Neural Networks Can Learn Logical Semantics,2015,0.0018009266152699645,2
W2962832505,Regularizing and Optimizing LSTM Language Models,2017,0.0018008721056958612,2
W4403659146,Enhancing Chinese comprehension and reasoning for large language models: an efficient LoRA fine-tuning and tree of thoughts framework,2024,0.0017999319670690352,2
W4327644053,Parameter-Efficient Sparse Retrievers and Rerankers Using Adapters,2023,0.001797879355194413,2
W2791941932,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,2018,0.0017961404506496113,2
W2963028402,Training Classifiers with Natural Language Explanations,2018,0.0017954899468162044,2
W4310854123,Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale,2022,0.0017951095835300665,2
W4399109812,DUVEL: an active-learning annotated biomedical corpus for the recognition of oligogenic combinations,2024,0.001793405821361266,2
W4385571894,ThinkSum: Probabilistic reasoning over sets using large language models,2023,0.0017928879877703824,2
W4283778430,Chemical identification and indexing in PubMed full-text articles using deep learning and heuristics,2022,0.0017889532970704685,2
W4316015028,Data-driven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models,2023,0.0017848567013910356,2
W4392673844,Effective Natural Language Processing Algorithms for Early Alerts of Gout Flares from Chief Complaints,2024,0.001784168468809438,2
W4389523721,TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks,2023,0.0017817923343812015,2
W4225110351,Modern Baselines for SPARQL Semantic Parsing,2022,0.0017814592246509335,2
W4220864053,Text Data Augmentation for the Korean Language,2022,0.00176943043884956,2
W4389519226,LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models,2023,0.0017666412930162796,2
W4410498908,Neural Topic Generation Utilizing Attention Mechanisms With Transformer‐Based Embeddings for Root‐Cause Analysis of Manufacturing Defects in Electronic Products,2025,0.0017656238302498587,2
W3117696238,A Survey of the State of Explainable AI for Natural Language Processing,2020,0.0017626353302736054,2
W4389524372,Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data,2023,0.0017606033139764866,2
W2769099080,Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning,2017,0.0017573139268317077,2
W4389521054,Batch Prompting: Efficient Inference with Large Language Model APIs,2023,0.0017567846692510237,2
W4403499178,Large language models in law: A survey,2024,0.0017536086908040923,2
W4226325130,Out-of-Domain Semantics to the Rescue! Zero-Shot Hybrid Retrieval Models,2022,0.0017490882604919542,2
W4407571831,Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking,2025,0.0017484097538565556,2
W4399286424,Extracting patient lifestyle characteristics from Dutch clinical text with BERT models,2024,0.0017483251268253118,2
W4323568419,A Multi-Modal Story Generation Framework with AI-Driven Storyline Guidance,2023,0.0017477729456949957,2
W4389988752,Zero-shot Bilingual App Reviews Mining with Large Language Models,2023,0.00174742311916838,2
W2810134635,Subword-augmented Embedding for Cloze Reading Comprehension,2018,0.0017468987452205161,2
W4389523957,Is ChatGPT a General-Purpose Natural Language Processing Task Solver?,2023,0.0017451440981342727,2
W4367189613,Generative Relevance Feedback with Large Language Models,2023,0.001743035768931451,2
W4385571886,Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers,2023,0.0017410094960693733,2
W3201012227,Graph-based Retrieval for Claim Verification over Cross-document Evidence,2022,0.001740997543300222,2
W3176111903,Generalizing Cross-Document Event Coreference Resolution Across Multiple Corpora,2021,0.0017407211618563998,2
W3181342329,Testing challenges for NLP-intensive bots,2021,0.001740073595803903,2
W4389010498,Syndicom: Improving Conversational Commonsense with Error-Injection and Natural Language Feedback,2023,0.0017397160442524857,2
W4400721704,Do Generative Large Language Models Need Billions of Parameters?,2024,0.0017389389472731672,2
W4400200617,Effectiveness of large language models in automated evaluation of argumentative essays: finetuning vs. zero-shot prompting,2024,0.00173826190205699,2
W2915589364,The State of Sparsity in Deep Neural Networks,2019,0.001736891806776606,2
W4407214653,Re-SciBERT: An Entity-Enriched Language Model to Enhance Biomedical Relation Extraction,2024,0.0017359082522059837,2
W4284691483,ArchivalQA,2022,0.0017335173426515742,2
W2625541525,S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension,2017,0.0017309090415473544,2
W2786685006,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,2018,0.0017291746154643605,2
W2986889180,Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment,2019,0.0017254870043040143,2
W2948130861,Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP,2019,0.0017243544693441406,2
W4368353189,Towards Practical Few-shot Federated NLP,2023,0.0017241851812017156,2
W4408389185,Semantic information-based attention mapping network for few-shot knowledge graph completion,2025,0.0017227778302386183,2
W4388131010,Sentence Similarity Using Modified Latent Semantic Analysis and Semantic Relations,2023,0.0017213483008669227,2
W4385532506,Multi-level Adversarial Training for Stock Sentiment Prediction,2023,0.0017194770570606453,2
W4367309809,Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models,2023,0.001717988434419537,2
W3106394530,Look before you Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion,2019,0.0017142087678880882,2
W2891691791,Multi-Source Domain Adaptation with Mixture of Experts,2018,0.0017139710601551748,2
W3153233475,Data-Efficient Pretraining via Contrastive Self-Supervision,2020,0.0017108306026420018,2
W4393398286,Exploiting Graph Embeddings from Knowledge Bases for Neural Biomedical Relation Extraction,2024,0.0017067848629275112,2
W4402794779,Low-Parameter Federated Learning with Large Language Models,2024,0.0017046112314602904,2
W4281650060,Specialized document embeddings for aspect-based similarity of research papers,2022,0.0017044248004305853,2
W2962863107,Towards Explainable NLP: A Generative Explanation Framework for Text Classification,2019,0.0016972465846527803,2
W4401692510,AdIn-DETR: Adapting Detection Transformer for End-to-End Real-Time Power Line Insulator Defect Detection,2024,0.0016966336126539334,2
W4409836394,Exploring Methodologies for Computing Sentence Similarity in Natural Language Processing,2025,0.001695687279797798,2
W3094444847,TwinBERT,2020,0.0016896176005106926,2
W4407567330,Optimizing Tumor Information Extraction from Pathology Reports Using Large Language Models: The Advantage of Few-Shot Learning,2025,0.0016892244065960613,2
W4385570683,Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction,2023,0.0016866394730827715,2
W4383665117,Resume Shortlisting and Ranking with Transformers,2023,0.001681154970633344,2
W4386771634,GIMM: A graph convolutional network-based paraphrase identification model to detecting duplicate questions in QA communities,2023,0.0016788084847358486,2
W4401862830,Killing Two Birds with One Stone: Cross-modal Reinforced Prompting for Graph and Language Tasks,2024,0.0016755653657062057,2
W4378512336,MGeo: Multi-Modal Geographic Language Model Pre-Training,2023,0.00167546371993619,2
W4407598256,ChatCNC: Conversational machine monitoring via large language model and real-time data retrieval augmented generation,2025,0.001674340583338256,2
W4381802325,Mining legal arguments in court decisions,2023,0.0016724789206766881,2
W4406207370,A Primer on Large Language Models and their Limitations,2025,0.0016706685353700447,2
W2889577585,Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces,2018,0.0016703342782685704,2
W4379031423,Using Siamese BiLSTM Models for Identifying Text Semantic Similarity,2023,0.0016689425004091274,2
W4408769333,An automated classification pipeline for tables in pharmacokinetic literature,2025,0.0016685047684843963,2
W4205881250,Comparison of different feature extraction methods for applicable automated ICD coding,2022,0.0016682963332059532,2
W2803267010,Neural Text Generation in Stories Using Entity Representations as Context,2018,0.0016681783006425863,2
W4409060877,Enhancing Plant Protection Knowledge with Large Language Models: A Fine-Tuned Question-Answering System Using LoRA,2025,0.0016670455699617495,2
W2952191002,Long Short-Term Memory-Networks for Machine Reading,2016,0.0016662362164215562,2
W4394569842,Let's Speak Trajectories: A Vision to Use NLP Models for Trajectory Analysis Tasks,2024,0.0016641263754605265,2
W2757276219,Deep Neural Solver for Math Word Problems,2017,0.0016640013888602509,2
W2864258299,Deep Enhanced Representation for Implicit Discourse Relation Recognition,2018,0.001655245224454639,2
W3154451896,Process-Level Representation of Scientific Protocols with Interactive Annotation,2021,0.0016533912764553908,2
W4407601505,Knowledge Neurons in the Knowledge Graph-based Link Prediction Models,2025,0.0016532799500876333,2
W4210984920,Neural Network Methods for Natural Language Processing,2017,0.0016502366951603247,2
W4382202515,Real or Fake Text?: Investigating Human Ability to Detect Boundaries between Human-Written and Machine-Generated Text,2023,0.0016500878460556348,2
W4307647693,Understanding models understanding language,2022,0.0016446725652195801,2
W4407039591,Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?,2025,0.001644087708518985,2
W3176357828,A Targeted Assessment of Incremental Processing in Neural Language Models and Humans,2021,0.001642274857660415,2
W4383860112,End-to-End Multimodal Fact-Checking and Explanation Generation: A Challenging Dataset and Models,2023,0.0016420007108069415,2
W4385571775,Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker,2023,0.001640350516742694,2
W4396620533,Infusing internalized knowledge of language models into hybrid prompts for knowledgeable dialogue generation,2024,0.001637772288236747,2
W3125701197,Towards Rare Disease Knowledge Graph Learning from Social Posts of Patients,2021,0.0016376611178685544,2
W4382240075,Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding,2023,0.0016375594510186651,2
W4287887196,Robust Conversational Agents against Imperceptible Toxicity Triggers,2022,0.0016367951698805622,2
W4409979535,Bigram Knowledge Extraction from GPT-2,2025,0.0016357207592516863,2
W4379507314,Just Tell Me: Prompt Engineering in Business Process Management,2023,0.0016347400609772523,2
W2891488835,Understanding Convolutional Neural Networks for Text Classification,2018,0.0016285811166550398,2
W4290599668,Comparison of Pretraining Models and Strategies for Health-Related Social Media Text Classification,2022,0.001627959907864245,2
W4293581639,Beyond word embeddings: A survey,2022,0.0016275561319433977,2
W4407150845,Using Large Language Model to Fill in Web Forms to Support Automated Web Application Testing,2025,0.001626263209882113,2
W4407857790,"Urdu Word Sense Disambiguation: Leveraging Contextual Stacked Embedding, Siamese Transformer Encoder 1DCNN-BiLSTM, and Gloss Data Augmentation",2025,0.0016242811399272076,2
W4404132457,"Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability",2024,0.0016236657199800693,2
W4368373721,Pairwise contrastive learning for sentence semantic equivalence identification with limited supervision,2023,0.0016215439203855306,2
W4396929025,An in-depth evaluation of federated learning on biomedical natural language processing for information extraction,2024,0.0016212778416649567,2
W4407596815,Large language models can better understand knowledge graphs than we thought,2025,0.0016187228236133142,2
W4406407909,Large Language Models Struggle to Encode Medical Concepts - A Multilingual Benchmarking and Comparative Analysis,2025,0.0016124522666822085,2
W4396665924,Automatically Correcting Large Language Models: <i>Surveying the Landscape of Diverse Automated Correction Strategies</i>,2024,0.0016093709216308315,2
W4398186161,An Algorithm based on Semantic Similarity to Extract Candidate Answers in Question Answering Systems,2024,0.0016091231570706734,2
W4386587781,Graph-Enriched Biomedical Entity Representation Transformer,2023,0.001608442674740101,2
W2932637973,Unsupervised Recurrent Neural Network Grammars,2019,0.0016076587914396786,2
W4387105248,A Benchmark Dataset to Distinguish Human-Written and Machine-Generated Scientific Papers,2023,0.0016011543956623985,2
W3182653761,Mining Numbers in Text: A Survey,2021,0.0016000651987958146,2
W4393187322,Automatic categorization of self-acknowledged limitations in randomized controlled trial publications,2024,0.00159682872553246,2
W4410049458,Large Language Model Fine-tuning with Low-Rank Adaptation: A Performance Exploration,2025,0.0015951274725302916,2
W2949611393,Beyond BLEU:Training Neural Machine Translation with Semantic Similarity,2019,0.0015946709223438505,2
W4385570848,How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech,2023,0.0015931062443338272,2
W4386002638,ChatAgri: Exploring potentials of ChatGPT on cross-linguistic agricultural text classification,2023,0.0015906999049695046,2
W2798698226,Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement,2018,0.0015894039148254113,2
W4407870828,Knowledge assimilation: Implementing knowledge-guided agricultural large language model,2025,0.0015868237567368848,2
W3123802026,Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues,2021,0.0015863977155575466,2
W3177466351,Rational LAMOL: A Rationale-based Lifelong Learning Framework,2021,0.0015839276099620977,2
W3168591151,Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention,2021,0.0015831056711499483,2
W4407838598,Weakly supervised veracity classification with LLM-predicted credibility signals,2025,0.001579462482309099,2
W4382010844,Finding Structure in One Child's Linguistic Experience,2023,0.0015793029776064134,2
W4308931255,Chinese sentence semantic matching based on multi-level relevance extraction and aggregation for intelligent human–robot interaction,2022,0.0015787800630318164,2
W2512531235,Deep Fusion LSTMs for Text Semantic Matching,2016,0.0015759108311264689,2
W4327644088,Query Performance Prediction for Neural IR: Are We There Yet?,2023,0.0015749351464521741,2
W4399659401,Building a Large Corpus and Pre-trained Language Models from National and Local Assembly Minutes,2024,0.0015745743026043299,2
W4385301120,Chatbot Integration for Metaverse - A University Platform Prototype,2023,0.0015739229680954265,2
W4404136417,Exploring the effectiveness of instruction tuning in biomedical language processing,2024,0.0015723936620206118,2
W3134687547,Information to Wisdom: Commonsense Knowledge Extraction and Compilation,2021,0.0015717118801888553,2
W4221151670,Incorporating Commonsense Knowledge into Story Ending Generation via Heterogeneous Graph Networks,2022,0.001567296787490307,2
W4392267027,Large-scale benchmark yields no evidence that language model surprisal explains syntactic disambiguation difficulty,2024,0.001566837617435327,2
W4367047445,Hierarchy-Aware Multi-Hop Question Answering over Knowledge Graphs,2023,0.0015645631159746725,2
W3003404702,Bringing Stories Alive: Generating Interactive Fiction Worlds,2020,0.0015595067193751974,2
W4393146990,"Translate Meanings, Not Just Words: IdiomKB’s Role in Optimizing Idiomatic Translation with Language Models",2024,0.0015594268252837524,2
W4393160124,Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-Based Retrofitting,2024,0.0015584780057418068,2
W2550837020,Words or Characters? Fine-grained Gating for Reading Comprehension,2016,0.0015572071551843104,2
W4402407635,Prompt Engineering Paradigms for Medical Applications: Scoping Review,2024,0.0015565693023169044,2
W2415755012,Gated-Attention Readers for Text Comprehension,2016,0.0015559039143085594,2
W4311711471,Reinforcement learning-driven deep question generation with rich semantics,2022,0.0015492645064641194,2
W4409651727,LLM-KGMQA: large language model-augmented multi-hop question-answering system based on knowledge graph in medical field,2025,0.0015462474784755948,2
W4407914316,A Review of the Challenges with Massive Web-Mined Corpora Used in Large Language Models Pre-training,2025,0.0015451536540916106,2
W4224311069,A simple neural vector space model for medical concept normalization using concept embeddings,2022,0.0015444395071336488,2
W4395688865,Answering Spatial Commonsense Questions by Learning Domain-Invariant Generalization Knowledge,2024,0.001543059748281417,2
W4367843743,Heart disease risk factors detection from electronic health records using advanced NLP and deep learning techniques,2023,0.0015385766267704863,2
W4406737213,Has machine paraphrasing skills approached humans? Detecting automatically and manually generated paraphrased cases,2025,0.0015384804234960948,2
W4392904187,Retrieval-Generation Synergy Augmented Large Language Models,2024,0.0015319706955325358,2
W4224308764,Efficient Neural Ranking using Forward Indexes,2022,0.0015318224562094734,2
W4396871174,Non-Alpha-Num: a novel architecture for generating adversarial examples for bypassing NLP-based clickbait detection mechanisms,2024,0.0015317387956816475,2
W4389523811,Look-back Decoding for Open-Ended Text Generation,2023,0.0015242974713728893,2
W4313591059,Knowledge Adaptive Multi-Way Matching Network for Biomedical Named Entity Recognition via Machine Reading Comprehension,2023,0.0015225703065253832,2
W4407079389,Multilingual Computational Models Reveal Shared Brain Responses to 21 Languages,2025,0.0015204711143029944,2
W3109431378,"Survey and open problems in privacy-preserving knowledge graph: merging, query, representation, completion, and applications",2024,0.001520095167642017,2
W4410218224,Prompting large language models with knowledge graphs for question answering involving long-tail facts,2025,0.0015189297015790117,2
W4406828411,Inflect-text: a novel mechanism to evade neural text classifiers by leveraging word inflectional perturbations,2025,0.0015178137603127605,2
W4406297511,Tool learning with large language models: a survey,2025,0.001514304463370017,2
W3116073702,Explainability in deep reinforcement learning,2020,0.0015142461771199811,2
W4409599055,Research on the discrimination of translation difficulty level based on spoken language signal processing technology,2025,0.0015124603758280648,2
W4306249555,Enriching Biomedical Knowledge for Low-resource Language Through Translation,2022,0.001511905269922885,2
W2963923670,A comparison of word embeddings for the biomedical natural language processing,2018,0.0015095478801935415,2
W4407069513,cosmosage: A natural-language assistant for cosmology,2025,0.0015094556389379655,2
W4407269018,Parentheses insertion based sentence-level text adversarial attack,2025,0.001508105024506791,2
W3091534907,Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking,2020,0.0015078716566492905,2
W4381661201,Continual Pre-Training of Language Models for Concept Prerequisite Learning with Graph Neural Networks,2023,0.0015070065590372722,2
W4402646312,Enhancing Small Language Models via ChatGPT and Dataset Augmentation,2024,0.0015047923081685794,2
W3103662468,STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,2020,0.001496958873074533,2
W2914855263,Strategies for Structuring Story Generation,2019,0.0014956289784730861,2
W4393343917,Alignment of brain embeddings and artificial contextual embeddings in natural language points to common geometric patterns,2024,0.001493156938152597,2
W4385278011,Retrieval-Augmented Knowledge Graph Reasoning for Commonsense Question Answering,2023,0.0014929604731558615,2
W4405105409,CPLLM: Clinical prediction with large language models,2024,0.0014909661480570731,2
W4389519219,Med-HALT: Medical Domain Hallucination Test for Large Language Models,2023,0.001490828062817492,2
W4362602149,Position-Aware Relational Transformer for Knowledge Graph Embedding,2023,0.0014906748968375068,2
W3176075895,Conversational Neuro-Symbolic Commonsense Reasoning,2021,0.0014893952783113452,2
W4406418717,Metadata Conditioning Accelerates Language Model Pre-training,2025,0.0014884675915118016,2
W4221142212,BERN2: an advanced neural biomedical named entity recognition and normalization tool,2022,0.0014879442703213056,2
W4289638300,A hierarchy of linguistic predictions during natural language comprehension,2022,0.001487687902320308,2
W4226163610,Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation,2021,0.0014868023103296914,2
W4387424307,Contrastive sentence representation learning with adaptive false negative cancellation,2023,0.0014839752711938317,2
W4224616393,Multilabel classification of medical concepts for patient clinical profile identification,2022,0.0014834857845653388,2
W3159682551,Did they answer? Subjective acts and intents in conversational discourse,2021,0.001483420988702503,2
W4389519056,A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis,2023,0.0014784950278498615,2
W4408089088,Fixer-level supervised contrastive learning for bug assignment,2025,0.0014774949948021124,2
W4399513151,Towards knowledge-infused automated disease diagnosis assistant,2024,0.001475534378861124,2
W3166550823,A Comparative Study of Using Pre-trained Language Models for Toxic Comment Classification,2021,0.0014754957579565353,2
W4292938144,Overview of BioASQ 2022: The Tenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering,2022,0.0014750569604841022,2
W4292199287,A pre-trained BERT for Korean medical natural language processing,2022,0.0014748502387126597,2
W4385532435,SAKP: A Korean Sentiment Analysis Model via Knowledge Base and Prompt Tuning,2023,0.001474493009932687,2
W4387686358,Connecting AI: Merging Large Language Models and Knowledge Graph,2023,0.0014743145203473416,2
W4401242770,FinSoSent: Advancing Financial Market Sentiment Analysis through Pretrained Large Language Models,2024,0.0014737506451006693,2
W4399798455,Whither developmental psycholinguistics?,2024,0.0014734625251105473,2
W4402402588,A Dynamic Retrieval-Augmented Generation Framework for Border Inspection Legal Question Answering,2024,0.0014730484947115066,2
W3172750682,Pre-trained Language Model for Web-scale Retrieval in Baidu Search,2021,0.0014730048654563977,2
W4391044610,Commonsense Reasoning and Explainable Artificial Intelligence Using Large Language Models,2024,0.001472188652806013,2
W4309729081,An Overview of Knowledge Graph Reasoning: Key Technologies and Applications,2022,0.0014715915891583286,2
W4392951810,A Model Ensemble Approach with LLM for Chinese Text Classification,2024,0.0014706543771328619,2
W4391680357,Revisiting Bag of Words Document Representations for Efficient Ranking with Transformers,2024,0.0014704607560060143,2
W4287855173,Explaining Toxic Text via Knowledge Enhanced Text Generation,2022,0.0014653232128454853,2
W4407487400,Predicting learning performance using NLP: an exploratory study using two semantic textual similarity methods,2025,0.0014648361612370627,2
W4291012446,Identifying the Perceived Severity of Patient-Generated Telemedical Queries Regarding COVID: Developing and Evaluating a Transfer Learning–Based Solution,2022,0.001462168470954776,2
W4402567773,Biomedical knowledge graph-optimized prompt generation for large language models,2024,0.001460685395032655,2
W4389523927,Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA,2023,0.0014600399922743961,2
W4396723160,Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs,2024,0.0014584412142142772,2
W2964146920,Lingke: a Fine-grained Multi-turn Chatbot for Customer Service,2018,0.0014564677702985909,2
W4392484147,Large language model augmented exercise retrieval for personalized language learning,2024,0.00145237998039064,2
W4391133530,Performance of 4 Pre-Trained Sentence Transformer Models in the Semantic Query of a Systematic Review Dataset on Peri-Implantitis,2024,0.0014507806493934182,2
W4381713888,University Student Dropout Prediction Using Pretrained Language Models,2023,0.001450398021735632,2
W4402722849,SpikingMiniLM: energy-efficient spiking transformer for natural language understanding,2024,0.0014477099021711287,2
W2611099133,Neural Models for Information Retrieval,2017,0.0014476445206843703,2
W4406731238,Towards normalized clinical information extraction in Chinese radiology report with large language models,2025,0.001447203834501057,2
W4407558735,Automatically resolving conflicts between expert systems: An experimental approach using large language models and fuzzy cognitive maps from participatory modeling studies,2025,0.0014471135118066332,2
W4406174106,Leveraging Retrieval-Augmented Generation for Swahili Language Conversation Systems,2025,0.0014440514081226039,2
W4382541143,A shared linguistic space for transmitting our thoughts from brain to brain in natural conversations,2023,0.0014412499132091152,2
W4410443327,Aspect-Enhanced Prompting Method for Unsupervised Domain Adaptation in Aspect-Based Sentiment Analysis,2025,0.0014404514694222905,2
W4385372118,HOMOCHAR: A novel adversarial attack framework for exposing the vulnerability of text based neural sentiment classifiers,2023,0.0014380177853721237,2
W4206693003,A hybrid approach of Weighted Fine-Tuned BERT extraction with deep Siamese Bi – LSTM model for semantic text similarity identification,2022,0.001437859865647806,2
W4367047001,Learning Denoised and Interpretable Session Representation for Conversational Search,2023,0.0014370460701311736,2
W2963502184,Siamese CBOW: Optimizing Word Embeddings for Sentence Representations,2016,0.0014366069754048589,2
W4224315235,Topic Discovery via Latent Space Clustering of Pretrained Language Model Representations,2022,0.0014353166300589562,2
W2962782614,Text Processing Like Humans Do: Visually Attacking and Shielding,2019,0.0014327964401304968,2
W4286567174,“Note Bloat” impacts deep learning-based NLP models for clinical prediction tasks,2022,0.0014321328134105867,2
W4360850261,Enabling Early Health Care Intervention by Detecting Depression in Users of Web-Based Forums using Language Models: Longitudinal Analysis and Evaluation,2023,0.0014311226638505535,2
W4402919780,Automated annotation of scientific texts for ML-based keyphrase extraction and validation,2024,0.0014290669885976767,2
W4409651300,Event Detection and Analysis from Social Media Data Using N-gram and Distil-BERT Model,2025,0.0014276848127608088,2
W2964301649,Audio Adversarial Examples: Targeted Attacks on Speech-to-Text,2018,0.0014264764789703802,2
W4367322784,Finding Patient Zero and Tracking Narrative Changes in the Context of Online Disinformation Using Semantic Similarity Analysis,2023,0.001421744338252816,2
W4389518298,GYM at Qur’an QA 2023 Shared Task: Multi-Task Transfer Learning for Quranic Passage Retrieval and Question Answering with Large Language Models,2023,0.001420605598698463,2
W4408336406,How to Write Effective Prompts for Screening Biomedical Literature Using Large Language Models,2025,0.0014195892841592408,2
W4401940777,Integrating deep learning architectures for enhanced biomedical relation extraction: a pipeline approach,2024,0.001419514908603149,2
W2989622715,Semantic Textual Similarity with Siamese Neural Networks,2019,0.0014154059924126217,2
W4313476629,Extractive Explanations for Interpretable Text Ranking,2022,0.0014132843807244167,2
W2799051010,Learning Thematic Similarity Metric from Article Sections Using Triplet Networks,2018,0.001412806917834806,2
W4401555054,"Correctness Comparison of <scp>ChatGPT</scp>‐4, Gemini, Claude‐3, and Copilot for Spatial Tasks",2024,0.0014118228121029076,2
W4390489251,Investigating ChatGPT’s Potential to Assist in Requirements Elicitation Processes,2023,0.001410769400739204,2
W4378232119,Automated Classification for Open-Ended Questions with BERT,2023,0.0014096103790113299,2
W3214637114,The Future is not One-dimensional: Complex Event Schema Induction by Graph Modeling for Event Prediction,2021,0.0014081767496104416,2
W4379534564,RISC: Generating Realistic Synthetic Bilingual Insurance Contract,2023,0.0014080722846587371,2
W2986213397,Commonsense Properties from Query Logs and Question Answering Forums,2019,0.00140781036167017,2
W4401626333,Document-Level Event Extraction with Definition-Driven ICL,2024,0.0014052404930361244,2
W3169432135,Characterizing English Variation across Social Media Communities with BERT,2021,0.0014049773919853157,2
W4401907507,Answering Spatial Commonsense Questions Based on Chain-of-Thought Reasoning with Adaptive Complexity,2024,0.0014040250639062966,2
W4391544525,ChIP-GPT: a managed large language model for robust data extraction from biomedical database records,2024,0.0014008892519900365,2
W2280395961,Sentence Similarity Learning by Lexical Decomposition and Composition,2016,0.0013988670249212608,2
W4385572952,Interpreting Language Models with Contrastive Explanations,2022,0.001398838189816089,2
W4400338820,Implicit and explicit commonsense for multi-sentence video captioning,2024,0.0013988214294827074,2
W4408583140,MARRO: multi-headed attention for rhetorical role labeling in legal documents,2025,0.0013987733530025645,2
W4409671337,TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision,2025,0.001398654729399475,2
W2963740900,Unsupervised sentence representations as word information series: Revisiting TF–IDF,2019,0.0013979343137510696,2
W4404021255,Biomedical Information Retrieval with Positive-Unlabeled Learning and Knowledge Graphs,2024,0.0013973800351408854,2
W4397010262,Few-shot biomedical relation extraction using data augmentation and domain information,2024,0.001396822823944372,2
W4393200435,Data Augmentation and Large Language Model for Legal Case Retrieval and Entailment,2024,0.0013954145916793654,2
W4362696539,Strong Prediction: Language Model Surprisal Explains Multiple N400 Effects,2023,0.0013942392342479366,2
W4327644584,Learning Query-Space Document Representations for High-Recall Retrieval,2023,0.0013940660382843221,2
W4392902639,A Soft Contrastive Learning-Based Prompt Model for Few-Shot Sentiment Analysis,2024,0.001393042169292022,2
W2962940365,Interpretable Charge Predictions for Criminal Cases: Learning to Generate Court Views from Fact Descriptions,2018,0.0013925937331712589,2
W4406846967,SPIRIT: Structural Entropy Guided Prefix Tuning for Hierarchical Text Classification,2025,0.0013924648158142728,2
W4409332187,"Multimodal Data Fusion for Tabular and Textual Data: Zero-Shot, Few-Shot, and Fine-Tuning of Generative Pre-Trained Transformer Models",2025,0.0013920627276397986,2
W4396913708,PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking,2024,0.0013910937224280192,2
W2952831501,Putting Words in Context: LSTM Language Models and Lexical Ambiguity,2019,0.001391030467706786,2
W4389519321,Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering,2023,0.0013907011706607054,2
W4388551933,Unsupervised Contrastive Learning of Sentence Embeddings Through Optimized Sample Construction and Knowledge Distillation,2023,0.001386813685456453,2
W2518510348,Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM,2016,0.0013836123936994198,2
W4408735899,Using similarity network analysis to improve text similarity calculations,2025,0.001383169219938113,2
W4310529830,Can language models automate data wrangling?,2022,0.0013831269680106978,2
W4403646833,Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications,2024,0.0013803574354527368,2
W2962817854,Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers,2018,0.0013801646146623142,2
W2963001247,Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext,2017,0.0013791264107308274,2
W2890801081,DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning,2018,0.0013750680707757448,2
W2969219365,The compositionality of neural networks: integrating symbolism and connectionism.,2019,0.0013742136538476568,2
W4406550498,On the suitability of hugging face hub for empirical studies,2025,0.0013736934968108883,2
W4389524268,"Fine-tuned LLMs Know More, Hallucinate Less with Few-Shot Sequence-to-Sequence Semantic Parsing over Wikidata",2023,0.001371165830219761,2
W4401414182,FLTRNN: Faithful Long-Horizon Task Planning for Robotics with Large Language Models,2024,0.0013702521061113194,2
W4403563666,E-code: Mastering efficient code generation through pretrained models and expert encoder group,2024,0.0013701288356850437,2
W2760327630,Interactive Visualization and Manipulation of Attention-based Neural Machine Translation,2017,0.0013698763384732636,2
W3101652466,PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,2020,0.0013689088909091798,2
W2995409942,LAMOL: LAnguage MOdeling for Lifelong Language Learning,2019,0.0013687315092596373,2
W4396498719,Large language models leverage external knowledge to extend clinical insight beyond language boundaries,2024,0.0013678305157383101,2
W4410202920,Ontology matching with Large Language Models and prioritized depth-first search,2025,0.0013677685414649663,2
W4409211441,Debate divides: Argument relation-based contrastive opinion summarization via multi-task learning for online discussions,2025,0.0013674539129926563,2
W2972896975,Hierarchical Representation in Neural Language Models: Suppression and Recovery of Expectations,2019,0.0013649449163239931,2
W2517782820,Improved Representation Learning for Question Answer Matching,2016,0.0013646556698207089,2
W4313071293,Evidence-Based Document-Level Event Factuality Identification,2022,0.001364551298098967,2
W4393154423,SAM-PARSER: Fine-Tuning SAM Efficiently by Parameter Space Reconstruction,2024,0.0013642229434610252,2
W4392167104,Automated Scoring of Translations with BERT Models: Chinese and English Language Case Study,2024,0.0013641387381252198,2
W4407852890,Expert-level policy style measurement via knowledge distillation with large language model collaboration,2025,0.0013635344402404056,2
W4396833205,ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality,2024,0.001361354252231424,2
W3090395639,A Survey of the State of Explainable AI for Natural Language Processing,2020,0.001357117366251651,2
W4394895012,Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction (Preprint),2024,0.001354179775581306,2
W4392692817,LLM–Assisted Data Augmentation for Chinese Dialogue–Level Dependency Parsing,2024,0.0013530711611270505,2
W2951976932,Tracking the World State with Recurrent Entity Networks,2016,0.0013527409318134997,2
W4398250357,Predicting the next sentence (not word) in large language models: What model-brain alignment tells us about discourse comprehension,2024,0.0013521588510692836,2
W4409183375,Hierarchical Skip Decoding for Efficient Autoregressive Language Model,2025,0.001351836807754759,2
W4394828474,"AHAM: Adapt, Help, Ask, Model Harvesting LLMs for Literature Mining",2024,0.0013453169339763824,2
W4404295456,TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation,2024,0.001345211231609031,2
W4372260073,Bert is Robust! A Case Against Word Substitution-Based Adversarial Attacks,2023,0.0013439367427638986,2
W4397001891,Acquiring and modeling abstract commonsense knowledge via conceptualization,2024,0.0013430532533975786,2
W4389523987,Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation,2023,0.0013429681319042648,2
W4389040036,An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing,2023,0.0013399927114927425,2
W4410041388,Harnessing Pre-trained Language Models for Efficient Move Recognition in Biomedical Abstracts,2025,0.0013390619532921747,2
W4388214929,Harnessing GPT-3.5-Turbo for Rhetorical Role Prediction in Legal Cases,2023,0.001338438144608947,2
W2606089314,Neural Network Methods for Natural Language Processing,2017,0.0013360908408098363,2
W4407345433,Topic modeling based BERT &amp; SBERT transformer pretrained language modeling: A survey (2019-2023),2025,0.0013355566642548318,2
W3154790171,Measuring and Improving Faithfulness of Attention in Neural Machine Translation,2021,0.0013339480793029472,2
W4410304050,Similarity Measurement Model for Standard Clauses Based on RAG,2025,0.0013330084191243105,2
W2952208026,The emergence of number and syntax units in LSTM language models,2019,0.0013329453973460008,2
W4396982467,Advancing Multimodal Diagnostics: Integrating Industrial Textual Data and Domain Knowledge with Large Language Models,2024,0.0013309567357339578,2
W4394894051,Preventing the Immense Increase in the Life-Cycle Energy and Carbon Footprints of LLM-Powered Intelligent Chatbots,2024,0.0013272562985438146,2
W4409622428,Integrating domain-specific knowledge and fine-tuned general-purpose large language models for question-answering in construction engineering management,2025,0.0013252597261389928,2
W3104958909,Causal Inference of Script Knowledge,2020,0.00132510363561884,2
W4395025756,Indian Annual Report Assessment Using Large Language Models,2024,0.0013210677418878107,2
W4293140814,A Review of Knowledge Graph Completion,2022,0.0013206975878060538,2
W2996919866,Stepwise Reasoning for Multi-Relation Question Answering over Knowledge Graph with Weak Supervision,2020,0.0013206785429459252,2
W3154773080,GPT-2’s activations predict the degree of semantic comprehension in the human brain,2021,0.0013201403988064276,2
W4399280731,Fine-tuning large language models for rare disease concept normalization,2024,0.0013199081033474292,2
W4363625523,A-maze of Natural Stories: Comprehension and surprisal in the Maze task,2023,0.001318856153271351,2
W4409468566,Text-augmented long-term relation dependency learning for knowledge graph representation,2025,0.0013178228998018847,2
W4281482237,"How to Approach Ambiguous Queries in Conversational Search: A Survey of Techniques, Approaches, Tools, and Challenges",2022,0.001315144941375292,2
W4385573380,One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks,2022,0.001313188413659197,2
W2951105272,Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs,2019,0.001313041506061819,2
W4396808727,VisionVerse: Dynamic Video Question Answering Through Retrieval-Augmented Generation,2024,0.001312867706810123,2
W3016169217,Neural Tree Indexers for Text Understanding,2016,0.0013103120381115034,2
W2986650838,Team SVMrank: Leveraging Feature-rich Support Vector Machines for Ranking Explanations to Elementary Science Questions,2019,0.001309032975840439,2
W4393113494,Neural Data Augmentation for Legal Overruling Task: Small Deep Learning Models vs. Large Language Models,2024,0.00130626750636214,2
W4409651000,FZeroTC: fully zero-shot text classification for simultaneously discovering and labeling unseen classes,2025,0.0013015162504441875,2
W4395112660,SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification,2024,0.0013006468988812092,2
W3205803342,ZeRO-infinity,2021,0.001297823998668946,2
W4327499388,"Legal IR and NLP: The History, Challenges, and State-of-the-Art",2023,0.0012927945514669577,2
W2460937040,Adversarial examples in the physical world,2016,0.0012905590529378398,2
W4323354733,AI chatbots not yet ready for clinical use,2023,0.0012888468601513932,2
W4407088958,Integration of biomedical concepts for enhanced medical literature retrieval,2025,0.0012874919615230005,2
W4312767993,Benchmarking library recognition in tweets,2022,0.0012872209996296579,2
W4409159698,mFollowIR: A Multilingual Benchmark for Instruction Following in Retrieval,2025,0.0012866952958736453,2
W4385570718,The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code,2023,0.0012857519457418207,2
W2963838657,Learning to Rank Question-Answer Pairs Using Hierarchical Recurrent Encoder with Latent Topic Clustering,2018,0.0012812527413307204,2
W4409902375,Enhancing food safety review classification with large language models and label embedding,2025,0.001278882118964208,2
W4403257570,Learning to match patients to clinical trials using large language models,2024,0.0012781477795529854,2
W4401042387,Assessing Logical Puzzle Solving in Large Language Models: Insights from a Minesweeper Case Study,2024,0.001277859756316427,2
W4409166952,LSTM-Based Selective Dense Text Retrieval Guided by Sparse Lexical Retrieval,2025,0.0012772926654114499,2
W4389524159,Large Language Models are Better Reasoners with Self-Verification,2023,0.0012720238966890298,2
W4408257283,A Comprehensive Ontology Knowledge Evaluation System for Large Language Models,2025,0.0012718669839185918,2
W4409581847,Arch-Eval benchmark for assessing chinese architectural domain knowledge in large language models,2025,0.0012691706993218047,2
W4400928211,MedT2T: An adaptive pointer constrain generating method for a new medical text-to-table task,2024,0.0012689194702528049,2
W4393147146,Can Large Language Models Understand Real-World Complex Instructions?,2024,0.0012676732493064075,2
W4391108544,A novel technique using graph neural networks and relevance scoring to improve the performance of knowledge graph-based question answering systems,2024,0.0012674125510539261,2
W4327644099,CoSPLADE: Contextualizing SPLADE for Conversational Information Retrieval,2023,0.001265187839563615,2
W4318464905,Transfer learning for the efficient detection of COVID-19 from smartphone audio data,2023,0.001264271504547973,2
W4399730482,A criteria-based classification model using augmentation and contrastive learning for analyzing imbalanced statement data,2024,0.0012633538549157392,2
W2913293259,An Efficient Framework for Sentence Similarity Modeling,2019,0.0012632108690715255,2
W4409030971,Making sense of transformer success,2025,0.0012607845318074644,2
W2995359496,Deep Learning for Symbolic Mathematics,2019,0.0012596659688232502,2
W4408257260,Manu-Eval: A Chinese Language Understanding Benchmark for Manufacturing Industry,2025,0.0012578188426087642,2
W4409826996,Unravelling the semantic mysteries of transformers layer by layer,2025,0.0012569155708190554,2
W4392909818,Sparsely Shared Lora on Whisper for Child Speech Recognition,2024,0.0012559675290281737,2
W3099236585,Distributional Semantics and Linguistic Theory,2019,0.0012526824781132699,2
W4388666116,Evaluating Large Language Models in Relationship Extraction from Unstructured Data: Empirical Study from Holocaust Testimonies,2023,0.001250181720477405,2
W4394579747,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study,2024,0.001248967532337599,2
W3015646487,MedLinker: Medical Entity Linking with Neural Representations and Dictionary Matching,2020,0.0012480618168233528,2
W4385363516,Large language models as models of human cognition,2023,0.001244341621625709,2
W4386424132,Prompting meaning: a hermeneutic approach to optimising prompt engineering with ChatGPT,2023,0.0012417147202729683,2
W4284685693,Curriculum Contrastive Context Denoising for Few-shot Conversational Dense Retrieval,2022,0.001241465902234129,2
W4365148488,AI chatbots not yet ready for clinical use,2023,0.0012392227159693035,2
W4400222836,KnowledgeNavigator: leveraging large language models for enhanced reasoning over knowledge graph,2024,0.0012365419360342318,2
W4396698999,"Benchmarking of Commercial Large Language Models: ChatGPT, Mistral, and Llama",2024,0.001235219331136963,2
W4409376916,Topic Words‐Based Multilingual Hateful Linguistic Resources Construction for Developing Multilingual Hateful Content Detection Model Using Deep Learning Technique,2025,0.0012350100507075618,2
W4389523998,Token Prediction as Implicit Classification to Identify LLM-Generated Text,2023,0.001233712381636551,2
W4407855931,Osteosarcoma KGQA system: deep learning-based knowledge graph and large language model fusion,2025,0.0012332670621977352,2
W4397029679,A Language Model Based Framework for New Concept Placement in Ontologies,2024,0.0012330506235876365,2
W2787752464,Model compression via distillation and quantization,2018,0.0012329673509122905,2
W4406810072,Synthetically generated text for supervised text analysis,2025,0.0012328632621376769,2
W4383481224,Modeling Structure‐Building in the Brain With CCG Parsing and Large Language Models,2023,0.0012325834706343295,2
W4378472501,Constructing a disease database and using natural language processing to capture and standardize free text clinical information,2023,0.0012314597169887486,2
W4396721893,nach0: multimodal natural and chemical languages foundation model,2024,0.0012314249130789016,2
W4404823262,Streamlining Attention for Text Classification: Sequence Length Reduction with Pooling Attention,2024,0.0012292267386059782,2
W3128128460,Cone-KG: A Semantic Knowledge Graph with News Content and Social Context for Studying Covid-19 News Articles on Social Media,2020,0.0012288298429799372,2
W3156316299,Development and evaluation of novel ophthalmology domain-specific neural word embeddings to predict visual prognosis,2021,0.0012285310769624093,2
W3022131108,Compositionality decomposed: how do neural networks generalise?,2019,0.0012272336088746528,2
W4400971828,"The Effects of the Training Sample Size, Ground Truth Reliability, and NLP Method on Language-Based Automatic Interview Scores’ Psychometric Properties",2024,0.00122584747722321,2
W4408559683,Towards Building Urdu Language Document Retrieval Framework,2025,0.0012236390191511352,2
W4223546521,Generative Biomedical Entity Linking via Knowledge Base-Guided Pre-training and Synonyms-Aware Fine-tuning,2022,0.001223024650022344,2
W4386932193,Fake Review Detection via Heterogeneous Graph Attention Network,2023,0.0012202423772413663,2
W4408224376,Scaling language model size yields diminishing returns for single-message political persuasion,2025,0.0012192687328592437,2
W4388144292,SORBET: A Siamese Network for Ontology Embeddings Using a Distance-Based Regression Loss and BERT,2023,0.0012191434893256295,2
W4394810471,Natural language processing (NLP) to facilitate abstract review in medical research: the application of BioBERT to exploring the 20-year use of NLP in medical research,2024,0.0012157490824054784,2
W4324142688,"Creation, Analysis and Evaluation of AnnoMI, a Dataset of Expert-Annotated Counselling Dialogues",2023,0.0012122021218596646,2
W2963041663,Open-World Knowledge Graph Completion,2018,0.001211682978089176,2
W4386155665,Marie and BERT─A Knowledge Graph Embedding Based Question Answering System for Chemistry,2023,0.0012116498452135677,2
W4409815088,Legal judgment prediction via legal knowledge extraction and fusion,2025,0.0012109471776986253,2
W3147509388,"The CLEF-2021 CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News",2021,0.0012101975956541205,2
W4313563595,Data Augmentation for Improving Emotion Recognition in Software Engineering Communication,2022,0.001210197400077656,2
W4376619178,Investigating the Extent to which Distributional Semantic Models Capture a Broad Range of Semantic Relations,2023,0.0012022904297608855,2
W4407991112,BERT Mutation: Deep Transformer Model for Masked Uniform Mutation in Genetic Programming,2025,0.001199802754383641,2
W4407959012,Toward a Large Language Model-Driven Medical Knowledge Retrieval and QA System: Framework Design and Evaluation,2025,0.0011969354639821093,2
W4327928482,Text-Defend: Detecting Adversarial Examples using Local Outlier Factor,2023,0.0011963137441892038,2
W4410332729,Overcoming Data Shortage in Critical Domains With Data Augmentation for Natural Language Software Requirements,2025,0.0011961985857989507,2
W4360765003,Zero-Shot Text Matching for Automated Auditing using Sentence Transformers,2022,0.001196128421355628,2
W2808281579,Hermitian Co-Attention Networks for Text Matching in Asymmetrical Domains,2018,0.0011960536706688283,2
W4403942912,W2CL: A Multi-task Learning Approach to Improve Domain-Specific Sentence Classification Through Word Classification and Contrastive Learning,2024,0.001189133282123237,2
W3099729825,Imitation Attacks and Defenses for Black-box Machine Translation Systems,2020,0.0011886053380326988,2
W3100458477,HyperText: Endowing FastText with Hyperbolic Geometry,2020,0.0011871284010048526,2
W4393277515,Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs,2024,0.0011868758675093287,2
W4410533142,Diacritical Manipulations as Adversarial Attacks in Arabic NLP Systems,2025,0.0011865644566726955,2
W3113763975,SemEval-2020 Task 11: Detection of Propaganda Techniques in News Articles,2020,0.0011839345810655073,2
W4406026208,Explainable Security Requirements Classification Through Transformer Models,2025,0.0011827234255353037,2
W4391559941,Fine-tuning ChatGPT for automatic scoring,2024,0.001182595363822658,2
W4408417706,A definition and taxonomy of digital twins: case studies with machine learning and scientific applications,2025,0.0011825433330416428,2
W4408999580,Named entity recognition for construction documents based on fine-tuning of large language models with low-quality datasets,2025,0.0011822343194852199,2
W4407605136,"The ""Podcast"" ECoG dataset for modeling neural activity during natural language comprehension",2025,0.0011755874099333072,2
W4410345129,U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models,2025,0.0011746371794801624,2
W4398265013,CoRTEx: contrastive learning for representing terms via explanations with applications on constructing biomedical knowledge graphs,2024,0.00117127411879854,2
W4385562652,Contrastive Learning of Stress-specific Word Embedding for Social Media based Stress Detection,2023,0.0011707111987024237,2
W4396767636,From explainable to interpretable deep learning for natural language processing in healthcare: How far from reality?,2024,0.0011684127601470746,2
W4393166638,"Quality, Accuracy, and Bias in ChatGPT-Based Summarization of Medical Abstracts",2024,0.001168157532057833,2
W3106378800,LibKGE - A knowledge graph embedding library for reproducible research,2020,0.0011679561703002949,2
W4399205954,ArguSense: Argument-Centric Analysis of Online Discourse,2024,0.0011661641408948536,2
W4386968362,Biomedical generative pre-trained based transformer language model for age-related disease target discovery,2023,0.0011645706337985284,2
W2889234142,One-Shot Relational Learning for Knowledge Graphs,2018,0.001164054074108891,2
W2769216919,Modelling Domain Relationships for Transfer Learning on Retrieval-based Question Answering Systems in E-commerce,2018,0.0011638778622931074,2
W4295942986,The AI‐IP: Minimizing the guesswork of personality scale item development through artificial intelligence,2022,0.001162231992724076,2
W4382279906,Deep speech-to-text models capture the neural basis of spontaneous speech in everyday conversations,2023,0.0011622086800650566,2
W4318559756,Improving biomedical named entity recognition through transfer learning and asymmetric tri-training,2023,0.0011616607359667508,2
W4361008252,Accurate and Reliable Classification of Unstructured Reports on Their Diagnostic Goal Using BERT Models,2023,0.0011593755684494828,2
W4409386825,Detecting emergencies in patient portal messages using large language models and knowledge graph-based retrieval-augmented generation,2025,0.0011592841247330416,2
W3012568767,Enhanced-RCNN: An Efficient Method for Learning Sentence Similarity,2020,0.0011582048945883966,2
W4318983406,Building a knowledge graph to enable precision medicine,2023,0.001158160457104361,2
W4315781034,Natural Language Processing Applications for Computer-Aided Diagnosis in Oncology,2023,0.0011580361338399845,2
W4317423646,Sentence embedding and fine-tuning to automatically identify duplicate bugs,2023,0.0011576514822926327,2
W4407604969,Linguistic coupling between neural systems for speech production and comprehension during real-time dyadic conversations,2025,0.0011551603889133329,2
W4408250731,TCM-KLLaMA: Intelligent generation model for Traditional Chinese Medicine Prescriptions based on knowledge graph and large language model,2025,0.0011548924567923583,2
W3193158708,MT-clinical BERT: scaling clinical information extraction with multitask learning,2021,0.001153305139970217,2
W4391404146,Sentiment Analysis in Portuguese Restaurant Reviews: Application of Transformer Models in Edge Computing,2024,0.0011490464016128674,2
W4409277341,A Novel Retrieval-Augmented Generation Framework Using Large Language Models for Lyrics and Song Composition,2025,0.001148419336305254,2
W4390825188,Simple knowledge graph completion model based on PU learning and prompt learning,2024,0.0011465308339773071,2
W3122172846,Interpreting Graph Neural Networks for NLP With Differentiable Edge Masking,2021,0.0011464405374938993,2
W4389520259,"HuatuoGPT, Towards Taming Language Model to Be a Doctor",2023,0.0011448875819037696,2
W3012932209,Leveraging Sentiment Distributions to Distinguish Figurative From Literal Health Reports on Twitter,2020,0.001143624981390915,2
W4200594764,Detecting computer-generated disinformation,2021,0.00113870585280929,2
W4408204750,Towards semantic versioning of open pre-trained language model releases on hugging face,2025,0.0011387034666527416,2
W3100239257,Message Passing for Hyper-Relational Knowledge Graphs,2020,0.0011385377929279223,2
W4402761492,Learning to Score: A Coding System for Constructed Response Items via Interactive Clustering,2024,0.0011359421609605641,2
W4210744365,Improving Crisis Events Detection Using DistilBERT with Hunger Games Search Algorithm,2022,0.001133958641689411,2
W2806198715,SemEval 2018 Task 2: Multilingual Emoji Prediction,2018,0.0011339393881371687,2
W4386165948,Improving short text classification with augmented data using GPT-3,2023,0.0011335718454076394,2
W4385572060,Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods,2023,0.0011287100454028451,2
W4403825982,PRISM: Patient Records Interpretation for Semantic clinical trial Matching system using large language models,2024,0.00112870814068631,2
W3213472637,Generative Pre-Trained Transformer for Design Concept Generation: An Exploration,2022,0.0011285033020504228,2
W2971142670,Adapting Meta Knowledge Graph Information for Multi-Hop Reasoning over Few-Shot Relations,2019,0.0011282491422871198,2
W3085380432,Captum: A unified and generic model interpretability library for PyTorch,2020,0.0011242895939877673,2
W3035103838,Mining Implicit Relevance Feedback from User Behavior for Web Question Answering,2020,0.001123586746574888,2
W4409837816,"Vox Populi, Vox AI? Using Large Language Models to Estimate German Vote Choice",2025,0.00112204536839563,2
W4394999741,Scaling Implicit Bias Analysis across Transformer-Based Language Models through Embedding Association Test and Prompt Engineering,2024,0.0011190828279897046,2
W4392377312,The Promises and Perils of Foundation Models in Dermatology,2024,0.0011185556258829041,2
W4312125493,Clinical Application of Detecting COVID-19 Risks: A Natural Language Processing Approach,2022,0.001118255817404272,2
W4398173700,Hybrid Retrieval-Augmented Generation Approach for LLMs Query Response Enhancement,2024,0.0011174308104729247,2
W4390590960,Intelligent Practices of Large Language Models in Digital Government Services,2024,0.0011164542918457678,2
W2949399644,Compound Probabilistic Context-Free Grammars for Grammar Induction,2019,0.0011143573629516222,2
W4387846279,CLosER: Conversational Legal Longformer with Expertise-Aware Passage Response Ranker for Long Contexts,2023,0.0011125166267622057,2
W4407830444,Behind the mask: Random and selective masking in transformer models applied to specialized social science texts,2025,0.0011093644876693618,2
W4387824566,Assessing student errors in experimentation using artificial intelligence and large language models: A comparative study with human raters,2023,0.0011079640876388958,2
W4409100492,A Modified Word Saliency-Based Adversarial Attack on Text Classification Models,2025,0.0011067862972871626,2
W4392119501,BioEmoDetector: A flexible platform for detecting emotions from health narratives,2024,0.0011058359842622252,2
W2963900105,Attention-Based Convolutional Neural Network for Machine Comprehension,2016,0.0011036046527015684,2
W4375869145,Discriminative Speaker Representation Via Contrastive Learning with Class-Aware Attention in Angular Space,2023,0.0011017757149390933,2
W4410089370,TourRank: Utilizing Large Language Models for Documents Ranking with a Tournament-Inspired Strategy,2025,0.0011017391614425332,2
W4229335055,A Deep Language Model for Symptom Extraction From Clinical Text and its Application to Extract COVID-19 Symptoms From Social Media,2021,0.0010999546553324905,2
W3176634681,Unleash GPT-2 Power for Event Detection,2021,0.0010994844588230034,2
W2951576127,Incorporating Priors with Feature Attribution on Text Classification,2019,0.0010993441351704687,2
W4288051127,A knowledge inference model for question answering on an incomplete knowledge graph,2022,0.0010983818445103868,2
W2515385951,Pruning Filters for Efficient ConvNets,2016,0.001098082683132646,2
W2953150860,ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs,2015,0.0010970372488385237,2
W4406870927,LLMs4OM: Matching Ontologies with Large Language Models,2025,0.001096656315129854,2
W4387847108,XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters,2023,0.0010957021879273588,2
W4394743995,On Extracting Specialized Code Abilities from Large Language Models: A Feasibility Study,2024,0.0010951498692723966,2
W4407191181,Large Language Models for Electronic Health Record De-Identification in English and German,2025,0.0010928162231237388,2
W2963059228,On the Practical Computational Power of Finite Precision RNNs for Language Recognition,2018,0.0010925223362722918,2
W4288048729,Multi-granularity interaction model based on pinyins and radicals for Chinese semantic matching,2022,0.0010910949102560675,2
W4379280737,NASTyLinker: NIL-Aware Scalable Transformer-Based Entity Linker,2023,0.001089045023302918,2
W4403768141,Multi-Hop Arabic LLM Reasoning in Complex QA,2024,0.001086702576087637,2
W4405989740,Success and failure of compositional generalisation in distributional models of language,2025,0.001085373172365334,2
W3124523525,Deep Learning-Based Natural Language Processing for Screening Psychiatric Patients,2021,0.00108308880195232,2
W2946809241,A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric Interpretation,2019,0.001082662840334708,2
W4394782456,PMC-LLaMA: toward building open-source language models for medicine,2024,0.0010818626397768366,2
W4400477691,Advancing multimodal diagnostics: Integrating industrial textual data and domain knowledge with large language models,2024,0.0010803967595733315,2
W4394975332,Distilling large language models for matching patients to clinical trials,2024,0.0010794289061935134,2
W4386647425,ProVe: A pipeline for automated provenance verification of knowledge graphs against textual sources,2023,0.0010790606660844705,2
W4386002582,Can ChatGPT provide intelligent diagnoses? A comparative study between predictive models and ChatGPT to define a new medical diagnostic bot,2023,0.0010780148751930742,2
W4409451928,Benchmarking domain-specific pretrained language models to identify the best model for methodological rigor in clinical studies,2025,0.001077537839649535,2
W2963157366,Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval,2018,0.0010773873413612483,2
W4406459274,LLM Augmentations to support Analytical Reasoning over Multiple Documents,2024,0.0010772178708840876,2
W4410515601,Improving Knowledge Tracing through Multi-Source Scaling with Decoder-Only Transformers,2025,0.001077098136647748,2
W4407241465,Out-of-distribution generalization via composition: A lens through induction heads in Transformers,2025,0.0010731418822364563,2
W4387848863,NOVO: Learnable and Interpretable Document Identifiers for Model-Based IR,2023,0.0010730094249231813,2
W4382052727,Bypassing Deep Learning based Sentiment Analysis from Business Reviews,2023,0.0010729326644239876,2
W4282967944,Prediction as a basis for skilled reading: insights from modern language models,2022,0.0010728328210780245,2
W4397003497,Clinical Text Datasets for Medical Artificial Intelligence and Large Language Models — A Systematic Review,2024,0.0010724979179563923,2
W4393161039,SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research,2024,0.0010717126118231432,2
W4403938784,Integrating Multi-view Analysis: Multi-view Mixture-of-Expert for Textual Personality Detection,2024,0.0010713097560707322,2
W4284689799,A Non-Factoid Question-Answering Taxonomy,2022,0.001070127664311824,2
W4200136492,A contextual multi-task neural approach to medication and adverse events identification from clinical text,2021,0.001069241760038489,2
W4408705811,Transformers and large language models are efficient feature extractors for electronic health record studies,2025,0.001065405958408343,2
W4404193844,Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification,2024,0.0010645427015534364,2
W2945655519,,2019,0.0010612352300454636,2
W4385574240,Are Neural Topic Models Broken?,2022,0.0010607905408041756,2
W4408257726,Adaptive Factual Decoding for Hallucination Mitigation with Part-of-Speech Based Critics,2025,0.0010584318188679041,2
W2904996081,Towards Sentence-Level Brain Decoding with Distributed Representations,2019,0.0010571457852486916,2
W2952113915,Bilateral Multi-Perspective Matching for Natural Language Sentences,2017,0.0010556412811452058,2
W4409245884,Complex knowledge base question answering with difficulty-aware active data augmentation,2025,0.0010541467658818319,2
W2917049430,Transformer-Based Neural Network for Answer Selection in Question Answering,2019,0.0010512475238031557,2
W4386710827,<scp>MapIntel</scp>: A visual analytics platform for competitive intelligence,2023,0.0010507601052223046,2
W4409688433,Applications and Modeling of Keystroke Logs in Writing Assessments,2025,0.0010490440508357107,2
W4388406741,Automating Intended Target Identification for Paraphasias in Discourse Using a Large Language Model,2023,0.0010472640330580478,2
W4392546128,Systematic evaluation of common natural language processing techniques to codify clinical notes,2024,0.001045699781568469,2
W3168767448,Why Attentions May Not Be Interpretable?,2021,0.0010448105252087209,2
W2740205663,Chinese Medical Question Answer Matching Using End-to-End Character-Level Multi-Scale CNNs,2017,0.0010421718506457931,2
W4410368723,Dissociable frequency effects attenuate as large language model surprisal predictors improve,2025,0.001041786810625684,2
W4409166550,Large Language Models Are Human-Like Annotators,2025,0.001038579960685873,2
W3189827190,Knowledge-aware Zero-Shot Learning: Survey and Perspective,2021,0.0010383662601564701,2
W3038046627,BadNL: Backdoor Attacks Against NLP Models,2020,0.0010376862364915864,2
W4402227510,Multi-level Shared Knowledge Guided Learning for Knowledge Graph Completion,2024,0.001037520253053901,2
W4399485854,Evaluating Generative Language Models with Prompt Engineering for Categorizing User Stories to its Sector Domains,2024,0.0010342159382567527,2
W4389668679,Many but not all deep neural network audio models capture brain responses and exhibit correspondence between model stages and brain regions,2023,0.0010334863398606853,2
W3147450229,Question Answering Systems: A Systematic Literature Review,2021,0.0010317853610606637,2
W4379469539,Using Social Media to Help Understand Patient-Reported Health Outcomes of Post–COVID-19 Condition: Natural Language Processing Approach,2023,0.0010288973068475775,2
W4366779175,Identifying stroke-related quantified evidence from electronic health records in real-world studies,2023,0.0010265635706226785,2
W4392612897,Empowering digital twins with large language models for global temporal feature learning,2024,0.0010236216136397282,2
W4409438762,DEANE: Context-Aware Dual-Craft Graph Contrastive Learning for Enhanced Extractive Question Answering,2025,0.0010224528377358183,2
W4406371523,COTR: Efficient Job Task Recognition for Occupational Information Systems with Class-Incremental Learning,2025,0.0010202009886298036,2
W4389519053,Multilingual estimation of political-party positioning: From label aggregation to long-input Transformers,2023,0.001019754587977985,2
W3184259550,Event-Centric Natural Language Processing,2021,0.0010192389102434663,2
W4400127166,Improving Text Classification with Large Language Model-Based Data Augmentation,2024,0.001019113710458996,2
W4386071594,Learning Federated Visual Prompt in Null Space for MRI Reconstruction,2023,0.001016779962612812,2
W4406640860,Human-interpretable clustering of short text using large language models,2025,0.001016456740082233,2
W4387645476,Fine-Tuning Large Enterprise Language Models via Ontological Reasoning,2023,0.00101588593209965,2
W4288679905,A novel locality-sensitive hashing relational graph matching network for semantic textual similarity measurement,2022,0.0010158043006487738,2
W4400485910,Optimizing Tourism Accommodation Offers by Integrating Language Models and Knowledge Graph Technologies,2024,0.0010152998702044889,2
W4406016552,Brain-model neural similarity reveals abstractive summarization performance,2025,0.0010136739271959453,2
W3034397670,A Reinforced Generation of Adversarial Examples for Neural Machine Translation,2020,0.0010136119227334593,2
W4311762778,Predicting medical specialty from text based on a domain-specific pre-trained BERT,2022,0.0010131817059870994,2
W4385565358,Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction,2023,0.001012730233422355,2
W3103252405,Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation,2020,0.0010116730662606667,2
W3157651102,Comparing Pre-trained and Feature-Based Models for Prediction of Alzheimer's Disease Based on Speech,2021,0.0010108005538810398,2
W3035597164,An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models,2020,0.00101078102051057,2
W4403577386,Multimodal Misinformation Detection using Large Vision-Language Models,2024,0.0010101096096106477,2
W4386938228,A Transformer-Based Framework for Biomedical Information Retrieval Systems,2023,0.0010076803810639041,2
W4388751616,Fictionalism about Chatbots,2023,0.0010065059965992137,2
W4400724805,"The potential and pitfalls of using a large language model such as ChatGPT, GPT-4, or LLaMA as a clinical assistant",2024,0.0010047024864739374,2
W4407866557,Adaptive data augmentation for salient sentence identification in Indian judicial decisions,2025,0.0010031152080880001,2
W4408573322,Benchmarking Interpretability in Healthcare Using Pattern Discovery and Disentanglement,2025,0.0010024404624455847,2
W4392266005,AskIt: Unified Programming Interface for Programming with Large Language Models,2024,0.0010007748064690314,2
W4406596702,Clinical entity augmented retrieval for clinical information extraction,2025,0.0009995198367502044,2
W4408984182,Automated Knowledge Extraction from IS Research Articles Combining Sentence Classification and Ontological Annotation,2025,0.000999215659198985,2
W4408345962,LLM-IE: a python package for biomedical generative information extraction with large language models,2025,0.0009979663386982841,2
W4391132527,Can large language models help augment English psycholinguistic datasets?,2024,0.0009975049779776027,2
W3137216496,Studying Catastrophic Forgetting in Neural Ranking Models,2021,0.0009963748381522127,2
W4409670780,Paths-over-Graph: Knowledge Graph Empowered Large Language Model Reasoning,2025,0.0009958140096625917,2
W2798966449,Generating Natural Language Adversarial Examples,2018,0.0009955338154841536,2
W3166464178,Single‐Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty,2021,0.000993981480114769,2
W4379197269,An analysis of entity normalization evaluation biases in specialized domains,2023,0.0009938062997002855,2
W4399879615,Enhancing Natural Language Query to SQL Query Generation Through Classification-Based Table Selection,2024,0.0009917076615177883,2
W4396654496,Oversampling effect in pretraining for bidirectional encoder representations from transformers (BERT) to localize medical BERT and enhance biomedical BERT,2024,0.0009913579793799257,2
W4397006720,The performance of large language models on quantitative and verbal ability tests: Initial evidence and implications for unproctored high‐stakes testing,2024,0.0009895338252463642,2
W4387430414,Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings,2023,0.0009889587996492301,2
W4407058661,Qur’an Passage Ranking Using Transformer Models,2025,0.000987975578290755,2
W4394811216,Supporting Text Entry in Virtual Reality with Large Language Models,2024,0.0009850342396688252,2
W4362632884,A Neural Topic Modeling Study Integrating SBERT and Data Augmentation,2023,0.0009826484663457348,2
W3211949750,Neurocomputational Models of Language Processing,2021,0.0009822798713206827,2
W4406458141,Aligning the Representation of Knowledge Graph and Large Language Model for Causal Question Answering,2024,0.0009819810193892379,2
W4406749242,ARCH: Large-scale knowledge graph via aggregated narrative codified health records analysis,2025,0.000981831398819153,2
W4405989608,AMPLE: Emotion-Aware Multimodal Fusion Prompt Learning for Fake News Detection,2025,0.0009803882299453458,2
W3094057023,Mapping ESG Trends by Distant Supervision of Neural Language Models,2020,0.000979862727766405,2
W4407340340,Abstract Operations Research Modeling Using Natural Language Inputs,2025,0.0009798485151005312,2
W4406152279,Toward expert-level medical question answering with large language models,2025,0.0009781480181021759,2
W4327644612,Improving Neural Topic Models with Wasserstein Knowledge Distillation,2023,0.0009781009317909768,2
W2968561320,A Study of BERT for Non-Factoid Question-Answering under Passage Length Constraints,2019,0.000977632331165836,2
W4407811419,DeB3RTa: A Transformer-Based Model for the Portuguese Financial Domain,2025,0.0009755257373645713,2
W3104298168,RNNs can generate bounded hierarchical languages with optimal memory,2020,0.0009751078927539698,2
W3080429061,Medical Information Extraction in the Age of Deep Learning,2020,0.0009735133067275382,2
W4403211586,Comprehensive Modeling and Question Answering of Cancer Clinical Practice Guidelines using LLMs,2024,0.0009714333967652818,2
W4382468457,Fine-Grained Retrieval Prompt Tuning,2023,0.0009711188916434457,2
W4399767739,Evaluating ChatGPT: Strengths and Limitations in NLP Problem Solving,2024,0.0009704309635445434,2
W3046467166,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,2020,0.0009699296383064639,2
W4386392742,Contrastive Graph Prompt-tuning for Cross-domain Recommendation,2023,0.000969595145509847,2
W4408585377,ICARE: cross-domain text classification with incremental class-aware representation and distillation learning,2025,0.0009691145748969959,2
W4408804730,Context Matters: Understanding Socially Appropriate Affective Responses Via Sentence Embeddings,2025,0.0009688715222142793,2
W4407887224,The Design and Practice of an Enhanced Search for Maritime Transportation Knowledge Graph Based on Semi-Schema Constraints,2025,0.0009645075195241212,2
W4407914295,Assessing Generalization Capability of Text Ranking Models in Polish,2025,0.0009635905365398829,2
W4409024488,Large Language Models' Ability to Assess Main Concepts in Story Retelling: A Proof-of-Concept Comparison of Human Versus Machine Ratings,2025,0.000963339987988138,2
W4408209324,Conversational Agents in the Legal Domain: A Systematic Review of the Literature,2025,0.0009616876685407919,2
W4396843973,NoteLLM: A Retrievable Large Language Model for Note Recommendation,2024,0.0009610080335278948,2
W4283318325,BioRED: a rich biomedical relation extraction dataset,2022,0.0009605272480771299,2
W4318147441,Media Coverage and Public Perception of Distance Learning During the COVID-19 Pandemic: A Topic Modeling Approach Based on BERTopic,2022,0.0009603164524835375,2
W4377138005,An Analysis of Fusion Functions for Hybrid Retrieval,2023,0.0009596634152575145,2
W4382317573,CodeAttack: Code-Based Adversarial Attacks for Pre-trained Programming Language Models,2023,0.0009587947322156722,2
W4319986894,Dialogue Logic Aware and Key Utterance Decoupling Model for Multi-Party Dialogue Reading Comprehension,2023,0.0009577041624708075,2
W4319730829,Information extraction from German radiological reports for general clinical text and language understanding,2023,0.000957154959037903,2
W4225378553,Analysis of community question‐answering issues via machine learning and deep learning: State‐of‐the‐art review,2022,0.0009565482173669192,2
W4388266051,Integrating legal event and context information for Chinese similar case analysis,2023,0.0009535025319568177,2
W4392282268,Taiyi: a bilingual fine-tuned large language model for diverse biomedical tasks,2024,0.000953242339325144,2
W4401943355,Leveraging Large Language Models for Patient Engagement: The Power of Conversational AI in Digital Health,2024,0.0009526990139423941,2
W3198980621,A Survey on Recent Named Entity Recognition and Relationship Extraction Techniques on Clinical Texts,2021,0.0009524156976429959,2
W4407945631,Comprehensive Analysis of Machine Learning and Deep Learning models on Prompt Injection Classification using Natural Language Processing techniques,2025,0.0009498430921420219,2
W4380789085,Semantic similarity models for automated fact-checking: ClaimCheck as a claim matching tool,2023,0.0009484340069701808,2
W4388422777,Extracting laboratory test information from paper-based reports,2023,0.0009481866791893769,2
W4366595110,A Comprehensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT,2023,0.0009480782310379385,2
W2889446948,Toward Fast and Accurate Neural Discourse Segmentation,2018,0.0009423800097443262,2
W4399326970,BioInstruct: instruction tuning of large language models for biomedical natural language processing,2024,0.0009399240115144754,2
W4210794564,Adversarial Machine Learning in Text Processing: A Literature Survey,2022,0.0009398376488531507,2
W2765742249,Drug–drug interaction extraction via hierarchical RNNs on sequence and shortest dependency paths,2017,0.0009382478969041162,2
W4228996479,Effective use of BERT in graph embeddings for sparse knowledge graph completion,2022,0.0009380491597644314,2
W4403577774,A GAIL Fine-Tuned LLM Enhanced Framework for Low-Resource Knowledge Graph Question Answering,2024,0.0009371132942707428,2
W4405247702,From Fact Drafts to Operational Systems: Semantic Search in Legal Decisions Using Fact Drafts,2024,0.000935678962444595,2
W2788751659,Memorize or generalize? Searching for a compositional RNN in a haystack,2018,0.0009299501484670275,2
W4310157637,Combining computational controls with natural text reveals aspects of meaning composition,2022,0.0009285700789158057,2
W4409159827,CREDIFY: contextualized retrieval of evidence for open-domain fact verification,2025,0.000927183070067395,2
W4313531743,Research on semantic representation and citation recommendation of scientific papers with multiple semantics fusion,2023,0.0009256576970465459,2
W2964152081,"Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks",2017,0.0009255733682903781,2
W2605089588,Neural Network-based Question Answering over Knowledge Graphs on Word and Character Level,2017,0.0009251815538676807,2
W2998443519,A survey of semantic relatedness evaluation datasets and procedures,2019,0.0009251768632695919,2
W3104727373,Combining computational controls with natural text reveals new aspects of meaning composition,2020,0.0009248011205397628,2
W4297238006,Stepwise relation prediction with dynamic reasoning network for multi-hop knowledge graph question answering,2022,0.000924664581006804,2
W4403603005,Health Care Language Models and Their Fine-Tuning for Information Extraction: Scoping Review,2024,0.0009234811135188546,2
W4379769651,Health system-scale language models are all-purpose prediction engines,2023,0.0009230737688801128,2
W2997738974,Few-Shot Knowledge Graph Completion,2020,0.0009228592352919308,2
W2910453440,A Survey of Zero-Shot Learning,2019,0.0009140379005068727,2
W3095645723,Scaling Laws for Autoregressive Generative Modeling,2020,0.0009115841447677097,2
W4387878813,Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues,2023,0.000911430785422874,2
W4410092719,The bewitching AI: The Illusion of Communication with Large Language Models,2025,0.0009082156453965343,2
W3035294872,A Label Attention Model for ICD Coding from Clinical Text,2020,0.0009050390773680348,2
W4408341583,Semantic embeddings reveal and address taxonomic incommensurability in psychological measurement,2025,0.0009047023935526421,2
W2789244308,Comparing deep learning and concept extraction based methods for patient phenotyping from clinical narratives,2018,0.0009042778917377838,2
W4382366638,Information Retrieval Using Domain Adapted Language Models: Application to Resume Documents for HR Recruitment Assistance,2023,0.0009041679000849287,2
W4389747470,A Semantic Search System for the Supremo Tribunal de Justiça,2023,0.0009029880512161781,2
W4401379766,Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese,2024,0.000902945339527888,2
W4200427693,An attentive joint model with transformer-based weighted graph convolutional network for extracting adverse drug event relation,2021,0.0009000509927432473,2
W4387211709,Text-Guided Foundation Model Adaptation for Pathological Image Classification,2023,0.0008998037883233094,2
W4400984866,Dr.ICL: Demonstration-Retrieved In-context Learning,2024,0.0008996998103969528,2
W3099543642,Lifelong Language Knowledge Distillation,2020,0.0008989571876084977,2
W3102485638,How well does surprisal explain N400 amplitude under different experimental conditions?,2020,0.0008985337599354816,2
W4399276598,Viability of Open Large Language Models for Clinical Documentation in German Health Care: Real-World Model Evaluation Study,2024,0.0008982343893239033,2
W4407194866,CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare,2025,0.0008973465696249434,2
W4402584557,Cross-Lingual Short-Text Semantic Similarity for Kannada–English Language Pair,2024,0.0008968370439451505,2
W4404396281,Fine-tuning language model embeddings to reveal domain knowledge: An explainable artificial intelligence perspective on medical decision making,2024,0.0008958513230281703,2
W4384642795,Adapting Learned Sparse Retrieval for Long Documents,2023,0.0008953701059657673,2
W3034444624,Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?,2020,0.000893964396680561,2
W4401068660,"Prompts and Large Language Models: A New Tool for Drafting, Reviewing and Interpreting Contracts?",2024,0.0008926511283428669,2
W4403343568,Federated learning-based natural language processing: a systematic literature review,2024,0.000892277583613352,2
W2910705748,"Definitions, methods, and applications in interpretable machine learning",2019,0.0008886820123916495,2
W4392384310,LEAD: Liberal Feature-based Distillation for Dense Retrieval,2024,0.0008869278902642436,2
W3033057983,Assessing the accuracy of automatic speech recognition for psychotherapy,2020,0.0008817356985080088,2
W4409172641,Legal Literacy in Indonesia: Leveraging Semantic-Based AI and NLP for Enhanced Civil Law Access,2025,0.0008795968780125337,2
W4410465595,Comparative analysis of text mining and clustering techniques for assessing functional dependency between manual test cases,2025,0.0008795968780125337,2
W4312064036,Combining BERT with numerical variables to classify injury leave based on accident description,2022,0.0008779470068757733,2
W4409157766,LLM-Eraser: Optimizing Large Language Model Unlearning through Selective Pruning,2025,0.000877472853984984,2
W2999791086,Ensemble Approach for Natural Language Question Answering Problem,2019,0.0008756427907593725,2
W3100083965,A Qualitative Evaluation of Language Models on Automatic Question-Answering for COVID-19,2020,0.0008752539744453277,2
W4399282265,Improving Vietnamese Legal Question–Answering System Based on Automatic Data Enrichment,2024,0.0008750729090435831,2
W4225891352,The CLEF-2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and Fake News Detection,2022,0.00087379322864895,2
W4409386930,Causality-Driven Patent Valuation: Integrating Domain Knowledge and Language Models in a Structured Interview-Like Selection Process,2025,0.000872190134024301,2
W4292939077,Overview of the CLEF–2022 CheckThat! Lab on Fighting the COVID-19 Infodemic and Fake News Detection,2022,0.0008710110205579276,2
W4387428045,MarkBERT: Marking Word Boundaries Improves Chinese BERT,2023,0.0008702294629216537,2
W3102199783,Learning Numeral Embedding,2020,0.0008691905550863953,2
W4409286293,Natural language processing models reveal neural dynamics of human conversation,2025,0.000868928681822896,2
W2962941914,Finding syntax in human encephalography with beam search,2018,0.0008674904366220409,2
W4394567450,An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study (Preprint),2023,0.0008673444654777161,2
W2892197424,Dialog-to-action: conversational question answering over a large-scale knowledge base,2018,0.0008645612687719424,2
W4312426142,Context-Free Word Importance Scores for Attacking Neural Networks,2022,0.0008635878329219453,2
W2936405048,Pun Generation with Surprise,2019,0.0008628658454128398,2
W4309652662,Active learning for transformer models in direction query tagging,2022,0.0008624727655591713,2
W4388514671,OLaLa: Ontology Matching with Large Language Models,2023,0.0008600292276345566,2
W3094756469,Accenture at CheckThat! 2020: If you say so: Post-hoc fact-checking of claims using transformer-based models,2020,0.0008596636808560254,2
W4388805583,The power and potentials of Flexible Query Answering Systems: A critical and comprehensive analysis,2023,0.0008550018680378089,2
W4322757661,Assessment of Natural Language Processing of Electronic Health Records to Measure Goals-of-Care Discussions as a Clinical Trial Outcome,2023,0.0008543455979403477,2
W4409160182,Graph-Convolutional Networks: Named Entity Recognition and Large Language Model Embedding in Document Clustering,2025,0.0008540713185221913,2
W4327652408,Contextualized Graph Embeddings for Adverse Drug Event Detection,2023,0.000853662591433538,2
W3175462020,Developing a Vietnamese Tourism Question Answering System Using Knowledge Graph and Deep Learning,2021,0.0008523707014428979,2
W4385893847,GreenKGC: A Lightweight Knowledge Graph Completion Method,2023,0.0008511575034241027,2
W4296143727,Heterogeneous deep graph convolutional network with citation relational BERT for COVID-19 inline citation recommendation,2022,0.0008504681037838748,2
W4366824871,Contextualized medication information extraction using Transformer-based deep learning architectures,2023,0.0008501794426809463,2
W2774918944,Active Learning for Convolutional Neural Networks: A Core-Set Approach,2017,0.000849371050097042,2
W4408044443,Generalizable and Robust Log Anomaly Detection Based on Transformer,2025,0.0008486874026261029,2
W2963861211,Simple Question Answering by Attentive Convolutional Neural Network,2016,0.0008481518977889956,2
W4391644728,Investigating the Impact of Prompt Engineering on the Performance of Large Language Models for Standardizing Obstetric Diagnosis Text: Comparative Study,2024,0.0008436902089099096,2
W2970014349,Human-grounded Evaluations of Explanation Methods for Text Classification,2019,0.0008434731196554894,2
W4384891026,The Tale of Two MSMARCO - and Their Unfair Comparisons,2023,0.0008431844317186121,2
W4285134704,Model Distillation for Faithful Explanations of Medical Code Predictions,2022,0.0008427024086937244,2
W4406132119,Natural language processing for chest X‐ray reports in the transformer era: BERT‐like encoders for comprehension and GPT‐like decoders for generation,2025,0.0008426994385024269,2
W3101950626,Semantic Structure and Interpretability of Word Embeddings,2018,0.0008425765927846745,2
W4389518889,Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset,2023,0.0008425156826071909,2
W2511929605,Inner Attention based Recurrent Neural Networks for Answer Selection,2016,0.0008392052476507368,2
W4409120200,DALL-M: Context-aware clinical data augmentation with large language models,2025,0.0008375447256415972,2
W4382318497,COSMOS: Catching Out-of-Context Image Misuse Using Self-Supervised Learning,2023,0.0008368629176335044,2
W2963053846,A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations,2015,0.000836330391347436,2
W3043747042,Adversarial active learning for the identification of medical concepts and annotation inconsistency,2020,0.0008360546873239102,2
W4393213239,ChatGPT Incorrectness Detection in Software Reviews,2024,0.000834392776639753,2
W4410590444,Evaluating the Performance of Complex Text Generated by Large Language Models,2025,0.0008335589486775625,2
W4402827393,Larger and more instructable language models become less reliable,2024,0.0008319446198192238,2
W4290613782,Applications of natural language processing in ophthalmology: present and future,2022,0.0008308280588853637,2
W2897007327,Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record,2018,0.0008304046773061253,2
W4385763801,Generalizing to Unseen Elements: A Survey on Knowledge Extrapolation for Knowledge Graphs,2023,0.000828467657548673,2
W4401813696,AMGPT: A large language model for contextual querying in additive manufacturing,2024,0.0008280931887996589,2
W4400520877,Multi-schema prompting powered token-feature woven attention network for short text classification,2024,0.000827236541126144,2
W4403989939,NSSC: a neuro-symbolic AI system for enhancing accuracy of named entity recognition and linking from oncologic clinical notes,2024,0.0008257540120606508,2
W4403791697,Enhancing Question Answering for Enterprise Knowledge Bases using Large Language Models,2024,0.0008254294298765752,2
W2894740066,SNIP: Single-shot Network Pruning based on Connection Sensitivity,2018,0.0008254217626364454,2
W3180374035,Wordcraft: a Human-AI Collaborative Editor for Story Writing,2021,0.0008253621339625351,2
W4410398307,End-to-end Chinese clinical event extraction based on large language model,2025,0.000824439022257865,2
W4390347092,Differentiating ChatGPT-Generated and Human-Written Medical Texts: Quantitative Study,2023,0.0008243191778718805,2
W4409150672,GraphLoRA: Structure-Aware Contrastive Low-Rank Adaptation for Cross-Graph Transfer Learning,2025,0.0008233167681415591,2
W4389524566,Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication,2023,0.0008231417282383028,2
W3184324824,Detecting formal thought disorder by deep contextualized word representations,2021,0.0008205582970667786,2
W4313201586,Multi-granularity Hierarchical Feature Extraction for Question-Answering Understanding,2022,0.0008204475148308122,2
W4402856262,Language Model-Based Text Augmentation System for Cerebrovascular Disease Related Medical Report,2024,0.0008203651485654733,2
W4284685989,BERT-ER,2022,0.0008159439841291485,2
W4367047490,KRACL: Contrastive Learning with Graph Context Modeling for Sparse Knowledge Graph Completion,2023,0.0008151267762044552,2
W2890560945,A State-transition Framework to Answer Complex Questions over Knowledge Base,2018,0.0008148042684263674,2
W4385571981,AntContentTech at SemEval-2023 Task 6: Domain-adaptive Pretraining and Auxiliary-task Learning for Understanding Indian Legal Texts,2023,0.0008143324271374405,2
W4389165057,Augmenting interpretable models with large language models during training,2023,0.0008140400960938849,2
W4402034088,Backdoor Attacks with Input-Unique Triggers in NLP,2024,0.0008133436688327821,2
W2949579048,Visually Grounded Neural Syntax Acquisition,2019,0.0008125306461674331,2
W3183569911,SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images,2021,0.0008120229804265617,2
W4405962550,An Automated Hybrid Exam Evaluation Framework for Textual Courses Using AI,2024,0.0008112877288625718,2
W4361829659,An Exploratory Analysis of GSDMM and BERTopic on Short Text Topic Modelling,2022,0.0008111379508326711,2
W4385541615,Read it Twice: Towards Faithfully Interpretable Fact Verification by Revisiting Evidence,2023,0.0008104850995887918,2
W3182536263,"Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey",2021,0.0008094248641676624,2
W4385572206,ActiveAED: A Human in the Loop Improves Annotation Error Detection,2023,0.0008091971638164992,2
W4408095424,Leveraging Unstructured Text Data for Federated Instruction Tuning of Large Language Models,2025,0.0008089676459689194,2
W4386450708,"Sentences Similarity Model Based on Fusion of Semantic, Syntactic and Word Order Multi-Features",2023,0.0008087579051693468,2
W4410486481,A Discourse Analysis Framework for Legislative and Social Media Debates,2025,0.0008087562000592023,2
W4327927574,Exposing the Vulnerabilities of Deep Learning Models in News Classification,2023,0.0008080240201726121,2
W4396843991,"Text-Attributed Graph Representation Learning: Methods, Applications, and Challenges",2024,0.00080633697162026,2
W4387059191,2SCE-4SL: a 2-stage causality extraction framework for scientific literature,2023,0.0008059724723961664,2
W2963897632,Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks,2018,0.0008057002265672967,2
W4401945007,Heterogeneous-branch integration framework: Introducing first-order predicate logic in Logical Reasoning Question Answering,2024,0.0008052814309447792,2
W3169141681,"Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",2021,0.0008031026085654005,2
W4317611844,Automated Creation of an Intent Model for Conversational Agents,2023,0.000802752656238418,2
W4407766364,Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches,2025,0.0008022461343259689,2
W2963738886,CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases,2016,0.0008013887942883,2
W4401103265,Improving biomedical entity linking for complex entity mentions with LLM-based text simplification,2024,0.0008012825082375975,2
W4407682194,Efficiency and Performance Optimization in Large Language Models through IB Fine-Tuning,2025,0.0008012017875067561,2
W4402703473,From NLP to Taxonomy: Identifying and Classifying Key Functionality Concepts of Multi-level Project Planning and Control Systems,2024,0.0007981424166332915,2
W4312531598,Computational Understanding of Narratives: A Survey,2022,0.0007977856833600139,2
W2797585226,Discourse-Aware Neural Rewards for Coherent Text Generation,2018,0.0007961887916575777,2
W3152858897,Toward Automated Factchecking,2021,0.0007948707258441834,2
W4401866067,Understanding Logical Reasoning Ability of Large Language Models,2024,0.0007942753808553087,2
W4389518770,Discovering Universal Geometry in Embeddings with ICA,2023,0.0007934096028468332,2
W4396819963,Ranked List Truncation for Large Language Model-based Re-Ranking,2024,0.0007928252582546296,2
W2962927633,The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers,2019,0.0007911664265329027,2
W4406122607,Annotated corpus for traditional formula-disease relationships in biomedical articles,2025,0.0007894177506897398,2
W4394617362,ACP-DRL: an anticancer peptides recognition method based on deep representation learning,2024,0.0007887825051045784,2
W4366824677,Joint modeling for early predictions of Li-ion battery cycle life and degradation trajectory,2023,0.0007878710672892134,2
W4408960620,Large language model for interpreting research policy using adaptive two-stage retrieval augmented fine-tuning method,2025,0.0007868528352147438,2
W4393074274,ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality,2024,0.0007858438575684701,2
W4409164491,How do LLMs perform on Turkish? A multi-faceted multi-prompt evaluation,2025,0.000784249592252799,2
W4409378335,ReranKGC: A cooperative retrieve-and-rerank framework for multi-modal knowledge graph completion,2025,0.0007836801526660071,2
W4403780660,Enhancing Transformer-based Semantic Matching for Few-shot Learning through Weakly Contrastive Pre-training,2024,0.0007804770352487864,2
W4399184154,Optimization techniques for sentiment analysis based on LLM (GPT-3),2024,0.0007792548887847664,2
W4221150520,HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing,2022,0.0007786029164502399,2
W4409663470,Qibo: A Large Language Model for traditional Chinese medicine,2025,0.0007771667379718127,2
W4220835136,Unrestricted multi-hop reasoning network for interpretable question answering over knowledge graph,2022,0.0007764999955886518,2
W2963682631,Character-Level Question Answering with Attention,2016,0.0007760919369530279,2
W4407295345,Automated Extraction of Key Entities from Non-English Mammography Reports Using Named Entity Recognition with Prompt Engineering,2025,0.0007753273230803469,2
W4307807303,MADLINK: Attentive multihop and entity descriptions for link prediction in knowledge graphs,2022,0.0007751541518060243,2
W4224068866,A Transfer Learning Method for Detecting Alzheimer's Disease Based on Speech and Natural Language Processing,2022,0.0007749429911865672,2
W3173561451,Biomedical and clinical English model packages for the Stanza Python NLP library,2021,0.0007721244137589288,2
W4393094733,Advancing entity recognition in biomedicine via instruction tuning of large language models,2024,0.0007713093051537265,2
W4409295452,Enhancing clinical information processing with ICRE: an intelligent chain-refinement extraction framework for precision data mining,2025,0.0007712424192879033,2
W3203241482,"Artificial intelligence in the construction industry: A review of present status, opportunities and future challenges",2021,0.0007702720950849175,2
W4407839089,stEELlm: An LLM for Generating Semantic Annotations of Tabular Data,2025,0.0007695881810189731,2
W4388478664,Named Entity Recognition in Electronic Health Records: A Methodological Review,2023,0.000768990628721339,2
W4408165783,Medical foundation large language models for comprehensive text analysis and beyond,2025,0.0007686558149976192,2
W4281687075,A comparative evaluation of biomedical similar article recommendation,2022,0.0007677434065327461,2
W4307138871,"Automated clinical coding: what, why, and where we are?",2022,0.000766393032741414,2
W2610536450,Adversarial Connective-exploiting Networks for Implicit Discourse Relation Classification,2017,0.0007631923482897125,2
W4388501732,Comparing text mining and manual coding methods: Analysing interview data on quality of care in long-term care for older adults,2023,0.0007628731818220228,2
W4407058821,MAGENTA: Generating and Detecting Arabic Machine-Generated Text in Multiple Domains,2025,0.0007614436887521829,2
W2952604841,Auditing Data Provenance in Text-Generation Models,2019,0.0007591022896041725,2
W3200167504,RNN-Test: Towards Adversarial Testing for Recurrent Neural Network Systems,2021,0.0007572846346275048,2
W4401824528,Designing Retrieval-Augmented Language Models for Clinical Decision Support,2024,0.0007566726191230563,2
W4283079132,Automatic data extraction to support meta-analysis statistical analysis: a case study on breast cancer,2022,0.0007531490239041931,2
W4229447062,Interactive Model Cards: A Human-Centered Approach to Model Documentation,2022,0.0007505178825410753,2
W2970254524,Reconstructing Capsule Networks for Zero-shot Intent Classification,2019,0.0007504260753428597,2
W3021347125,PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,2020,0.0007498491704604097,2
W4408529278,"Lost in translation: using global fact-checks to measure multilingual misinformation prevalence, spread, and evolution",2025,0.0007497881529829731,2
W4389524356,FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge,2023,0.0007442900045186531,2
W4388593316,Assessing the Strengths and Weaknesses of Large Language Models,2023,0.0007420934247337825,2
W4320854797,Mixed Multi-Model Semantic Interaction for Graph-based Narrative Visualizations,2023,0.0007406103015615891,2
W4406873853,Natural language processing‐based classification of early Alzheimer's disease from connected speech,2025,0.0007354485913859632,2
W4309773749,Data augmentation techniques in natural language processing,2022,0.0007343072597463218,2
W4391428886,Automated Smell Detection and Recommendation in Natural Language Requirements,2024,0.0007343034025193511,2
W3206644354,Do travelers' reviews depend on the destination? An analysis in coastal and urban peer‐to‐peer lodgings,2021,0.0007334005674314503,2
W4401834344,A Scalable Framework for Benchmarking Embedding Models for Semantic Medical Tasks,2024,0.0007317350491176739,2
W4391724817,MASTERKEY: Automated Jailbreaking of Large Language Model Chatbots,2024,0.0007306927649976222,2
W2985173696,EHR Coding with Multi-scale Feature Attention and Structured Knowledge Graph Propagation,2019,0.0007273475448555946,2
W4403672405,Discovering Hidden Patterns: Applying Topic Modeling in Qualitative Research,2024,0.0007256668867406651,2
W3119145505,Medical concept normalization in French using multilingual terminologies and contextual embeddings,2021,0.0007251129429654334,2
W4407797263,Effectiveness in retrieving legal precedents: exploring text summarization and cutting-edge language models toward a cost-efficient approach,2025,0.0007246755508984121,2
W2515860461,Metaphor as a Medium for Emotion: An Empirical Study,2016,0.0007231651308069731,2
W4404440409,Decoding text from electroencephalography signals: A novel Hierarchical Gated Recurrent Unit with Masked Residual Attention Mechanism,2024,0.0007224908246971731,2
W2951125449,Condensed Memory Networks for Clinical Diagnostic Inferencing,2017,0.0007219949716142155,2
W4394726121,Enhancing Zero-Shot Stance Detection with Contrastive and Prompt Learning,2024,0.0007218567223942299,2
W4386173035,Applications of the Natural Language Processing Tool ChatGPT in Clinical Practice: Comparative Study and Augmented Systematic Review,2023,0.0007202515932758247,2
W4383818636,Unifying Sentence Transformer Embedding and Softmax Voting Ensemble for Accurate News Category Prediction,2023,0.0007200052210431665,2
W2995755016,Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples,2022,0.0007166179671964953,2
W3200076410,"Overview of the CLEF–2021 CheckThat! Lab on Detecting Check-Worthy Claims, Previously Fact-Checked Claims, and Fake News",2021,0.000716392055279167,2
W4393411284,Topic Modeling for Mining Opinion Aspects from a Customer Feedback Corpus,2024,0.000715469351848621,2
W4312782030,A Neighborhood Re-Ranking Model With Relation Constraint for Knowledge Graph Completion,2022,0.0007151436065260062,2
W3037404357,Scalable and explainable legal prediction,2020,0.0007139976408183932,2
W2888213795,A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation,2018,0.0007138728322734864,2
W4393160377,A Learnable Discrete-Prior Fusion Autoencoder with Contrastive Learning for Tabular Data Synthesis,2024,0.0007117821670594316,2
W3174583470,Math Word Problem Solving with Explicit Numerical Values,2021,0.0007113048259043827,2
W4389109075,NLFOA: Natural Language Focused Ontology Alignment,2023,0.0007085884906439365,2
W4402635336,"Overview of the CLEF-2024 CheckThat! Lab: Check-Worthiness, Subjectivity, Persuasion, Roles, Authorities, and Adversarial Robustness",2024,0.0007081755550861805,2
W4409158849,Leveraging social media for public health: NLP implementations for blood donation data analysis in Japan,2025,0.0007081209339382431,2
W4295808856,"Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond",2022,0.0007080375399958719,2
W4408254499,Guiding Prototype Networks with label semantics for few-shot text classification,2025,0.00070749579714489,2
W4394910399,Understanding the Role of Self-Attention in a Transformer Model for the Discrimination of SCD From MCI Using Resting-State EEG,2024,0.0007072279346242203,2
W4402533779,"RegulaTome: a corpus of typed, directed, and signed relations between biomedical entities in the scientific literature",2024,0.0007061807525458182,2
W4364368785,Drug–drug interaction extraction‐based system: An <scp>natural language processing</scp> approach,2023,0.000706116879753543,2
W4404818016,Concepts and Relations Features Are All You Need for Embedding-Based Ontology Matching,2024,0.0007058275165733773,2
W4384664813,RegEMR: a natural language processing system to automatically identify premature ovarian decline from Chinese electronic medical records,2023,0.0007048280812841122,2
W4221142755,CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion,2022,0.0007048217679214268,2
W4281737296,NuPS: A Parameter Server for Machine Learning with Non-Uniform Parameter Access,2022,0.0007027517534442052,2
W4386947298,Prognostic models of in-hospital mortality of intensive care patients using neural representation of unstructured text: A systematic review and critical appraisal,2023,0.0007025478766805674,2
W2968210605,Attention is not not Explanation,2019,0.0006984566718779864,2
W4396900090,Introduction to Large Language Models (LLMs) for dementia care and research,2024,0.0006973445463193552,2
W4212830955,Doc2KG,2022,0.0006973436207382015,2
W4389579575,STRING-ing together protein complexes: corpus and methods for extracting physical protein interactions from the biomedical literature,2023,0.0006965192560519125,2
W4388714782,"The good, the bad, and the ambivalent: Extrapolating affective values for 38,000+ Chinese words via a computational model",2023,0.0006963175310690246,2
W4409657176,MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot,2025,0.0006962484801421916,2
W4391653771,"Screening Smarter, Not Harder: A Comparative Analysis of Machine Learning Screening Algorithms and Heuristic Stopping Criteria for Systematic Reviews in Educational Research",2024,0.0006941312199666164,2
W4389523673,NormDial: A Comparable Bilingual Synthetic Dialog Dataset for Modeling Social Norm Adherence and Violation,2023,0.0006936906446134468,2
W3001093703,Interactive knowledge-enhanced attention network for answer selection,2020,0.0006925630904902864,2
W4388537645,Poisoning scientific knowledge using large language models,2023,0.0006912190300906808,2
W4408742984,Evaluating explainability in language classification models: A unified framework incorporating feature attribution methods and key factors affecting faithfulness,2025,0.0006906814992992372,2
W4294989880,Investigating Topic Modeling Techniques to Extract Meaningful Insights in Italian Long COVID Narration,2022,0.0006903519825789758,2
W4385573017,A Survey of Active Learning for Natural Language Processing,2022,0.0006897681548975897,2
W3215981572,Unsupervised Topic Discovery in User Comments,2021,0.0006880295968235796,2
W4400985603,PIM GPT a hybrid process in memory accelerator for autoregressive transformers,2024,0.0006872395192868941,2
W4410129427,fastText-Based Siamese Network for Hindi Semantic Textual Similarity,2025,0.0006862395084475314,2
W4381715962,A Transformer-Based Model Trained on Large Scale Claims Data for Prediction of Severe COVID-19 Disease Progression,2023,0.0006856167944954399,2
W3102561203,Building Legal Case Retrieval Systems with Lexical Matching and Summarization using A Pre-Trained Phrase Scoring Model,2019,0.0006855350775046022,2
W4391839376,Analyzing public demands on China’s online government inquiry platform: A BERTopic-Based topic modeling study,2024,0.0006851020820290032,2
W2963374347,Visual interpretability for deep learning: a survey,2018,0.0006834631508622303,2
W4407859403,Assisting Drafting of Chinese Legal Documents Using Fine-Tuned Pre-trained Large Language Models,2025,0.0006829282243924619,2
W4404900783,"Multi-modal large language models in radiology: principles, applications, and potential",2024,0.0006821155222809132,2
W4406617454,Using the Retrieval-Augmented Generation to Improve the Question-Answering System in Human Health Risk Assessment: The Development and Application,2025,0.000681820813891245,2
W4328053377,Development and external validation of automated ICD-10 coding from discharge summaries using deep learning approaches,2023,0.0006810133331309864,2
W4309630085,Incorporating anticipation embedding into reinforcement learning framework for multi-hop knowledge graph question answering,2022,0.0006793695735045398,2
W4327498003,ECIR 23 Tutorial: Neuro-Symbolic Approaches for Information Retrieval,2023,0.0006788128769027901,2
W4409253829,Enhancing Vietnamese Mental Health Question-Answering Systems: Adaptive Retrieval Augmented Generation Pipeline and Data Augmentation for Large Language Models,2025,0.0006779099691766448,2
W2965962253,Controllable Neural Story Plot Generation via Reward Shaping,2019,0.0006778101249853717,2
W4400617496,A knowledge graph completion model based on triple level interaction and contrastive learning,2024,0.0006766879152015952,2
W4284687473,Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding,2022,0.0006758220024694694,2
W4306247398,Over-reliance on English hinders cognitive science,2022,0.0006745512210274264,2
W2772121968,Medical subdomain classification of clinical notes using a machine learning-based natural language processing approach,2017,0.0006733775859694148,2
W3116099552,PharmKG: a dedicated knowledge graph benchmark for bomedical data mining,2020,0.0006725991664343014,2
W4388491879,Classification of Human- and AI-Generated Texts: Investigating Features for ChatGPT,2023,0.0006719495672854465,2
W4388821834,On reading and interpreting black box deep neural networks,2023,0.000670907804205506,2
W4407634256,Teaching a Conversational Agent using Natural Language: Effect on Learning and Engagement,2025,0.0006694084328949748,2
W4306770848,Differentiation in microenterprises,2022,0.0006694084328949748,2
W4407750010,Enhancing Recommender Systems: Deep Modality Alignment with Large Multi-Modal Encoders,2025,0.0006694084328949748,2
W4220973773,Active Learning for Reducing Labeling Effort in Text Classification Tasks,2022,0.0006694084328949748,2
W4390145017,Syntax through rapid synaptic changes,2023,0.0006693325092296337,2
W4406073177,Unveiling the power of language models in chemical research question answering,2025,0.0006684286829207143,2
W4391873151,Phenomics Assistant: An Interface for LLM-based Biomedical Knowledge Graph Exploration,2024,0.0006683253397251036,2
W4407624627,Improving Systematic Review Updates with Natural Language Processing: A Study on Screening Model Efficiency Through Component Classification and Selection (Preprint),2024,0.0006663418967243974,2
W3017463390,BioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale,2020,0.0006656741388864078,2
W4386780600,Open-world story generation with structured knowledge enhancement: A comprehensive survey,2023,0.0006655367539499491,2
W4282960293,How can natural language processing help model informed drug development?: a review,2022,0.0006639160378732197,2
W3101747393,Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture,2017,0.0006637582005768941,2
W4393381464,Using rhetorical strategies to design prompts: a human-in-the-loop approach to make AI useful,2024,0.000663392428320439,2
W4394824046,Triple alignment-enhanced complex question answering over knowledge bases,2024,0.0006614565772860033,2
W4408826549,ATIRS: Towards Adaptive Threat Analysis with Intelligent Log Summarization and Response Recommendation,2025,0.0006614145985751953,2
W4394822252,Floating-Point Embedding: Enhancing the Mathematical Comprehension of Large Language Models,2024,0.0006588667554689873,2
W4381930847,ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge,2023,0.000657728728626479,2
W4385570204,DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models,2023,0.0006561605543989438,2
W4389520349,CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks,2023,0.000655494308801377,2
W4407590023,Enhancing Adverse Event Reporting With Clinical Language Models: Inpatient Falls,2025,0.0006539831390396686,2
W2916562859,What can linguistics and deep learning contribute to each other? Response to Pater,2019,0.0006530161059333568,2
W4403577485,Aligning Large Language Models to a Domain-specific Graph Database for NL2GQL,2024,0.0006529871843069492,2
W4226084703,Establishing Strong Baselines For TripClick Health Retrieval,2022,0.0006529097945987452,2
W4407410388,Natural language processing techniques applied to the electronic health record in clinical research and practice - an introduction to methodologies,2025,0.0006514167408165375,2
W3099641295,De-Biased Court’s View Generation with Causality,2020,0.000650130117324568,2
W4405627622,Knowledge Distillation in RNN-Attention Models for Early Prediction of Student Performance,2025,0.000649201462753439,2
W4304128530,Systematic Evaluation of Common Natural Language Processing Techniques to Codify Clinical Notes,2022,0.0006470429529539274,2
W4401892472,Spatial–Temporal Transformer Networks for Traffic Flow Forecasting Using a Pre-Trained Language Model,2024,0.0006469015940981431,2
W4221151629,Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs,2022,0.0006463441500374645,2
W3093778498,FAQ-Based Question Answering via Knowledge Anchors,2020,0.0006456137876582346,2
W2963572446,Towards Automatic Generation of Shareable Synthetic Clinical Notes Using Neural Language Models,2019,0.0006452608862242399,2
W4206700446,Predicting permeability from 3D rock images based on CNN with physical information,2022,0.0006438938210958902,2
W4406869923,NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning,2025,0.0006435030871364459,2
W2265289447,A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations,2016,0.0006417592637157277,2
W4312158512,Boosting question answering over knowledge graph with reward integration and policy evaluation under weak supervision,2022,0.0006403305434702512,2
W3117745048,FACE-KEG: Fact Checking Explained using KnowledgE Graphs,2021,0.000639614774195435,2
W4378839671,Ask and Ye shall be Answered: Bayesian tag-based collaborative recommendation of trustworthy experts over time in community question answering,2023,0.0006374563634053494,2
W4390512419,SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding,2024,0.0006373060317543306,2
W2964072618,Variational Knowledge Graph Reasoning,2018,0.0006363506918020258,2
W3171364227,Fully Hyperbolic Neural Networks,2022,0.0006356894468827413,2
W2526174222,Image-embodied Knowledge Representation Learning,2017,0.0006346584730768301,2
W2946595845,Proppy: Organizing the news based on their propagandistic content,2019,0.0006343331156100923,2
W2586597293,Neural Discourse Structure for Text Categorization,2017,0.0006328959085063212,2
W4393160208,Small Language Model Can Self-Correct,2024,0.0006316483371595903,2
W4389616438,Understanding the sentiment associated with cultural ecosystem services using images and text from social media,2023,0.0006316194470725909,2
W4396498274,Empower LlaMa 2 for Advanced Logical Reasoning in Natural Language Understanding,2024,0.0006297090776958654,2
W4385571396,Contrastive Learning with Generated Representations for Inductive Knowledge Graph Embedding,2023,0.0006288744911451637,2
W2101848544,Compositional Vector Space Models for Knowledge Base Completion,2015,0.0006284526356121154,2
W4393564733,Infusing behavior science into large language models for activity coaching,2024,0.0006281980941146473,2
W2952409498,Deep Unknown Intent Detection with Margin Loss,2019,0.0006266578205478648,2
W4407953233,Towards Reliable Latent Knowledge Estimation in LLMs: Zero-Prompt Many-Shot Based Factual Knowledge Extraction,2025,0.0006235398942129928,2
W4285202171,Rare Tokens Degenerate All Tokens: Improving Neural Text Generation via Adaptive Gradient Gating for Rare Token Embeddings,2022,0.0006195897403682302,2
W4384705126,BLADE: Combining Vocabulary Pruning and Intermediate Pretraining for Scaleable Neural CLIR,2023,0.000619060467089371,2
W4388164796,Named Entity Recognition and Linking for Entity Extraction from Italian Civil Judgements,2023,0.0006187344773070547,2
W4410478455,Transformer Generative AI Model for Enhanced Molecular Property Prediction,2025,0.0006162528841301589,2
W4410454493,Enhanced effective convolutional attention network with squeeze-and-excitation inception module for multi-label clinical document classification,2025,0.000615720442252359,2
W4220798854,CEQE to SQET: A study of contextualized embeddings for query expansion,2022,0.0006152942783336577,2
W4308590518,Information extraction from electronic medical documents: state of the art and future research directions,2022,0.0006144761298966347,2
W4403780651,Bridging Gaps in Content and Knowledge for Multimodal Entity Linking,2024,0.0006141596025244076,2
W4406522012,Historical facts learning from Long-Short Terms with Language Model for Temporal Knowledge Graph Reasoning,2025,0.0006134614304139233,2
W4409341352,Integrating TCM’s “One Root of Medicine and Food” Principle Into Dietary Recommendations with Retrieval-Augmented LLMs,2025,0.000612757734610402,2
W4293463901,Probabilistic atlas for the language network based on precision fMRI data from &gt;800 individuals,2022,0.0006127475815615036,2
W4389523857,CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation,2023,0.0006114965673511104,2
W4383336963,Global information-aware argument mining based on a top-down multi-turn QA model,2023,0.0006114829908024789,2
W3034470897,Reverse Engineering Configurations of Neural Text Generation Models,2020,0.0006113277390897556,2
W3171530662,A Review on Medical Textual Question Answering Systems Based on Deep Learning Approaches,2021,0.0006097147079778368,2
W4406152280,A generalist medical language model for disease diagnosis assistance,2025,0.0006089557524308283,2
W4311501806,Information flow across the cortical timescale hierarchy during narrative construction,2022,0.0006076812808118205,2
W4409166473,A Reproducibility Study on Consistent LLM Reasoning for Natural Language Inference over Clinical Trials,2025,0.0006034040971310972,2
W2989202909,Generating Diverse Story Continuations with Controllable Semantics,2019,0.0006028437603725501,2
W4385571728,Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models,2023,0.0006017153681910996,2
W4402923149,Towards building multilingual language model for medicine,2024,0.000601254727043323,2
W4284880643,Faster Learned Sparse Retrieval with Guided Traversal,2022,0.0006012118103185776,2
W4409202661,SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation,2025,0.0006006636756633844,2
W2962998183,Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention,2016,0.0006002106586341788,2
W4408366965,Exploring the potential of Claude 2 for risk of bias assessment: Using a large language model to assess randomized controlled trials with RoB 2,2025,0.0005974797942528872,2
W4408135179,A simplified retriever to improve accuracy of phenotype normalizations by large language models,2025,0.0005972859159816094,2
W3101786725,A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving,2020,0.0005968387025256344,2
W3196704868,Contextualized Knowledge-aware Attentive Neural Network: Enhancing Answer Selection with Knowledge,2021,0.0005965274184136319,2
W4308791833,Improving embedded knowledge graph multi-hop question answering by introducing relational chain reasoning,2022,0.0005962551661927784,2
W2895351178,Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings,2018,0.0005960883942090759,2
W4409626371,Optimizing Prompt Refinement: Algorithmic Strategies for Llm-Driven Text Classification Tasks,2025,0.0005943132237848927,2
W4220771481,TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters,2022,0.000592681395187244,2
W2954141754,Team Bertha von Suttner at SemEval-2019 Task 4: Hyperpartisan News Detection using ELMo Sentence Representation Convolutional Network,2019,0.0005926488067846755,2
W4408146479,Robustness of Large Language Models Against Adversarial Attacks,2024,0.0005893454980771426,2
W4410314721,VIKCSE: Visual-knowledge enhanced contrastive learning with prompts for sentence embedding,2025,0.0005878801704197048,2
W3175530221,Stylized Story Generation with Style-Guided Planning,2021,0.0005878084617765097,2
W4405284316,Classification of English Words into Grammatical Notations Using Deep Learning Technique,2024,0.0005872667410426062,2
W4407416297,Enriching RDF Data with LLM Based Named Entity Recognition and Linking on Embedded Natural Language Annotations,2025,0.000586777865223987,2
W4406273314,Applications and Case Studies in Natural Language Understanding,2025,0.000586731626877128,2
W4407772393,Evaluating and Advancing Large Language Models for Water Knowledge Tasks in Engineering and Research,2025,0.0005846443793268885,2
W4408385008,Conversational AI Model for Effective Responses with Augmented Retrieval (CAMERA) Based Chatbot on NVIDIA Jetson Nano,2025,0.0005830448830482979,2
W4281492987,Natural Language Processing for Information Extraction of Gastric Diseases and Its Application in Large-Scale Clinical Research,2022,0.0005810100450218642,2
W4394752750,The language network as a natural kind within the broader landscape of the human brain,2024,0.0005807596956193676,2
W4407058961,Question Answering over the Arabic Hadith Sharif Using Transformer Models,2025,0.0005805561252721129,2
W4396225444,Cross-Domain Knowledge Transfer without Retraining to Facilitating Seamless Knowledge Application in Large Language Models,2024,0.0005779611464082938,2
W4391691195,Retrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) Performance for Clinical Trial Screening,2024,0.0005773097117253618,2
W4408147356,Detecting AI-Generated Text Using Fine-Tuned Transformers: A Study on Academic Integrity,2025,0.0005754983083865547,2
W3116878798,Intent Mining from past conversations for Conversational Agent,2020,0.0005753069987415808,2
W4407601674,A Legal Fact-Finding Model Based on the T5 and LexiLaw Large Language Models,2024,0.0005752980746782758,2
W4393156546,MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models,2024,0.0005752443333260842,2
W4391841474,Combining prompt-based language models and weak supervision for labeling named entity recognition on legal documents,2024,0.000572777090216511,2
W4317797582,Grounding the Vector Space of an Octopus: Word Meaning from Raw Text,2023,0.0005707882945491762,2
W4297478855,ALBERT with Knowledge Graph Encoder Utilizing Semantic Similarity for Commonsense Question Answering,2022,0.0005676907244494423,2
W3174225409,Story Ending Generation with Multi-Level Graph Convolutional Networks over Dependency Trees,2021,0.000566419674952023,2
W3159986071,Compositional Processing Emerges in Neural Networks Solving Math Problems,2021,0.0005660403043084177,2
W4410453176,Implementation and Performance Comparison Study of a Retrieval-Augmented Generation (RAG)-based Chatbot for Korean Medical Consultation (Preprint),2025,0.0005648469155718279,2
W4403582540,Two Heads are Better than One: Zero-shot Cognitive Reasoning via Multi-LLM Knowledge Fusion,2024,0.0005635966379236917,2
W3015378124,Adversarial Attacks on Time Series,2020,0.0005635391325640886,2
W4391126287,Evaluating the ChatGPT family of models for biomedical reasoning and classification,2024,0.0005634070567559171,2
W4224215750,Adversarial attack and defense technologies in natural language processing: A survey,2022,0.0005627623566779392,2
W3188599330,Explainable zero-shot learning via attentive graph convolutional network and knowledge graphs,2021,0.0005576167847907478,2
W4392904549,Large Language Models As A Proxy For Human Evaluation In Assessing The Comprehensibility Of Disordered Speech Transcription,2024,0.0005522228541257815,2
W2963916998,Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks,2018,0.0005510334193836132,2
W4407622810,Augmenting general-purpose large-language models with domain-specific multimodal knowledge graph for question-answering in construction project management,2025,0.0005498662099506037,2
W4327644597,Domain-Aligned Data Augmentation for Low-Resource and Imbalanced Text Classification,2023,0.0005494141938369773,2
W4407268941,"Enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics",2025,0.0005486842886005666,2
W4398183427,Evaluating the accuracy of a state-of-the-art large language model for prediction of admissions from the emergency room,2024,0.0005484773169139182,2
W4295714264,<scp>Semi‐automatic</scp> coding of <scp>open‐ended</scp> text responses in <scp>large‐scale</scp> assessments,2022,0.0005464517149615029,2
W2977268464,Comparing gated and simple recurrent neural network architectures as models of human sentence processing,2018,0.0005458837609183368,2
W4409254068,Self-explaining Neural Network for Multi-criteria Sentiment Analysis,2025,0.0005450907607626587,2
W4404210426,A fine-tuning enhanced RAG system with quantized influence measure as AI judge,2024,0.0005450305298981403,2
W3170841641,Pre-trained Language Model based Ranking in Baidu Search,2021,0.0005448775809256669,2
W4392283598,Large-scale evidence for logarithmic effects of word predictability on reading time,2024,0.000544660391367185,2
W4393160809,EcomGPT: Instruction-Tuning Large Language Models with Chain-of-Task Tasks for E-commerce,2024,0.0005436279499221299,2
W2760057941,Multi-task Attention-based Neural Networks for Implicit Discourse Relationship Representation and Identification,2017,0.0005431975090101,2
W2952068915,Embedding Logical Queries on Knowledge Graphs,2018,0.0005423600981625716,2
W4401577991,Gauging Airbnb review sentiments and critical key-topics by small area estimation,2024,0.0005418187589436467,2
W4409448707,Shades of zero: Distinguishing impossibility from inconceivability,2025,0.0005417989130169419,2
W4409527720,Stimulus dependencies—rather than next-word prediction—can explain pre-onset brain encoding during natural listening,2025,0.0005407393575853379,2
W4409527751,Stimulus dependencies—rather than next-word prediction—can explain pre-onset brain encoding during natural listening,2025,0.0005407393575853379,2
W4408742190,Towards a self-cognitive complex product design system: A fine-grained multi-modal feature recognition and semantic understanding approach using large language models in mechanical engineering,2025,0.00053984028362269,2
W4409068762,Optimising Contract Interpretations with Large Language Models: A Comparative Evaluation of a Vector Database-Powered Chatbot vs. ChatGPT,2025,0.0005396376519382317,2
W4403997208,Language as a cognitive and social tool at the time of large language models,2024,0.0005393901372806113,2
W4393949013,A Generative Artificial Intelligence Using Multilingual Large Language Models for ChatGPT Applications,2024,0.0005390618878189123,2
W4401778192,IMPLEMENTING RETRIEVAL-AUGMENTED GENERATION AND VECTOR DATABASES FOR CHATBOTS IN PUBLIC SERVICES AGENCIES CONTEXT,2024,0.0005387699910076371,2
W4401876324,Language models align with human judgments on key grammatical constructions,2024,0.0005379826178886757,2
W4407011914,Natural language processing for scalable feature engineering and ultra-high-dimensional confounding adjustment in healthcare database studies.,2025,0.0005364880792730142,2
W4405745605,Transformer-Based Tool for Automated Fact-Checking: A Pilot Study on Online Health Information (Preprint),2024,0.0005364041179510419,2
W2965362971,Discourse Analysis and Its Applications,2019,0.0005363188192086388,2
W4391653102,MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model,2024,0.0005356982828172521,2
W4389520015,Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information,2023,0.0005356709199373643,2
W4408132544,An Inference Method for Professional Texts with Computational Expressions Under Few-Shot Scenarios,2025,0.0005337380020557954,2
W4407743121,Can ChatGPT recognize impoliteness? An exploratory study of the pragmatic awareness of a large language model,2025,0.0005324808143324886,2
W4393199286,Investigating machine learning and natural language processing techniques applied for detecting eating disorders: a systematic literature review,2024,0.0005321185400772489,2
W4410300584,Dataset for Legal Question Answering System in the Indian Judiciary Context,2025,0.0005311030597716898,2
W4390426502,The role of large language models in medical image processing: a narrative review,2023,0.0005301832676909782,2
W4319985906,Natural Language Processing in Electronic Health Records in relation to healthcare decision-making: A systematic review,2023,0.0005292242788450093,2
W2951107864,Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions,2019,0.000526985314387891,2
W4392619039,Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning,2024,0.0005266032330803703,2
W4407773580,NNBSVR: Neural Network-Based Semantic Vector Representations of ICD-10 codes,2025,0.0005261532012850965,2
W4390573468,Discovering significant topics from legal decisions with selective inference,2024,0.0005255421390136865,2
W4404424464,Enhancing systematic review efficiency in hand surgery using artificial intelligence (natural language processing) for abstract screening,2024,0.0005255343127057956,2
W4229377239,"Knowledge graphs: Introduction, history, and perspectives",2022,0.000523621388893799,2
W4410056248,Exploring formal defeasible reasoning of large language models: A Chain-of-Thought approach,2025,0.0005235289171466097,2
W4393950419,Collaborative and privacy-enhancing workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions,2024,0.0005199785071793418,2
W4407719186,Extend Adversarial Policy Against Neural Machine Translation via Unknown Token,2025,0.0005199112615483617,2
W4409807000,Injecting Bias into Text Classification Models Using Backdoor Attacks,2025,0.0005187932739214875,2
W4388019311,The Chronicles of ChatGPT: Generating and Evaluating Visual Novel Narratives on Climate Change Through ChatGPT,2023,0.0005184994431933054,2
W4408229742,irAE-GPT: Leveraging large language models to identify immune-related adverse events in electronic health records and clinical trial datasets,2025,0.0005175422101402081,2
W3108978252,A hierarchy of linguistic predictions during natural language comprehension,2020,0.0005167539537458658,2
W4408725130,Robust privacy amidst innovation with large language models through a critical assessment of the risks,2025,0.0005161915626363775,2
W4224920338,Bytecover2: Towards Dimensionality Reduction of Latent Embedding for Efficient Cover Song Identification,2022,0.0005153656573219837,2
W4317738119,A Textual Backdoor Defense Method Based on Deep Feature Classification,2023,0.0005153641211678939,2
W4377289963,Drug–disease association prediction with literature based multi-feature fusion,2023,0.0005153067884471217,2
W4409975498,Weakly supervised text classification on free-text comments in patient-reported outcome measures,2025,0.0005144741463503333,2
W3035134435,Low-Dimensional Hyperbolic Knowledge Graph Embeddings,2020,0.0005138017342146034,2
W2966033979,Post-Processing of Word Representations via Variance Normalization and Dynamic Embedding,2019,0.0005120935699252394,2
W4399932275,CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs for Legal Question Answering,2024,0.0005112197871359336,2
W4404826238,On the creativity of large language models,2024,0.0005109882234225104,2
W4409002188,Leveraging large language models to mimic domain expert labeling in unstructured text-based electronic healthcare records in non-english languages,2025,0.0005094777556925917,2
W4408112574,Domain-Specific Question-Answering Systems: A Case Study of a Carbon Neutrality Knowledge Base,2025,0.0005091021160213748,2
W4408042595,Data transformation of unstructured electroencephalography reports by natural language processing: improving data usability for large-scale epilepsy studies,2025,0.0005088605874568019,2
W4385430086,Emergent analogical reasoning in large language models,2023,0.0005085288902473384,2
W4394744221,Traces of Memorisation in Large Language Models for Code,2024,0.0005078808336868353,2
W4393148027,Mutual-Modality Adversarial Attack with Semantic Perturbation,2024,0.0005072515476665042,2
W2970169050,Event Representation Learning Enhanced with External Commonsense Knowledge,2019,0.0005067004010066863,2
W4306661594,Natural Language Processing Techniques for Text Classification of Biomedical Documents: A Systematic Review,2022,0.0005065588211020382,2
W4409705281,Exploring the Green AI Potential of Adapter Tuning for Language Models,2025,0.0005061693311076264,2
W4220844058,Deep learning-based methods for natural hazard named entity recognition,2022,0.0005042680168912509,2
W4406805597,Urinary Bladder Acute Inflammations and Nephritis of the Renal Pelvis: Diagnosis Using Fine-Tuned Large Language Models,2025,0.0005014658070532366,2
W4409775117,Construction of Journal Knowledge Graph Based on Deep Learning and LLM,2025,0.0005003769085695677,2
W4389669106,"Systematic testing of three Language Models reveals low language accuracy, absence of response stability, and a yes-response bias",2023,0.0005001080420619562,2
W4385270408,Relational Message Passing for Fully Inductive Knowledge Graph Completion,2023,0.000499932834067907,2
W4408095440,FedLegal: A Real-World Federated Learning Benchmark for Legal Natural Language Processing,2025,0.0004984023486934735,2
W4391511770,Transformers in health: a systematic review on architectures for longitudinal data analysis,2024,0.0004981585020347081,2
W4392903658,Interpretable Multimodal Out-of-Context Detection with Soft Logic Regularization,2024,0.0004971112009952924,2
W4221160815,Rethinking Graph Convolutional Networks in Knowledge Graph Completion,2022,0.0004969704264795475,2
W4406870107,From Liberating to Questioning Tabular Data in Documents Using Knowledge Graphs,2025,0.0004950525955502534,2
W2971169991,Asking Clarification Questions in Knowledge-Based Question Answering,2019,0.0004938280412240872,2
W3156782505,Word Senses as Clusters of Meaning Modulations: A Computational Model of Polysemy,2021,0.0004937348213562416,2
W4390011054,Prompting Metalinguistic Awareness in Large Language Models: ChatGPT and Bias Effects on the Grammar of Italian and Italian Varieties,2023,0.0004934928269414744,2
W4410495528,More than meets the eye: Feature concerns and suggestions in mobile XR app reviews,2025,0.0004919332569891499,2
W4402101192,LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages,2024,0.0004916985910909659,2
W4408321847,Multimodal retrieval-augmented generation for financial documents: image-centric analysis of charts and tables with large language models,2025,0.0004912406140252106,2
W4409270119,Securing NLP Systems: A Comprehensive AI-Based Approach,2025,0.0004907302000779121,2
W4407336703,(Chinavis 2024) TextLens: large language models-powered visual analytics enhancing text clustering,2025,0.000490533583406158,2
W2966774251,Knowledgeable Storyteller: A Commonsense-Driven Generative Model for Visual Storytelling,2019,0.0004898971774512331,2
W4384656691,Benchmarking Middle-Trained Language Models for Neural Search,2023,0.0004892887175699979,2
W4409767054,Emerging Data Practices: Data Work in the Era of Large Language Models,2025,0.00048823525165442484,2
W4402353454,Exploring and Improving Consistency in Large Language Models for Multiple-Choice Question Assessment,2024,0.00048813797183722775,2
W2898936689,Sequence Classification with Human Attention,2018,0.00048812503534037885,2
W4385516995,Pre-Trained Model-Based NFR Classification: Overcoming Limited Data Challenges,2023,0.0004867080908565262,2
W2995925258,You CAN Teach an Old Dog New Tricks! On Training Knowledge Graph Embeddings,2020,0.00048589425380709313,2
W4409254463,Research on Legal Question Answering System with Retrieval-Augmented Large Language Models,2025,0.00048536236101329774,2
W4408466330,Negation Scope Conversion for Unifying Negation-Annotated Datasets,2025,0.00048369854428463675,2
W4403791927,Making Large Language Models Perform Better in Knowledge Graph Completion,2024,0.00048322516308286605,2
W4386587785,Overview of BioASQ 2023: The Eleventh BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering,2023,0.0004832200999164444,2
W4390493562,Semantic Compression with Large Language Models,2023,0.00048287633408298364,2
W3209145439,Improving legal judgment prediction through reinforced criminal element extraction,2021,0.00048285644212216894,2
W4387267812,Knowledge-Driven Online Multimodal Automated Phenotyping System,2023,0.00048267559396088315,2
W4200455899,Deep learning for patent landscaping using transformer and graph embedding,2021,0.00048102840394726186,2
W2962970011,Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving,2017,0.00048091750100416836,2
W4399487368,ARG-Net: Adversarial Robust Generalized Network to Defend Against Word-Level Textual Adversarial Attacks,2024,0.00047749415968208393,2
W3113170987,Communicative Message Passing for Inductive Relation Reasoning,2021,0.0004767152282705709,2
W4290738699,Thirty years of artificial intelligence and law: the third decade,2022,0.0004754248694381691,2
W3175115403,"Anonymisation Models for Text Data: State of the art, Challenges and Future Directions",2021,0.0004753995150572093,2
W4401298243,Assessing Dimensions of Thought Disorder with Large Language Models: The Tradeoff of Accuracy and Consistency,2024,0.0004742482629575056,2
W3013629728,Shaping Visual Representations with Language for Few-Shot Classification,2020,0.0004736955065256149,2
W4398138749,MOSS: An Open Conversational Large Language Model,2024,0.00046821791699271195,2
W4394000434,Interpretable answer retrieval based on heterogeneous network embedding,2024,0.00046772613357283466,2
W3217331066,Data science approaches to confronting the COVID-19 pandemic: a narrative review,2021,0.000466253474110421,2
W3093811011,A novel joint biomedical event extraction framework via two-level modeling of documents,2020,0.00046618783364188953,2
W3040863728,Meta-Learning Requires Meta-Augmentation,2020,0.00046587310959093437,2
W3124180615,Sentiment Polarity Detection for Software Development,2017,0.0004655831995047327,2
W3097513514,A Survey of Text Data Augmentation,2020,0.00046339592572426695,2
W4387846647,Neural Disentanglement of Query Difficulty and Semantics,2023,0.0004630588077066623,2
W2955845608,Hierarchical Matching Network for Crime Classification,2019,0.00046166753397987184,2
W4392876888,Difficulty-controllable question generation over knowledge graphs: A counterfactual reasoning approach,2024,0.0004598218488583133,2
W4320507246,Evaluation of Incremental Entity Extraction with Background Knowledge and Entity Linking,2022,0.00045903947702307415,2
W4385483069,CAKT: Coupling contrastive learning with attention networks for interpretable knowledge tracing,2023,0.0004589420835941659,2
W4288384170,Learning representations for gene ontology terms by jointly encoding graph structure and textual node descriptors,2022,0.00045733081729810483,2
W2991361764,It Takes Nine to Smell a Rat: Neural Multi-Task Learning for Check-Worthiness Prediction,2019,0.0004544499532301592,2
W4391572936,VEM$$^2$$L: an easy but effective framework for fusing text and structure knowledge on sparse knowledge graph completion,2024,0.00045348234774086893,2
W4402704595,Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation,2024,0.00045218334526922645,2
W2950738719,A Decomposable Attention Model for Natural Language Inference,2016,0.00045010587171681273,2
W4396901469,Efficient Compression of Large Language Models: A Case Study on Llama 2 with 13B Parameters,2024,0.00044918953683903016,2
W3003943678,Transfer Learning for Risk Classification of Social Media Posts: Model Evaluation Study,2020,0.0004485714690972559,2
W3004280374,Vec2graph: A Python Library for Visualizing Word Embeddings as Graphs,2020,0.0004484153168693853,2
W2971094176,Tree-structured Decoding for Solving Math Word Problems,2019,0.000447782447324301,2
W2798139452,Adversarial Example Generation with Syntactically Controlled Paraphrase Networks,2018,0.000444163640529133,2
W4382630911,KR4SL: knowledge graph reasoning for explainable prediction of synthetic lethality,2023,0.00044400705679474594,2
W4400723242,Arithmetic with language models: From memorization to computation,2024,0.00044198680053044625,2
W4409254893,Detecting Persuasion in Financial Short Texts: A Computational Approach,2025,0.00044065111009524235,2
W4406378170,Personalized federated knowledge graph embedding with client-wise relation graph,2025,0.00043819174572415625,2
W4385567756,Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies,2023,0.00043801880824091173,2
W4409918358,Including Co-Relation via Concatenate Operator for Static and Temporal Knowledge Graph Embedding,2025,0.00043671764879786514,2
W4408071852,Pretrained transformers applied to clinical studies improve predictions of treatment efficacy and associated biomarkers,2025,0.00043584955717889014,2
W4327498760,"The CLEF-2023 CheckThat! Lab: Checkworthiness, Subjectivity, Political Bias, Factuality, and Authority",2023,0.0004346460134669341,2
W3034374701,A Re-evaluation of Knowledge Graph Completion Methods,2020,0.00043328903389981715,2
W3037843601,Feature-Based Explanations Don't Help People Detect Misclassifications of Online Toxicity,2020,0.000433104534178091,2
W4282829375,"A Concise Survey on Datasets, Tools and Methods for Biomedical Text",2022,0.000432237123170476,2
W4406460250,SR2ACM: A Methodical Approach for Translating Natural Language Security Requirements to Access Control Model,2024,0.00043154053781275023,2
W4388776291,Relphormer: Relational Graph Transformer for Knowledge Graph Representations,2023,0.0004314764523978736,2
W4408366404,Generalizable and scalable multistage biomedical concept normalization leveraging large language models,2025,0.00043006540509302746,2
W4310993626,A Paradigm Shift from “Human Writing” to “Machine Generation” in Personality Test Development: an Application of State-of-the-Art Natural Language Processing,2022,0.0004292559327744411,2
W4399476954,Automatic question-answer pairs generation using pre-trained large language models in higher education,2024,0.0004289986538465288,2
W2751627669,Visual Exploration of Semantic Relationships in Neural Word Embeddings,2017,0.00042887552639304987,2
W4392369006,"Evaluating Large Language Models: ChatGPT-4, Mistral 8x7B, and Google Gemini Benchmarked Against MMLU",2024,0.00042801307786356643,2
W4406151683,Using Computational Models to Detect Autistic Tendencies for Children from their Story Book Narratives,2025,0.00042768070510880646,2
W4409962616,Beyond words: evaluating large language models in transportation planning,2025,0.00042681416649744026,2
W3089285634,Ape210K: A Large-Scale and Template-Rich Dataset of Math Word Problems,2020,0.0004268053781721208,2
W4384890771,SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval,2023,0.00042567713446985036,2
W4390532736,Enhancing Heterogeneous Knowledge Graph Completion with a Novel GAT-based Approach,2024,0.0004255640361800318,2
W4410374560,Sentiment analysis and summarization with ChatGPT: implications for sales prediction,2025,0.0004243359128570633,2
W4406039094,Temporal knowledge graph completion based on product space and contrastive learning of commonsense,2025,0.00042306768427997557,2
W4224981510,Monant Medical Misinformation Dataset,2022,0.0004224771665346726,2
W4399391277,RA-CFGPT: Chinese financial assistant with retrieval-augmented large language model,2024,0.00042202320870343615,2
W4402324956,Large language multimodal models for new-onset type 2 diabetes prediction using five-year cohort electronic health records,2024,0.00042162953010660653,2
W4392908117,The Inadequacy of Reinforcement Learning From Human Feedback—Radicalizing Large Language Models via Semantic Vulnerabilities,2024,0.00042136803060366265,2
W4391992225,Application of large language models in professional fields,2023,0.0004212418038348041,2
W4296580689,Precision fMRI reveals that the language-selective network supports both phrase-structure building and lexical access during language production,2022,0.00042042019786424454,2
W4385571952,Enhancing Neural Topic Model with Multi-Level Supervisions from Seed Words,2023,0.00042033514048264026,2
W4386616609,Prediction during language comprehension: what is next?,2023,0.00042033091220542546,2
W4406313140,"Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology, Results, and Challenges",2025,0.0004201078843195673,2
W2946606955,Old is Gold: Linguistic Driven Approach for Entity and Relation Linking of Short Text,2019,0.00041981388001378987,2
W4391578379,Claude 2.0 large language model: Tackling a real-world classification problem with a new iterative prompt engineering approach,2024,0.0004193335421768548,2
W4385570989,Unsupervised Melody-to-Lyrics Generation,2023,0.00041842244393564335,2
W4405867157,xMEN: a modular toolkit for cross-lingual medical entity normalization,2024,0.0004183629749513421,2
W2981133974,Classifying cancer pathology reports with hierarchical self-attention networks,2019,0.00041562731631816097,2
W4406917752,Evolving Chain-of-Thought and automatic text annotation based intent classification method for wireless network,2025,0.00041555462692976374,2
W4212867238,Beyond NED,2022,0.0004154832964192823,2
W4408130327,A neurobiologically inspired model of sentence comprehension,2025,0.00041543356961388773,2
W4399557940,CLSESSP: Contrastive learning of sentence embedding with strong semantic prototypes,2024,0.00041439674234093507,2
W3167292670,Relational Message Passing for Knowledge Graph Completion,2021,0.0004126351333098415,2
W4393994727,Language models accurately infer correlations between psychological items and scales from text alone,2024,0.0004123060349629684,2
W4396823873,Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering,2024,0.0004105564609637726,2
W2806168354,Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia,2018,0.0004095902520882894,2
W4226150360,ALP: Data Augmentation Using Lexicalized PCFGs for Few-Shot Text Classification,2022,0.00040754405581030096,2
W4393906041,SeSICL: Semantic and Structural Integrated Contrastive Learning for Knowledge Graph Error Detection,2024,0.0004073187655248463,2
W4321597234,A large dataset of semantic ratings and its computational extension,2023,0.0004073080235570892,2
W4311165730,KGML-xDTD: A Knowledge Graph-based Machine Learning Framework for Drug Treatment Prediction and Mechanism Description,2022,0.00040584602927053393,2
W4400255044,Teaching Materials for Reading in a Professional Context to Improve the Reading Skills of English Education Program Students,2024,0.00040510075745336103,2
W4406761441,An Ensemble Framework for Text Classification,2025,0.0004031386124183653,2
W4401241231,"Efficiency-Driven Custom Chatbot Development: Unleashing LangChain, RAG, and Performance-Optimized LLM Fusion",2024,0.00040237259121494415,2
W4387250954,A Theoretical Framework for AI Models Explainability with Application in Biomedicine,2023,0.0004007125965646308,2
W4396760140,SIDU-TXT: An XAI algorithm for NLP with a holistic assessment approach,2024,0.0004006913360654985,2
W4410552705,Identifying Disinformation on the Extended Impacts of COVID-19: Methodological Investigation Using a Fuzzy Ranking Ensemble of Natural Language Processing Models (Preprint),2025,0.0004000634276531088,2
W4410556555,Identifying Disinformation on the Extended Impacts of COVID-19: Methodological Investigation Using a Fuzzy Ranking Ensemble of Natural Language Processing Models,2025,0.0004000634276531088,2
W4385767986,FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer,2023,0.0003983459031815185,2
W4392372817,Incorporating evidence into mental health Q&amp;A: a novel method to use generative language models for validated clinical content extraction,2024,0.00039749497889737464,2
W3029492690,Annotating and Analyzing Biased Sentences in News Articles using Crowdsourcing,2020,0.0003974417430845054,2
W4392384220,Multi-Granular Text Classification with Minimal Supervision,2024,0.000396812179801087,2
W2958089299,A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI,2020,0.00039491646613790034,2
W4366999416,SAILER: Structure-aware Pre-trained Language Model for Legal Case Retrieval,2023,0.00039458386424427814,2
W4387968110,ECENet: Explainable and Context-Enhanced Network for Muti-modal Fact verification,2023,0.00039427094419912297,2
W4389955489,An information fusion based approach to context-based fine-tuning of GPT models,2023,0.0003937578618707855,2
W4410392558,Cross-Lingual Entity Linking Using GPT Models in Radiology Abstracts,2025,0.0003931095268255528,2
W4394733949,Entity neighborhood awareness and hierarchical message aggregation for inductive relation prediction,2024,0.0003928777356305447,2
W4400524696,GraphGPT: Graph Instruction Tuning for Large Language Models,2024,0.00039226299158248105,2
W2969263964,Are We Safe Yet? The Limitations of Distributional Features for Fake News Detection.,2019,0.0003921770202362916,2
W4399803382,Language is primarily a tool for communication rather than thought,2024,0.00039101233537931475,2
W4406163678,The Frontier of Data Erasure: A Survey on Machine Unlearning for Large Language Models,2025,0.00038911160642592725,2
W4401907124,Complex Knowledge Base Question Answering via Structure and Content Dual-Driven Method,2024,0.0003887720605662864,2
W4402057292,A large-scale audit of dataset licensing and attribution in AI,2024,0.00038634512411899266,2
W4393239266,Large Language Models and the Future of Organization Theory,2024,0.0003854146339231903,2
W2927032858,A clinical text classification paradigm using weak supervision and deep representation,2019,0.0003821876904406598,2
W4407695356,Do LLMs write like humans? Variation in grammatical and rhetorical styles,2025,0.00038166798195226644,2
W4410486525,Emails by LLMs: A Comparison of Language in AI-Generated and Human-Written Emails,2025,0.000381654739872973,2
W4293262073,Making Adversarially-Trained Language Models Forget with Model Retraining: A Case Study on Hate Speech Detection,2022,0.00038129002805506235,2
W4385572070,Team ISCL_WINTER at SemEval-2023 Task 12:AfriSenti-SemEval: Sentiment Analysis for Low-resource African Languages using Twitter Dataset,2023,0.0003809857037743803,2
W3086560451,"Explanation in AI and law: Past, present and future",2020,0.00038083067006749804,2
W4410176770,Semantic Processing of Argument Structure during Naturalistic Story Listening: Evidence from Computational Modeling on fMRI,2025,0.0003807997567294054,2
W4403582442,Towards Completeness-Oriented Tool Retrieval for Large Language Models,2024,0.00037970369801753343,2
W3000752281,Graph Constrained Reinforcement Learning for Natural Language Action Spaces,2020,0.00037766553003416446,2
W4386852102,"A systematic review on media bias detection: What is media bias, how it is expressed, and how to detect it",2023,0.0003775609711046322,2
W3156450083,OCTIS: Comparing and Optimizing Topic models is Simple!,2021,0.0003775188899627283,2
W4281293760,ARNN-QA: Adaptive Recurrent Neural Network with feature optimization for incremental learning-based Question Answering system,2022,0.0003761001823320267,2
W4249111509,Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020,2020,0.000375742228533256,2
W4392056731,Using natural language processing to analyze unstructured patient-reported outcomes data derived from electronic health records for cancer populations: a systematic review,2024,0.00037329572772648665,2
W4386504863,Can GPT-3 Perform Statutory Reasoning?,2023,0.00037240106502411084,2
W4406947254,Toward cultural interpretability: A linguistic anthropological framework for describing and evaluating large language models,2025,0.0003722378326027261,2
W3186135811,Towards Unifying Feature Attribution and Counterfactual Explanations: Different Means to the Same End,2021,0.00037004303053102944,2
W4220686467,Improving exchange rate forecasting via a new deep multimodal fusion model,2022,0.000369493524283822,2
W3115652744,Self-Supervised Hyperboloid Representations from Logical Queries over Knowledge Graphs,2021,0.0003676438333583213,2
W3130138360,Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics,2021,0.0003669362052929848,2
W4391577874,Extracting adverse drug events from clinical Notes: A systematic review of approaches used,2024,0.0003662782224931713,2
W4409098905,Mental Health in the Digital Era-NLP Models for Depression and Suicidal Tendency Detection,2025,0.00036614565145818727,2
W4387185528,The Emotions of the Crowd: Learning Image Sentiment from Tweets via Cross-Modal Distillation,2023,0.0003660234105791153,2
W3128431233,Spark NLP: Natural Language Understanding at Scale,2021,0.0003651666786982935,2
W3090760060,Overview of CheckThat! 2020: Automatic Identification and Verification of Claims in Social Media,2020,0.00036490354166670723,2
W3013605954,Clinical Text Data in Machine Learning: Systematic Review,2020,0.0003645148010562822,2
W2742940593,A Hybrid Framework for Text Modeling with Convolutional RNN,2017,0.00036448556152291536,2
W4386390787,A template-based approach for question answering over knowledge bases,2023,0.00036402204262874524,2
W4400773813,Automated Scoring of Constructed Response Items in Math Assessment Using Large Language Models,2024,0.0003615947145194714,2
W4393946785,Improving dense retrieval models with LLM augmented data for dataset search,2024,0.0003615554740255765,2
W3100117936,Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks,2020,0.00035811502485724706,2
W4393333899,Automatic information extraction from Employment Tribunal judgements using large language models,2024,0.000356957184650236,2
W4381573351,Representation Sparsification with Hybrid Thresholding for Fast SPLADE-based Document Retrieval,2023,0.0003564983446503434,2
W4317932826,An Automatic Generation of Heterogeneous Knowledge Graph for Global Disease Support: A Demonstration of a Cancer Use Case,2023,0.0003551298630522863,2
W4409968028,Knowledge-Guided Reasoning Chain of Pre-trained LLM in Industrial Domain,2025,0.00035456124614638424,2
W4390694654,A Case for Business Process-Specific Foundation Models,2024,0.000353162291851368,2
W4406942015,InRanker: Distilled Rankers for Zero-Shot Information Retrieval,2025,0.0003505173148576315,2
W4406277430,Estimating textual treatment effect via causal disentangled representation learning,2025,0.00035051098725788905,2
W4229072483,Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction – two sides of the same coin?,2022,0.00034944784344014196,2
W2799079975,Data-Driven Methods for Solving Algebra Word Problems,2018,0.0003490228679786601,2
W4392632244,Rag-Fusion: A New Take on Retrieval Augmented Generation,2024,0.00034883615956734165,2
W4392740458,CORAL: Expert-Curated Oncology Reports to Advance Language Model Inference,2024,0.0003487573261918501,2
W4401094131,Show Criminals’ True Color: Chinese Variant Toxic Text Restoration Based on Pointer-Generator Network,2024,0.000347793066375336,2
W4397008743,Multi-view fusion for instruction mining of large language model,2024,0.00034755211134874023,2
W4392902780,Forgetting Private Textual Sequences in Language Models Via Leave-One-Out Ensemble,2024,0.0003474339540183732,2
W4389670098,SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding,2023,0.0003466374223914116,2
W4408258990,A large language model-enabled machining process knowledge graph construction method for intelligent process planning,2025,0.0003444244797165142,2
W4409054952,Exploring the prospects of multimodal large language models for Automated Emotion Recognition in education: Insights from Gemini,2025,0.0003443069479204818,2
W4394994622,High-Order Neighbors Aware Representation Learning for Knowledge Graph Completion,2024,0.0003439836848776202,2
W4386588806,Supervised Machine-Generated Text Detectors: Family and Scale Matters,2023,0.0003423646268627003,2
W4409183263,Large Language Model Can Be a Foundation for Hidden Rationale-Based Retrieval,2025,0.0003407547214260464,2
W3082665562,Understanding the role of individual units in a deep neural network,2020,0.00033990781860388034,2
W4385453176,ArEmotive Bridging the Gap: Automatic Ontology Augmentation Using Zero-Shot Classification for Fine-Grained Sentiment Analysis of Arabic Text,2023,0.0003391338839523017,2
W4408072827,Comparing Traditional Book Wisdom with Large Language Model’s Guidance on Time and Stress Management,2025,0.00033908527232968326,2
W4293344661,Path and future of artificial intelligence in the field of justice: a systematic literature review and a research agenda,2022,0.0003390759480818279,2
W4408242371,On the Gap Between AI-Generated and Human-Written Patent Texts,2025,0.0003390403681037182,2
W4391230471,Mconvkgc: a novel multi-channel convolutional model for knowledge graph completion,2024,0.00033824119388836866,2
W4409732945,Augmenting Dark Patterns Text Data by Leveraging Large Language Models: A Multi-agent Framework and Parameter-Efficient Fine-Tuning,2025,0.0003379540764217537,2
W4406771743,A Flexible Knowledge Graph Error Detection Framework Combined with Semantic Information,2025,0.0003376350244861557,2
W2773143256,A deep network model for paraphrase detection in short text messages,2018,0.0003352341251109479,2
W4390607929,Identifying and Extracting Rare Diseases and Their Phenotypes with Large Language Models,2024,0.0003346965836191384,2
W4226112939,Transfer Learning Approaches for Building Cross-Language Dense Retrieval Models,2022,0.0003344242041918015,2
W4409852358,Exploring Prompt Injection: Methodologies and Risks with an Interactive Chatbot Demonstration,2025,0.0003338876597524296,2
W4396831993,Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels,2024,0.0003315624182488643,2
W2752337926,Automated Crowdturfing Attacks and Defenses in Online Review Systems,2017,0.0003301586872960031,2
W4388034436,Dissecting neural computations in the human auditory pathway using deep neural networks for speech,2023,0.0003297672455715831,2
W4390450387,Towards Mental Health Analysis in Social Media for Low-resourced Languages,2023,0.00032877661096284826,2
W4320507288,RAILD: Towards Leveraging Relation Features for Inductive Link Prediction In Knowledge Graphs,2022,0.00032746214863043065,2
W4399269329,"Enhancing Complex Linguistic Tasks Resolution Through Fine-Tuning LLMs, RAG and Knowledge Graphs (Short Paper)",2024,0.0003271585260767078,2
W4391836239,"Mathematics, word problems, common sense, and artificial intelligence",2024,0.000327047486604902,2
W4361199350,Technological forecasting based on estimation of word embedding matrix using LSTM networks,2023,0.00032698069298221164,2
W3100776995,Visually Grounded Compound PCFGs,2020,0.0003260050802821003,2
W4385572830,Re3: Generating Longer Stories With Recursive Reprompting and Revision,2022,0.0003253478863840926,2
W4392240059,H3D-Transformer: A Heterogeneous 3D (H3D) Computing Platform for Transformer Model Acceleration on Edge Devices,2024,0.0003220872249369214,2
W4384342195,The default network dominates neural responses to evolving movie stories,2023,0.00032193930683528535,2
W4406518786,Hypnos: A domain-specific large language model for anesthesiology,2025,0.00031994687013503113,2
W4406071956,"Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting",2025,0.0003184526562336454,2
W2951515642,A2N: Attending to Neighbors for Knowledge Graph Inference,2019,0.0003180558627323258,2
W4408800659,Explainable TabNet Transformer-based on Google Vizier Optimizer for Anomaly Intrusion Detection System,2025,0.0003177510751241977,2
W4390064615,Zero-shot information extraction from radiological reports using ChatGPT,2023,0.00031774018217932225,2
W2557579533,Deep Variational Information Bottleneck,2016,0.0003175034782562438,2
W4407028442,Knowledge Graph Applications and Multi-Relation Learning for Drug Repurposing: A Scoping Review,2025,0.0003156154124231509,2
W4396722687,Back to the Future: Towards Explainable Temporal Reasoning with Large Language Models,2024,0.00031454760963413444,2
W4402261986,Investigating the Robustness of Arabic Offensive Language Transformer-Based Classifiers to Adversarial Attacks,2024,0.00031424332951840287,2
W3111538899,AIST: An Interpretable Attention-Based Deep Learning Model for Crime Prediction,2023,0.0003138116786570491,2
W2953647525,Team QCRI-MIT at SemEval-2019 Task 4: Propaganda Analysis Meets Hyperpartisan News Detection,2019,0.0003132154269845183,2
W4400526005,Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive Multimodal Knowledge Graph Completion,2024,0.0003131293135955986,2
W3203041483,The reporting quality of natural language processing studies: systematic review of studies of radiology reports,2021,0.00031211666700723626,2
W4408751761,Entity Disambiguation Using Ensemble Classification,2025,0.0003120489061634506,2
W4407611747,Optimizing Self-training Sample Selection for Euphemism Detection in Special Scenarios,2025,0.0003118609901715774,2
W4395050972,Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework,2024,0.0003103680079163053,2
W4386333898,What are large language models supposed to model?,2023,0.00031007041136278066,2
W4407042009,Event extraction based on self-data augmentation with large language models,2025,0.00030976246455530513,2
W4375840656,Automated Test Case Generation Using T5 and GPT-3,2023,0.00030914449348355493,2
W4281989354,Disentangled Ontology Embedding for Zero-shot Learning,2022,0.0003065150431880714,2
W4393048251,Inclusion in Linguistics,2024,0.0003061315806363445,2
W4400651718,Simple Data Transformations for Mitigating the Syntactic Similarity to Improve Sentence Embeddings at Supervised Contrastive Learning,2024,0.00030357326060147883,2
W4408342914,Automatic evaluation and enhancement of reading strategies in English reading comprehension based on the BERT model,2025,0.0003034841990256241,2
W4366548757,False perspectives on human language: Why statistics needs linguistics,2023,0.0003031917377114665,2
W4407830908,AUTOMATED DETECTION OF EMOTION IN CENTRAL BANK COMMUNICATION: A WARNING,2025,0.00030292270676547325,2
W4321434205,Feature-Based Graph Backdoor Attack in the Node Classification Task,2023,0.0003024369258028188,2
W3089862415,Machine Learning and Natural Language Processing in Mental Health: Systematic Review,2020,0.0003020866363458924,2
W4206254927,On the relationship between similar requirements and similar software,2022,0.00030017418640557436,2
W3183229893,Report on the future conversations workshop at CHIIR 2021,2021,0.00029903701977388787,2
W2198584637,An End-to-End Neural Network for Polyphonic Piano Music Transcription,2016,0.00029903701977388787,2
W4405114593,Simple and effective embedding model for single-cell biology built from ChatGPT,2024,0.0002983285613103097,2
W3008918149,Explainable Authorship Verification in Social Media via Attention-based Similarity Learning,2019,0.0002979744325991199,2
W3174368915,Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks,2021,0.00029697027022895724,2
W4409762035,Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base for Geospatial Code Generation with Large Language Models,2025,0.0002967996862115482,2
W2990953743,QKD: Quantization-aware Knowledge Distillation,2019,0.00029580067331130404,2
W4406642447,Large Language Models vs Human for Classifying Clinical Documents,2025,0.00029531889508939565,2
W4385460063,The Politics of Right and Wrong: Moral Appeals in Political Communication over Six Decades in Ten Western Democracies,2023,0.00029406664577324954,2
W4282916217,JiuZhang: A Chinese Pre-trained Language Model for Mathematical Problem Understanding,2022,0.00029319222499357987,2
W4409991618,Research on Automatic Question Generation Methods for Niche Subjects Based on Large Language Models,2025,0.0002931430880983959,2
W4389132345,TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records,2023,0.0002926446229250144,2
W4221148482,Exploring and Adapting Chinese GPT to Pinyin Input Method,2022,0.000292550342026585,2
W3094242471,Decoupled Graph Convolution Network for Inferring Substitutable and Complementary Items,2020,0.0002924999585843573,2
W4398764576,Natural language processing systems for extracting information from electronic health records about activities of daily living. A systematic review,2024,0.00029209461585063705,2
W2991642412,Two Distinct Neural Timescales for Predictive Speech Processing,2019,0.00029130772001826147,2
W4362654035,COLIEE 2022 Summary: Methods for Legal Document Retrieval and Entailment,2023,0.000290871277093964,2
W4390822940,"Overview and Discussion of the Competition on Legal Information, Extraction/Entailment (COLIEE) 2023",2024,0.00029061672590857874,2
W4398243240,Large language models: Expectations for semantics-driven systems engineering,2024,0.0002902796362554531,2
W4226204319,Voxelwise Encoding Models Show That Cerebellar Language Representations Are Highly Conceptual,2021,0.0002898555760855779,2
W2980346149,Deep Feature Fusion Model for Sentence Semantic Matching,2019,0.00028794984142826313,2
W4406799871,ChatGPT and L2 Chinese writing: evaluating the impact of model version and prompt language on automated corrective feedback,2025,0.000287276353067608,2
W3216092102,Class imbalance in out-of-distribution datasets: Improving the robustness of the TextCNN for the classification of rare cancer types,2021,0.00028678887147242933,2
W4312140420,Complex Knowledge Base Question Answering for Intelligent Bridge Management Based on Multi-Task Learning and Cross-Task Constraints,2022,0.0002855952281758819,2
W4361017888,Semantic Representations during Language Comprehension Are Affected by Context,2023,0.0002846512020469694,2
W3016564562,Privacy-Preserving Deep Learning NLP Models for Cancer Registries,2020,0.00028456224144643964,2
W3155775551,Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion,2021,0.00028448222698800805,2
W4285605214,Meta-Learning Based Knowledge Extrapolation for Knowledge Graphs in the Federated Setting,2022,0.000284454728586838,2
W4285217123,Uncovering Values: Detecting Latent Moral Content from Natural Language with Explainable and Non-Trained Methods,2022,0.00028408597660342687,2
W3035451094,Using Language Processing and Speech Analysis for the Identification of Psychosis and Other Disorders,2020,0.00028296439674675976,2
W4376614852,Automated Subject Identification using the Universal Decimal Classification: The ANN Approach,2023,0.0002824128184700172,2
W3164109169,Generating Synthetic Training Data for Supervised De-Identification of Electronic Health Records,2021,0.00028199146061569635,2
W4408411093,Enhancing diagnostic capability with multi-agents conversational large language models,2025,0.0002809393169726061,2
W4384636969,Recipe-MPR: A Test Collection for Evaluating Multi-aspect Preference-based Natural Language Retrieval,2023,0.0002801018177234989,2
W4322625799,Top-down information shapes lexical processing when listening to continuous speech,2023,0.0002798720215458676,2
W3094657717,Prediction of breast cancer distant recurrence using natural language processing and knowledge-guided convolutional neural network,2020,0.00027915057583994006,2
W3006631416,Sparse low rank factorization for deep neural network compression,2020,0.0002789783769563536,2
W4280604898,Machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching,2022,0.0002789750601463031,2
W4409852399,ASC: Aggregating Sentence-Level Classifications for Multi-label Long Text Classification,2025,0.0002786280905211252,2
W4285495954,A Narrative Literature Review of Natural Language Processing Applied to the Occupational Exposome,2022,0.00027839568183964623,2
W3115686121,SMART: A Situation Model for Algebra Story Problems via Attributed Grammar,2021,0.00027803718666371234,2
W4387185333,Cognitive Effects in Large Language Models,2023,0.0002777985763126705,2
W4212989157,Gradient Acceptability and Linguistic Theory,2021,0.00027740066193415795,2
W4389010426,OpinionConv: Conversational Product Search with Grounded Opinions,2023,0.000276845865859929,2
W4388471935,Multi-hop community question answering based on multi-aspect heterogeneous graph,2023,0.00027357562362813527,2
W2963106052,Multi-Task Learning for Argumentation Mining in Low-Resource Settings,2018,0.00027311443189203833,2
W4389245454,"An Improved Transformer-based Model for Detecting Phishing, Spam, and Ham: A Large Language Model Approach",2023,0.0002729631900784581,2
W4394995253,"Enhancing Customer Segmentation Using Large Language Models (LLMs) and Deterministic, Independent-of-Corpus Embeddings (DICE)",2024,0.00027250569694247344,2
W4319332823,Large language models can segment narrative events similarly to humans,2025,0.00027236650137496207,2
W2607219512,Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks,2018,0.0002710366115439153,2
W4304183734,An Entity Linking Algorithm Derived from Graph Convolutional Network and Contextualized Semantic Relevance,2022,0.00026893054140035447,2
W4390990543,Unraveling energy justice in NYC urban buildings through social media sentiment analysis and transformer deep learning,2024,0.0002688861388770301,2
W4406603177,CARDBiomedBench: A Benchmark for Evaluating Large Language Model Performance in Biomedical Research,2025,0.00026544255644938707,2
W4367609927,Semantic reconstruction of continuous language from non-invasive brain recordings,2023,0.0002646490363443271,2
W4408102719,Solving the enigma: Enhancing faithfulness and comprehensibility in explanations of deep networks,2025,0.000264595416628023,2
W4408985659,Skill Learning Using Process Mining for Large Language Model Plan Generation,2025,0.00026374617669924175,2
W3100144085,Adversarial Examples for CNN-Based SAR Image Classification: An Experience Study,2020,0.0002626685528074248,2
W4392606373,Beyond Words: A Comparative Analysis of LLM Embeddings for Effective Clustering,2024,0.0002625963769383285,2
W3007523830,Comprehend Medical: A Named Entity Recognition and Relationship Extraction Web Service,2019,0.0002620243306674307,2
W4406033330,Using transformer-based models and social media posts for heat stroke detection,2025,0.00026200804656453,2
W4393090023,"The CLEF-2024 CheckThat! Lab: Check-Worthiness, Subjectivity, Persuasion, Roles, Authorities, and Adversarial Robustness",2024,0.0002618730347068092,2
W4400872202,Lingdan: enhancing encoding of traditional Chinese medicine knowledge for clinical reasoning tasks with large language models,2024,0.0002618678240235273,2
W4317382484,Vietnamese Fact Checking based on the Knowledge Graph and Deep Learning,2022,0.0002617519613592483,2
W3099836348,CoDEx: A Comprehensive Knowledge Graph Completion Benchmark,2020,0.0002612188125445957,2
W3211781298,"COVID-19 in Bulgarian Social Media: Factuality, Harmfulness, Propaganda, and Framing",2021,0.0002605466665177165,2
W4308294451,Feature-space selection with banded ridge regression,2022,0.0002604785612358881,2
W4400527956,A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models,2024,0.0002585201155379988,2
W4391676924,Enhancing Continuous Auditing with Large Language Models: A Framework for Cross-Verification Using Exogenous Textual Data,2024,0.0002583248626530992,2
W4385525774,Comparing sustainable product hashtags: Insights from a historical twitter dataset,2023,0.0002583135833690025,2
W3204615515,A semantic approach to post-retrieval query performance prediction,2021,0.0002575687942021313,2
W4290927860,Joint Knowledge Graph Completion and Question Answering,2022,0.0002574596839090954,2
W4389519810,Joint Learning for Legal Text Retrieval and Textual Entailment: Leveraging the Relationship between Relevancy and Affirmation,2023,0.00025649257979717987,2
W2804439688,Moon IME: Neural-based Chinese Pinyin Aided Input Method with Customizable Association,2018,0.000255710074576852,2
W3035336738,Knowledge Graph-based Event Embedding Framework for Financial Quantitative Investments,2020,0.0002552338332896077,2
W4390403247,Fine-tuning Large Language Models for Rare Disease Concept Normalization,2023,0.00025282082804376383,2
W4408546772,Prompt Framework for Extracting Scale-Related Knowledge Entities from Chinese Medical Literature: Development and Evaluation Study,2025,0.00025268938542225515,2
W4391385527,Emergence of syntax and word prediction in an artificial neural circuit of the cerebellum,2024,0.0002510372411842279,2
W4378627669,Dual-use implications of AI text generation,2023,0.00024870598233933374,2
W2915478146,Improving Neural Network Quantization without Retraining using Outlier Channel Splitting,2019,0.00024749625989731774,2
W4283784468,Multisource financial sentiment analysis for detecting Bitcoin price change indications using deep learning,2022,0.0002472357801867489,2
W4384642334,Relation-Aware Multi-Positive Contrastive Knowledge Graph Completion with Embedding Dimension Scaling,2023,0.00024694626900583535,2
W4409989924,A disambiguation method for potential ambiguities in Chinese based on knowledge graphs and large language model,2025,0.00024630960715275207,2
W4294325323,Use of Data Augmentation Techniques in Detection of Antisocial Behavior Using Deep Learning Methods,2022,0.0002455546912197898,2
W4407187061,Targeted generative data augmentation for automatic metastases detection from free-text radiology reports,2025,0.0002454486814461753,2
W2996681978,Graph Constrained Reinforcement Learning for Natural Language Action Spaces,2020,0.0002445966469092786,2
W4406940792,ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation,2025,0.00024366147065069333,2
W4394895522,A Quantization Approach for the Reduced Size of Large Language Models,2024,0.00024287241964696365,2
W2911462778,Evaluation and accurate diagnoses of pediatric diseases using artificial intelligence,2019,0.00024222403229896047,2
W4408224774,MedicalGLM: A Pediatric Medical Question Answering Model with a quality evaluation mechanism,2025,0.00024058349061031498,2
W4403577762,LeDQA: A Chinese Legal Case Document-based Question Answering Dataset,2024,0.00023932321733535137,2
W4408955021,Enhancing Radiology Report Interpretation with Large-Scale Language Models: A Two-Stage Fine-Tuning Approach,2025,0.0002383838874195425,2
W4394943312,Large Language Models Are Poor Medical Coders — Benchmarking of Medical Code Querying,2024,0.0002379098374956846,2
W2927599034,Natural Language Processing for the Identification of Silent Brain Infarcts From Neuroimaging Reports,2019,0.00023697017856071036,2
W3035707368,ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding,2020,0.00023651992955813687,2
W4398218462,GastroBot: a Chinese gastrointestinal disease chatbot based on the retrieval-augmented generation,2024,0.00023622857421141082,2
W4390823535,Explainable text-based features in predictive models of crowdfunding campaigns,2024,0.00023596553452115575,2
W4310645210,Hi-BEHRT: Hierarchical Transformer-Based Model for Accurate Prediction of Clinical Events Using Multimodal Longitudinal Electronic Health Records,2022,0.00023508453835818912,2
W3081317618,Clinical information extraction for preterm birth risk prediction,2020,0.00023431578683801583,2
W4379207303,Making Sense of Citizens’ Input through Artificial Intelligence: A Review of Methods for Computational Text Analysis to Support the Evaluation of Contributions in Public Participation,2023,0.00023427620341119062,2
W4387966945,"Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet)",2023,0.00023354157414103512,2
W4384644539,Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models,2023,0.00023328562532332598,2
W4229029478,The Entropy of Morphological Systems in Natural Languages Is Modulated by Functional and Semantic Properties,2022,0.0002324365756422284,2
W4407392849,"Grammar Induction from Visual, Speech and Text",2025,0.00023075923352542017,2
W4410429768,Generating Clarifying Questions for Conversational Legal Case Retrieval without External Knowledge,2025,0.00023006114216037266,2
W4396661911,Semantic- and relation-based graph neural network for knowledge graph completion,2024,0.0002297293476905103,2
W4409951379,Detoxifying language model outputs: combining multi-agent debates and reinforcement learning for improved summarization,2025,0.00022698091547001254,2
W4406105510,Uncertainty modeling for inductive knowledge graph embedding,2025,0.00022637145590406863,2
W4409759796,Understanding the Student Confidence in Viva Responses Through Audio Analysis,2025,0.00022609051686386597,2
W3093937844,A Multidimensional Dataset Based on Crowdsourcing for Analyzing and Detecting News Bias,2020,0.00022587934701896964,2
W4409877648,Autonomous embodied navigation task generation from natural language dialogues,2025,0.00022574971181684106,2
W4406857279,Research on Parameter-Efficient Knowledge Graph Completion Methods and Their Performance in the Cybersecurity Field,2025,0.00022473479057294511,2
W3006942207,Text Similarity in Vector Space Models: A Comparative Study,2019,0.00022438446945690025,2
W4410315685,XLR-KGDD: leveraging LLM and RAG for knowledge graph-based explainable disease diagnosis using multimodal clinical information,2025,0.00022371644231360184,2
W3110875274,HAPI,2020,0.00022342847027250422,2
W4394789132,Advancing NLP models with strategic text augmentation: A comprehensive study of augmentation methods and curriculum strategies,2024,0.00022188167431312443,2
W4309623083,BERT-Log: Anomaly Detection for System Logs Based on Pre-trained Language Model,2022,0.00022110547954283554,2
W4409605213,Fake news detection using Hashtag context,2025,0.00022096401290957717,2
W2787296320,Deep neural networks for bot detection,2018,0.0002203609535297189,2
W4394718458,AI and narrative embeddings detect PTSD following childbirth via birth stories,2024,0.0002201873645845706,2
W2768377508,Hierarchical attention networks for information extraction from cancer pathology reports,2017,0.00022006743091374547,2
W4409281855,Elevating large language model reasoning ability with auto-enhanced zero-shot prompts,2025,0.000219643105531166,2
W4408075204,To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models,2025,0.00021893139111956036,2
W2963102202,EARL: Joint Entity and Relation Linking for Question Answering over Knowledge Graphs,2018,0.00021816437768530674,2
W4256179132,Modeling misretrieval and feature substitution in agreement attraction: A computational evaluation,2020,0.000216159860608862,2
W4385568110,Commonsense Knowledge Graph towards Super APP and Its Applications in Alipay,2023,0.00021438560395363102,2
W4321480042,Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts,2023,0.00021393998992386696,2
W4385694164,"Distinguishing ChatGPT(-3.5, -4)-generated and human-written papers through Japanese stylometric analysis",2023,0.00021374794554039798,2
W4407900932,Self-supervised learning for neural topic models with variance–invariance–covariance regularization,2025,0.00021333324421868633,2
W4407942790,Interleaving and cross-attention presents efficient knowledge graph embedding,2025,0.00021328976415845938,2
W4285601829,Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning,2022,0.0002127789611736717,2
W2912860574,Predicate constraints based question answering over knowledge graph,2019,0.00021181956046203754,2
W4408886313,Bilingual Dialogue Dataset with Personality and Emotion Annotations for Personality Recognition in Education,2025,0.00021094931395344805,2
W4405316960,Diff-PC: Identity-preserving and 3D-aware controllable diffusion for zero-shot portrait customization,2024,0.0002093618925362464,2
W4382239759,Analogical Inference Enhanced Knowledge Graph Embedding,2023,0.00020731392498421917,2
W4213450276,Efficient Prediction of Court Judgments Using an LSTM+CNN Neural Network Model with an Optimal Feature Set,2022,0.00020675984517715194,2
W3034751553,Encoding History with Context-aware Representation Learning for Personalized Search,2020,0.00020593371971674938,2
W4404970290,Mitigating Hallucination in Large Language Model by Leveraging Decoder Layer Contrasting,2024,0.00020562054459234637,2
W4409778058,Enhancing Accuracy and Explainability in Anomaly Classification with Large Language Models,2025,0.00020491403397668182,2
W4388144893,Detecting and Unmasking AI-Generated Texts through Explainable Artificial Intelligence using Stylistic Features,2023,0.00020487207095571286,2
W4400099044,Assessing the performance of large language models in literature screening for pharmacovigilance: a comparative study,2024,0.00020455928961919732,2
W4404823333,Mitigating the Bias of Large Language Model Evaluation,2024,0.00020278172867566285,2
W4391887109,MAEDAY: MAE for few- and zero-shot AnomalY-Detection,2024,0.0002026948179411115,2
W4405400063,Automatic essay scoring for natural language processing: feature extraction and scoring models,2024,0.00020242239699171718,2
W4292676168,Automated diagnosing primary open-angle glaucoma from fundus image by simulating human’s grading with deep learning,2022,0.00020239426403185996,2
W4406469500,Structure-Aware Conversational Legal Case Retrieval,2025,0.00020189799208041452,2
W4385570924,Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text,2023,0.00020104050797769483,2
W4408128017,Question-Answering (QA) Model for a Personalized Learning Assistant for Arabic Language,2025,0.00020009125554711706,2
W4386914096,SRSCL: A strong-relatedness-sequence-based fine-grained collective entity linking method for heterogeneous information networks,2023,0.00019991918225939836,2
W4400528385,LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset,2024,0.000199503071871432,2
W4399035903,CAPTAIN at COLIEE 2024: Large Language Model for Legal Text Retrieval and Entailment,2024,0.0001992101941641644,2
W4385807433,Artificial Intelligence’s new clothes? A system technology perspective,2023,0.00019841891356031747,2
W4399483233,On Artificial and Post-artificial Texts: Machine Learning and the Reader's Expectations of Literary and Non-literary Writing,2024,0.00019841891356031747,2
W3013555162,Born-Again Tree Ensembles,2020,0.00019820338784250707,2
W4366547579,Tracing and Visualizing Human-ML/AI Collaborative Processes through Artifacts of Data Work,2023,0.00019801875209544888,2
W2768346313,Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients,2018,0.00019799599373327236,2
W4409189376,Unified link prediction modeling for enhanced knowledge graph completion task,2025,0.0001978083891137746,2
W4385570675,Compounding Geometric Operations for Knowledge Graph Completion,2023,0.00019732780387630392,2
W4385571582,VTCC-NLP at SemEval-2023 Task 6:Long-Text Representation Based on Graph Neural Network for Rhetorical Roles Prediction,2023,0.00019641891773046014,2
W4406740223,"Categorising Corruption in the Vaccine Discourse: A General Taxonomy, Data Set, and Evaluation of LLMs for Classifying Corruption Dialogue in Social Media",2025,0.0001961870644606029,2
W4396655051,Using Large Language Models to Detect Self-Regulated Learning in Think-Aloud Protocols,2024,0.00019590020322380594,2
W4409720527,TELL-ME: Toward Personalized Explanations of Large Language Models,2025,0.0001950515965370742,2
W2963359213,Knowledge Graph Embedding With Iterative Guidance From Soft Rules,2018,0.00019467726812051606,2
W4394958222,Evaluating the language abilities of Large Language Models vs. humans: Three caveats,2024,0.00019431216334594994,2
W3166859509,BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction,2021,0.00019414163220588578,2
W3181074959,Effective Sparsification of Neural Networks with Global Sparsity Constraint,2021,0.00019255899994518862,2
W3102589552,Detecting Media Bias in News Articles using Gaussian Bias Distributions,2020,0.0001924612359677227,2
W4403203115,Topic Modeling for Faster Literature Screening Using Transformer-Based Embeddings,2024,0.00019205281821174362,2
W4323359744,EHR foundation models improve robustness in the presence of temporal distribution shift,2023,0.00019205002149845358,2
W4315705628,Natural Backdoor Attacks on Speech Recognition Models,2023,0.00019168466180679322,2
W2996899616,Relational Graph Neural Network with Hierarchical Attention for Knowledge Graph Completion,2020,0.00019112063775185567,2
W4387506531,A Comprehensive Review of the Latest Advancements in Large Generative AI Models,2023,0.00018877075367621524,2
W4408935240,Student Enrollment Consultation Q&amp;A Robot Based on Large Language Model,2025,0.0001883828361466612,2
W4406919506,Integrating Speech Recognition and NLP for Efficient Transcription Solutions,2025,0.0001883828361466612,2
W4397013501,Identification of patients’ smoking status using an explainable AI approach: a Danish electronic health records case study,2024,0.00018816126415983737,2
W2902516827,Clinical text classification with rule-based features and knowledge-guided convolutional neural networks,2019,0.00018814792083116533,2
W4385251138,"Using ChatGPT Standard Prompt Engineering Techniques in Lesson Preparation: Role, Instructions and Seed-Word Prompts",2023,0.00018806840248519491,2
W3125997628,A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence,2021,0.00018707648978578658,2
W3136558904,Communicating artificial neural networks develop efficient color-naming systems,2021,0.0001869963123525799,2
W2963408280,Neural Machine Translation Inspired Binary Code Similarity Comparison beyond Function Pairs,2019,0.00018698022889604044,2
W2996775350,Knowledge Graph Embedding via Graph Attenuated Attention Networks,2019,0.0001869655521242596,2
W2912907847,Automatic ICD code assignment of Chinese clinical notes based on multilayer attention BiRNN,2019,0.00018685925286512198,2
W4298144575,Semantic reconstruction of continuous language from non-invasive brain recordings,2022,0.00018611915272043672,2
W4313260873,Stock Price Prediction Using a Frequency Decomposition Based GRU Transformer Neural Network,2022,0.00018597249347625968,2
W4408080196,Text Summarization Using the T5 Transformer with Rouge Score,2025,0.00018594687276753496,2
W4391968384,Sustainable Shift: Analyzing Drivers for Low-Carbon Transportation Adoption in California’s Heavy-Duty and Off-Road Sectors,2024,0.00018594687276753496,2
W2794609696,AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation,2018,0.00018580322444947638,2
W4395028878,DF-DM: A foundational process model for multimodal data fusion in the artificial intelligence era,2024,0.00018574930312410725,2
W3115897805,Predicting mortality in critically ill patients with diabetes using machine learning and clinical notes,2020,0.000185397265888972,2
W3094602619,Predictive Student Modeling in Game-Based Learning Environments with Word Embedding Representations of Reflection,2020,0.00018457342345323027,2
W4409330220,Exploring Consumer Bias Patterns in Fashion E-Commerce Through LLM-Based Sentiment and Network Analysis,2025,0.00018454232893621238,2
W3215147048,Backdoor Attacks on Image Classification Models in Deep Neural Networks,2022,0.0001838319625785629,2
W2995543961,Maze Made Easy: Better and easier measurement of incremental processing difficulty,2019,0.00018353606219132009,2
W4410446500,The “LLM World of Words” English free association norms generated by large language models,2025,0.00018318740079267714,2
W4410592342,Deepfake Text Detection Using Deep Convolutional Neural Networks with Explainable Ai,2025,0.00018127547087544134,2
W4406859636,Scalable information extraction from free text electronic health records using large language models,2025,0.00018107190258710227,2
W4406421570,"Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines",2025,0.00018088150351882899,2
W4322631694,Using Natural Language Processing to Analyze Political Party Manifestos from New Zealand,2023,0.00018013503291558087,2
W4401622681,Relation labeling in product knowledge graphs with large language models for e-commerce,2024,0.00017995216720687762,2
W2810150893,Beyond Precision: A Study on Recall of Initial Retrieval with Neural Representations,2023,0.00017736781844708343,2
W4386574930,Improving information retrieval through correspondence analysis instead of latent semantic analysis,2023,0.00017736781844708343,2
W4409825362,Large language models for intelligent RDF knowledge graph construction: results from medical ontology mapping,2025,0.00017699383590523936,2
W4402512774,Large language models predict human sensory judgments across six modalities,2024,0.00017659176824604393,2
W4384789477,Information Retrieval from Legal Documents with Ontology and Graph Embeddings Approach,2023,0.0001763694096256094,2
W3015158377,Knowledge graph fusion for smart systems: A Survey,2020,0.00017563781710286342,2
W4402654757,Improved Models for Media Bias Detection and Subcategorization,2024,0.00017512991383767304,2
W3130747775,GHS-NET a generic hybridized shallow neural network for multi-label biomedical text classification,2021,0.0001744029879609808,2
W4401305431,Providing Citations to Support Fact-Checking: Contextualizing Detection of Sentences Needing Citation on Small Wikipedias,2024,0.00017437265819082652,2
W4408363728,Enhancing Traditional Chinese Medicine Question Answering and Semantic Reasoning via Historical Exam Retrieval and Sentence Similarity,2025,0.00017399548803870516,2
W4377232895,Relation-attention semantic-correlative knowledge graph embedding for inductive link prediction,2023,0.00017349916895587997,2
W3156584161,AraFacts: The First Large Arabic Dataset of Naturally Occurring Claims,2021,0.00017274285095967431,2
W2767852858,Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques,2017,0.0001723750946686897,2
W4389518735,From Words to Wires: Generating Functioning Electronic Devices from Natural Language Descriptions,2023,0.00017132345954954956,2
W4407196512,Accelerating knowledge graph and ontology engineering with large language models,2025,0.000168150581856823,2
W4319988693,Membership Inference Attacks With Token-Level Deduplication on Korean Language Models,2023,0.0001671601951891734,2
W4205149420,Named entity disambiguation in short texts over knowledge graphs,2022,0.00016527232232633137,2
W2967056613,Use of Natural Language Processing to Extract Clinical Cancer Phenotypes from Electronic Medical Records,2019,0.00016495175822854642,2
W4309625848,Self-attention presents low-dimensional knowledge graph embeddings for link prediction,2022,0.00016468923581963713,2
W4409852380,Multi-agent Chatbot for Efficient Interaction with Blockchain APIs,2025,0.0001638225681566179,2
W4400765516,Harnessing large language models to auto-evaluate the student project reports,2024,0.00016313889482771698,2
W4405491233,Extraction of patients subpopulations with psychiatric symptoms using a transformer architecture,2024,0.00016271477015311327,2
W4408252562,Semantic Annotation Model and Method Based on Internet Open Dataset,2025,0.00016225059096866223,2
W4377141577,A general text mining method to extract echocardiography measurement results from echocardiography documents,2023,0.00016120071717297881,2
W4381705785,NLP techniques for automating responses to customer queries: a systematic review,2023,0.000161057187680488,2
W2869198903,Interactive Analysis of Word Vector Embeddings,2018,0.00016100443920210106,2
W4410545705,Unpacking media bias in the growing divide between cable and network news,2025,0.00016092857140664662,2
W3169228325,Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion,2021,0.0001602406087690848,2
W4409157940,Multi-level Matching Network for Multimodal Entity Linking,2025,0.00016005483064814252,2
W3154075676,Wiki2Prop: A Multimodal Approach for Predicting Wikidata Properties from Wikipedia,2021,0.00015966619516016696,2
W4391092830,Prompting Large Language Models for Topic Modeling,2023,0.00015942375556246446,2
W4361204756,Overlap in meaning is a stronger predictor of semantic activation in GPT-3 than in humans,2023,0.00015901555307508433,2
W4399391263,LLM-Generated Word Association Norms,2024,0.00015846031361020792,2
W4399516266,Transformer-based Language Models and Homomorphic Encryption: An Intersection with BERT-tiny,2024,0.00015818770164325804,2
W4385688710,Balanced Knowledge Distillation with Contrastive Learning for Document Re-ranking,2023,0.00015773878524022195,2
W4362650374,Legal Textual Entailment Using Ensemble of Rule-Based and BERT-Based Method with Data Augmentation by Related Article Generation,2023,0.0001576782167930962,2
W4214933629,Developing a Cancer Digital Twin: Supervised Metastases Detection From Consecutive Structured Radiology Reports,2022,0.00015696927274366359,2
W4288059328,Improving the Quality of Students’ Written Reflections Using Natural Language Processing: Model Design and Classroom Evaluation,2022,0.0001565989946726614,2
W3209819950,Venomave: Targeted Poisoning Against Speech Recognition,2023,0.00015659016432938155,2
W4407410901,Are Natural Language Processing methods applicable to EPS forecasting in Poland?,2025,0.00015644188269945826,2
W4409278937,Linking Symptom Inventories Using Semantic Textual Similarity,2025,0.00015571835762370447,2
W3185329530,Information asymmetry in Wikipedia across different languages: A statistical analysis,2021,0.00015413417086354746,2
W4409349005,Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge,2025,0.0001537618892372201,2
W3043240831,COVID-19 SignSym: a fast adaptation of a general clinical NLP tool to identify and normalize COVID-19 signs and symptoms to OMOP common data model,2021,0.00015308363428714086,2
W3213176494,A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in Qatar,2021,0.00015208647714724726,2
W4390751407,Research on a Mongolian Text to Speech Model Based on Ghost and ILPCnet,2024,0.00015192653198323118,2
W4400529373,LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction,2024,0.00015190478226768695,2
W3141847762,Natural language processing in medicine: A review,2021,0.00015050118246136905,2
W4407930496,Issues and trends in generative AI technologies for decision making,2025,0.0001500731543132505,2
W4406703812,Exploring the Behavior and Performance of Large Language Models: Can LLMs Infer Answers to Questions Involving Restricted Information?,2025,0.00014946092087409732,2
W4398201981,Use of Artificial Intelligence Chatbots in Interpretation of Pathology Reports,2024,0.00014856650971398493,2
W4408838056,Large scale summarization using ensemble prompts and in context learning approaches,2025,0.0001481599705190513,2
W3129758539,Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks,2021,0.00014598874906400088,2
W4318541692,Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression,2023,0.0001455799382984883,2
W2810065831,How To Backdoor Federated Learning,2018,0.00014541217145857833,2
W4392251150,Large language models as a substitute for human experts in annotating political text,2024,0.00014477960139372385,2
W2726375170,Recurrent neural networks for classifying relations in clinical notes,2017,0.00014390238377619155,2
W4400525285,Combining Large Language Models and Crowdsourcing for Hybrid Human-AI Misinformation Detection,2024,0.00014353234148996232,2
W4409607349,Leveraging Large Language Models for Sentiment Analysis and Investment Strategy Development in Financial Markets,2025,0.00014330918844769372,2
W4391114242,Towards Generative Search and Recommendation: A keynote at RecSys 2023,2023,0.00014326644441743034,2
W4402404817,"Right to be forgotten in the Era of large language models: implications, challenges, and solutions",2024,0.00014313161971304668,2
W4221074805,Comparing PSO-based clustering over contextual vector embeddings to modern topic modeling,2022,0.00014299663615668408,2
W4390628433,Learning dual disentangled representation with self-supervision for temporal knowledge graph reasoning,2024,0.00014251401420242615,2
W4406750975,Enhancing doctor-patient communication using large language models for pathology report interpretation,2025,0.00014246745676269488,2
W2973252307,Targeted Adversarial Examples for Black Box Audio Systems,2019,0.00014235079140578012,2
W4386630107,Short Text Analytics based on BERT by using Multivariate Filter Methods for Feature Selection,2023,0.00014146841469024572,2
W4376225338,MSRDL: Deep learning framework for service recommendation in mashup creation,2023,0.00014146841469024572,2
W2963505471,Team Fernando-Pessa at SemEval-2019 Task 4: Back to Basics in Hyperpartisan News Detection,2019,0.00014032590490541285,2
W3110926788,Prediction of Stroke Outcome Using Natural Language Processing-Based Machine Learning of Radiology Report of Brain MRI,2020,0.0001402967467176241,2
W4280622095,Information Theory as a Bridge Between Language Function and Language Form,2022,0.00013994146902694015,2
W4310640599,Transforming epilepsy research: A systematic review on natural language processing applications,2022,0.00013948478961258698,2
W2924606678,Argument Mining for Understanding Peer Reviews,2019,0.00013945174775462598,2
W4385562555,Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks,2023,0.00013882172215590478,2
W4396218105,Does More Advice Help? The Effects of Second Opinions in AI-Assisted Decision Making,2024,0.00013814979997082792,2
W4392405493,LEVA: Using Large Language Models to Enhance Visual Analytics,2024,0.00013789450440626706,2
W4391994107,A Systematic Review of NLP Applications in Clinical Healthcare: Advancement and Challenges,2024,0.00013647734846631562,2
W4410253067,A Rhetorical Role Relatedness (RRR) framework for Legal Case Brief Generation,2025,0.00013588582969811702,2
W4392370696,Structural complexity predicts consensus readability in online discussions,2024,0.00013575199733345785,2
W4408258137,Towards Semantic Classification: An Experimental Study on Automated Understanding of the Meaning of Verbal Utterances,2025,0.00013575199733345785,2
W4408134531,Fusion of Domain Dependent Sensitive Semantics and Large Language Model for Research Text Security Classification Category,2025,0.00013575199733345785,2
W4221161137,Type-aware Embeddings for Multi-Hop Reasoning over Knowledge Graphs,2022,0.00013508348800969256,2
W4289222848,Speech disturbances in schizophrenia: Assessing cross-linguistic generalizability of NLP automated measures of coherence,2022,0.00013487365171242275,2
W4397029707,Navigating Ontology Development with Large Language Models,2024,0.00013428174794031386,2
W4410173160,Linear self-attention with multi-relational graph for knowledge graph completion,2025,0.000133961881337542,2
W4408529625,Large language model agents can use tools to perform clinical calculations,2025,0.0001338267457334689,2
W3001497429,Universals of word order reflect optimization of grammars for efficient communication,2020,0.00013358556504031036,2
W4406634532,An Application of Natural Language Processing for Hypoglycemic Event Identification in Patients with Diabetes Mellitus,2025,0.00013172671832217486,2
W2900069888,Extraction of Information Related to Adverse Drug Events from Electronic Health Record Notes: Design of an End-to-End Model Based on Deep Learning,2018,0.0001313549165480706,2
W4319597833,A Comparison of Different Topic Modeling Methods through a Real Case Study of Italian Customer Care,2023,0.0001313246804489189,2
W4386567268,Multisource hierarchical neural network for knowledge graph embedding,2023,0.00012910045348796595,2
W4410540745,Detection of Duplicate Questions Using Universal Sentence Encoder with Learning,2025,0.00012873398373831627,2
W4406572162,Patient2Trial: From Patient to Participant in Clinical Trials Using Large Language Models,2025,0.00012860343481265453,2
W4408519190,Oil Price Volatility Classification and Prediction with Sentiment Analysis: Machine Learning Approach,2025,0.00012833479625108443,2
W2551087083,LSTM-Based System-Call Language Modeling and Robust Ensemble Method for Designing Host-Based Intrusion Detection Systems,2016,0.00012818785029816726,2
W4389205282,Zero-shot interpretable phenotyping of postpartum hemorrhage using large language models,2023,0.00012731358163311533,2
W4380757924,Chinese mineral question and answering system based on knowledge graph,2023,0.00012660879191253964,2
W4281561077,Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion,2022,0.00012638890851030443,2
W4324142429,A Review of Deep Transfer Learning and Recent Advancements,2023,0.00012629202305513917,2
W4402265970,FIN2SUM: Advancing AI-Driven Financial Text Summarization with LLMs,2024,0.00012606262218734884,2
W2983721890,Layerwise Relevance Visualization in Convolutional Text Graph Classifiers,2019,0.00012571843555983755,2
W4401039927,The Role of Information Technology in Combating Hoaxes and Misinformation,2024,0.00012539645087208988,2
W4375869076,Going in Style: Audio Backdoors Through Stylistic Transformations,2023,0.00012538009328042423,2
W4225989348,Comparison of machine-learning algorithms for the prediction of Current Procedural Terminology (CPT) codes from pathology reports,2022,0.00012530518632454965,2
W4392157497,A bibliometric analysis of artificial intelligence in L2 teaching and applied linguistics between 1995 and 2022,2024,0.00012499893972986207,2
W4283818721,Exploring Relational Semantics for Inductive Knowledge Graph Completion,2022,0.00012445659745901672,2
W4393347796,Uncovering Flat and Hierarchical Topics by Community Discovery on Word Co-occurrence Network,2024,0.0001238101172858732,2
W4400529431,Preventing and Detecting Misinformation Generated by Large Language Models,2024,0.0001236783763124704,2
W4409183272,Approximate Bag-of-Words Top-k Corpus Graphs,2025,0.00012363238832418096,2
W4407057467,Parallel hierarchical encoding of linguistic representations in the human auditory cortex and recurrent automatic speech recognition systems,2025,0.0001230507997448361,2
W2993303751,Requirements Classification with Interpretable Machine Learning and Dependency Parsing,2019,0.00012229560225786814,2
W3036353984,Exploiting non-taxonomic relations for measuring semantic similarity and relatedness in WordNet,2020,0.00012198998068140025,2
W4409262680,Enhancing creativity with covert neurofeedback: causal evidence for default-executive network coupling in creative thinking,2025,0.00012158296167068971,2
W4407861121,How Creative is This Child?: Student Demographics Affect How Teachers Rate Creativity,2025,0.00012158296167068971,2
W4408919020,The Geometry of Concepts: Sparse Autoencoder Feature Structure,2025,0.00011937289199355982,2
W4407596266,Evaluating diabetes dataset for knowledge graph embedding based link prediction,2025,0.00011937289199355982,2
W4406248884,NLP and education,2025,0.00011937289199355982,2
W4408052763,Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models,2025,0.00011937289199355982,2
W4392752389,MCFCN: Multi-scale capsule-weighted fusion classification network for lung disease classification based on chest CT scans,2024,0.0001188781837488769,2
W3112631310,Artificial Intelligence in mental health and the biases of language based models,2020,0.00011852497817873535,2
W4383376720,One-Class Learning for AI-Generated Essay Detection,2023,0.00011765230663601566,2
W4322741012,WEIGHTED ENTITY-LINKING AND INTEGRATION ALGORITHM FOR MEDICAL KNOWLEDGE GRAPH GENERATION,2023,0.00011730464808249952,2
W4396953196,AI-based disease category prediction model using symptoms from low-resource Ethiopian language: Afaan Oromo text,2024,0.00011707264795449375,2
W4403811090,Automatic categorization of medical documents in Afaan Oromo using ensemble machine learning techniques,2024,0.00011707264795449364,2
W4409166199,Verifying Cross-Modal Entity Consistency in News Using Vision-Language Models,2025,0.00011639358462016617,2
W4406234910,Identification of Naloxone in Emergency Medical Services Data Substantially Improves by Processing Unstructured Patient Care Narratives,2025,0.00011588310360939464,2
W4392190680,Bridging the gap in biomedical information retrieval: Harnessing machine learning for enhanced search results and query semantics,2024,0.00011445841781845564,2
W4385202102,Moving thoughts: emotion concepts from the perspective of context dependent embodied simulation,2023,0.00011417746766517612,2
W4288741487,A hybrid CNN + BILSTM deep learning-based DSS for efficient prediction of judicial case decisions,2022,0.00011354262375573596,2
W4401129934,Topic Modeling for Faster Literature Screening Using Transformer-Based Embeddings,2024,0.00011324981587407114,2
W4408377435,Electronic Health Record classification and analysis using NLP Techniques,2025,0.00011287117304963691,2
W3189998517,Self-Supervised Adversarial Distribution Regularization for Medication Recommendation,2021,0.0001125985719360869,2
W4221166361,TrustAL: Trustworthy Active Learning Using Knowledge Distillation,2022,0.00011223521547989539,2
W4400529044,SuicidEmoji: Derived Emoji Dataset and Tasks for Suicide-Related Social Content,2024,0.00011213648995188975,2
W4386587410,Overview of the CLEF-2023 LongEval Lab on Longitudinal Evaluation of Model Performance,2023,0.00011033448350453975,2
W4320065330,Extracting Medical Information From Free-Text and Unstructured Patient-Generated Health Data Using Natural Language Processing Methods: Feasibility Study With Real-world Data,2023,0.00011030931560604419,2
W4360992621,Knowledge graph representation learning with simplifying hierarchical feature propagation,2023,0.00010998134735941016,2
W4383875393,The defeat of the Winograd Schema Challenge,2023,0.00010993864544617672,2
W2981175810,Efficiently embedding dynamic knowledge graphs,2022,0.00010949793388856506,2
W4392012809,Generating Context-Aware Contrastive Explanations in Rule-based Systems,2024,0.00010910933516131358,2
W4410221225,Contrastive zero-shot relational learning for knowledge graph completion,2025,0.00010900283994900172,2
W4401522875,Large language model based collaborative robot system for daily task assistance,2024,0.00010782846022570763,2
W4410252907,Developing Natural Language Processing Algorithms to Fact-Check Speech or Text,2025,0.00010741033816419047,2
W3098147269,The Go Transformer: Natural Language Modeling for Game Play,2020,0.00010681014316647873,2
W4382240599,"Relief in Sight? Chatbots, In-baskets, and the Overwhelmed Primary Care Clinician",2023,0.00010672188358122226,2
W4406424213,Use of Natural Language Processing to Retrospectively Identify and Improve Follow-Up of Adrenal Incidentalomas Based on Patient and Nodule Characteristics,2025,0.00010672188358122226,2
W4409663614,Natural Language Processing (NLP)- and Machine Learning (ML)-Enabled Operating Room Optimization: A Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) Systematic Review Anchored in Project Planning Theory,2025,0.00010672188358122226,2
W4408733089,Do multimodal large language models understand welding?,2025,0.0001064810585475956,2
W4408029885,Image processing on linguistics for biometric NLP trainers using AI-text analysis systems,2025,0.00010411664404769773,2
W4408200739,Transforming hematological research documentation with large language models: an approach to scientific writing and data analysis,2025,0.00010402626731813811,2
W3093681740,DeText,2020,0.0001023017601191495,2
W3109550251,Data Augmentation For Chinese Text Classification Using Back-Translation,2020,0.00010114482107499373,2
W4394902770,Uncovering Gender Bias within Journalist-Politician Interaction in Indian Twitter,2024,0.00010061483044890827,2
W4388267292,Narratives and Valuations,2023,0.00010061483044890827,2
W4376133440,Hospital-wide natural language processing summarising the health data of 1 million patients,2023,0.00010059211876964751,2
W4410083102,Grammaticality representation in ChatGPT as compared to linguists and laypeople,2025,0.0001000884883640874,2
W4407986790,Literature-scaled immunological gene set annotation using AI-powered immune cell knowledge graph (ICKG),2025,9.957655638320115e-05,2
W4408231555,A unified acoustic-to-speech-to-language embedding space captures the neural basis of natural language processing in everyday conversations,2025,9.8950768907551e-05,2
W4409167058,"ELOQUENT CLEF Shared Tasks for Evaluation of Generative Language Model Quality, 2025 Edition",2025,9.806702173231049e-05,2
W4407079550,Deep Learning-Driven Ontology Learning: A Systematic Mapping Study,2025,9.766372686581742e-05,2
W3207160263,Deep Learning Methods for Vessel Trajectory Prediction Based on Recurrent Neural Networks,2021,9.671288146191681e-05,2
W4388230782,Large-Scale Simulation Study of Active Learning Models for Systematic Reviews,2023,9.54406250060753e-05,2
W4221081166,Darling: A Web Application for Detecting Disease-Related Biomedical Entity Associations with Literature Mining,2022,9.5359109159277e-05,2
W4312092237,Classifying Customers’ Journey from Online Reviews of Amazon Fresh via Sentiment Analysis and Topic Modelling,2022,9.5359109159277e-05,2
W4308558768,e-TSN: an interactive visual exploration platform for target–disease knowledge mapping from literature,2022,9.5359109159277e-05,2
W4372259985,PQLM - Multilingual Decentralized Portable Quantum Language Model,2023,9.51400476869673e-05,2
W4406354428,The Impact of Linguistic Signals on Cognitive Change in Support Seekers in Online Mental Health Communities: Text Analysis and Empirical Study,2025,9.47661749628574e-05,2
W3129717984,An ecologically motivated image dataset for deep learning yields better models of human vision,2021,9.42616066031854e-05,2
W4410096736,Evaluation of Effectiveness of Large Language Models in Ontology and Knowledge Graph Creation,2025,9.419829180655001e-05,2
W4400526908,Large Language Models can Accurately Predict Searcher Preferences,2024,9.404250704659245e-05,2
W3004088204,Deep Learning for Natural Language Processing in Radiology—Fundamentals and a Systematic Review,2020,9.353162872860067e-05,2
W4323262115,"Master coach The Consequences Workplace of the Head Nurse's Adherence to Nursing Professional Educational Ethics and Knowledge of Moral Dilemmas at King Abdulaziz Hospital, Makkah, Saudi Arabia",2023,9.350528600232856e-05,2
W3199757199,The Language of Dreams: Application of Linguistics-Based Approaches for the Automated Analysis of Dream Experiences,2021,9.345391975219731e-05,2
W4410412168,Ontology-Based Learning Assistant Chatbot: Enhancing Accurate and Explanatory Knowledge Provision in Myanmar’s Primary Education,2025,9.321645589858892e-05,2
W4365393312,Explainable Causal Analysis of Mental Health on Social Media Data,2023,9.254167525569577e-05,2
W4381617132,Constructionist Approaches,2023,9.212593740498022e-05,2
W4315436249,Negation detection in Dutch clinical texts: an evaluation of rule-based and machine learning methods,2023,9.142020161100315e-05,2
W4409012087,Large language model for patent concept generation,2025,9.089616835655982e-05,2
W3036927603,FasTag: Automatic text classification of unstructured medical narratives,2020,9.003664959520746e-05,2
W2749581528,Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates,2017,8.80871483274053e-05,2
W4391609107,"Embodied human language models vs. Large Language Models, or why Artificial Intelligence cannot explain the modal be able to",2024,8.800487155092032e-05,2
W4384659456,Learn from Relational Correlations and Periodic Events for Temporal Knowledge Graph Reasoning,2023,8.767673899872082e-05,2
W4399854538,A comparative study of large language model-based zero-shot inference and task-specific supervised classification of breast cancer pathology reports,2024,8.588419783343043e-05,2
W4390510809,Siamese capsule network with position correlation and integrating articles of law for Chinese similar case matching,2024,8.58665354404511e-05,2
W4389639285,Network intrusion detection system by learning jointly from tabular and text‐based features,2023,8.52534492896082e-05,2
W2951073610,Understanding artificial intelligence ethics and safety.,2019,8.523633898875353e-05,2
W4386307206,Integrating Text Classification into Topic Discovery Using Semantic Embedding Models,2023,8.5235728816136e-05,2
W4296606109,Learning knowledge graph embedding with a dual-attention embedding network,2022,8.477283256682049e-05,2
W4409535830,How Good Are Large Language Models at Arithmetic Reasoning in Low-Resource Language Settings?—A Study on Yorùbá Numerical Probes with Minimal Contamination,2025,8.464068762810981e-05,2
W2926850261,Natural language processing of radiology reports for identification of skeletal site-specific fractures,2019,8.366086660194767e-05,2
W4401855268,Understanding Narratives of Uncertainty in Fertility Intentions of Dutch Women: A Neural Topic Modeling Approach,2024,8.352148511642215e-05,2
W4406412494,LAMGCN:Traditional Chinese Medicine Herb Recommendation via LSTMs with Attention Mechanisms and Graph Convolutional Networks,2025,8.34315282900332e-05,2
W4225891344,Word2Vec: Optimal hyperparameters and their impact on natural language processing downstream tasks,2022,8.310999546488895e-05,2
W4285294253,Shallow Neural Network and Ontology-Based Novel Semantic Document Indexing for Information Retrieval,2022,8.293404132836214e-05,2
W4407442143,A knowledge graph attention network for the cold‐start problem in intelligent manufacturing: Interpretability and accuracy improvement,2025,8.279392873992976e-05,2
W4407843264,Geographic Named Entity Matching and Evaluation Recommendation Using Multi-Objective Tasks: A Study Integrating a Large Language Model (LLM) and Retrieval-Augmented Generation (RAG),2025,8.265901856999936e-05,2
W3170852693,Social media crowdsourcing for rapid damage assessment following a sudden-onset natural hazard event,2021,8.26470098118421e-05,2
W4393322202,Similarity Matching for Patent Documents Using Ensemble BERT-Related Model and Novel Text Processing Method,2024,8.249140932639628e-05,2
W4391648585,Establishment of the Norwegian Public Relations Club in the Context of the Cold War,2024,8.216068835089645e-05,2
W4206292538,COVID-19 increased censorship circumvention and access to sensitive topics in China,2022,8.155439572042604e-05,2
W4407415852,OWL2Vec4OA: Tailoring Knowledge Graph Embeddings for Ontology Alignment,2025,8.150103943986406e-05,2
W4409485225,HyEED: embedding learning of knowledge graphs with entity description in hyperbolic space,2025,8.139813155915677e-05,2
W4313007581,Contrastive Multi-Modal Knowledge Graph Representation Learning,2022,8.103548291481713e-05,2
W4406696010,Question Answer Summary Generation from Unstructured Texts by Using LLMs,2025,8.072574298264098e-05,2
W4391929391,A novel approach to voice of customer extraction using GPT-3.5 Turbo: linking advanced NLP and Lean Six Sigma 4.0,2024,8.008233017734107e-05,2
W4409387699,A TCM Syndrome Differentiation Thinking Method Based on Chain of Thought and Knowledge Retrieval Augmentation,2025,8.008163975764449e-05,2
W4392179379,Talkin' 'Bout AI Generation,2024,7.744096115533835e-05,2
W4406752992,Which Reveals Ideology Better? Comparing Self-Presentation and Public Rhetoric in the Facebook Climate Debate via Embeddings Analysis,2025,7.722361711988798e-05,2
W3157844032,RE-BERT,2021,7.700891458552811e-05,2
W4226119767,Progressive changes in descriptive discourse in First Episode Schizophrenia: a longitudinal computational semantics study,2022,7.645907908974625e-05,2
W4408925093,Specialized Large Language Model Outperforms Neurologists at Complex Diagnosis in Blinded Case-Based Evaluation,2025,7.642351837583523e-05,2
W4401468394,Development of a natural language processing pipeline for assessment of cardiovascular risk in myeloproliferative neoplasms,2024,7.640087741971974e-05,2
W3207974842,Machine Learning Techniques for Biomedical Natural Language Processing: A Comprehensive Review,2021,7.597686735168458e-05,2
W4393158201,Bootstrapping Cognitive Agents with a Large Language Model,2024,7.54402060515829e-05,2
W2972984751,Evaluating shallow and deep learning strategies for the 2018 n2c2 shared task on clinical text classification,2019,7.487080855787139e-05,2
W4401411847,Natural Language Processing Accurately Differentiates Cancer Symptom Information in Electronic Health Record Narratives,2024,7.463368584720096e-05,2
W4409800684,Knowledge Graph-Based Legal Query System with LLM and Retrieval Augmented Generation,2025,7.459373623386888e-05,2
W3025279871,An Analysis of Adversarial Attacks and Defenses on Autonomous Driving Models,2020,7.298321064010805e-05,2
W4405427519,MetaphorPrompt - An Analogical Reasoning Approach for Extracting Causal Links from Biological Text,2024,7.279850665626836e-05,2
W3035403290,DGL-KE,2020,7.165445945398632e-05,2
W3137983801,Making costly manufacturing smart with transfer learning under limited data: A case study on composites autoclave processing,2021,7.142171064486109e-05,2
W4391255995,Embedding-Based Entity Alignment of Cross-Lingual Temporal Knowledge Graphs,2024,7.12738747586571e-05,2
W3153018527,Political Ideology Prediction from Bengali Text Using Word Embedding Models,2021,7.12018686666141e-05,2
W4409737518,Patient-Centered Research Through Artificial Intelligence to Identify Priorities in Cancer Care,2025,7.027286616404744e-05,2
W4387533115,Adversarial attack and training for deep neural network based power quality disturbance classification,2023,6.926218600511973e-05,2
W4405537947,Unpacking the Outputs of Generative AI Platforms and Revealing Gender and Social Re-Presentations,2024,6.9150734268653e-05,2
W3156311852,Medical Specialty Recommendations by an Artificial Intelligence Chatbot on a Smartphone: Development and Deployment,2021,6.914629020514968e-05,2
W4410116425,Dynamics of auditory word form encoding in human speech cortex,2025,6.866318948095687e-05,2
W4392763134,Large language models are able to downplay their cognitive abilities to fit the persona they simulate,2024,6.837508452977503e-05,2
W3138149011,A Survey on Knowledge Graph Embeddings for Link Prediction,2021,6.825252737179349e-05,2
W4393159663,Contributing Dimension Structure of Deep Feature for Coreset Selection,2024,6.824602515330739e-05,2
W4285404855,GFCNet: Utilizing graph feature collection networks for coronavirus knowledge graph embeddings,2022,6.807240891223585e-05,2
W4396985403,Sample Size Considerations for Fine-Tuning Large Language Models for Named Entity Recognition Tasks: Methodological Study,2024,6.750396609432267e-05,2
W4407958102,Identifying Useful Answers on Community-Based Question Answering Platforms: A Novel Unified Answer Comment-Based Approach,2025,6.682389138224301e-05,2
W4385626383,Duplicate Quora Questions Pair Detection using Siamese Bert and Ma-LSTM,2023,6.680457982549548e-05,2
W4362650778,Less is Better: Constructing Legal Question Answering System by Weighing Longest Common Subsequence of Disjunctive Union Text,2023,6.675333533967285e-05,2
W4319302991,Buried Object Characterization Using Ground Penetrating Radar Assisted by Data-Driven Surrogate-Models,2023,6.369397266986639e-05,2
W4394815468,A dilated convolution‐based method with time series fine tuning for data‐driven crack length estimation,2024,6.369397266986639e-05,2
W2883657435,Developing a portable natural language processing based phenotyping system,2019,6.227106490150499e-05,2
W4221155892,Deep Learning with Logical Constraints,2022,6.219263087378285e-05,2
W4408029418,Mario: Near Zero-cost Activation Checkpointing in Pipeline Parallelism,2025,6.170077934883344e-05,2
W4368340828,A hybrid deep learning approach for detecting sentiment polarities and knowledge graph representation on monkeypox tweets,2023,6.165054021466416e-05,2
W4313895174,Text-to-Ontology Mapping via Natural Language Processing with Application to Search for Relevant Ontologies in Catalysis,2023,6.125774935697308e-05,2
W3041169705,How managerial responses to online reviews affect customer satisfaction: An empirical study based on additional reviews,2020,5.9644147680328314e-05,2
W4400353916,How to classify domain entities into top-level ontology concepts using large language models,2024,5.962408882299632e-05,2
W4315777907,Image Embedding and Classification using Pre-Trained Deep Learning Architectures,2022,5.877535624542003e-05,2
W3117559565,Authorship Identification of a Russian-Language Text Using Support Vector Machine and Deep Neural Networks,2020,5.732448949807086e-05,2
W3164786495,Convolutional Complex Knowledge Graph Embeddings,2021,5.73192882254337e-05,2
W4283078630,Constructing Domain Ontology for Alzheimer Disease Using Deep Learning Based Approach,2022,5.6987760378029606e-05,2
W4390264399,StackER: a novel SMILES-based stacked approach for the accelerated and efficient discovery of ERα and ERβ antagonists,2023,5.5103530823196194e-05,2
W2919057541,Chinese medical question answer selection via hybrid models based on CNN and GRU,2019,5.433969498371827e-05,2
W4403181807,The Synergy of Clinical Psychology and Affective Computing: Advancements in Emotion Recognition and Therapy,2024,5.4170848315825414e-05,2
W4408741271,Exploring Classification Consistency of Natural Language Requirements Using GPT-4o,2025,5.389855846977237e-05,2
W3046238651,"The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies",2020,5.347527388464425e-05,2
W4394866292,AI for crop production – Where can large language models (LLMs) provide substantial value?,2024,5.2425990408914115e-05,2
W3100075909,Scaling Hidden Markov Language Models,2020,5.1013742089669406e-05,2
W4409011559,The cortical architecture representing the linguistic hierarchy of the conversational speech,2025,5.101142388379134e-05,2
W4284899843,Towards Understanding the Skill Gap in Cybersecurity,2022,4.924396915303849e-05,2
W2561975083,Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics,2017,4.914740340627424e-05,2
W4407407865,Artificially Intelligent or Artificially Inflated? Determinants and Informativeness of Corporate AI Disclosures,2025,4.912189027205746e-05,2
W4392542324,Trust in Generative AI among Students: An exploratory study,2024,4.9051913671929274e-05,2
W4389506426,Visual clustering network-based intelligent power lines inspection system,2023,4.801910558957782e-05,2
W4388452371,Rhythmic modulation of prediction errors: A top-down gating role for the beta-range in speech processing,2023,4.7796835448692284e-05,2
W4290989514,Improving Prediction of Arabic Fake News Using Fuzzy Logic and Modified Random Forest Model,2022,4.754746963032721e-05,2
W4362653953,Semantic-Based Classification of Relevant Case Law,2023,4.736723457789928e-05,2
W4409373574,A brief study on evaluation metrics for knowledge graph embeddings,2025,4.7044699859233865e-05,2
W4400916398,Dissociating prosodic from syntactic delta activity during natural speech comprehension,2024,4.4931133343037235e-05,2
W4395098217,Two-Stage Knowledge Graph Completion Based on Semantic Features and High-Order Structural Features,2024,4.4776054805295115e-05,2
W4327814177,FRD-LSTM: a novel technique for fake reviews detection using DCWR with the Bi-LSTM method,2023,4.447153203243043e-05,2
W4407134183,Multimodal Convolutional Neural Networks for the Prediction of Acute Kidney Injury in the Intensive Care,2025,4.422970962903799e-05,2
W2997514453,Artificial intelligence approaches using natural language processing to advance EHR-based clinical research,2019,4.396414984546256e-05,2
W4396577390,MetaMate: Large Language Model to the Rescue of Automated Data Extraction for Educational Systematic Reviews and Meta-analyses,2024,4.386918257587672e-05,2
W4410544973,Japanese Author Attribution Using BERT Finetuning with Stylometric Features,2025,4.3761436761527285e-05,2
W4399897292,Privileged representational axes in biological and artificial neural networks,2024,4.311098176593668e-05,2
W4386324432,Synthesize high-dimensional longitudinal electronic health records via hierarchical autoregressive language model,2023,4.216091550130371e-05,2
W3083667980,Evolving CNN-LSTM Models for Time Series Prediction Using Enhanced Grey Wolf Optimizer,2020,4.193323323931351e-05,2
W4402263913,Advancing Ear Biometrics: Enhancing Accuracy and Robustness Through Deep Learning,2024,4.176637750048662e-05,2
W4312679697,Systematized Nomenclature of Medicine–Clinical Terminology (SNOMED CT) Clinical Use Cases in the Context of Electronic Health Record Systems: Systematic Literature Review,2022,4.153851497938895e-05,2
W4393262489,Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?,2024,4.1417067428917085e-05,2
W4311404181,A Survey on Negative Transfer,2022,4.1248786967529956e-05,2
W3201352722,A Path for Translation of Machine Learning Products into Healthcare Delivery,2020,4.07147783216008e-05,2
W4392791588,Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‐4's efficacy in screening and extracting data from peer‐reviewed and grey literature in multiple languages,2024,4.0080546472850765e-05,2
W4386356152,A comparison of large language model versus manual chart review for extraction of data elements from the electronic health record,2023,4.0014468206061215e-05,2
W4400462656,Flexible multitask computation in recurrent networks utilizes shared dynamical motifs,2024,3.9237279531563705e-05,2
W4387560121,Integrating Graphs With Large Language Models: Methods and Prospects,2024,3.9209034287340596e-05,2
W4205184268,Machine Learning and Hebrew NLP for Automated Assessment of Open-Ended Questions in Biology,2022,3.9093676888271074e-05,2
W4229073922,Question answering system for chemistry—A semantic agent extension,2022,3.8798043930675155e-05,2
W4407554043,Event Log Extraction for Process Mining Using Large Language Models,2025,3.854888662368186e-05,2
W4410104494,"Dual‐stream algorithms for dementia detection: Harnessing structured and unstructured electronic health record data, a novel approach to prevalence estimation",2025,3.853785745782036e-05,2
W3011373955,A Survey on Computational Metaphor Processing,2020,3.7416340107087996e-05,2
W4408854660,Dual-view cross attention enhanced semi-supervised learning method for discourse cognitive engagement classification in online course discussions,2025,3.739903945282455e-05,2
W4406628184,Large language models for data extraction from unstructured and semi-structured electronic health records: a multiple model performance evaluation,2025,3.728169688658759e-05,2
W4391519594,Application of Large Language Models to DDoS Attack Detection,2024,3.640905016977394e-05,2
W4406159163,A hybrid model for the detection of multi-agent written news articles based on linguistic features and BERT,2025,3.6345573987763534e-05,2
W4311970748,Indonesian Scientists’ Behavior Relative to Research Data Governance in Preventing WMD-Applicable Technology Transfer,2022,3.634546563014446e-05,2
W4400482513,Ontology matching and repair based on semantic association and probabilistic logic,2024,3.634546563014446e-05,2
W4393218806,Bayesian-knowledge driven ontologies: A framework for fusion of semantic knowledge under uncertainty and incompleteness,2024,3.634546563014446e-05,2
W4367297437,Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective,2023,3.634546563014446e-05,2
W4220758630,Complex graph convolutional network for link prediction in knowledge graphs,2022,3.5064841965458876e-05,2
W4296701495,A lexicon-based approach to examine depression detection in social media: the case of Twitter and university community,2022,3.401571329311444e-05,2
W3199475877,Deep Learning Based Wheat Crop Yield Prediction Model in Punjab Region of North India,2021,3.311758826376433e-05,2
W4393338077,Advancing Breast Cancer Research Through Collaborative Computing: Harnessing Google Colab for Innovation,2024,3.2603777903437886e-05,2
W4410052319,Breast Cancer Detection Using Convolutional Neural Networks: A Deep Learning-Based Approach,2025,3.2603777903437886e-05,2
W4406870075,OntoChat: A Framework for Conversational Ontology Engineering Using Language Models,2025,3.201715368846834e-05,2
W4410104073,Are transformers truly foundational for robotics?,2025,2.956054051038484e-05,2
W4221086307,A 28nm 15.59µJ/Token Full-Digital Bitline-Transpose CIM-Based Sparse Transformer Accelerator with Pipeline/Parallel Reconfigurable Modes,2022,2.8774084933324197e-05,2
W4399740794,Leveraging LLM: Implementing an Advanced AI Chatbot for Healthcare,2024,2.8698074459116412e-05,2
W4400527438,Weighted KL-Divergence for Document Ranking Model Refinement,2024,2.85288002880017e-05,2
W4409108674,Who Is Leading in Communication Tone? Wavelet Analysis of the Fed and the ECB,2025,2.810708644837378e-05,2
W4409214646,The News in Earnings Announcement Disclosures: Capturing Word Context Using LLM Methods,2025,2.810708644837378e-05,2
W4394691488,Utilizing Machine Learning Techniques for Classifying Translated and Non-Translated Corporate Annual Reports,2024,2.810708644837378e-05,2
W4390686303,The determinants of linguistic features in key audit matters: Empirical evidence from Europe,2024,2.810708644837378e-05,2
W4407214475,Evaluation and Analysis of Large Language Models Performance in English Exam,2024,2.8106249584730838e-05,2
W2963485691,Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness Against Adversarial Attack,2019,2.7771169652561316e-05,2
W3017720918,Enabling Fast and Universal Audio Adversarial Attack Using Generative Model,2021,2.7607082515738324e-05,2
W4286008391,A Review on the Application of Knowledge Graph Technology in the Medical Field,2022,2.5843227360250987e-05,2
W4382405173,TABASCO: A transformer based contextualization toolkit,2023,2.456698035058227e-05,2
W4408162838,Automated Requirements Terminology Extraction,2025,2.456698035058227e-05,2
W4224921946,Using Natural Language Processing and Machine Learning to Preoperatively Predict Lymph Node Metastasis for Non–Small Cell Lung Cancer With Electronic Medical Records: Development and Validation Study,2022,2.4564367398243295e-05,2
W2764024122,Interpretable Convolutional Neural Networks,2018,2.4434382078449188e-05,2
W3119911952,A natural language processing approach for identifying temporal disease onset information from mental healthcare text,2021,2.358507329284201e-05,2
W4408235728,"From Pure Language to Word Embeddings: Benjamin, Metaphysics, and Machine Translation",2025,1.851610881163619e-05,2
W4392928544,Leveraging large language models for generating responses to patient messages—a subjective analysis,2024,1.8462189097550566e-05,2
W4389461439,Deep LSTM and LSTM-Attention Q-learning based reinforcement learning in oil and gas sector prediction,2023,1.7720076036347465e-05,2
W3110715780,Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems,2020,1.753151217001337e-05,2
W4408599531,Case ID Revealed HERE: Hybrid Elusive Case Repair Method for Transformer-Driven Business Process Event Log Enhancement,2025,1.6610163255202785e-05,2
W3035475567,Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations,2020,1.6149137846168363e-05,2
W4212975397,Detecting Language Associated With Home Healthcare Patient’s Risk for Hospitalization and Emergency Department Visit,2022,1.5503427027065605e-05,2
W4407930738,"Legal information extraction and classification using BERT, Bi-LSTM, and CRF models",2025,1.4609715438329046e-05,2
W4320501880,Intelligent Question Answering System Based on Domain Knowledge Graph,2023,1.4609715438329046e-05,2
W4383878309,Research on the Classification Methods of Social Bots,2023,1.4609715438329046e-05,2
W4353021271,Latent Factors of Language Disturbance and Relationships to Quantitative Speech Features,2023,1.3869631108098714e-05,2
W3025047062,Neural Networks-Based Aerodynamic Data Modeling: A Comprehensive Review,2020,1.3791978476427269e-05,2
W3202547399,"Evaluating the Cybersecurity Risk of Real-world, Machine Learning Production Systems",2022,1.311303067165464e-05,2
W4364376167,A distributional assessment of rivalry in word formation,2023,1.2858831974914778e-05,2
W4406776968,The role of meaning in the rivalry of <i>-ity</i> and <i>-ness</i>: evidence from distributional semantics,2025,1.2858831974914413e-05,2
W4364375221,Affix rivalry: Theoretical and methodological challenges,2023,1.28588319749144e-05,2
W4319341221,Distributional evidence for derivational paradigms,2023,1.2858831974914293e-05,2
W4392895803,Extending the Architecture of Language From a Multimodal Perspective,2024,1.2728176769556159e-05,2
W4235453268,Effects of Language on Visual Perception,2020,1.2728176769556159e-05,2
W4388181629,"Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs",2023,1.2728176769556159e-05,2
W4409176541,Extensive compositionality in the vocal system of bonobos,2025,1.2728176769556159e-05,2
W4308749892,Informative pseudo-labeling for graph neural networks with few labels,2022,1.2059533129203142e-05,2
W4407588883,sunflower: an R package for handling multiple response attempts and conducting error analysis in aphasia and related disorders,2025,1.0640974045743932e-05,2
W4210452949,Construct validity for computational linguistic metrics in individuals at clinical risk for psychosis: Associations with clinical ratings,2022,9.707892421118232e-06,2
W4410215381,Transforming education: tackling the two sigma problem with AI in journal clubs – a proof of concept,2025,9.379109476603124e-06,2
W4210518974,Delta-band neural activity primarily tracks sentences instead of semantic properties of words,2022,9.270875288654959e-06,2
W4392647583,Predictive models for flexible pavement fatigue cracking based on machine learning,2024,9.029663895899916e-06,2
W4366597729,Improving Multiparty Interactions with a Robot Using Large Language Models,2023,8.337476764680776e-06,2
W4397006802,Makita—A workflow generator for large-scale and reproducible simulation studies mimicking text labeling,2024,8.02263158515121e-06,2
W3203283460,LiterallyWikidata - A Benchmark for Knowledge Graph Completion Using Literals,2021,7.959496672594314e-06,2
W4200557907,"“YOU POST, I TRAVEL.” Bloggers' credibility, digital engagement, and travelers' behavioral intention: The mediating role of hedonic and utilitarian motivations",2021,7.451890026609133e-06,2
W4407034894,"Advanced Text Analysis, Simplification, Classification, and Synthesis Techniques",2025,7.36768165693365e-06,2
W4321452549,Single-cell biological network inference using a heterogeneous graph transformer,2023,7.245345566386566e-06,2
W4366265333,Improving ransomware detection based on portable executable header using xception convolutional neural network,2023,7.194182984121148e-06,2
W2964031179,Examining CNN Representations With Respect to Dataset Bias,2018,7.192760409304061e-06,2
W4391066510,Principles and applications of convolutional neural network for spectral analysis in food quality evaluation: A review,2024,6.944489087733374e-06,2
W3199891962,"More collaboration, less seriousness: Investigating new strategies for promoting youth engagement in government-generated videos during the COVID-19 pandemic in China",2021,6.842012318863453e-06,2
W4392881223,The application of multimodal large language models in medicine,2024,6.618873954594595e-06,2
W3004528428,From online texts to Landscape Character Assessment: Collecting and analysing first-person landscape perception computationally,2020,6.418381503787203e-06,2
W4403983018,Reconciling the contrasting narratives on the environmental impact of large language models,2024,5.172852530223416e-06,2
W4404172291,Transformer-Based Quantification of the Echo Chamber Effect in Online Communities,2024,4.705018599819104e-06,2
W4389724194,Back Translation-EDA and Transformer for Hate Speech Classification in Indonesian,2023,3.5227224420456822e-06,2
W2784186414,Integrating Prior Knowledge into Deep Learning,2017,2.521540474904508e-06,2
W2976258142,10 years of health-care reform in China: progress and gaps in Universal Health Coverage,2019,2.4611746031438674e-06,2
W3156187258,Supporting policy-making with social media and e-participation platforms data: A policy analytics framework,2021,2.3804188068545024e-06,2
W3165643650,Technological progress in electronic health record system optimization: Systematic review of systematic literature reviews,2021,4.467081487396074e-07,2
W4379467046,Enhanced bare-bones particle swarm optimization based evolving deep neural networks,2023,4.260725399408702e-07,2
W4213112533,Machine learning to detect invalid text responses: Validation and comparison to existing detection methods,2022,2.3964171866307865e-07,2
W4210551272,Classifying the content of social media images to support cultural ecosystem service assessments using deep learning models,2022,6.521548419660315e-08,2
