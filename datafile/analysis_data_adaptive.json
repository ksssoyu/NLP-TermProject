[
  {
    "window_start": 2012,
    "window_end": 2014,
    "paper_count": 71,
    "edge_count": 207,
    "dbcv_score": 0.7711562749384846,
    "optimal_params": {
      "n_neighbors": 25,
      "min_cluster_size": 15,
      "min_samples": 10
    },
    "topics": [
      {
        "cluster_id": 1,
        "size": 36,
        "keywords": [
          "word",
          "representations",
          "language",
          "neural",
          "networks",
          "translation",
          "modeling",
          "distributed",
          "linguistic",
          "embeddings",
          "space",
          "based",
          "models",
          "machine",
          "2012"
        ],
        "confidence": 0.8374599495216235
      },
      {
        "cluster_id": 0,
        "size": 18,
        "keywords": [
          "neural",
          "networks",
          "recurrent",
          "translation",
          "sequence",
          "models",
          "machine",
          "continuous",
          "recognition",
          "learning",
          "deep",
          "speech",
          "statistical",
          "space",
          "modeling"
        ],
        "confidence": 0.9937507601834371
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Adam: A Method for Stochastic Optimization",
          "score": 149584
        },
        {
          "title": "ImageNet classification with deep convolutional neural networks",
          "score": 119504
        },
        {
          "title": "Dropout: a simple way to prevent neural networks from overfitting",
          "score": 39709
        },
        {
          "title": "Distributed Representations of Words and Phrases and their Compositionality",
          "score": 33467
        },
        {
          "title": "GloVe: Global Vectors for Word Representation",
          "score": 32016
        },
        {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "score": 31416
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 27210
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 23254
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 20484
        },
        {
          "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
          "score": 12647
        },
        {
          "title": "Distributed Representations of Sentences and Documents",
          "score": 9231
        },
        {
          "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "score": 8448
        },
        {
          "title": "How transferable are features in deep neural networks?",
          "score": 8313
        },
        {
          "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
          "score": 7335
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 6760
        },
        {
          "title": "ADADELTA: An Adaptive Learning Rate Method",
          "score": 6620
        },
        {
          "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
          "score": 6177
        },
        {
          "title": "Show and tell: A neural image caption generator",
          "score": 6012
        },
        {
          "title": "On the difficulty of training recurrent neural networks",
          "score": 5322
        },
        {
          "title": "Generating Sequences With Recurrent Neural Networks",
          "score": 4026
        }
      ],
      "by_pagerank": [
        {
          "title": "Statistical Language Models Based on Neural Networks",
          "score": 0.05559672593586292
        },
        {
          "title": "ImageNet classification with deep convolutional neural networks",
          "score": 0.04635385444118192
        },
        {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "score": 0.04454435136974035
        },
        {
          "title": "Improving Word Representations via Global Context and Multiple Word Prototypes",
          "score": 0.041025421882741275
        },
        {
          "title": "Linguistic Regularities in Continuous Space Word Representations",
          "score": 0.03270424664460781
        },
        {
          "title": "Large Scale Distributed Deep Networks",
          "score": 0.02476515972316235
        },
        {
          "title": "Bilingual Word Embeddings for Phrase-Based Machine Translation",
          "score": 0.02153065469275417
        },
        {
          "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
          "score": 0.02103132704317716
        },
        {
          "title": "Continuous Space Translation Models with Neural Networks",
          "score": 0.020898125148160838
        },
        {
          "title": "A fast and simple algorithm for training neural probabilistic language models",
          "score": 0.020553783284646087
        },
        {
          "title": "Distributed Representations of Words and Phrases and their Compositionality",
          "score": 0.01969915616614607
        },
        {
          "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "score": 0.01917560896216416
        },
        {
          "title": "Generating Sequences With Recurrent Neural Networks",
          "score": 0.017936717059609183
        },
        {
          "title": "Theano: new features and speed improvements",
          "score": 0.01792101135425042
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 0.01759470561869189
        },
        {
          "title": "On the difficulty of training recurrent neural networks",
          "score": 0.01745039153981355
        },
        {
          "title": "Maxout Networks",
          "score": 0.01702293107710368
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 0.016479807779183887
        },
        {
          "title": "Recurrent Continuous Translation Models",
          "score": 0.015852678516605205
        },
        {
          "title": "CoNLL-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes",
          "score": 0.015091181681601633
        }
      ],
      "by_authority": [
        {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "score": 0.13108913577324163
        },
        {
          "title": "Linguistic Regularities in Continuous Space Word Representations",
          "score": 0.08817425718312936
        },
        {
          "title": "Distributed Representations of Words and Phrases and their Compositionality",
          "score": 0.08144264893147314
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 0.049824895176455296
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 0.04791498507443635
        },
        {
          "title": "Recurrent Continuous Translation Models",
          "score": 0.044998964190738476
        },
        {
          "title": "Generating Sequences With Recurrent Neural Networks",
          "score": 0.04463973597483594
        },
        {
          "title": "Bilingual Word Embeddings for Phrase-Based Machine Translation",
          "score": 0.036378863917544635
        },
        {
          "title": "On the difficulty of training recurrent neural networks",
          "score": 0.03420258084171319
        },
        {
          "title": "Statistical Language Models Based on Neural Networks",
          "score": 0.030015564725288324
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 0.02483425450832908
        },
        {
          "title": "Improving Word Representations via Global Context and Multiple Word Prototypes",
          "score": 0.024586364145653496
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 0.02447358736814412
        },
        {
          "title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
          "score": 0.020872278906037295
        },
        {
          "title": "Audio Chord Recognition with Recurrent Neural Networks",
          "score": 0.019276103671581144
        },
        {
          "title": "Maxout Networks",
          "score": 0.01897628560568288
        },
        {
          "title": "Multilingual Distributed Representations without Word Alignment",
          "score": 0.01814710055957005
        },
        {
          "title": "Sequence Transduction with Recurrent Neural Networks",
          "score": 0.015391303250146088
        },
        {
          "title": "Combining Heterogeneous Models for Measuring Relational Similarity",
          "score": 0.015032907007388453
        },
        {
          "title": "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
          "score": 0.014620013860628946
        }
      ]
    }
  },
  {
    "window_start": 2013,
    "window_end": 2015,
    "paper_count": 71,
    "edge_count": 240,
    "dbcv_score": 0.18323661294061325,
    "optimal_params": {
      "n_neighbors": 15,
      "min_cluster_size": 15,
      "min_samples": 10
    },
    "topics": [],
    "top_papers": {
      "by_citation": [
        {
          "title": "Deep Residual Learning for Image Recognition",
          "score": 192834
        },
        {
          "title": "Adam: A Method for Stochastic Optimization",
          "score": 149584
        },
        {
          "title": "Dropout: a simple way to prevent neural networks from overfitting",
          "score": 39709
        },
        {
          "title": "Distributed Representations of Words and Phrases and their Compositionality",
          "score": 33467
        },
        {
          "title": "GloVe: Global Vectors for Word Representation",
          "score": 32016
        },
        {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "score": 31416
        },
        {
          "title": "Rethinking the Inception Architecture for Computer Vision",
          "score": 27242
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 27210
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 23254
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 20484
        },
        {
          "title": "Distilling the Knowledge in a Neural Network",
          "score": 19498
        },
        {
          "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
          "score": 12647
        },
        {
          "title": "Distributed Representations of Sentences and Documents",
          "score": 9231
        },
        {
          "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "score": 8448
        },
        {
          "title": "How transferable are features in deep neural networks?",
          "score": 8313
        },
        {
          "title": "Effective Approaches to Attention-based Neural Machine Translation",
          "score": 7948
        },
        {
          "title": "Neural Machine Translation of Rare Words with Subword Units",
          "score": 7701
        },
        {
          "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
          "score": 7335
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 6760
        },
        {
          "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
          "score": 6177
        }
      ],
      "by_pagerank": [
        {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "score": 0.08805934024293573
        },
        {
          "title": "Linguistic Regularities in Continuous Space Word Representations",
          "score": 0.08235292628921627
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 0.036424837911282065
        },
        {
          "title": "Generating Sequences With Recurrent Neural Networks",
          "score": 0.036090721299791906
        },
        {
          "title": "Distributed Representations of Words and Phrases and their Compositionality",
          "score": 0.035269158011655866
        },
        {
          "title": "Maxout Networks",
          "score": 0.035167485672198254
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 0.030945924923175102
        },
        {
          "title": "Recurrent Continuous Translation Models",
          "score": 0.02873461931163299
        },
        {
          "title": "Combining Heterogeneous Models for Measuring Relational Similarity",
          "score": 0.02759998911779347
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 0.026359926740299276
        },
        {
          "title": "Bilingual Word Embeddings for Phrase-Based Machine Translation",
          "score": 0.025161457970212235
        },
        {
          "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "score": 0.02368437209056922
        },
        {
          "title": "Learning Semantic Representations for the Phrase Translation Model",
          "score": 0.01872435677316901
        },
        {
          "title": "Joint Language and Translation Modeling with Recurrent Neural Networks",
          "score": 0.01619756005809951
        },
        {
          "title": "Dropout: a simple way to prevent neural networks from overfitting",
          "score": 0.014605488649671634
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 0.01445074295033207
        },
        {
          "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
          "score": 0.01270715245189703
        },
        {
          "title": "Skip-Thought Vectors",
          "score": 0.011952765374571789
        },
        {
          "title": "Multilingual Distributed Representations without Word Alignment",
          "score": 0.011852354972389227
        },
        {
          "title": "Show and tell: A neural image caption generator",
          "score": 0.011674763412782984
        }
      ],
      "by_authority": [
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 0.12377083618625888
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 0.11388009338019253
        },
        {
          "title": "Recurrent Continuous Translation Models",
          "score": 0.08211029532452159
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 0.07698521852737751
        },
        {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "score": 0.07195684967773318
        },
        {
          "title": "Generating Sequences With Recurrent Neural Networks",
          "score": 0.04805655688497151
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 0.04423308731808874
        },
        {
          "title": "Distributed Representations of Words and Phrases and their Compositionality",
          "score": 0.032677835189955144
        },
        {
          "title": "Linguistic Regularities in Continuous Space Word Representations",
          "score": 0.03239947654481465
        },
        {
          "title": "Audio Chord Recognition with Recurrent Neural Networks",
          "score": 0.02605177717580235
        },
        {
          "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "score": 0.025623744503344946
        },
        {
          "title": "Skip-Thought Vectors",
          "score": 0.02431466950489614
        },
        {
          "title": "Show and tell: A neural image caption generator",
          "score": 0.02303673371340933
        },
        {
          "title": "Multilingual Distributed Representations without Word Alignment",
          "score": 0.021465741373212405
        },
        {
          "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
          "score": 0.019520938562918057
        },
        {
          "title": "GloVe: Global Vectors for Word Representation",
          "score": 0.018162983451027032
        },
        {
          "title": "A Neural Network for Factoid Question Answering over Paragraphs",
          "score": 0.01589126469742873
        },
        {
          "title": "Grammar as a Foreign Language",
          "score": 0.015697086964101853
        },
        {
          "title": "Adam: A Method for Stochastic Optimization",
          "score": 0.01542025346631807
        },
        {
          "title": "Fast and Accurate Shift-Reduce Constituent Parsing",
          "score": 0.014142737404554712
        }
      ]
    }
  },
  {
    "window_start": 2014,
    "window_end": 2016,
    "paper_count": 80,
    "edge_count": 303,
    "dbcv_score": 0.6263819685414087,
    "optimal_params": {
      "n_neighbors": 25,
      "min_cluster_size": 15,
      "min_samples": 10
    },
    "topics": [
      {
        "cluster_id": 1,
        "size": 49,
        "keywords": [
          "learning",
          "neural",
          "language",
          "word",
          "machine",
          "end",
          "translation",
          "networks",
          "models",
          "lstm",
          "representations",
          "vectors",
          "network",
          "sentences",
          "sequence"
        ],
        "confidence": 0.9967195621521621
      },
      {
        "cluster_id": 0,
        "size": 20,
        "keywords": [
          "neural",
          "networks",
          "sequence",
          "deep",
          "learning",
          "machine",
          "translation",
          "recurrent",
          "time",
          "empirical",
          "network",
          "dropout",
          "training",
          "encoder",
          "decoder"
        ],
        "confidence": 0.9977196971677802
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Deep Residual Learning for Image Recognition",
          "score": 192834
        },
        {
          "title": "Adam: A Method for Stochastic Optimization",
          "score": 149584
        },
        {
          "title": "Dropout: a simple way to prevent neural networks from overfitting",
          "score": 39709
        },
        {
          "title": "GloVe: Global Vectors for Word Representation",
          "score": 32016
        },
        {
          "title": "Rethinking the Inception Architecture for Computer Vision",
          "score": 27242
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 27210
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 23254
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 20484
        },
        {
          "title": "Distilling the Knowledge in a Neural Network",
          "score": 19498
        },
        {
          "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
          "score": 14474
        },
        {
          "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
          "score": 12647
        },
        {
          "title": "Layer Normalization",
          "score": 10421
        },
        {
          "title": "Enriching Word Vectors with Subword Information",
          "score": 9944
        },
        {
          "title": "Distributed Representations of Sentences and Documents",
          "score": 9231
        },
        {
          "title": "How transferable are features in deep neural networks?",
          "score": 8313
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 8081
        },
        {
          "title": "Effective Approaches to Attention-based Neural Machine Translation",
          "score": 7948
        },
        {
          "title": "Neural Machine Translation of Rare Words with Subword Units",
          "score": 7701
        },
        {
          "title": "The Stanford CoreNLP Natural Language Processing Toolkit",
          "score": 7335
        },
        {
          "title": "Matching Networks for One Shot Learning",
          "score": 7295
        }
      ],
      "by_pagerank": [
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 0.09921434386793057
        },
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 0.07424978107284956
        },
        {
          "title": "An Autoencoder Approach to Learning Bilingual Word Representations",
          "score": 0.06826201580589805
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 0.054852836580718684
        },
        {
          "title": "Fast and Robust Neural Network Joint Models for Statistical Machine Translation",
          "score": 0.03892173944227638
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 0.030884676110331907
        },
        {
          "title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
          "score": 0.02948778972710567
        },
        {
          "title": "Dropout: a simple way to prevent neural networks from overfitting",
          "score": 0.024106977431498932
        },
        {
          "title": "Distributed Representations of Sentences and Documents",
          "score": 0.02297891829941327
        },
        {
          "title": "Adam: A Method for Stochastic Optimization",
          "score": 0.02237208529222802
        },
        {
          "title": "GloVe: Global Vectors for Word Representation",
          "score": 0.021813532299472253
        },
        {
          "title": "Overcoming the Curse of Sentence Length for Neural Machine Translation using Automatic Segmentation",
          "score": 0.01857746445494767
        },
        {
          "title": "Skip-Thought Vectors",
          "score": 0.016994064159628324
        },
        {
          "title": "Training Very Deep Networks",
          "score": 0.016247628390511006
        },
        {
          "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
          "score": 0.015711307329691424
        },
        {
          "title": "Linguistic Regularities in Sparse and Explicit Word Representations",
          "score": 0.014285613662552658
        },
        {
          "title": "Grammar as a Foreign Language",
          "score": 0.01423139653002717
        },
        {
          "title": "Show and tell: A neural image caption generator",
          "score": 0.012592282787147028
        },
        {
          "title": "Character-Aware Neural Language Models",
          "score": 0.012004254072605839
        },
        {
          "title": "Multi-task Sequence to Sequence Learning",
          "score": 0.011152508324902506
        }
      ],
      "by_authority": [
        {
          "title": "Sequence to Sequence Learning with Neural Networks",
          "score": 0.1672083273906329
        },
        {
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
          "score": 0.15401710399667068
        },
        {
          "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation",
          "score": 0.1421457916097611
        },
        {
          "title": "Adam: A Method for Stochastic Optimization",
          "score": 0.051794240363941854
        },
        {
          "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
          "score": 0.03748672630312621
        },
        {
          "title": "Skip-Thought Vectors",
          "score": 0.03731422772002108
        },
        {
          "title": "Grammar as a Foreign Language",
          "score": 0.03408744057770206
        },
        {
          "title": "Effective Approaches to Attention-based Neural Machine Translation",
          "score": 0.029793637268757122
        },
        {
          "title": "Deep Residual Learning for Image Recognition",
          "score": 0.02853052344478502
        },
        {
          "title": "Show and tell: A neural image caption generator",
          "score": 0.024076568358347136
        },
        {
          "title": "GloVe: Global Vectors for Word Representation",
          "score": 0.023172272528294773
        },
        {
          "title": "Edinburgh\u2019s Phrase-based Machine Translation Systems for WMT-14",
          "score": 0.01956810601326999
        },
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 0.01912074203508886
        },
        {
          "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
          "score": 0.016905960544570327
        },
        {
          "title": "Neural GPUs Learn Algorithms",
          "score": 0.016737425594075248
        },
        {
          "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
          "score": 0.015127654125621457
        },
        {
          "title": "Dropout: a simple way to prevent neural networks from overfitting",
          "score": 0.013180177956941262
        },
        {
          "title": "A Neural Network for Factoid Question Answering over Paragraphs",
          "score": 0.011952115631734131
        },
        {
          "title": "Distributed Representations of Sentences and Documents",
          "score": 0.011720123152921442
        },
        {
          "title": "A large annotated corpus for learning natural language inference",
          "score": 0.011437106121742584
        }
      ]
    }
  },
  {
    "window_start": 2015,
    "window_end": 2017,
    "paper_count": 94,
    "edge_count": 319,
    "dbcv_score": 0.753673628947073,
    "optimal_params": {
      "n_neighbors": 10,
      "min_cluster_size": 15,
      "min_samples": 10
    },
    "topics": [
      {
        "cluster_id": 2,
        "size": 36,
        "keywords": [
          "learning",
          "neural",
          "networks",
          "end",
          "machine",
          "deep",
          "attention",
          "translation",
          "language",
          "model",
          "sequence",
          "memory",
          "natural",
          "shot",
          "structured"
        ],
        "confidence": 0.7006850702688933
      },
      {
        "cluster_id": 1,
        "size": 22,
        "keywords": [
          "neural",
          "models",
          "word",
          "learning",
          "language",
          "disambiguation",
          "sense",
          "evaluation",
          "character",
          "coreference",
          "embedding",
          "bidirectional",
          "vectors",
          "sequence",
          "end"
        ],
        "confidence": 0.9716614844446841
      },
      {
        "cluster_id": 0,
        "size": 16,
        "keywords": [
          "comprehension",
          "reading",
          "machine",
          "language",
          "natural",
          "inference",
          "question",
          "large",
          "networks",
          "learning",
          "attention",
          "answering",
          "scale",
          "dataset",
          "visual"
        ],
        "confidence": 0.9984348912881748
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Deep Residual Learning for Image Recognition",
          "score": 192834
        },
        {
          "title": "Attention is All you Need",
          "score": 130102
        },
        {
          "title": "Rethinking the Inception Architecture for Computer Vision",
          "score": 27242
        },
        {
          "title": "Decoupled Weight Decay Regularization",
          "score": 22656
        },
        {
          "title": "Distilling the Knowledge in a Neural Network",
          "score": 19498
        },
        {
          "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
          "score": 14474
        },
        {
          "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
          "score": 11810
        },
        {
          "title": "Layer Normalization",
          "score": 10421
        },
        {
          "title": "Enriching Word Vectors with Subword Information",
          "score": 9944
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 8081
        },
        {
          "title": "Effective Approaches to Attention-based Neural Machine Translation",
          "score": 7948
        },
        {
          "title": "Neural Machine Translation of Rare Words with Subword Units",
          "score": 7701
        },
        {
          "title": "Matching Networks for One Shot Learning",
          "score": 7295
        },
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 6772
        },
        {
          "title": "VQA: Visual Question Answering",
          "score": 5436
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 4452
        },
        {
          "title": "A large annotated corpus for learning natural language inference",
          "score": 4264
        },
        {
          "title": "Neural Architectures for Named Entity Recognition",
          "score": 4005
        },
        {
          "title": "Optimization as a Model for Few-Shot Learning",
          "score": 3394
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 3279
        }
      ],
      "by_pagerank": [
        {
          "title": "Skip-Thought Vectors",
          "score": 0.15631160828672577
        },
        {
          "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books",
          "score": 0.1391954654603023
        },
        {
          "title": "Training Very Deep Networks",
          "score": 0.03398545666514756
        },
        {
          "title": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation",
          "score": 0.02635166809009596
        },
        {
          "title": "Effective Approaches to Attention-based Neural Machine Translation",
          "score": 0.021596140451786965
        },
        {
          "title": "A large annotated corpus for learning natural language inference",
          "score": 0.02136204868562581
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 0.018718977782104698
        },
        {
          "title": "Character-Aware Neural Language Models",
          "score": 0.01845652007412848
        },
        {
          "title": "Deep Residual Learning for Image Recognition",
          "score": 0.017508924253058694
        },
        {
          "title": "End-To-End Memory Networks",
          "score": 0.015768726138139615
        },
        {
          "title": "Neural GPUs Learn Algorithms",
          "score": 0.0152280248434853
        },
        {
          "title": "Long Short-Term Memory-Networks for Machine Reading",
          "score": 0.015075143043969775
        },
        {
          "title": "Neural Machine Translation of Rare Words with Subword Units",
          "score": 0.014444202996634663
        },
        {
          "title": "End-to-end learning of semantic role labeling using recurrent neural networks",
          "score": 0.013089575271662757
        },
        {
          "title": "Multi-task Sequence to Sequence Learning",
          "score": 0.01225034102217686
        },
        {
          "title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
          "score": 0.01202320742163527
        },
        {
          "title": "Rethinking the Inception Architecture for Computer Vision",
          "score": 0.011982394875188086
        },
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 0.011292047607034462
        },
        {
          "title": "Bidirectional Attention Flow for Machine Comprehension",
          "score": 0.010534688467590656
        },
        {
          "title": "A Decomposable Attention Model for Natural Language Inference",
          "score": 0.010484637958576215
        }
      ],
      "by_authority": [
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 0.09769847016167149
        },
        {
          "title": "Effective Approaches to Attention-based Neural Machine Translation",
          "score": 0.07811880292524646
        },
        {
          "title": "Deep Residual Learning for Image Recognition",
          "score": 0.07686398442691739
        },
        {
          "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
          "score": 0.06243619128143109
        },
        {
          "title": "A large annotated corpus for learning natural language inference",
          "score": 0.05308120796793419
        },
        {
          "title": "Skip-Thought Vectors",
          "score": 0.052311284372568295
        },
        {
          "title": "Neural Machine Translation of Rare Words with Subword Units",
          "score": 0.04497440526734645
        },
        {
          "title": "Exploring the Limits of Language Modeling",
          "score": 0.033639673667586516
        },
        {
          "title": "End-To-End Memory Networks",
          "score": 0.030636395832247244
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 0.026371495079993403
        },
        {
          "title": "Neural Machine Translation in Linear Time",
          "score": 0.025987286442469216
        },
        {
          "title": "Character-Aware Neural Language Models",
          "score": 0.024623163370477853
        },
        {
          "title": "A Decomposable Attention Model for Natural Language Inference",
          "score": 0.024376075765135844
        },
        {
          "title": "Bidirectional Attention Flow for Machine Comprehension",
          "score": 0.02405198922119138
        },
        {
          "title": "End-to-end learning of semantic role labeling using recurrent neural networks",
          "score": 0.021015247286880305
        },
        {
          "title": "Attention is All you Need",
          "score": 0.019804731496053288
        },
        {
          "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
          "score": 0.019683829175638302
        },
        {
          "title": "Layer Normalization",
          "score": 0.01733399355212744
        },
        {
          "title": "Long Short-Term Memory-Networks for Machine Reading",
          "score": 0.015473150097966258
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 0.014981791842746334
        }
      ]
    }
  },
  {
    "window_start": 2016,
    "window_end": 2018,
    "paper_count": 1812,
    "edge_count": 1221,
    "dbcv_score": 0.5827113712207659,
    "optimal_params": {
      "n_neighbors": 15,
      "min_cluster_size": 15,
      "min_samples": 10
    },
    "topics": [
      {
        "cluster_id": 4,
        "size": 647,
        "keywords": [
          "language",
          "learning",
          "natural",
          "processing",
          "neural",
          "deep",
          "using",
          "text",
          "based",
          "data",
          "word",
          "networks",
          "machine",
          "analysis",
          "network"
        ],
        "confidence": 0.9956268008071252
      },
      {
        "cluster_id": 1,
        "size": 25,
        "keywords": [
          "comprehension",
          "question",
          "reading",
          "answering",
          "machine",
          "inference",
          "language",
          "natural",
          "attention",
          "challenge",
          "networks",
          "learning",
          "dataset",
          "multi",
          "scale"
        ],
        "confidence": 0.8343157386232087
      },
      {
        "cluster_id": 0,
        "size": 23,
        "keywords": [
          "language",
          "survey",
          "processing",
          "neural",
          "learning",
          "natural",
          "data",
          "deep",
          "bias",
          "gender",
          "methods",
          "analysis",
          "word",
          "embeddings",
          "model"
        ],
        "confidence": 0.8950161782811543
      },
      {
        "cluster_id": 3,
        "size": 21,
        "keywords": [
          "attention",
          "neural",
          "text",
          "structured",
          "summarization",
          "networks",
          "language",
          "natural",
          "machine",
          "translation",
          "network",
          "generation",
          "need",
          "sequence",
          "transformers"
        ],
        "confidence": 0.8463463148830096
      },
      {
        "cluster_id": 2,
        "size": 20,
        "keywords": [
          "learning",
          "deep",
          "natural",
          "language",
          "processing",
          "text",
          "machine",
          "based",
          "graph",
          "neural",
          "convolutional",
          "networks",
          "survey",
          "omics",
          "models"
        ],
        "confidence": 0.8932189284246828
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Attention is All you Need",
          "score": 130102
        },
        {
          "title": "Decoupled Weight Decay Regularization",
          "score": 22656
        },
        {
          "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
          "score": 14474
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 11860
        },
        {
          "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
          "score": 11810
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 11526
        },
        {
          "title": "Layer Normalization",
          "score": 10421
        },
        {
          "title": "Enriching Word Vectors with Subword Information",
          "score": 9944
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 8081
        },
        {
          "title": "Matching Networks for One Shot Learning",
          "score": 7295
        },
        {
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "score": 7103
        },
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 6772
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 4452
        },
        {
          "title": "Neural Architectures for Named Entity Recognition",
          "score": 4005
        },
        {
          "title": "Universal Language Model Fine-tuning for Text Classification",
          "score": 3628
        },
        {
          "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
          "score": 3499
        },
        {
          "title": "Optimization as a Model for Few-Shot Learning",
          "score": 3394
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 3279
        },
        {
          "title": "A Call for Clarity in Reporting BLEU Scores",
          "score": 2958
        },
        {
          "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
          "score": 2954
        }
      ],
      "by_pagerank": [
        {
          "title": "Enriching Word Vectors with Subword Information",
          "score": 0.014966849909398703
        },
        {
          "title": "Charagram: Embedding Words and Sentences via Character n-grams",
          "score": 0.01463352861918119
        },
        {
          "title": "Neural Architectures for Named Entity Recognition",
          "score": 0.014184453095953264
        },
        {
          "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
          "score": 0.012571508266005208
        },
        {
          "title": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation",
          "score": 0.012234829740634718
        },
        {
          "title": "Exploring the Limits of Language Modeling",
          "score": 0.011286539912142584
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 0.010478025585597411
        },
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 0.010353975820235065
        },
        {
          "title": "Attention is All you Need",
          "score": 0.009058816700066622
        },
        {
          "title": "Long Short-Term Memory-Networks for Machine Reading",
          "score": 0.00884316737533028
        },
        {
          "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
          "score": 0.006088673654995847
        },
        {
          "title": "Bidirectional Attention Flow for Machine Comprehension",
          "score": 0.006017222428159962
        },
        {
          "title": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF",
          "score": 0.005728049382126613
        },
        {
          "title": "A Decomposable Attention Model for Natural Language Inference",
          "score": 0.005553324961541771
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 0.005439317340183869
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 0.0036345946564634384
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 0.003392729237073299
        },
        {
          "title": "Learning Global Features for Coreference Resolution",
          "score": 0.0033129464723872784
        },
        {
          "title": "Neural Machine Translation in Linear Time",
          "score": 0.0031211917606619667
        },
        {
          "title": "Layer Normalization",
          "score": 0.003075915966520947
        }
      ],
      "by_authority": [
        {
          "title": "Deep Contextualized Word Representations",
          "score": 0.07743993855928498
        },
        {
          "title": "Attention is All you Need",
          "score": 0.06833748277754813
        },
        {
          "title": "Bidirectional Attention Flow for Machine Comprehension",
          "score": 0.06125357208085061
        },
        {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "score": 0.05452937364304794
        },
        {
          "title": "Learned in Translation: Contextualized Word Vectors",
          "score": 0.04397057276223448
        },
        {
          "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
          "score": 0.043799783303417536
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 0.03118227547382111
        },
        {
          "title": "Enhanced LSTM for Natural Language Inference",
          "score": 0.02705921756104184
        },
        {
          "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
          "score": 0.026300360844617277
        },
        {
          "title": "A Decomposable Attention Model for Natural Language Inference",
          "score": 0.02276180328583482
        },
        {
          "title": "Neural Architectures for Named Entity Recognition",
          "score": 0.02189098662927833
        },
        {
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation",
          "score": 0.019615317335149506
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 0.01874408295730616
        },
        {
          "title": "Semi-supervised sequence tagging with bidirectional language models",
          "score": 0.018140616794873117
        },
        {
          "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks",
          "score": 0.017313262789932667
        },
        {
          "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
          "score": 0.017039606415822788
        },
        {
          "title": "Enriching Word Vectors with Subword Information",
          "score": 0.01522622019985891
        },
        {
          "title": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
          "score": 0.015104240344612389
        },
        {
          "title": "Stochastic Answer Networks for Machine Reading Comprehension",
          "score": 0.015099075712977021
        },
        {
          "title": "Learning Distributed Representations of Sentences from Unlabelled Data",
          "score": 0.014507332803392925
        }
      ]
    }
  },
  {
    "window_start": 2017,
    "window_end": 2019,
    "paper_count": 3646,
    "edge_count": 3818,
    "dbcv_score": 0.5444772157656491,
    "optimal_params": {
      "n_neighbors": 15,
      "min_cluster_size": 25,
      "min_samples": 15
    },
    "topics": [
      {
        "cluster_id": 3,
        "size": 1597,
        "keywords": [
          "language",
          "learning",
          "natural",
          "neural",
          "processing",
          "deep",
          "using",
          "text",
          "based",
          "analysis",
          "networks",
          "machine",
          "word",
          "sentiment",
          "classification"
        ],
        "confidence": 0.9961466201194876
      },
      {
        "cluster_id": 2,
        "size": 278,
        "keywords": [
          "language",
          "processing",
          "natural",
          "learning",
          "clinical",
          "using",
          "deep",
          "based",
          "health",
          "machine",
          "data",
          "medical",
          "biomedical",
          "electronic",
          "text"
        ],
        "confidence": 0.9952781441836547
      },
      {
        "cluster_id": 1,
        "size": 32,
        "keywords": [
          "learning",
          "language",
          "deep",
          "natural",
          "processing",
          "text",
          "based",
          "graph",
          "attention",
          "neural",
          "analysis",
          "sentiment",
          "survey",
          "networks",
          "convolutional"
        ],
        "confidence": 0.8998549654147283
      },
      {
        "cluster_id": 0,
        "size": 25,
        "keywords": [
          "survey",
          "language",
          "processing",
          "natural",
          "neural",
          "data",
          "learning",
          "word",
          "embeddings",
          "deep",
          "bias",
          "gender",
          "methods",
          "based",
          "analysis"
        ],
        "confidence": 1.0
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Attention is All you Need",
          "score": 130102
        },
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 94099
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 24213
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 22682
        },
        {
          "title": "Decoupled Weight Decay Regularization",
          "score": 22656
        },
        {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "score": 19888
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 11860
        },
        {
          "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
          "score": 11810
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 11526
        },
        {
          "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "score": 10743
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 8395
        },
        {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "score": 7422
        },
        {
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "score": 7103
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 6424
        },
        {
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "score": 5602
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 4452
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 3715
        },
        {
          "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
          "score": 3664
        },
        {
          "title": "Universal Language Model Fine-tuning for Text Classification",
          "score": 3628
        },
        {
          "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
          "score": 3499
        }
      ],
      "by_pagerank": [
        {
          "title": "Attention is All you Need",
          "score": 0.023275748031957935
        },
        {
          "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
          "score": 0.018074231634763786
        },
        {
          "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
          "score": 0.016443866345449924
        },
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 0.015202273440774416
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 0.014776252868343042
        },
        {
          "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
          "score": 0.014550720089579909
        },
        {
          "title": "A Structured Self-attentive Sentence Embedding",
          "score": 0.01355907359117489
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 0.009518796771517185
        },
        {
          "title": "Neural Sequence Learning Models for Word Sense Disambiguation",
          "score": 0.008129281526641594
        },
        {
          "title": "Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison",
          "score": 0.007919855945992706
        },
        {
          "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
          "score": 0.005670035451645768
        },
        {
          "title": "Learned in Translation: Contextualized Word Vectors",
          "score": 0.005223174322103531
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 0.005062603890220212
        },
        {
          "title": "Semi-supervised sequence tagging with bidirectional language models",
          "score": 0.004550815324187448
        },
        {
          "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
          "score": 0.004247158028809257
        },
        {
          "title": "Deep Semantic Role Labeling: What Works and What\u2019s Next",
          "score": 0.004192775190395484
        },
        {
          "title": "Recent Trends in Deep Learning Based Natural Language Processing",
          "score": 0.004083696762729109
        },
        {
          "title": "On the State of the Art of Evaluation in Neural Language Models",
          "score": 0.003609172372212936
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 0.0035428811444596915
        },
        {
          "title": "End-to-end Neural Coreference Resolution",
          "score": 0.0031900148557482896
        }
      ],
      "by_authority": [
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 0.14048826184127763
        },
        {
          "title": "Attention is All you Need",
          "score": 0.0985269008906527
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 0.08626347341002673
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 0.055669304972748544
        },
        {
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "score": 0.03187109122595791
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 0.0312321142709426
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 0.030159180509569616
        },
        {
          "title": "Universal Language Model Fine-tuning for Text Classification",
          "score": 0.026598106021919903
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 0.024493000993205633
        },
        {
          "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
          "score": 0.0213479718273539
        },
        {
          "title": "Cross-lingual Language Model Pretraining",
          "score": 0.01839744259830984
        },
        {
          "title": "Learned in Translation: Contextualized Word Vectors",
          "score": 0.016897933036684052
        },
        {
          "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
          "score": 0.015138232357151102
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 0.0139905180984991
        },
        {
          "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
          "score": 0.013816789366998962
        },
        {
          "title": "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
          "score": 0.0118936060525098
        },
        {
          "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
          "score": 0.00828995260099118
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 0.008233626400324972
        },
        {
          "title": "Convolutional Sequence to Sequence Learning",
          "score": 0.00810480665612462
        },
        {
          "title": "Semi-supervised sequence tagging with bidirectional language models",
          "score": 0.00782937574480895
        }
      ]
    }
  },
  {
    "window_start": 2018,
    "window_end": 2020,
    "paper_count": 5511,
    "edge_count": 8505,
    "dbcv_score": 0.44129649701105655,
    "optimal_params": {
      "n_neighbors": 10,
      "min_cluster_size": 35,
      "min_samples": 10
    },
    "topics": [
      {
        "cluster_id": 0,
        "size": 3145,
        "keywords": [
          "language",
          "natural",
          "learning",
          "processing",
          "using",
          "deep",
          "based",
          "text",
          "neural",
          "analysis",
          "machine",
          "survey",
          "models",
          "data",
          "sentiment"
        ],
        "confidence": 0.9823048656724312
      },
      {
        "cluster_id": 2,
        "size": 533,
        "keywords": [
          "language",
          "processing",
          "natural",
          "learning",
          "clinical",
          "using",
          "health",
          "based",
          "medical",
          "data",
          "machine",
          "text",
          "deep",
          "electronic",
          "biomedical"
        ],
        "confidence": 0.8712746826330788
      },
      {
        "cluster_id": 1,
        "size": 84,
        "keywords": [
          "learning",
          "deep",
          "language",
          "using",
          "natural",
          "chemical",
          "processing",
          "prediction",
          "protein",
          "machine",
          "molecular",
          "based",
          "sequence",
          "modeling",
          "extraction"
        ],
        "confidence": 0.9393743927842955
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 94099
        },
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 41302
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 24213
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 22682
        },
        {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "score": 19888
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 11860
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 11526
        },
        {
          "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "score": 10743
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 8395
        },
        {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "score": 7422
        },
        {
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "score": 7103
        },
        {
          "title": "Training data-efficient image transformers & distillation through attention",
          "score": 6690
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 6424
        },
        {
          "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
          "score": 6136
        },
        {
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "score": 5602
        },
        {
          "title": "Scaling Laws for Neural Language Models",
          "score": 4707
        },
        {
          "title": "Measuring Massive Multitask Language Understanding",
          "score": 4304
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 3715
        },
        {
          "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
          "score": 3664
        },
        {
          "title": "Universal Language Model Fine-tuning for Text Classification",
          "score": 3628
        }
      ],
      "by_pagerank": [
        {
          "title": "Deep Contextualized Word Representations",
          "score": 0.04530512785658746
        },
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 0.037115328303132016
        },
        {
          "title": "Annotation Artifacts in Natural Language Inference Data",
          "score": 0.01213142332489723
        },
        {
          "title": "Visual Referring Expression Recognition: What Do Systems Actually Learn?",
          "score": 0.009455701187906448
        },
        {
          "title": "Universal Language Model Fine-tuning for Text Classification",
          "score": 0.008598958509965459
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 0.006818763229753433
        },
        {
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "score": 0.006634423871080369
        },
        {
          "title": "Dissecting Contextual Word Embeddings: Architecture and Representation",
          "score": 0.00593607469697303
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 0.005459413801978047
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 0.0051877340258884965
        },
        {
          "title": "Semi-Supervised Sequence Modeling with Cross-View Training",
          "score": 0.004976808210242773
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 0.00473446941682174
        },
        {
          "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension",
          "score": 0.004306564493001072
        },
        {
          "title": "Character-Level Language Modeling with Deeper Self-Attention",
          "score": 0.004061700982894758
        },
        {
          "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform",
          "score": 0.0036167612701192733
        },
        {
          "title": "Neural Network Acceptability Judgments",
          "score": 0.003055622936069519
        },
        {
          "title": "Learning Word Vectors for 157 Languages",
          "score": 0.0029297590366419123
        },
        {
          "title": "Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning",
          "score": 0.0026554926519513355
        },
        {
          "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
          "score": 0.002648191222252582
        },
        {
          "title": "Contextual String Embeddings for Sequence Labeling",
          "score": 0.0026049941338521223
        }
      ],
      "by_authority": [
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 0.1721991088102247
        },
        {
          "title": "Deep Contextualized Word Representations",
          "score": 0.06370713858166296
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 0.06115174592263094
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 0.04169549351583821
        },
        {
          "title": "Improving Language Understanding by Generative Pre-Training",
          "score": 0.039867071331911336
        },
        {
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "score": 0.039039656712231995
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 0.03603433212372863
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 0.02764895334230798
        },
        {
          "title": "Universal Language Model Fine-tuning for Text Classification",
          "score": 0.021970019392859697
        },
        {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "score": 0.017996431926545178
        },
        {
          "title": "Cross-lingual Language Model Pretraining",
          "score": 0.015920326406895773
        },
        {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "score": 0.014156571552333814
        },
        {
          "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
          "score": 0.013950193888267086
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 0.012193388980609466
        },
        {
          "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD",
          "score": 0.011235124521213016
        },
        {
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "score": 0.010106347215095748
        },
        {
          "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
          "score": 0.008429692830626612
        },
        {
          "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
          "score": 0.007406177911170892
        },
        {
          "title": "Contextual String Embeddings for Sequence Labeling",
          "score": 0.007216680430739239
        },
        {
          "title": "Patient Knowledge Distillation for BERT Model Compression",
          "score": 0.007210994636000413
        }
      ]
    }
  },
  {
    "window_start": 2019,
    "window_end": 2021,
    "paper_count": 5582,
    "edge_count": 11216,
    "dbcv_score": 0.3899422184281573,
    "optimal_params": {
      "n_neighbors": 10,
      "min_cluster_size": 15,
      "min_samples": 15
    },
    "topics": [
      {
        "cluster_id": 4,
        "size": 586,
        "keywords": [
          "language",
          "natural",
          "processing",
          "learning",
          "clinical",
          "using",
          "health",
          "machine",
          "medical",
          "text",
          "based",
          "data",
          "electronic",
          "records",
          "deep"
        ],
        "confidence": 0.9851897398887044
      },
      {
        "cluster_id": 25,
        "size": 261,
        "keywords": [
          "sentiment",
          "analysis",
          "using",
          "based",
          "learning",
          "language",
          "deep",
          "natural",
          "processing",
          "text",
          "classification",
          "social",
          "twitter",
          "machine",
          "arabic"
        ],
        "confidence": 0.884480187555169
      },
      {
        "cluster_id": 1,
        "size": 230,
        "keywords": [
          "adversarial",
          "language",
          "learning",
          "natural",
          "detection",
          "processing",
          "using",
          "deep",
          "based",
          "attacks",
          "privacy",
          "models",
          "machine",
          "text",
          "attack"
        ],
        "confidence": 0.990471567238145
      },
      {
        "cluster_id": 33,
        "size": 125,
        "keywords": [
          "language",
          "brain",
          "natural",
          "processing",
          "human",
          "linguistic",
          "neural",
          "semantic",
          "representations",
          "word",
          "languages",
          "learning",
          "comprehension",
          "effects",
          "structure"
        ],
        "confidence": 0.8382964144082607
      },
      {
        "cluster_id": 13,
        "size": 117,
        "keywords": [
          "chatbot",
          "language",
          "chatbots",
          "using",
          "natural",
          "ai",
          "conversational",
          "based",
          "processing",
          "learning",
          "intelligence",
          "artificial",
          "human",
          "intelligent",
          "virtual"
        ],
        "confidence": 0.6808499153045807
      },
      {
        "cluster_id": 0,
        "size": 104,
        "keywords": [
          "learning",
          "deep",
          "language",
          "using",
          "protein",
          "sequence",
          "chemical",
          "molecular",
          "natural",
          "prediction",
          "models",
          "based",
          "data",
          "processing",
          "model"
        ],
        "confidence": 0.9663873839233031
      },
      {
        "cluster_id": 8,
        "size": 96,
        "keywords": [
          "bias",
          "gender",
          "language",
          "natural",
          "word",
          "processing",
          "embeddings",
          "biases",
          "models",
          "fairness",
          "nlp",
          "learning",
          "analysis",
          "inference",
          "evaluating"
        ],
        "confidence": 0.6550618189349845
      },
      {
        "cluster_id": 24,
        "size": 86,
        "keywords": [
          "classification",
          "text",
          "learning",
          "deep",
          "based",
          "neural",
          "using",
          "networks",
          "model",
          "attention",
          "multi",
          "label",
          "bert",
          "survey",
          "network"
        ],
        "confidence": 0.6806130948258662
      },
      {
        "cluster_id": 50,
        "size": 80,
        "keywords": [
          "learning",
          "deep",
          "survey",
          "neural",
          "applications",
          "networks",
          "reinforcement",
          "machine",
          "review",
          "architectures",
          "based",
          "recent",
          "using",
          "research",
          "optimization"
        ],
        "confidence": 0.8116207472491148
      },
      {
        "cluster_id": 46,
        "size": 78,
        "keywords": [
          "translation",
          "machine",
          "language",
          "neural",
          "processing",
          "natural",
          "survey",
          "arabic",
          "english",
          "using",
          "kurdish",
          "bangla",
          "corpus",
          "review",
          "learning"
        ],
        "confidence": 0.9024435414131634
      },
      {
        "cluster_id": 39,
        "size": 76,
        "keywords": [
          "processing",
          "natural",
          "language",
          "text",
          "summarization",
          "using",
          "based",
          "extraction",
          "nlp",
          "analysis",
          "information",
          "literature",
          "mining",
          "approach",
          "automatic"
        ],
        "confidence": 0.691329321184997
      },
      {
        "cluster_id": 15,
        "size": 70,
        "keywords": [
          "entity",
          "recognition",
          "named",
          "relation",
          "model",
          "neural",
          "based",
          "extraction",
          "survey",
          "bert",
          "nested",
          "using",
          "pre",
          "ner",
          "learning"
        ],
        "confidence": 0.6897915179531686
      },
      {
        "cluster_id": 45,
        "size": 70,
        "keywords": [
          "word",
          "embeddings",
          "embedding",
          "language",
          "representations",
          "sense",
          "survey",
          "processing",
          "natural",
          "similarity",
          "using",
          "neural",
          "learning",
          "evaluation",
          "models"
        ],
        "confidence": 0.7683955857664408
      },
      {
        "cluster_id": 48,
        "size": 69,
        "keywords": [
          "language",
          "model",
          "bert",
          "multilingual",
          "models",
          "cross",
          "lingual",
          "pre",
          "training",
          "pretraining",
          "based",
          "learning",
          "transformer",
          "transfer",
          "understanding"
        ],
        "confidence": 0.8369285828301791
      },
      {
        "cluster_id": 54,
        "size": 66,
        "keywords": [
          "bert",
          "language",
          "models",
          "distillation",
          "knowledge",
          "pruning",
          "pre",
          "compression",
          "trained",
          "structured",
          "natural",
          "transformers",
          "large",
          "model",
          "transformer"
        ],
        "confidence": 0.8432899901464643
      },
      {
        "cluster_id": 12,
        "size": 66,
        "keywords": [
          "language",
          "learning",
          "automated",
          "education",
          "natural",
          "using",
          "processing",
          "scoring",
          "essay",
          "based",
          "artificial",
          "intelligence",
          "student",
          "analysis",
          "research"
        ],
        "confidence": 0.7477575684561029
      },
      {
        "cluster_id": 41,
        "size": 64,
        "keywords": [
          "requirements",
          "language",
          "natural",
          "using",
          "based",
          "processing",
          "test",
          "approach",
          "models",
          "generation",
          "nlp",
          "automated",
          "model",
          "software",
          "process"
        ],
        "confidence": 0.5915319330587302
      },
      {
        "cluster_id": 42,
        "size": 57,
        "keywords": [
          "code",
          "language",
          "source",
          "natural",
          "deep",
          "programming",
          "generation",
          "based",
          "learning",
          "models",
          "neural",
          "using",
          "embeddings",
          "bug",
          "model"
        ],
        "confidence": 0.7681320935449691
      },
      {
        "cluster_id": 51,
        "size": 56,
        "keywords": [
          "learning",
          "language",
          "natural",
          "processing",
          "transfer",
          "data",
          "deep",
          "survey",
          "text",
          "shot",
          "augmentation",
          "multi",
          "neural",
          "nlp",
          "task"
        ],
        "confidence": 0.9520097075154998
      },
      {
        "cluster_id": 26,
        "size": 55,
        "keywords": [
          "fake",
          "news",
          "detection",
          "using",
          "learning",
          "language",
          "natural",
          "processing",
          "based",
          "machine",
          "deep",
          "data",
          "model",
          "classification",
          "analysis"
        ],
        "confidence": 0.7469231481146065
      },
      {
        "cluster_id": 27,
        "size": 54,
        "keywords": [
          "transformer",
          "transformers",
          "vision",
          "image",
          "survey",
          "segmentation",
          "visual",
          "point",
          "based",
          "classification",
          "remote",
          "sensing",
          "cloud",
          "audio",
          "attention"
        ],
        "confidence": 0.7764901050749407
      },
      {
        "cluster_id": 17,
        "size": 46,
        "keywords": [
          "emotion",
          "recognition",
          "detection",
          "analysis",
          "multi",
          "based",
          "multimodal",
          "text",
          "learning",
          "using",
          "speech",
          "sentiment",
          "network",
          "attention",
          "social"
        ],
        "confidence": 0.8489016574058362
      },
      {
        "cluster_id": 40,
        "size": 44,
        "keywords": [
          "language",
          "processing",
          "natural",
          "learning",
          "deep",
          "survey",
          "models",
          "review",
          "machine",
          "pre",
          "neural",
          "techniques",
          "trained",
          "translation",
          "representation"
        ],
        "confidence": 0.7005715009161273
      },
      {
        "cluster_id": 44,
        "size": 43,
        "keywords": [
          "question",
          "answering",
          "knowledge",
          "reading",
          "machine",
          "comprehension",
          "survey",
          "dataset",
          "language",
          "base",
          "bert",
          "questions",
          "based",
          "learning",
          "using"
        ],
        "confidence": 0.8602856880952541
      },
      {
        "cluster_id": 30,
        "size": 42,
        "keywords": [
          "image",
          "captioning",
          "caption",
          "learning",
          "using",
          "deep",
          "text",
          "based",
          "generation",
          "generator",
          "language",
          "survey",
          "attention",
          "natural",
          "cnn"
        ],
        "confidence": 0.710541932399344
      },
      {
        "cluster_id": 3,
        "size": 41,
        "keywords": [
          "language",
          "processing",
          "natural",
          "speech",
          "based",
          "alzheimer",
          "disease",
          "detection",
          "dementia",
          "models",
          "using",
          "cognitive",
          "prediction",
          "learning",
          "analysis"
        ],
        "confidence": 0.723335161655743
      },
      {
        "cluster_id": 52,
        "size": 41,
        "keywords": [
          "deep",
          "processing",
          "learning",
          "efficient",
          "neural",
          "language",
          "based",
          "accelerator",
          "fpga",
          "transformer",
          "natural",
          "networks",
          "transformers",
          "hardware",
          "architecture"
        ],
        "confidence": 0.7996234349513869
      },
      {
        "cluster_id": 10,
        "size": 37,
        "keywords": [
          "learning",
          "stock",
          "using",
          "based",
          "analysis",
          "forecasting",
          "prediction",
          "recurrent",
          "sentiment",
          "deep",
          "networks",
          "neural",
          "natural",
          "language",
          "processing"
        ],
        "confidence": 0.9014205267633686
      },
      {
        "cluster_id": 16,
        "size": 37,
        "keywords": [
          "speech",
          "end",
          "language",
          "spoken",
          "understanding",
          "learning",
          "transformer",
          "recognition",
          "based",
          "processing",
          "deep",
          "models",
          "automatic",
          "using",
          "self"
        ],
        "confidence": 0.6540738760086605
      },
      {
        "cluster_id": 35,
        "size": 35,
        "keywords": [
          "language",
          "natural",
          "processing",
          "using",
          "based",
          "learning",
          "accident",
          "analysis",
          "text",
          "machine",
          "reports",
          "safety",
          "fault",
          "aviation",
          "mining"
        ],
        "confidence": 0.8048921295478059
      },
      {
        "cluster_id": 29,
        "size": 34,
        "keywords": [
          "language",
          "vision",
          "navigation",
          "learning",
          "visual",
          "transformer",
          "models",
          "human",
          "grounding",
          "natural",
          "multimodal",
          "adversarial",
          "path",
          "attention",
          "tasks"
        ],
        "confidence": 0.9488561129460005
      },
      {
        "cluster_id": 22,
        "size": 34,
        "keywords": [
          "graph",
          "networks",
          "neural",
          "survey",
          "learning",
          "attention",
          "transformer",
          "self",
          "training",
          "recommendation",
          "sequential",
          "supervised",
          "network",
          "classification",
          "graphs"
        ],
        "confidence": 0.7037907423454169
      },
      {
        "cluster_id": 28,
        "size": 33,
        "keywords": [
          "visual",
          "question",
          "answering",
          "graph",
          "scene",
          "referring",
          "reasoning",
          "survey",
          "aware",
          "expression",
          "comprehension",
          "based",
          "knowledge",
          "datasets",
          "sensing"
        ],
        "confidence": 0.8492985743109843
      },
      {
        "cluster_id": 37,
        "size": 32,
        "keywords": [
          "natural",
          "language",
          "inference",
          "data",
          "annotation",
          "reasoning",
          "knowledge",
          "commonsense",
          "crowdsourcing",
          "text",
          "tasks",
          "annotator",
          "contrasting",
          "subjective",
          "nlp"
        ],
        "confidence": 0.9415440758201605
      },
      {
        "cluster_id": 32,
        "size": 31,
        "keywords": [
          "language",
          "learning",
          "natural",
          "human",
          "linguistic",
          "processing",
          "compositional",
          "reinforcement",
          "semantic",
          "learn",
          "grounding",
          "representations",
          "explanations",
          "neural",
          "tasks"
        ],
        "confidence": 0.9409124941368897
      },
      {
        "cluster_id": 53,
        "size": 31,
        "keywords": [
          "attention",
          "self",
          "transformer",
          "transformers",
          "memory",
          "multi",
          "language",
          "explicit",
          "sparse",
          "models",
          "head",
          "depth",
          "networks",
          "need",
          "modeling"
        ],
        "confidence": 0.9497298030704077
      },
      {
        "cluster_id": 43,
        "size": 31,
        "keywords": [
          "natural",
          "language",
          "semantic",
          "query",
          "ontologies",
          "knowledge",
          "ontology",
          "sql",
          "web",
          "processing",
          "legal",
          "data",
          "domain",
          "querying",
          "survey"
        ],
        "confidence": 0.917618894313372
      },
      {
        "cluster_id": 21,
        "size": 31,
        "keywords": [
          "hate",
          "speech",
          "detection",
          "using",
          "online",
          "learning",
          "language",
          "media",
          "social",
          "multilingual",
          "review",
          "bert",
          "data",
          "challenges",
          "natural"
        ],
        "confidence": 0.8755621511365136
      },
      {
        "cluster_id": 11,
        "size": 31,
        "keywords": [
          "dialogue",
          "language",
          "task",
          "natural",
          "generation",
          "oriented",
          "based",
          "systems",
          "learning",
          "conversational",
          "survey",
          "open",
          "domain",
          "pre",
          "dialogues"
        ],
        "confidence": 0.8503694485099397
      },
      {
        "cluster_id": 6,
        "size": 29,
        "keywords": [
          "covid",
          "19",
          "artificial",
          "intelligence",
          "pandemic",
          "language",
          "processing",
          "natural",
          "learning",
          "research",
          "machine",
          "deep",
          "data",
          "coronavirus",
          "ai"
        ],
        "confidence": 0.8625897372502452
      },
      {
        "cluster_id": 34,
        "size": 29,
        "keywords": [
          "legal",
          "language",
          "mining",
          "processing",
          "argument",
          "natural",
          "law",
          "text",
          "using",
          "computational",
          "analysis",
          "artificial",
          "intelligence",
          "understanding",
          "learning"
        ],
        "confidence": 0.9087406803227857
      },
      {
        "cluster_id": 14,
        "size": 29,
        "keywords": [
          "language",
          "natural",
          "processing",
          "construction",
          "artificial",
          "intelligence",
          "using",
          "ai",
          "management",
          "nlp",
          "learning",
          "review",
          "automated",
          "neuroscience",
          "requirements"
        ],
        "confidence": 0.9408130625355969
      },
      {
        "cluster_id": 7,
        "size": 29,
        "keywords": [
          "covid",
          "19",
          "twitter",
          "analysis",
          "language",
          "natural",
          "processing",
          "sentiment",
          "using",
          "pandemic",
          "data",
          "tweets",
          "study",
          "mining",
          "media"
        ],
        "confidence": 0.8980619776844393
      },
      {
        "cluster_id": 9,
        "size": 27,
        "keywords": [
          "language",
          "natural",
          "visual",
          "data",
          "interface",
          "storytelling",
          "visualization",
          "generating",
          "exploration",
          "processing",
          "multimodal",
          "3d",
          "interactive",
          "generative",
          "framework"
        ],
        "confidence": 0.9276936555810964
      },
      {
        "cluster_id": 36,
        "size": 27,
        "keywords": [
          "social",
          "data",
          "natural",
          "using",
          "media",
          "disaster",
          "learning",
          "smart",
          "machine",
          "mining",
          "language",
          "processing",
          "temporal",
          "disasters",
          "twitter"
        ],
        "confidence": 0.8704642951720805
      },
      {
        "cluster_id": 2,
        "size": 25,
        "keywords": [
          "quantum",
          "language",
          "processing",
          "natural",
          "computers",
          "learning",
          "deep",
          "advances",
          "intelligence",
          "hybrid",
          "question",
          "answering",
          "based",
          "grammar",
          "aware"
        ],
        "confidence": 0.7902739387634935
      },
      {
        "cluster_id": 49,
        "size": 25,
        "keywords": [
          "deep",
          "neural",
          "convolutional",
          "learning",
          "network",
          "cnn",
          "architectures",
          "networks",
          "survey",
          "models",
          "based",
          "analysis",
          "image",
          "processing",
          "applications"
        ],
        "confidence": 0.8499344204147696
      },
      {
        "cluster_id": 47,
        "size": 22,
        "keywords": [
          "language",
          "corpus",
          "english",
          "data",
          "natural",
          "dataset",
          "web",
          "inference",
          "representation",
          "question",
          "answering",
          "portuguese",
          "texts",
          "annotated",
          "brazilian"
        ],
        "confidence": 0.9938291197053271
      },
      {
        "cluster_id": 20,
        "size": 22,
        "keywords": [
          "detection",
          "cyberbullying",
          "learning",
          "language",
          "social",
          "using",
          "machine",
          "processing",
          "media",
          "natural",
          "cyber",
          "approaches",
          "abuse",
          "techniques",
          "framework"
        ],
        "confidence": 0.9238080171389856
      },
      {
        "cluster_id": 38,
        "size": 22,
        "keywords": [
          "language",
          "processing",
          "natural",
          "survey",
          "systems",
          "stemming",
          "study",
          "stopwords",
          "automatic",
          "text",
          "review",
          "kazakh",
          "word",
          "corpus",
          "applications"
        ],
        "confidence": 0.9828763497277009
      },
      {
        "cluster_id": 31,
        "size": 22,
        "keywords": [
          "video",
          "retrieval",
          "moment",
          "language",
          "learning",
          "network",
          "natural",
          "captioning",
          "localization",
          "review",
          "using",
          "deep",
          "text",
          "interaction",
          "weakly"
        ],
        "confidence": 0.9598972029446049
      },
      {
        "cluster_id": 18,
        "size": 21,
        "keywords": [
          "speech",
          "tagging",
          "using",
          "deep",
          "learning",
          "language",
          "approaches",
          "pos",
          "tagger",
          "based",
          "parts",
          "processing",
          "natural",
          "neural",
          "techniques"
        ],
        "confidence": 0.8662195559891411
      },
      {
        "cluster_id": 23,
        "size": 20,
        "keywords": [
          "graph",
          "knowledge",
          "completion",
          "language",
          "neural",
          "natural",
          "networks",
          "learning",
          "methods",
          "multi",
          "processing",
          "deep",
          "embedding",
          "based",
          "shot"
        ],
        "confidence": 0.9734989534602443
      },
      {
        "cluster_id": 19,
        "size": 17,
        "keywords": [
          "language",
          "sign",
          "speech",
          "using",
          "learning",
          "recognition",
          "translation",
          "avatar",
          "deep",
          "3d",
          "translator",
          "text",
          "impaired",
          "arabic",
          "deaf"
        ],
        "confidence": 0.9972753201664989
      },
      {
        "cluster_id": 5,
        "size": 16,
        "keywords": [
          "depression",
          "social",
          "media",
          "health",
          "using",
          "mental",
          "detection",
          "posts",
          "language",
          "natural",
          "processing",
          "surveillance",
          "data",
          "detecting",
          "arabic"
        ],
        "confidence": 0.9995552609496013
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 94099
        },
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 41302
        },
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 28903
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 24213
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 22682
        },
        {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "score": 19888
        },
        {
          "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "score": 10743
        },
        {
          "title": "LoRA: Low-Rank Adaptation of Large Language Models",
          "score": 10099
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 8395
        },
        {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "score": 7422
        },
        {
          "title": "Training data-efficient image transformers & distillation through attention",
          "score": 6690
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 6424
        },
        {
          "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
          "score": 6136
        },
        {
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "score": 5602
        },
        {
          "title": "Evaluating Large Language Models Trained on Code",
          "score": 5385
        },
        {
          "title": "Scaling Laws for Neural Language Models",
          "score": 4707
        },
        {
          "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
          "score": 4617
        },
        {
          "title": "Measuring Massive Multitask Language Understanding",
          "score": 4304
        },
        {
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
          "score": 3934
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 3715
        }
      ],
      "by_pagerank": [
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 0.11301303051134719
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 0.016758291703401366
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 0.015509629899824648
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 0.012089233064052149
        },
        {
          "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
          "score": 0.010072905103119496
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 0.007198891843270133
        },
        {
          "title": "Cross-lingual Language Model Pretraining",
          "score": 0.006447395448776733
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 0.005911839780287174
        },
        {
          "title": "Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding",
          "score": 0.004891632741853887
        },
        {
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "score": 0.00464275608449897
        },
        {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "score": 0.004518112970169536
        },
        {
          "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
          "score": 0.004370845713028373
        },
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 0.004155502611880304
        },
        {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "score": 0.0030701814535167944
        },
        {
          "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
          "score": 0.0028605422517129597
        },
        {
          "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
          "score": 0.002662611315588513
        },
        {
          "title": "Publicly Available Clinical BERT Embeddings",
          "score": 0.0024800605680422574
        },
        {
          "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them",
          "score": 0.0023326761546307636
        },
        {
          "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "score": 0.0021333199838863308
        },
        {
          "title": "Generating Long Sequences with Sparse Transformers",
          "score": 0.0019178624402590146
        }
      ],
      "by_authority": [
        {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "score": 0.21373435838862542
        },
        {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "score": 0.07776749037059516
        },
        {
          "title": "Language Models are Unsupervised Multitask Learners",
          "score": 0.04464151397189977
        },
        {
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
          "score": 0.04144610435382316
        },
        {
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "score": 0.03587485893819941
        },
        {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "score": 0.02600403720475763
        },
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 0.02323866595940754
        },
        {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "score": 0.02026813387442085
        },
        {
          "title": "Cross-lingual Language Model Pretraining",
          "score": 0.01665523922404995
        },
        {
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "score": 0.014569780459490115
        },
        {
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context",
          "score": 0.013075614306385227
        },
        {
          "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "score": 0.01295878111505958
        },
        {
          "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
          "score": 0.011686141379083442
        },
        {
          "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
          "score": 0.010527193239264048
        },
        {
          "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems",
          "score": 0.01039406607577764
        },
        {
          "title": "SciBERT: A Pretrained Language Model for Scientific Text",
          "score": 0.009834859720994767
        },
        {
          "title": "Patient Knowledge Distillation for BERT Model Compression",
          "score": 0.008393381678696318
        },
        {
          "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
          "score": 0.006940553444003174
        },
        {
          "title": "MASS: Masked Sequence to Sequence Pre-training for Language Generation",
          "score": 0.00691768202575706
        },
        {
          "title": "Publicly Available Clinical BERT Embeddings",
          "score": 0.006816596447957754
        }
      ]
    }
  },
  {
    "window_start": 2020,
    "window_end": 2022,
    "paper_count": 5579,
    "edge_count": 7223,
    "dbcv_score": 0.40573324315087905,
    "optimal_params": {
      "n_neighbors": 15,
      "min_cluster_size": 15,
      "min_samples": 15
    },
    "topics": [
      {
        "cluster_id": 1,
        "size": 812,
        "keywords": [
          "language",
          "natural",
          "processing",
          "using",
          "learning",
          "clinical",
          "health",
          "based",
          "medical",
          "machine",
          "review",
          "electronic",
          "data",
          "text",
          "records"
        ],
        "confidence": 0.9920337833747707
      },
      {
        "cluster_id": 26,
        "size": 240,
        "keywords": [
          "sentiment",
          "analysis",
          "using",
          "language",
          "based",
          "learning",
          "natural",
          "processing",
          "deep",
          "social",
          "machine",
          "text",
          "twitter",
          "approach",
          "media"
        ],
        "confidence": 0.9043902735574121
      },
      {
        "cluster_id": 49,
        "size": 128,
        "keywords": [
          "language",
          "translation",
          "machine",
          "natural",
          "processing",
          "english",
          "corpus",
          "languages",
          "multilingual",
          "based",
          "neural",
          "resource",
          "word",
          "low",
          "kurdish"
        ],
        "confidence": 0.9544047870484273
      },
      {
        "cluster_id": 0,
        "size": 127,
        "keywords": [
          "language",
          "learning",
          "deep",
          "using",
          "natural",
          "models",
          "protein",
          "model",
          "processing",
          "chemical",
          "data",
          "sequence",
          "prediction",
          "molecular",
          "based"
        ],
        "confidence": 0.8897478479604307
      },
      {
        "cluster_id": 31,
        "size": 119,
        "keywords": [
          "language",
          "natural",
          "processing",
          "text",
          "using",
          "extraction",
          "survey",
          "based",
          "analysis",
          "summarization",
          "information",
          "techniques",
          "methods",
          "review",
          "literature"
        ],
        "confidence": 0.753289497830776
      },
      {
        "cluster_id": 13,
        "size": 112,
        "keywords": [
          "text",
          "classification",
          "learning",
          "based",
          "deep",
          "using",
          "survey",
          "model",
          "graph",
          "multi",
          "neural",
          "label",
          "networks",
          "attention",
          "language"
        ],
        "confidence": 0.6655335432915388
      },
      {
        "cluster_id": 43,
        "size": 99,
        "keywords": [
          "language",
          "natural",
          "brain",
          "processing",
          "neural",
          "linguistic",
          "models",
          "structure",
          "comprehension",
          "representations",
          "semantic",
          "cognitive",
          "human",
          "corpus",
          "effects"
        ],
        "confidence": 0.6689670788506314
      },
      {
        "cluster_id": 35,
        "size": 92,
        "keywords": [
          "code",
          "language",
          "natural",
          "programming",
          "models",
          "based",
          "using",
          "source",
          "bug",
          "deep",
          "generation",
          "model",
          "software",
          "large",
          "neural"
        ],
        "confidence": 0.5196352766251088
      },
      {
        "cluster_id": 7,
        "size": 87,
        "keywords": [
          "bias",
          "gender",
          "language",
          "natural",
          "processing",
          "fairness",
          "nlp",
          "embeddings",
          "word",
          "survey",
          "models",
          "analysis",
          "systems",
          "using",
          "learning"
        ],
        "confidence": 0.7808086836919603
      },
      {
        "cluster_id": 2,
        "size": 87,
        "keywords": [
          "detection",
          "based",
          "using",
          "language",
          "learning",
          "processing",
          "natural",
          "machine",
          "deep",
          "phishing",
          "vulnerability",
          "security",
          "malware",
          "model",
          "attack"
        ],
        "confidence": 0.6542138467949878
      },
      {
        "cluster_id": 21,
        "size": 83,
        "keywords": [
          "entity",
          "named",
          "recognition",
          "based",
          "relation",
          "model",
          "extraction",
          "using",
          "bert",
          "deep",
          "learning",
          "neural",
          "language",
          "network",
          "crf"
        ],
        "confidence": 0.5856719272105118
      },
      {
        "cluster_id": 44,
        "size": 75,
        "keywords": [
          "language",
          "natural",
          "reasoning",
          "models",
          "inference",
          "explanations",
          "large",
          "understanding",
          "model",
          "knowledge",
          "symbolic",
          "processing",
          "survey",
          "commonsense",
          "nlp"
        ],
        "confidence": 0.7407454043196324
      },
      {
        "cluster_id": 38,
        "size": 75,
        "keywords": [
          "question",
          "answering",
          "based",
          "survey",
          "language",
          "learning",
          "knowledge",
          "models",
          "machine",
          "reading",
          "transformer",
          "essay",
          "comprehension",
          "automated",
          "answer"
        ],
        "confidence": 0.8046336561388685
      },
      {
        "cluster_id": 40,
        "size": 75,
        "keywords": [
          "language",
          "natural",
          "requirements",
          "processing",
          "engineering",
          "using",
          "nlp",
          "models",
          "based",
          "approach",
          "automated",
          "process",
          "model",
          "techniques",
          "systematic"
        ],
        "confidence": 0.5593995617368773
      },
      {
        "cluster_id": 48,
        "size": 72,
        "keywords": [
          "language",
          "text",
          "generation",
          "model",
          "pre",
          "models",
          "trained",
          "pretrained",
          "sequence",
          "based",
          "neural",
          "survey",
          "lingual",
          "bert",
          "training"
        ],
        "confidence": 0.9280761630670722
      },
      {
        "cluster_id": 8,
        "size": 69,
        "keywords": [
          "speech",
          "language",
          "end",
          "audio",
          "recognition",
          "spoken",
          "self",
          "learning",
          "understanding",
          "supervised",
          "training",
          "transformer",
          "processing",
          "pre",
          "music"
        ],
        "confidence": 0.6020446678089381
      },
      {
        "cluster_id": 14,
        "size": 69,
        "keywords": [
          "language",
          "learning",
          "natural",
          "education",
          "processing",
          "intelligence",
          "artificial",
          "based",
          "teaching",
          "using",
          "students",
          "research",
          "student",
          "ai",
          "review"
        ],
        "confidence": 0.7991060472841168
      },
      {
        "cluster_id": 20,
        "size": 67,
        "keywords": [
          "fake",
          "news",
          "detection",
          "learning",
          "using",
          "language",
          "natural",
          "processing",
          "based",
          "machine",
          "deep",
          "classification",
          "model",
          "techniques",
          "spam"
        ],
        "confidence": 0.6830819680106661
      },
      {
        "cluster_id": 5,
        "size": 64,
        "keywords": [
          "adversarial",
          "language",
          "robustness",
          "attacks",
          "natural",
          "nlp",
          "attack",
          "models",
          "training",
          "learning",
          "processing",
          "word",
          "text",
          "examples",
          "level"
        ],
        "confidence": 0.8030666668190852
      },
      {
        "cluster_id": 36,
        "size": 61,
        "keywords": [
          "transformers",
          "transformer",
          "vision",
          "survey",
          "image",
          "based",
          "classification",
          "point",
          "remote",
          "sensing",
          "visual",
          "cloud",
          "learning",
          "deep",
          "detection"
        ],
        "confidence": 0.825770923215984
      },
      {
        "cluster_id": 17,
        "size": 55,
        "keywords": [
          "chatbot",
          "chatbots",
          "language",
          "natural",
          "using",
          "processing",
          "ai",
          "review",
          "development",
          "based",
          "learning",
          "conversational",
          "nlp",
          "intelligent",
          "survey"
        ],
        "confidence": 0.7472524868867163
      },
      {
        "cluster_id": 33,
        "size": 50,
        "keywords": [
          "image",
          "captioning",
          "using",
          "caption",
          "learning",
          "deep",
          "generation",
          "based",
          "attention",
          "generator",
          "text",
          "neural",
          "model",
          "mechanism",
          "transformer"
        ],
        "confidence": 0.7027607744102622
      },
      {
        "cluster_id": 23,
        "size": 49,
        "keywords": [
          "covid",
          "19",
          "twitter",
          "analysis",
          "sentiment",
          "language",
          "using",
          "natural",
          "processing",
          "tweets",
          "vaccine",
          "pandemic",
          "learning",
          "data",
          "vaccination"
        ],
        "confidence": 0.7379426068600697
      },
      {
        "cluster_id": 34,
        "size": 48,
        "keywords": [
          "deep",
          "learning",
          "neural",
          "survey",
          "networks",
          "applications",
          "convolutional",
          "network",
          "based",
          "review",
          "cnn",
          "analysis",
          "application",
          "algorithm",
          "optimization"
        ],
        "confidence": 0.8838463273977389
      },
      {
        "cluster_id": 25,
        "size": 48,
        "keywords": [
          "detection",
          "hate",
          "speech",
          "sarcasm",
          "language",
          "using",
          "learning",
          "based",
          "processing",
          "natural",
          "review",
          "media",
          "social",
          "multimodal",
          "multilingual"
        ],
        "confidence": 0.6571520317812894
      },
      {
        "cluster_id": 50,
        "size": 47,
        "keywords": [
          "learning",
          "language",
          "natural",
          "processing",
          "survey",
          "task",
          "meta",
          "multi",
          "transfer",
          "neural",
          "sequence",
          "shot",
          "continual",
          "training",
          "tasks"
        ],
        "confidence": 0.9267392458827233
      },
      {
        "cluster_id": 3,
        "size": 46,
        "keywords": [
          "privacy",
          "learning",
          "federated",
          "language",
          "natural",
          "models",
          "processing",
          "text",
          "differential",
          "preserving",
          "private",
          "policies",
          "heterogeneous",
          "meets",
          "large"
        ],
        "confidence": 0.689389893758495
      },
      {
        "cluster_id": 29,
        "size": 44,
        "keywords": [
          "language",
          "learning",
          "reinforcement",
          "navigation",
          "robot",
          "visual",
          "vision",
          "multi",
          "manipulation",
          "data",
          "instructions",
          "conditioned",
          "robotic",
          "imitation",
          "modeling"
        ],
        "confidence": 0.8094345052666974
      },
      {
        "cluster_id": 6,
        "size": 40,
        "keywords": [
          "quantum",
          "language",
          "natural",
          "processing",
          "term",
          "neural",
          "near",
          "computers",
          "advances",
          "network",
          "learning",
          "wave",
          "functions",
          "recurrent",
          "intelligence"
        ],
        "confidence": 0.534992471853606
      },
      {
        "cluster_id": 10,
        "size": 38,
        "keywords": [
          "legal",
          "language",
          "natural",
          "processing",
          "mining",
          "prediction",
          "argument",
          "text",
          "using",
          "survey",
          "learning",
          "law",
          "argumentation",
          "judgment",
          "dataset"
        ],
        "confidence": 0.8355257348974824
      },
      {
        "cluster_id": 22,
        "size": 37,
        "keywords": [
          "emotion",
          "text",
          "recognition",
          "detection",
          "using",
          "learning",
          "based",
          "multi",
          "analysis",
          "deep",
          "approach",
          "network",
          "survey",
          "urdu",
          "speech"
        ],
        "confidence": 0.7868753341964588
      },
      {
        "cluster_id": 45,
        "size": 36,
        "keywords": [
          "language",
          "processing",
          "natural",
          "learning",
          "survey",
          "deep",
          "models",
          "representation",
          "neural",
          "machine",
          "review",
          "ai",
          "techniques",
          "translation",
          "overview"
        ],
        "confidence": 0.9977425035044827
      },
      {
        "cluster_id": 46,
        "size": 33,
        "keywords": [
          "word",
          "embeddings",
          "embedding",
          "survey",
          "language",
          "natural",
          "processing",
          "evaluation",
          "representations",
          "representation",
          "vector",
          "neural",
          "sense",
          "semantic",
          "meaning"
        ],
        "confidence": 0.8958144886290716
      },
      {
        "cluster_id": 27,
        "size": 33,
        "keywords": [
          "graph",
          "networks",
          "neural",
          "survey",
          "learning",
          "graphs",
          "network",
          "processing",
          "transformer",
          "self",
          "supervised",
          "applications",
          "node",
          "transformers",
          "deep"
        ],
        "confidence": 0.7250759793658711
      },
      {
        "cluster_id": 53,
        "size": 30,
        "keywords": [
          "based",
          "transformer",
          "processing",
          "fpga",
          "fpgas",
          "acceleration",
          "networks",
          "language",
          "efficient",
          "accelerating",
          "using",
          "inference",
          "natural",
          "deep",
          "accelerator"
        ],
        "confidence": 0.903785093227959
      },
      {
        "cluster_id": 41,
        "size": 30,
        "keywords": [
          "language",
          "natural",
          "sql",
          "query",
          "data",
          "text",
          "review",
          "queries",
          "querying",
          "parsing",
          "processing",
          "semantic",
          "interfaces",
          "interface",
          "systems"
        ],
        "confidence": 0.8832961489025649
      },
      {
        "cluster_id": 16,
        "size": 29,
        "keywords": [
          "dialogue",
          "based",
          "task",
          "generation",
          "oriented",
          "learning",
          "conversational",
          "open",
          "domain",
          "systems",
          "question",
          "dataset",
          "multi",
          "data",
          "bert"
        ],
        "confidence": 0.8515231353540276
      },
      {
        "cluster_id": 4,
        "size": 28,
        "keywords": [
          "backdoor",
          "models",
          "attacks",
          "language",
          "trained",
          "pre",
          "backdoors",
          "nlp",
          "textual",
          "natural",
          "survey",
          "defense",
          "text",
          "model",
          "processing"
        ],
        "confidence": 0.8454340583498864
      },
      {
        "cluster_id": 12,
        "size": 27,
        "keywords": [
          "stock",
          "using",
          "forecasting",
          "prediction",
          "based",
          "learning",
          "sentiment",
          "natural",
          "language",
          "price",
          "analysis",
          "processing",
          "news",
          "financial",
          "deep"
        ],
        "confidence": 0.8942160702748567
      },
      {
        "cluster_id": 39,
        "size": 27,
        "keywords": [
          "processing",
          "language",
          "natural",
          "using",
          "reports",
          "based",
          "aviation",
          "analysis",
          "machine",
          "learning",
          "safety",
          "factors",
          "accident",
          "industry",
          "text"
        ],
        "confidence": 0.9445669063086471
      },
      {
        "cluster_id": 30,
        "size": 27,
        "keywords": [
          "visual",
          "question",
          "answering",
          "sensing",
          "remote",
          "language",
          "graph",
          "survey",
          "reasoning",
          "based",
          "network",
          "transformers",
          "scene",
          "datasets",
          "grounding"
        ],
        "confidence": 0.9407186867979291
      },
      {
        "cluster_id": 24,
        "size": 26,
        "keywords": [
          "detection",
          "cyberbullying",
          "using",
          "language",
          "learning",
          "processing",
          "machine",
          "natural",
          "social",
          "media",
          "techniques",
          "cyber",
          "based",
          "approaches",
          "urdu"
        ],
        "confidence": 0.8468765040925443
      },
      {
        "cluster_id": 19,
        "size": 25,
        "keywords": [
          "social",
          "media",
          "natural",
          "based",
          "data",
          "mining",
          "smart",
          "machine",
          "learning",
          "disaster",
          "citizen",
          "urban",
          "language",
          "processing",
          "disasters"
        ],
        "confidence": 0.924401148015783
      },
      {
        "cluster_id": 18,
        "size": 24,
        "keywords": [
          "chatbot",
          "health",
          "artificial",
          "conversational",
          "intelligence",
          "mental",
          "based",
          "review",
          "medical",
          "chatbots",
          "agents",
          "virtual",
          "language",
          "natural",
          "processing"
        ],
        "confidence": 0.872428707379885
      },
      {
        "cluster_id": 32,
        "size": 24,
        "keywords": [
          "video",
          "moment",
          "retrieval",
          "language",
          "using",
          "text",
          "natural",
          "network",
          "learning",
          "localization",
          "person",
          "captioning",
          "alignment",
          "automatic",
          "search"
        ],
        "confidence": 0.968755222534681
      },
      {
        "cluster_id": 37,
        "size": 22,
        "keywords": [
          "segmentation",
          "medical",
          "transformer",
          "image",
          "transformers",
          "3d",
          "using",
          "imaging",
          "brain",
          "swin",
          "hybrid",
          "unet",
          "vision",
          "tumor",
          "mri"
        ],
        "confidence": 0.9659208310185715
      },
      {
        "cluster_id": 51,
        "size": 22,
        "keywords": [
          "attention",
          "self",
          "transformers",
          "transformer",
          "multi",
          "models",
          "head",
          "depth",
          "memory",
          "bounded",
          "heads",
          "rethinking",
          "bottleneck",
          "networks",
          "relaxed"
        ],
        "confidence": 0.9704197748762744
      },
      {
        "cluster_id": 52,
        "size": 21,
        "keywords": [
          "models",
          "language",
          "pruning",
          "kronecker",
          "transformer",
          "pre",
          "bert",
          "large",
          "trained",
          "compressing",
          "efficient",
          "based",
          "compression",
          "decomposition",
          "tuning"
        ],
        "confidence": 0.9769486834630263
      },
      {
        "cluster_id": 9,
        "size": 21,
        "keywords": [
          "language",
          "sign",
          "using",
          "speech",
          "translation",
          "recognition",
          "learning",
          "processing",
          "natural",
          "avatar",
          "deep",
          "languages",
          "3d",
          "neural",
          "translating"
        ],
        "confidence": 0.8745227632016088
      },
      {
        "cluster_id": 15,
        "size": 21,
        "keywords": [
          "tagging",
          "learning",
          "speech",
          "deep",
          "using",
          "language",
          "approaches",
          "parts",
          "pos",
          "tagger",
          "based",
          "languages",
          "techniques",
          "machine",
          "natural"
        ],
        "confidence": 0.9330054401934982
      },
      {
        "cluster_id": 28,
        "size": 20,
        "keywords": [
          "language",
          "vision",
          "models",
          "learning",
          "survey",
          "unified",
          "visual",
          "tasks",
          "pre",
          "prompt",
          "multi",
          "modal",
          "berts",
          "multimodal",
          "training"
        ],
        "confidence": 0.9692172753032378
      },
      {
        "cluster_id": 11,
        "size": 20,
        "keywords": [
          "natural",
          "language",
          "visualization",
          "data",
          "interfaces",
          "model",
          "interface",
          "processing",
          "interactive",
          "generating",
          "user",
          "interaction",
          "analysis",
          "computational",
          "visual"
        ],
        "confidence": 0.9669061165560547
      },
      {
        "cluster_id": 54,
        "size": 19,
        "keywords": [
          "bert",
          "quantization",
          "bit",
          "design",
          "models",
          "efficient",
          "low",
          "implications",
          "demystifying",
          "language",
          "training",
          "attention",
          "compression",
          "inference",
          "transformer"
        ],
        "confidence": 0.9928948132056264
      },
      {
        "cluster_id": 47,
        "size": 15,
        "keywords": [
          "language",
          "pre",
          "knowledge",
          "trained",
          "model",
          "natural",
          "models",
          "understanding",
          "enhanced",
          "survey",
          "training",
          "nlp",
          "intensive",
          "retrieval",
          "augmented"
        ],
        "confidence": 1.0
      },
      {
        "cluster_id": 42,
        "size": 15,
        "keywords": [
          "passage",
          "retrieval",
          "bert",
          "ranking",
          "text",
          "based",
          "dense",
          "pretrained",
          "deep",
          "framework",
          "pre",
          "matching",
          "query",
          "keyphrase",
          "model"
        ],
        "confidence": 1.0
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 41302
        },
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 28903
        },
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 12686
        },
        {
          "title": "LoRA: Low-Rank Adaptation of Large Language Models",
          "score": 10099
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 9151
        },
        {
          "title": "Training data-efficient image transformers & distillation through attention",
          "score": 6690
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 6159
        },
        {
          "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
          "score": 6136
        },
        {
          "title": "Evaluating Large Language Models Trained on Code",
          "score": 5385
        },
        {
          "title": "Scaling Laws for Neural Language Models",
          "score": 4707
        },
        {
          "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
          "score": 4617
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 4332
        },
        {
          "title": "Measuring Massive Multitask Language Understanding",
          "score": 4304
        },
        {
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
          "score": 3934
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 3533
        },
        {
          "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
          "score": 3350
        },
        {
          "title": "BEiT: BERT Pre-Training of Image Transformers",
          "score": 2790
        },
        {
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
          "score": 2698
        },
        {
          "title": "A Survey of Convolutional Neural Networks: Analysis, Applications, and Prospects",
          "score": 2675
        },
        {
          "title": "Transformers in Vision: A Survey",
          "score": 2478
        }
      ],
      "by_pagerank": [
        {
          "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
          "score": 0.04389484992654045
        },
        {
          "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
          "score": 0.041797448559663976
        },
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 0.031827082993554634
        },
        {
          "title": "Scaling Laws for Neural Language Models",
          "score": 0.006847413441462473
        },
        {
          "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
          "score": 0.0053685775357666865
        },
        {
          "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP",
          "score": 0.004759985738934852
        },
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 0.004623592700718184
        },
        {
          "title": "Adversarial Training for Large Neural Language Models",
          "score": 0.004428632815239665
        },
        {
          "title": "Training data-efficient image transformers & distillation through attention",
          "score": 0.004025757591301303
        },
        {
          "title": "Pretrained Transformers Improve Out-of-Distribution Robustness",
          "score": 0.0035872194614380414
        },
        {
          "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",
          "score": 0.0034488033905581868
        },
        {
          "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
          "score": 0.0034083188906997774
        },
        {
          "title": "StereoSet: Measuring stereotypical bias in pretrained language models",
          "score": 0.0032277611132198823
        },
        {
          "title": "Multilingual Denoising Pre-training for Neural Machine Translation",
          "score": 0.00295800036455434
        },
        {
          "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?",
          "score": 0.00291219962866675
        },
        {
          "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages",
          "score": 0.002871690360970313
        },
        {
          "title": "AraBERT: Transformer-based Model for Arabic Language Understanding",
          "score": 0.0027642241150761127
        },
        {
          "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System",
          "score": 0.002757440136393653
        },
        {
          "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT",
          "score": 0.002368701714876081
        },
        {
          "title": "Pre-trained models for natural language processing: A survey",
          "score": 0.0023322317601466636
        }
      ],
      "by_authority": [
        {
          "title": "Language Models are Few-Shot Learners",
          "score": 0.21214926189252298
        },
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 0.03236666577900117
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 0.021826465669139185
        },
        {
          "title": "Evaluating Large Language Models Trained on Code",
          "score": 0.02052744810884376
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 0.017401519025599922
        },
        {
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
          "score": 0.016553603140031393
        },
        {
          "title": "Scaling Laws for Neural Language Models",
          "score": 0.016079258340137174
        },
        {
          "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
          "score": 0.015824874416962426
        },
        {
          "title": "Training data-efficient image transformers & distillation through attention",
          "score": 0.014737876203972747
        },
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 0.014590865288264086
        },
        {
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
          "score": 0.01146813545244112
        },
        {
          "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
          "score": 0.011048619350926542
        },
        {
          "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?",
          "score": 0.01085644628597987
        },
        {
          "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
          "score": 0.010439869717776685
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 0.009690470904870983
        },
        {
          "title": "Linformer: Self-Attention with Linear Complexity",
          "score": 0.009476039909805955
        },
        {
          "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
          "score": 0.008596463351098304
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 0.008569202219972852
        },
        {
          "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
          "score": 0.008124704187923971
        },
        {
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "score": 0.00803235293429299
        }
      ]
    }
  },
  {
    "window_start": 2021,
    "window_end": 2023,
    "paper_count": 5684,
    "edge_count": 8779,
    "dbcv_score": 0.40733507038449646,
    "optimal_params": {
      "n_neighbors": 15,
      "min_cluster_size": 15,
      "min_samples": 10
    },
    "topics": [
      {
        "cluster_id": 3,
        "size": 760,
        "keywords": [
          "language",
          "natural",
          "processing",
          "using",
          "clinical",
          "learning",
          "health",
          "review",
          "models",
          "based",
          "medical",
          "electronic",
          "records",
          "machine",
          "large"
        ],
        "confidence": 0.975604768134018
      },
      {
        "cluster_id": 45,
        "size": 217,
        "keywords": [
          "sentiment",
          "analysis",
          "language",
          "based",
          "using",
          "learning",
          "natural",
          "processing",
          "deep",
          "text",
          "data",
          "aspect",
          "arabic",
          "social",
          "model"
        ],
        "confidence": 0.9292374521834481
      },
      {
        "cluster_id": 59,
        "size": 143,
        "keywords": [
          "language",
          "models",
          "multilingual",
          "languages",
          "natural",
          "processing",
          "model",
          "large",
          "speech",
          "low",
          "cross",
          "based",
          "tagging",
          "lingual",
          "corpus"
        ],
        "confidence": 0.9430877016514722
      },
      {
        "cluster_id": 26,
        "size": 126,
        "keywords": [
          "language",
          "models",
          "natural",
          "processing",
          "human",
          "brain",
          "neural",
          "learning",
          "word",
          "large",
          "representations",
          "structure",
          "linguistic",
          "using",
          "corpus"
        ],
        "confidence": 0.49358295188526247
      },
      {
        "cluster_id": 17,
        "size": 116,
        "keywords": [
          "code",
          "language",
          "models",
          "large",
          "natural",
          "generation",
          "test",
          "using",
          "bug",
          "model",
          "software",
          "source",
          "testing",
          "gpt",
          "based"
        ],
        "confidence": 0.6891940393816919
      },
      {
        "cluster_id": 60,
        "size": 101,
        "keywords": [
          "learning",
          "deep",
          "survey",
          "neural",
          "review",
          "networks",
          "applications",
          "convolutional",
          "models",
          "network",
          "machine",
          "data",
          "algorithms",
          "based",
          "cnn"
        ],
        "confidence": 0.9145396777369743
      },
      {
        "cluster_id": 66,
        "size": 101,
        "keywords": [
          "language",
          "learning",
          "models",
          "tuning",
          "shot",
          "fine",
          "task",
          "large",
          "natural",
          "processing",
          "pre",
          "text",
          "efficient",
          "based",
          "training"
        ],
        "confidence": 0.8942862666303307
      },
      {
        "cluster_id": 32,
        "size": 82,
        "keywords": [
          "bias",
          "language",
          "gender",
          "models",
          "natural",
          "fairness",
          "processing",
          "survey",
          "biases",
          "large",
          "nlp",
          "social",
          "analysis",
          "languages",
          "evaluating"
        ],
        "confidence": 0.6565965236001986
      },
      {
        "cluster_id": 57,
        "size": 79,
        "keywords": [
          "language",
          "learning",
          "reinforcement",
          "survey",
          "large",
          "models",
          "robot",
          "robotic",
          "based",
          "navigation",
          "autonomous",
          "multi",
          "model",
          "vision",
          "applications"
        ],
        "confidence": 0.9100825602084637
      },
      {
        "cluster_id": 34,
        "size": 76,
        "keywords": [
          "chatgpt",
          "language",
          "chatbots",
          "ai",
          "natural",
          "chatbot",
          "processing",
          "models",
          "using",
          "generative",
          "conversational",
          "survey",
          "gpt",
          "review",
          "development"
        ],
        "confidence": 0.7658030626817108
      },
      {
        "cluster_id": 36,
        "size": 68,
        "keywords": [
          "language",
          "learning",
          "natural",
          "chatgpt",
          "students",
          "education",
          "using",
          "processing",
          "based",
          "student",
          "assessment",
          "teaching",
          "large",
          "exploring",
          "research"
        ],
        "confidence": 0.8080247846466565
      },
      {
        "cluster_id": 64,
        "size": 68,
        "keywords": [
          "language",
          "reasoning",
          "models",
          "large",
          "natural",
          "survey",
          "logical",
          "causal",
          "commonsense",
          "reasoners",
          "symbolic",
          "thought",
          "chain",
          "logic",
          "knowledge"
        ],
        "confidence": 0.8000499899534063
      },
      {
        "cluster_id": 19,
        "size": 67,
        "keywords": [
          "chatgpt",
          "medical",
          "language",
          "ai",
          "health",
          "artificial",
          "chatbot",
          "intelligence",
          "future",
          "large",
          "review",
          "potential",
          "gpt",
          "natural",
          "models"
        ],
        "confidence": 0.6609661561701737
      },
      {
        "cluster_id": 5,
        "size": 64,
        "keywords": [
          "protein",
          "language",
          "models",
          "model",
          "learning",
          "sequence",
          "deep",
          "based",
          "using",
          "sequences",
          "prediction",
          "cell",
          "biological",
          "design",
          "processing"
        ],
        "confidence": 0.7536944408121493
      },
      {
        "cluster_id": 67,
        "size": 62,
        "keywords": [
          "transformers",
          "transformer",
          "vision",
          "survey",
          "image",
          "classification",
          "visual",
          "based",
          "networks",
          "remote",
          "sensing",
          "point",
          "deep",
          "attention",
          "3d"
        ],
        "confidence": 0.8786989781758284
      },
      {
        "cluster_id": 38,
        "size": 60,
        "keywords": [
          "language",
          "natural",
          "processing",
          "text",
          "extraction",
          "survey",
          "information",
          "data",
          "analysis",
          "based",
          "techniques",
          "using",
          "materials",
          "summarization",
          "literature"
        ],
        "confidence": 0.9629202620984045
      },
      {
        "cluster_id": 4,
        "size": 56,
        "keywords": [
          "language",
          "molecular",
          "models",
          "learning",
          "chemical",
          "using",
          "model",
          "molecules",
          "design",
          "materials",
          "property",
          "transformer",
          "prediction",
          "machine",
          "based"
        ],
        "confidence": 0.6513062485777904
      },
      {
        "cluster_id": 41,
        "size": 51,
        "keywords": [
          "language",
          "requirements",
          "natural",
          "processing",
          "engineering",
          "using",
          "models",
          "software",
          "systematic",
          "learning",
          "based",
          "classification",
          "techniques",
          "review",
          "systems"
        ],
        "confidence": 0.7439855212089653
      },
      {
        "cluster_id": 55,
        "size": 50,
        "keywords": [
          "dialogue",
          "conversational",
          "language",
          "based",
          "agents",
          "generation",
          "dataset",
          "learning",
          "model",
          "systems",
          "conversations",
          "task",
          "question",
          "human",
          "multi"
        ],
        "confidence": 0.7506691345652583
      },
      {
        "cluster_id": 22,
        "size": 50,
        "keywords": [
          "fake",
          "news",
          "detection",
          "learning",
          "language",
          "using",
          "processing",
          "natural",
          "based",
          "deep",
          "machine",
          "spam",
          "classification",
          "model",
          "analysis"
        ],
        "confidence": 0.7017008792118696
      },
      {
        "cluster_id": 10,
        "size": 50,
        "keywords": [
          "adversarial",
          "robustness",
          "language",
          "survey",
          "attacks",
          "nlp",
          "attack",
          "natural",
          "word",
          "defenses",
          "models",
          "learning",
          "deep",
          "textual",
          "based"
        ],
        "confidence": 0.7321968296540986
      },
      {
        "cluster_id": 48,
        "size": 49,
        "keywords": [
          "language",
          "vision",
          "models",
          "learning",
          "visual",
          "prompt",
          "survey",
          "context",
          "understanding",
          "pre",
          "multimodal",
          "modal",
          "multi",
          "unified",
          "scene"
        ],
        "confidence": 0.7555106361221074
      },
      {
        "cluster_id": 7,
        "size": 49,
        "keywords": [
          "speech",
          "language",
          "recognition",
          "self",
          "end",
          "learning",
          "processing",
          "supervised",
          "models",
          "model",
          "large",
          "representation",
          "tasks",
          "training",
          "automatic"
        ],
        "confidence": 0.6952769573430544
      },
      {
        "cluster_id": 42,
        "size": 48,
        "keywords": [
          "text",
          "classification",
          "based",
          "multi",
          "learning",
          "label",
          "model",
          "graph",
          "deep",
          "neural",
          "representation",
          "network",
          "bert",
          "networks",
          "using"
        ],
        "confidence": 0.721703000077147
      },
      {
        "cluster_id": 53,
        "size": 47,
        "keywords": [
          "image",
          "captioning",
          "learning",
          "using",
          "deep",
          "caption",
          "based",
          "generation",
          "review",
          "text",
          "transformer",
          "attention",
          "generator",
          "video",
          "survey"
        ],
        "confidence": 0.6985783801530456
      },
      {
        "cluster_id": 69,
        "size": 45,
        "keywords": [
          "transformer",
          "based",
          "accelerator",
          "acceleration",
          "processing",
          "language",
          "memory",
          "fpga",
          "efficient",
          "transformers",
          "natural",
          "accelerating",
          "networks",
          "inference",
          "vision"
        ],
        "confidence": 0.8152398079868924
      },
      {
        "cluster_id": 43,
        "size": 45,
        "keywords": [
          "covid",
          "19",
          "analysis",
          "sentiment",
          "twitter",
          "using",
          "language",
          "natural",
          "processing",
          "learning",
          "pandemic",
          "vaccine",
          "tweets",
          "vaccination",
          "deep"
        ],
        "confidence": 0.6615579824554305
      },
      {
        "cluster_id": 14,
        "size": 44,
        "keywords": [
          "recommendation",
          "recommender",
          "language",
          "models",
          "large",
          "based",
          "systems",
          "learning",
          "survey",
          "personalized",
          "news",
          "generative",
          "chatgpt",
          "model",
          "using"
        ],
        "confidence": 0.5629045247379598
      },
      {
        "cluster_id": 39,
        "size": 43,
        "keywords": [
          "language",
          "processing",
          "natural",
          "nlp",
          "survey",
          "research",
          "review",
          "applications",
          "models",
          "large",
          "analysis",
          "systematic",
          "role",
          "study",
          "techniques"
        ],
        "confidence": 0.9678722349366029
      },
      {
        "cluster_id": 0,
        "size": 40,
        "keywords": [
          "language",
          "processing",
          "natural",
          "learning",
          "deep",
          "survey",
          "using",
          "analysis",
          "models",
          "based",
          "machine",
          "representation",
          "nlp",
          "techniques",
          "approaches"
        ],
        "confidence": 0.5592204924804306
      },
      {
        "cluster_id": 1,
        "size": 40,
        "keywords": [
          "privacy",
          "language",
          "models",
          "differential",
          "private",
          "text",
          "preserving",
          "natural",
          "learning",
          "differentially",
          "large",
          "policies",
          "federated",
          "processing",
          "analysis"
        ],
        "confidence": 0.7704620299448022
      },
      {
        "cluster_id": 30,
        "size": 39,
        "keywords": [
          "question",
          "answering",
          "based",
          "learning",
          "survey",
          "language",
          "models",
          "using",
          "transformer",
          "deep",
          "answer",
          "grading",
          "comparative",
          "transformers",
          "automated"
        ],
        "confidence": 0.9053811326800124
      },
      {
        "cluster_id": 13,
        "size": 38,
        "keywords": [
          "detection",
          "language",
          "based",
          "using",
          "processing",
          "natural",
          "malware",
          "learning",
          "vulnerability",
          "code",
          "deep",
          "techniques",
          "classification",
          "android",
          "security"
        ],
        "confidence": 0.8595070000430076
      },
      {
        "cluster_id": 46,
        "size": 38,
        "keywords": [
          "hate",
          "speech",
          "detection",
          "language",
          "learning",
          "using",
          "natural",
          "processing",
          "based",
          "automatic",
          "challenges",
          "offensive",
          "media",
          "social",
          "review"
        ],
        "confidence": 0.6471327998428795
      },
      {
        "cluster_id": 23,
        "size": 38,
        "keywords": [
          "emotion",
          "text",
          "recognition",
          "based",
          "learning",
          "speech",
          "using",
          "deep",
          "detection",
          "classification",
          "approach",
          "review",
          "analysis",
          "personality",
          "challenges"
        ],
        "confidence": 0.7061190687757526
      },
      {
        "cluster_id": 58,
        "size": 37,
        "keywords": [
          "machine",
          "translation",
          "language",
          "neural",
          "english",
          "based",
          "processing",
          "natural",
          "statistical",
          "corpus",
          "using",
          "languages",
          "survey",
          "parallel",
          "level"
        ],
        "confidence": 0.7965686630000378
      },
      {
        "cluster_id": 56,
        "size": 36,
        "keywords": [
          "evaluation",
          "language",
          "nlp",
          "human",
          "metrics",
          "natural",
          "tasks",
          "generation",
          "survey",
          "models",
          "benchmark",
          "processing",
          "large",
          "data",
          "model"
        ],
        "confidence": 0.9358803690607326
      },
      {
        "cluster_id": 28,
        "size": 36,
        "keywords": [
          "entity",
          "recognition",
          "named",
          "model",
          "using",
          "based",
          "crf",
          "learning",
          "bert",
          "chinese",
          "domain",
          "deep",
          "nested",
          "review",
          "network"
        ],
        "confidence": 0.7517990037019208
      },
      {
        "cluster_id": 6,
        "size": 34,
        "keywords": [
          "audio",
          "language",
          "music",
          "captioning",
          "training",
          "pre",
          "models",
          "natural",
          "text",
          "generation",
          "supervision",
          "transformer",
          "representations",
          "large",
          "automated"
        ],
        "confidence": 0.7571769130433769
      },
      {
        "cluster_id": 29,
        "size": 33,
        "keywords": [
          "sql",
          "language",
          "natural",
          "text",
          "data",
          "database",
          "query",
          "survey",
          "querying",
          "models",
          "large",
          "processing",
          "systems",
          "queries",
          "learning"
        ],
        "confidence": 0.9276142245970196
      },
      {
        "cluster_id": 35,
        "size": 32,
        "keywords": [
          "education",
          "intelligence",
          "artificial",
          "language",
          "ai",
          "review",
          "systematic",
          "learning",
          "based",
          "future",
          "role",
          "intelligent",
          "teaching",
          "processing",
          "natural"
        ],
        "confidence": 0.8410999020713965
      },
      {
        "cluster_id": 47,
        "size": 30,
        "keywords": [
          "language",
          "detection",
          "cyberbullying",
          "using",
          "natural",
          "processing",
          "learning",
          "machine",
          "media",
          "social",
          "techniques",
          "urdu",
          "based",
          "cyber",
          "model"
        ],
        "confidence": 0.804290007124861
      },
      {
        "cluster_id": 9,
        "size": 30,
        "keywords": [
          "models",
          "language",
          "backdoor",
          "attacks",
          "trained",
          "pre",
          "backdoors",
          "natural",
          "processing",
          "review",
          "textual",
          "survey",
          "nlp",
          "large",
          "based"
        ],
        "confidence": 0.8845769021669804
      },
      {
        "cluster_id": 25,
        "size": 29,
        "keywords": [
          "processing",
          "language",
          "natural",
          "aviation",
          "safety",
          "reports",
          "using",
          "learning",
          "based",
          "machine",
          "application",
          "data",
          "factors",
          "text",
          "analysis"
        ],
        "confidence": 0.6609256941783311
      },
      {
        "cluster_id": 40,
        "size": 28,
        "keywords": [
          "natural",
          "language",
          "processing",
          "based",
          "information",
          "building",
          "ontology",
          "automated",
          "management",
          "construction",
          "using",
          "knowledge",
          "checking",
          "compliance",
          "semantic"
        ],
        "confidence": 0.9537575044320175
      },
      {
        "cluster_id": 65,
        "size": 28,
        "keywords": [
          "segmentation",
          "medical",
          "image",
          "transformer",
          "transformers",
          "3d",
          "brain",
          "unet",
          "review",
          "swin",
          "using",
          "tumor",
          "vision",
          "imaging",
          "architecture"
        ],
        "confidence": 0.8384618354689174
      },
      {
        "cluster_id": 37,
        "size": 26,
        "keywords": [
          "legal",
          "language",
          "natural",
          "processing",
          "law",
          "prediction",
          "models",
          "large",
          "survey",
          "judgment",
          "using",
          "domain",
          "retrieval",
          "brazilian",
          "case"
        ],
        "confidence": 0.8360732516667724
      },
      {
        "cluster_id": 51,
        "size": 25,
        "keywords": [
          "learning",
          "contrastive",
          "sentence",
          "representations",
          "embeddings",
          "representation",
          "unsupervised",
          "supervised",
          "language",
          "self",
          "text",
          "review",
          "model",
          "label",
          "understanding"
        ],
        "confidence": 0.8615016785663646
      },
      {
        "cluster_id": 16,
        "size": 25,
        "keywords": [
          "graph",
          "networks",
          "neural",
          "survey",
          "learning",
          "applications",
          "self",
          "supervised",
          "node",
          "processing",
          "deep",
          "classification",
          "methods",
          "attention",
          "transformer"
        ],
        "confidence": 0.8439350101534034
      },
      {
        "cluster_id": 52,
        "size": 22,
        "keywords": [
          "diffusion",
          "text",
          "image",
          "motion",
          "editing",
          "human",
          "driven",
          "models",
          "generation",
          "clip",
          "guided",
          "zero",
          "shot",
          "synthesis",
          "language"
        ],
        "confidence": 0.8789186248167202
      },
      {
        "cluster_id": 12,
        "size": 22,
        "keywords": [
          "cybersecurity",
          "threat",
          "intelligence",
          "based",
          "cyber",
          "language",
          "security",
          "natural",
          "learning",
          "systems",
          "processing",
          "nlp",
          "domain",
          "assessment",
          "automated"
        ],
        "confidence": 0.9026224393634447
      },
      {
        "cluster_id": 63,
        "size": 22,
        "keywords": [
          "language",
          "explanations",
          "nlp",
          "model",
          "models",
          "survey",
          "natural",
          "explainability",
          "explainable",
          "based",
          "explanation",
          "processing",
          "rationalization",
          "improving",
          "pretrained"
        ],
        "confidence": 0.9879444302820262
      },
      {
        "cluster_id": 68,
        "size": 22,
        "keywords": [
          "bert",
          "quantization",
          "transformer",
          "models",
          "language",
          "efficient",
          "quantized",
          "bit",
          "inference",
          "large",
          "training",
          "moe",
          "low",
          "high",
          "size"
        ],
        "confidence": 0.9634471607695321
      },
      {
        "cluster_id": 8,
        "size": 22,
        "keywords": [
          "language",
          "sign",
          "using",
          "recognition",
          "translation",
          "speech",
          "learning",
          "processing",
          "natural",
          "deep",
          "languages",
          "neural",
          "machine",
          "indian",
          "translating"
        ],
        "confidence": 0.8672977776081148
      },
      {
        "cluster_id": 49,
        "size": 21,
        "keywords": [
          "visual",
          "answering",
          "question",
          "sensing",
          "remote",
          "language",
          "survey",
          "transformers",
          "datasets",
          "image",
          "indic",
          "attention",
          "future",
          "reasoning",
          "challenges"
        ],
        "confidence": 0.9431711568842223
      },
      {
        "cluster_id": 27,
        "size": 21,
        "keywords": [
          "extraction",
          "language",
          "retrieval",
          "relation",
          "matching",
          "semantic",
          "based",
          "passage",
          "network",
          "bert",
          "neural",
          "graph",
          "text",
          "survey",
          "information"
        ],
        "confidence": 0.9968603822876376
      },
      {
        "cluster_id": 24,
        "size": 21,
        "keywords": [
          "stock",
          "sentiment",
          "prediction",
          "using",
          "based",
          "price",
          "market",
          "analysis",
          "learning",
          "natural",
          "language",
          "processing",
          "forecasting",
          "financial",
          "news"
        ],
        "confidence": 0.817057712680575
      },
      {
        "cluster_id": 50,
        "size": 20,
        "keywords": [
          "word",
          "embeddings",
          "language",
          "survey",
          "embedding",
          "processing",
          "natural",
          "learning",
          "representation",
          "neural",
          "deep",
          "evaluation",
          "models",
          "methods",
          "word2vec"
        ],
        "confidence": 0.9429683321012485
      },
      {
        "cluster_id": 15,
        "size": 20,
        "keywords": [
          "intelligence",
          "artificial",
          "ai",
          "applications",
          "management",
          "processing",
          "industry",
          "information",
          "impact",
          "library",
          "natural",
          "language",
          "future",
          "frontiers",
          "opportunities"
        ],
        "confidence": 0.9048866237434054
      },
      {
        "cluster_id": 21,
        "size": 20,
        "keywords": [
          "language",
          "natural",
          "data",
          "visualization",
          "interfaces",
          "survey",
          "based",
          "interactive",
          "visual",
          "analysis",
          "exploration",
          "authoring",
          "synthesis",
          "processing",
          "interface"
        ],
        "confidence": 0.9094138783643801
      },
      {
        "cluster_id": 61,
        "size": 18,
        "keywords": [
          "time",
          "series",
          "forecasting",
          "transformer",
          "survey",
          "models",
          "transformers",
          "pre",
          "based",
          "model",
          "analysis",
          "learning",
          "foundation",
          "trained",
          "multivariate"
        ],
        "confidence": 0.9788238099912524
      },
      {
        "cluster_id": 31,
        "size": 18,
        "keywords": [
          "answering",
          "question",
          "knowledge",
          "graphs",
          "graph",
          "based",
          "process",
          "querying",
          "approach",
          "domain",
          "bio",
          "enabling",
          "language",
          "natural",
          "soda"
        ],
        "confidence": 0.9414249525311053
      },
      {
        "cluster_id": 54,
        "size": 17,
        "keywords": [
          "language",
          "models",
          "large",
          "gpt",
          "natural",
          "financial",
          "model",
          "tasks",
          "comprehensive",
          "benchmark",
          "instruction",
          "use",
          "challenges",
          "evaluating",
          "llms"
        ],
        "confidence": 0.9998986469182434
      },
      {
        "cluster_id": 11,
        "size": 17,
        "keywords": [
          "phishing",
          "detection",
          "learning",
          "using",
          "language",
          "machine",
          "natural",
          "processing",
          "deep",
          "email",
          "model",
          "emails",
          "malicious",
          "detect",
          "based"
        ],
        "confidence": 0.962269397343561
      },
      {
        "cluster_id": 2,
        "size": 17,
        "keywords": [
          "quantum",
          "natural",
          "language",
          "processing",
          "intelligence",
          "approach",
          "near",
          "term",
          "neural",
          "attention",
          "network",
          "vision",
          "transformers",
          "mathematics",
          "artificial"
        ],
        "confidence": 0.9880284442125558
      },
      {
        "cluster_id": 62,
        "size": 17,
        "keywords": [
          "language",
          "models",
          "pre",
          "knowledge",
          "trained",
          "survey",
          "enhanced",
          "natural",
          "model",
          "understanding",
          "based",
          "processing",
          "transformer",
          "transformers",
          "learning"
        ],
        "confidence": 0.9972464705231836
      },
      {
        "cluster_id": 18,
        "size": 16,
        "keywords": [
          "language",
          "model",
          "geoscience",
          "understanding",
          "modeling",
          "large",
          "platform",
          "geospatially",
          "models",
          "challenges",
          "scientific",
          "building",
          "processes",
          "numerical",
          "earth"
        ],
        "confidence": 0.969912852545935
      },
      {
        "cluster_id": 20,
        "size": 16,
        "keywords": [
          "language",
          "personality",
          "chatgpt",
          "models",
          "processing",
          "natural",
          "large",
          "affective",
          "traits",
          "responses",
          "item",
          "development",
          "generation",
          "using",
          "emerge"
        ],
        "confidence": 0.9956276491743717
      },
      {
        "cluster_id": 44,
        "size": 16,
        "keywords": [
          "tourism",
          "language",
          "natural",
          "processing",
          "text",
          "based",
          "analysis",
          "mining",
          "fuzzy",
          "product",
          "industry",
          "approach",
          "decision",
          "group",
          "application"
        ],
        "confidence": 0.9985671962440599
      },
      {
        "cluster_id": 33,
        "size": 15,
        "keywords": [
          "language",
          "natural",
          "processing",
          "cities",
          "based",
          "sustainable",
          "intelligence",
          "smart",
          "governance",
          "research",
          "participation",
          "citizen",
          "innovation",
          "alignment",
          "service"
        ],
        "confidence": 1.0
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 28903
        },
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 12954
        },
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 12686
        },
        {
          "title": "LoRA: Low-Rank Adaptation of Large Language Models",
          "score": 10099
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 9151
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 6159
        },
        {
          "title": "Evaluating Large Language Models Trained on Code",
          "score": 5385
        },
        {
          "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
          "score": 4617
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 4332
        },
        {
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
          "score": 3934
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 3800
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 3533
        },
        {
          "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
          "score": 3350
        },
        {
          "title": "DINOv2: Learning Robust Visual Features without Supervision",
          "score": 3256
        },
        {
          "title": "BEiT: BERT Pre-Training of Image Transformers",
          "score": 2790
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 2659
        },
        {
          "title": "Transformers in Vision: A Survey",
          "score": 2478
        },
        {
          "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "score": 2364
        },
        {
          "title": "Learning to Prompt for Vision-Language Models",
          "score": 2355
        },
        {
          "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
          "score": 2098
        }
      ],
      "by_pagerank": [
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 0.029245891381565484
        },
        {
          "title": "Evaluating Large Language Models Trained on Code",
          "score": 0.015888210463972097
        },
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 0.01179776162456102
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 0.010621479530297684
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 0.00961695801045303
        },
        {
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
          "score": 0.008944398533787383
        },
        {
          "title": "BEiT: BERT Pre-Training of Image Transformers",
          "score": 0.007241796848021525
        },
        {
          "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning",
          "score": 0.006805959047116828
        },
        {
          "title": "What\u2019s in Your Head? Emergent Behaviour in Multi-Task Transformer Models",
          "score": 0.006019574367560406
        },
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 0.005849342252590573
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 0.0052063732812649265
        },
        {
          "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation",
          "score": 0.005080886272921371
        },
        {
          "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
          "score": 0.00424665299145642
        },
        {
          "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
          "score": 0.003928175188741948
        },
        {
          "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "score": 0.0036691592246864066
        },
        {
          "title": "QNLP in Practice: Running Compositional Models of Meaning on a Quantum Computer",
          "score": 0.00346913029867211
        },
        {
          "title": "LoRA: Low-Rank Adaptation of Large Language Models",
          "score": 0.0033803374608387415
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 0.003305643862953148
        },
        {
          "title": "lambeq: An Efficient High-Level Python Library for Quantum NLP",
          "score": 0.0028960907391549886
        },
        {
          "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
          "score": 0.0028239624053064782
        }
      ],
      "by_authority": [
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 0.0801950564079224
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 0.07000463787573757
        },
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 0.05400837457082346
        },
        {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "score": 0.05114597553640426
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 0.05051779291730285
        },
        {
          "title": "Evaluating Large Language Models Trained on Code",
          "score": 0.03914823608032608
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 0.03351459991519031
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 0.031879519375692895
        },
        {
          "title": "LoRA: Low-Rank Adaptation of Large Language Models",
          "score": 0.026678263500405714
        },
        {
          "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "score": 0.021449368471762896
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 0.014401004326142306
        },
        {
          "title": "Solving Quantitative Reasoning Problems with Language Models",
          "score": 0.01388580557816659
        },
        {
          "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
          "score": 0.013722793622837005
        },
        {
          "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
          "score": 0.013617566003828628
        },
        {
          "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
          "score": 0.011426946597224107
        },
        {
          "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
          "score": 0.007894038602820484
        },
        {
          "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
          "score": 0.006723676160245174
        },
        {
          "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
          "score": 0.006450152306661654
        },
        {
          "title": "PanGu-\u03b1: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation",
          "score": 0.006235823960860608
        },
        {
          "title": "Code as Policies: Language Model Programs for Embodied Control",
          "score": 0.006080275239058821
        }
      ]
    }
  },
  {
    "window_start": 2022,
    "window_end": 2024,
    "paper_count": 5921,
    "edge_count": 10079,
    "dbcv_score": 0.4217712079408517,
    "optimal_params": {
      "n_neighbors": 10,
      "min_cluster_size": 15,
      "min_samples": 15
    },
    "topics": [
      {
        "cluster_id": 4,
        "size": 824,
        "keywords": [
          "language",
          "natural",
          "processing",
          "clinical",
          "using",
          "medical",
          "models",
          "review",
          "large",
          "health",
          "learning",
          "model",
          "based",
          "artificial",
          "intelligence"
        ],
        "confidence": 0.992023299217266
      },
      {
        "cluster_id": 61,
        "size": 211,
        "keywords": [
          "language",
          "models",
          "translation",
          "large",
          "languages",
          "machine",
          "natural",
          "processing",
          "multilingual",
          "model",
          "english",
          "low",
          "survey",
          "resource",
          "arabic"
        ],
        "confidence": 0.9534082281424756
      },
      {
        "cluster_id": 3,
        "size": 179,
        "keywords": [
          "language",
          "models",
          "protein",
          "learning",
          "large",
          "model",
          "based",
          "using",
          "deep",
          "prediction",
          "molecular",
          "natural",
          "design",
          "drug",
          "processing"
        ],
        "confidence": 0.864369577316537
      },
      {
        "cluster_id": 17,
        "size": 145,
        "keywords": [
          "code",
          "language",
          "models",
          "generation",
          "large",
          "natural",
          "using",
          "software",
          "model",
          "test",
          "testing",
          "llm",
          "programming",
          "llms",
          "pre"
        ],
        "confidence": 0.6460732988208866
      },
      {
        "cluster_id": 34,
        "size": 138,
        "keywords": [
          "language",
          "education",
          "learning",
          "ai",
          "natural",
          "processing",
          "intelligence",
          "artificial",
          "chatgpt",
          "large",
          "models",
          "review",
          "based",
          "using",
          "teaching"
        ],
        "confidence": 0.8829128352551957
      },
      {
        "cluster_id": 27,
        "size": 116,
        "keywords": [
          "image",
          "text",
          "video",
          "language",
          "captioning",
          "generation",
          "learning",
          "models",
          "using",
          "visual",
          "diffusion",
          "retrieval",
          "editing",
          "deep",
          "model"
        ],
        "confidence": 0.8598763722331039
      },
      {
        "cluster_id": 35,
        "size": 107,
        "keywords": [
          "chatgpt",
          "language",
          "natural",
          "ai",
          "processing",
          "chatbots",
          "chatbot",
          "models",
          "review",
          "generative",
          "using",
          "gpt",
          "study",
          "exploring",
          "based"
        ],
        "confidence": 0.9510845858053153
      },
      {
        "cluster_id": 44,
        "size": 102,
        "keywords": [
          "language",
          "reasoning",
          "models",
          "large",
          "natural",
          "survey",
          "causal",
          "logical",
          "llms",
          "symbolic",
          "logic",
          "counterfactual",
          "model",
          "reasoners",
          "learning"
        ],
        "confidence": 0.7293186161763328
      },
      {
        "cluster_id": 20,
        "size": 100,
        "keywords": [
          "speech",
          "language",
          "audio",
          "models",
          "large",
          "end",
          "model",
          "generation",
          "recognition",
          "text",
          "natural",
          "self",
          "learning",
          "processing",
          "survey"
        ],
        "confidence": 0.4587163113222472
      },
      {
        "cluster_id": 25,
        "size": 93,
        "keywords": [
          "intelligence",
          "artificial",
          "ai",
          "management",
          "challenges",
          "applications",
          "language",
          "processing",
          "review",
          "learning",
          "machine",
          "natural",
          "role",
          "driven",
          "sustainable"
        ],
        "confidence": 0.6479133499437263
      },
      {
        "cluster_id": 36,
        "size": 91,
        "keywords": [
          "sentiment",
          "analysis",
          "learning",
          "using",
          "language",
          "natural",
          "processing",
          "based",
          "techniques",
          "deep",
          "machine",
          "review",
          "survey",
          "reviews",
          "approach"
        ],
        "confidence": 0.7673574815904596
      },
      {
        "cluster_id": 10,
        "size": 89,
        "keywords": [
          "language",
          "attacks",
          "models",
          "adversarial",
          "survey",
          "backdoor",
          "large",
          "attack",
          "defenses",
          "nlp",
          "learning",
          "robustness",
          "natural",
          "llms",
          "textual"
        ],
        "confidence": 0.9537368347332289
      },
      {
        "cluster_id": 26,
        "size": 80,
        "keywords": [
          "language",
          "vision",
          "visual",
          "models",
          "learning",
          "answering",
          "question",
          "survey",
          "natural",
          "multimodal",
          "multi",
          "reasoning",
          "modal",
          "knowledge",
          "pre"
        ],
        "confidence": 0.7078468507123169
      },
      {
        "cluster_id": 13,
        "size": 72,
        "keywords": [
          "bias",
          "language",
          "models",
          "gender",
          "fairness",
          "large",
          "natural",
          "processing",
          "survey",
          "social",
          "biases",
          "nlp",
          "model",
          "mitigating",
          "learning"
        ],
        "confidence": 0.9214899384423855
      },
      {
        "cluster_id": 42,
        "size": 71,
        "keywords": [
          "language",
          "natural",
          "processing",
          "legal",
          "models",
          "large",
          "survey",
          "nlp",
          "review",
          "using",
          "based",
          "analysis",
          "challenges",
          "domain",
          "prediction"
        ],
        "confidence": 0.49978291899485416
      },
      {
        "cluster_id": 63,
        "size": 66,
        "keywords": [
          "time",
          "series",
          "forecasting",
          "transformers",
          "transformer",
          "survey",
          "learning",
          "based",
          "models",
          "deep",
          "model",
          "prediction",
          "temporal",
          "attention",
          "networks"
        ],
        "confidence": 0.6562517449358125
      },
      {
        "cluster_id": 30,
        "size": 66,
        "keywords": [
          "detection",
          "language",
          "learning",
          "using",
          "processing",
          "natural",
          "hate",
          "speech",
          "based",
          "cyberbullying",
          "sarcasm",
          "social",
          "media",
          "deep",
          "machine"
        ],
        "confidence": 0.9109269132511965
      },
      {
        "cluster_id": 65,
        "size": 64,
        "keywords": [
          "language",
          "tuning",
          "fine",
          "models",
          "large",
          "efficient",
          "parameter",
          "learning",
          "model",
          "low",
          "training",
          "adaptation",
          "tasks",
          "task",
          "natural"
        ],
        "confidence": 0.7711131819288486
      },
      {
        "cluster_id": 67,
        "size": 60,
        "keywords": [
          "language",
          "large",
          "models",
          "inference",
          "model",
          "efficient",
          "quantization",
          "llm",
          "bit",
          "bert",
          "transformer",
          "decoding",
          "speculative",
          "kv",
          "energy"
        ],
        "confidence": 0.9462223933230912
      },
      {
        "cluster_id": 11,
        "size": 60,
        "keywords": [
          "detection",
          "language",
          "based",
          "using",
          "natural",
          "processing",
          "learning",
          "intelligence",
          "security",
          "model",
          "vulnerability",
          "cybersecurity",
          "code",
          "machine",
          "threat"
        ],
        "confidence": 0.9664822157997311
      },
      {
        "cluster_id": 62,
        "size": 57,
        "keywords": [
          "language",
          "models",
          "learning",
          "large",
          "natural",
          "prompt",
          "improving",
          "explanations",
          "survey",
          "explainability",
          "prompting",
          "generation",
          "context",
          "llms",
          "explainable"
        ],
        "confidence": 0.9323812221902561
      },
      {
        "cluster_id": 38,
        "size": 56,
        "keywords": [
          "language",
          "models",
          "brain",
          "natural",
          "neural",
          "human",
          "processing",
          "large",
          "fmri",
          "structure",
          "encoding",
          "comprehension",
          "learning",
          "deep",
          "eeg"
        ],
        "confidence": 0.706395103302606
      },
      {
        "cluster_id": 21,
        "size": 55,
        "keywords": [
          "sql",
          "language",
          "natural",
          "text",
          "data",
          "visualization",
          "models",
          "large",
          "database",
          "based",
          "survey",
          "querying",
          "interfaces",
          "generation",
          "queries"
        ],
        "confidence": 0.8644191804521307
      },
      {
        "cluster_id": 14,
        "size": 54,
        "keywords": [
          "emotion",
          "recognition",
          "text",
          "speech",
          "deep",
          "language",
          "learning",
          "using",
          "based",
          "models",
          "analysis",
          "emotions",
          "detection",
          "approach",
          "emotional"
        ],
        "confidence": 0.8240489723567831
      },
      {
        "cluster_id": 33,
        "size": 54,
        "keywords": [
          "conversational",
          "dialogue",
          "language",
          "based",
          "learning",
          "ai",
          "multi",
          "natural",
          "generation",
          "using",
          "models",
          "dataset",
          "search",
          "survey",
          "analysis"
        ],
        "confidence": 0.7768850967533284
      },
      {
        "cluster_id": 5,
        "size": 52,
        "keywords": [
          "recommendation",
          "recommender",
          "language",
          "large",
          "models",
          "systems",
          "based",
          "survey",
          "model",
          "generative",
          "llm",
          "generation",
          "chatgpt",
          "personalized",
          "user"
        ],
        "confidence": 0.6148588951165446
      },
      {
        "cluster_id": 40,
        "size": 52,
        "keywords": [
          "language",
          "requirements",
          "natural",
          "engineering",
          "models",
          "processing",
          "using",
          "large",
          "systematic",
          "software",
          "process",
          "business",
          "review",
          "automated",
          "study"
        ],
        "confidence": 0.8196520699829862
      },
      {
        "cluster_id": 60,
        "size": 51,
        "keywords": [
          "language",
          "large",
          "models",
          "survey",
          "evaluation",
          "model",
          "nlp",
          "generation",
          "metrics",
          "evaluating",
          "challenges",
          "human",
          "current",
          "alignment",
          "llm"
        ],
        "confidence": 0.8057585783974376
      },
      {
        "cluster_id": 56,
        "size": 49,
        "keywords": [
          "language",
          "robot",
          "large",
          "navigation",
          "models",
          "using",
          "vision",
          "robotic",
          "grasp",
          "natural",
          "survey",
          "manipulation",
          "model",
          "driven",
          "autonomous"
        ],
        "confidence": 0.815635961602167
      },
      {
        "cluster_id": 55,
        "size": 48,
        "keywords": [
          "language",
          "large",
          "agents",
          "models",
          "planning",
          "agent",
          "multi",
          "llm",
          "based",
          "model",
          "framework",
          "natural",
          "tasks",
          "collaboration",
          "reasoning"
        ],
        "confidence": 0.9755427112588694
      },
      {
        "cluster_id": 0,
        "size": 45,
        "keywords": [
          "language",
          "natural",
          "processing",
          "learning",
          "using",
          "analysis",
          "deep",
          "machine",
          "prediction",
          "construction",
          "based",
          "text",
          "sentiment",
          "research",
          "applications"
        ],
        "confidence": 0.5919170216395212
      },
      {
        "cluster_id": 28,
        "size": 45,
        "keywords": [
          "text",
          "classification",
          "based",
          "graph",
          "learning",
          "model",
          "neural",
          "language",
          "network",
          "networks",
          "using",
          "deep",
          "natural",
          "processing",
          "multi"
        ],
        "confidence": 0.7934167018490702
      },
      {
        "cluster_id": 64,
        "size": 44,
        "keywords": [
          "transformer",
          "transformers",
          "attention",
          "long",
          "models",
          "efficient",
          "context",
          "sequence",
          "modeling",
          "processing",
          "pruning",
          "language",
          "based",
          "pre",
          "network"
        ],
        "confidence": 0.792312008011053
      },
      {
        "cluster_id": 9,
        "size": 43,
        "keywords": [
          "privacy",
          "language",
          "models",
          "preserving",
          "large",
          "private",
          "natural",
          "differential",
          "processing",
          "model",
          "differentially",
          "analysis",
          "policies",
          "text",
          "data"
        ],
        "confidence": 0.7229893922392278
      },
      {
        "cluster_id": 49,
        "size": 41,
        "keywords": [
          "retrieval",
          "language",
          "augmented",
          "models",
          "generation",
          "large",
          "survey",
          "information",
          "question",
          "answering",
          "rag",
          "natural",
          "generative",
          "processing",
          "evaluation"
        ],
        "confidence": 0.8643726089379992
      },
      {
        "cluster_id": 54,
        "size": 41,
        "keywords": [
          "learning",
          "reinforcement",
          "language",
          "multi",
          "large",
          "imitation",
          "decision",
          "survey",
          "models",
          "data",
          "agent",
          "transformers",
          "sequence",
          "robotic",
          "transformer"
        ],
        "confidence": 0.8709919539079471
      },
      {
        "cluster_id": 58,
        "size": 40,
        "keywords": [
          "transformer",
          "vision",
          "transformers",
          "image",
          "survey",
          "based",
          "review",
          "classification",
          "using",
          "network",
          "detection",
          "deep",
          "visual",
          "segmentation",
          "remote"
        ],
        "confidence": 0.7780196524014069
      },
      {
        "cluster_id": 57,
        "size": 40,
        "keywords": [
          "neural",
          "convolutional",
          "networks",
          "learning",
          "review",
          "deep",
          "network",
          "based",
          "survey",
          "comprehensive",
          "detection",
          "applications",
          "cnn",
          "artificial",
          "application"
        ],
        "confidence": 0.6775397265647352
      },
      {
        "cluster_id": 47,
        "size": 38,
        "keywords": [
          "extraction",
          "language",
          "information",
          "models",
          "large",
          "natural",
          "event",
          "text",
          "model",
          "knowledge",
          "processing",
          "using",
          "survey",
          "relation",
          "entity"
        ],
        "confidence": 0.9002928674581111
      },
      {
        "cluster_id": 29,
        "size": 37,
        "keywords": [
          "news",
          "fake",
          "detection",
          "language",
          "learning",
          "processing",
          "natural",
          "based",
          "using",
          "machine",
          "deep",
          "spam",
          "analysis",
          "classification",
          "techniques"
        ],
        "confidence": 0.6753364820370803
      },
      {
        "cluster_id": 8,
        "size": 36,
        "keywords": [
          "language",
          "large",
          "models",
          "hallucination",
          "hallucinations",
          "llms",
          "survey",
          "mitigating",
          "detecting",
          "based",
          "open",
          "vision",
          "generation",
          "fine",
          "knowledge"
        ],
        "confidence": 0.7405131853198923
      },
      {
        "cluster_id": 43,
        "size": 34,
        "keywords": [
          "summarization",
          "text",
          "abstractive",
          "using",
          "language",
          "review",
          "based",
          "automatic",
          "natural",
          "models",
          "chart",
          "techniques",
          "large",
          "learning",
          "processing"
        ],
        "confidence": 0.8037804796013213
      },
      {
        "cluster_id": 51,
        "size": 32,
        "keywords": [
          "knowledge",
          "graph",
          "graphs",
          "question",
          "answering",
          "language",
          "large",
          "models",
          "based",
          "construction",
          "reasoning",
          "generation",
          "automated",
          "retrieval",
          "query"
        ],
        "confidence": 0.9414175946726423
      },
      {
        "cluster_id": 37,
        "size": 31,
        "keywords": [
          "analysis",
          "sentiment",
          "learning",
          "based",
          "deep",
          "language",
          "aspect",
          "lingual",
          "model",
          "multi",
          "cross",
          "word",
          "using",
          "arabic",
          "embedding"
        ],
        "confidence": 0.8963047440461631
      },
      {
        "cluster_id": 39,
        "size": 30,
        "keywords": [
          "language",
          "models",
          "large",
          "word",
          "understanding",
          "model",
          "processing",
          "expressions",
          "multiword",
          "lexicon",
          "english",
          "approach",
          "bert",
          "natural",
          "llms"
        ],
        "confidence": 0.93691120519896
      },
      {
        "cluster_id": 31,
        "size": 29,
        "keywords": [
          "stock",
          "language",
          "sentiment",
          "using",
          "market",
          "prediction",
          "analysis",
          "processing",
          "natural",
          "financial",
          "based",
          "news",
          "price",
          "models",
          "learning"
        ],
        "confidence": 0.8682880792336313
      },
      {
        "cluster_id": 66,
        "size": 29,
        "keywords": [
          "accelerator",
          "transformer",
          "based",
          "acceleration",
          "memory",
          "vision",
          "transformers",
          "efficient",
          "fpga",
          "bit",
          "hardware",
          "digital",
          "attention",
          "processing",
          "learning"
        ],
        "confidence": 0.784261328914776
      },
      {
        "cluster_id": 1,
        "size": 28,
        "keywords": [
          "language",
          "natural",
          "processing",
          "models",
          "survey",
          "learning",
          "large",
          "deep",
          "based",
          "using",
          "challenges",
          "news",
          "detection",
          "framework",
          "overview"
        ],
        "confidence": 0.7858711354505727
      },
      {
        "cluster_id": 50,
        "size": 27,
        "keywords": [
          "graph",
          "language",
          "large",
          "models",
          "graphs",
          "survey",
          "llms",
          "networks",
          "neural",
          "data",
          "learning",
          "text",
          "comprehensive",
          "model",
          "knowledge"
        ],
        "confidence": 0.861580882136839
      },
      {
        "cluster_id": 23,
        "size": 25,
        "keywords": [
          "3d",
          "scene",
          "language",
          "understanding",
          "point",
          "object",
          "large",
          "generation",
          "vision",
          "context",
          "indoor",
          "cloud",
          "detection",
          "semantic",
          "pre"
        ],
        "confidence": 0.8531280205971419
      },
      {
        "cluster_id": 16,
        "size": 25,
        "keywords": [
          "entity",
          "named",
          "recognition",
          "model",
          "based",
          "bert",
          "chinese",
          "using",
          "learning",
          "crf",
          "language",
          "deep",
          "approach",
          "legal",
          "transformer"
        ],
        "confidence": 0.9484799501275571
      },
      {
        "cluster_id": 12,
        "size": 24,
        "keywords": [
          "learning",
          "contrastive",
          "sentence",
          "representations",
          "representation",
          "unsupervised",
          "models",
          "language",
          "embeddings",
          "heterogeneous",
          "disentangled",
          "review",
          "label",
          "multi",
          "view"
        ],
        "confidence": 0.8271887294912723
      },
      {
        "cluster_id": 52,
        "size": 23,
        "keywords": [
          "transformers",
          "models",
          "language",
          "natural",
          "processing",
          "based",
          "review",
          "nlp",
          "transformer",
          "text",
          "bert",
          "learning",
          "using",
          "survey",
          "perspective"
        ],
        "confidence": 0.9892623043965465
      },
      {
        "cluster_id": 19,
        "size": 21,
        "keywords": [
          "diffusion",
          "motion",
          "text",
          "generation",
          "survey",
          "models",
          "human",
          "discrete",
          "model",
          "based",
          "conditioned",
          "self",
          "non",
          "autoregressive",
          "driven"
        ],
        "confidence": 0.9847036646229312
      },
      {
        "cluster_id": 32,
        "size": 21,
        "keywords": [
          "19",
          "covid",
          "using",
          "sentiment",
          "natural",
          "language",
          "processing",
          "analysis",
          "twitter",
          "vaccine",
          "public",
          "tweets",
          "vaccination",
          "learning",
          "models"
        ],
        "confidence": 0.9868432088557704
      },
      {
        "cluster_id": 48,
        "size": 20,
        "keywords": [
          "learning",
          "deep",
          "survey",
          "domain",
          "transfer",
          "active",
          "machine",
          "applications",
          "recent",
          "advances",
          "supervised",
          "data",
          "benchmark",
          "new",
          "adaptation"
        ],
        "confidence": 0.9937279545065764
      },
      {
        "cluster_id": 7,
        "size": 20,
        "keywords": [
          "quantum",
          "processing",
          "language",
          "natural",
          "network",
          "attention",
          "neural",
          "self",
          "term",
          "near",
          "text",
          "vision",
          "transformers",
          "gentle",
          "challenges"
        ],
        "confidence": 0.9180768663599558
      },
      {
        "cluster_id": 45,
        "size": 19,
        "keywords": [
          "wireless",
          "semantic",
          "learning",
          "model",
          "language",
          "models",
          "large",
          "deep",
          "communication",
          "multi",
          "foundation",
          "llm",
          "communications",
          "enabled",
          "distributed"
        ],
        "confidence": 0.9867892137026665
      },
      {
        "cluster_id": 59,
        "size": 19,
        "keywords": [
          "large",
          "language",
          "models",
          "financial",
          "benchmark",
          "open",
          "finance",
          "survey",
          "source",
          "evaluation",
          "chinese",
          "model",
          "domain",
          "data",
          "fingpt"
        ],
        "confidence": 0.9941711551761644
      },
      {
        "cluster_id": 18,
        "size": 18,
        "keywords": [
          "processing",
          "language",
          "natural",
          "safety",
          "using",
          "aviation",
          "assessment",
          "review",
          "reports",
          "models",
          "data",
          "analysis",
          "nlp",
          "learning",
          "based"
        ],
        "confidence": 0.9695360566895211
      },
      {
        "cluster_id": 53,
        "size": 18,
        "keywords": [
          "driving",
          "autonomous",
          "models",
          "language",
          "large",
          "survey",
          "gpt",
          "foundation",
          "model",
          "action",
          "scale",
          "end",
          "drive",
          "networks",
          "using"
        ],
        "confidence": 0.9778795683836409
      },
      {
        "cluster_id": 22,
        "size": 18,
        "keywords": [
          "language",
          "large",
          "models",
          "geoscience",
          "natural",
          "understanding",
          "model",
          "geospatially",
          "knowledge",
          "geospatial",
          "processing",
          "spatial",
          "grounded",
          "foundation",
          "challenges"
        ],
        "confidence": 0.9363763568799516
      },
      {
        "cluster_id": 24,
        "size": 18,
        "keywords": [
          "language",
          "natural",
          "tourism",
          "processing",
          "based",
          "urban",
          "text",
          "using",
          "information",
          "classification",
          "learning",
          "mining",
          "intelligence",
          "artificial",
          "methods"
        ],
        "confidence": 0.9995233083892945
      },
      {
        "cluster_id": 2,
        "size": 17,
        "keywords": [
          "federated",
          "learning",
          "language",
          "models",
          "large",
          "tuning",
          "personalized",
          "natural",
          "processing",
          "parameter",
          "heterogeneous",
          "meets",
          "pre",
          "comprehensive",
          "federatedscope"
        ],
        "confidence": 0.9835237722137331
      },
      {
        "cluster_id": 15,
        "size": 17,
        "keywords": [
          "language",
          "sign",
          "translation",
          "processing",
          "using",
          "natural",
          "speech",
          "learning",
          "large",
          "models",
          "free",
          "text",
          "indian",
          "based",
          "leveraging"
        ],
        "confidence": 0.9717990352103435
      },
      {
        "cluster_id": 41,
        "size": 17,
        "keywords": [
          "language",
          "models",
          "preference",
          "large",
          "learning",
          "human",
          "aligning",
          "self",
          "model",
          "preferences",
          "guided",
          "reward",
          "critiques",
          "bias",
          "active"
        ],
        "confidence": 0.999488800036962
      },
      {
        "cluster_id": 46,
        "size": 17,
        "keywords": [
          "language",
          "knowledge",
          "models",
          "pre",
          "large",
          "model",
          "natural",
          "understanding",
          "trained",
          "survey",
          "enhanced",
          "training",
          "know",
          "retrieval",
          "processing"
        ],
        "confidence": 0.9944841509645557
      },
      {
        "cluster_id": 6,
        "size": 17,
        "keywords": [
          "language",
          "natural",
          "processing",
          "resume",
          "using",
          "learning",
          "parsing",
          "automated",
          "job",
          "machine",
          "nlp",
          "recruitment",
          "approach",
          "recommendation",
          "based"
        ],
        "confidence": 0.9848054300038257
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 12954
        },
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 12686
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 9151
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 6159
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 4332
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 3800
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 3533
        },
        {
          "title": "DINOv2: Learning Robust Visual Features without Supervision",
          "score": 3256
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 2659
        },
        {
          "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "score": 2364
        },
        {
          "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
          "score": 2098
        },
        {
          "title": "Qwen Technical Report",
          "score": 1756
        },
        {
          "title": "A Survey on Evaluation of Large Language Models",
          "score": 1619
        },
        {
          "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications",
          "score": 1360
        },
        {
          "title": "A Survey on Large Language Model based Autonomous Agents",
          "score": 1225
        },
        {
          "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
          "score": 1224
        },
        {
          "title": "PaLM 2 Technical Report",
          "score": 1178
        },
        {
          "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
          "score": 1155
        },
        {
          "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images",
          "score": 1098
        },
        {
          "title": "RT-1: Robotics Transformer for Real-World Control at Scale",
          "score": 1084
        }
      ],
      "by_pagerank": [
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 0.04450878191407465
        },
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 0.04126836690231613
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 0.0377238420977756
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 0.02341924893013906
        },
        {
          "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
          "score": 0.020329069312455365
        },
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 0.018044960551094964
        },
        {
          "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "score": 0.01103989258504203
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 0.010722144475202476
        },
        {
          "title": "Designing Effective Sparse Expert Models",
          "score": 0.008470874113523843
        },
        {
          "title": "Solving Quantitative Reasoning Problems with Language Models",
          "score": 0.007018600574786965
        },
        {
          "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text",
          "score": 0.005700579487240293
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 0.0029401508664140793
        },
        {
          "title": "Natural Language to Code Translation with Execution",
          "score": 0.002747613806172255
        },
        {
          "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
          "score": 0.002642132423994821
        },
        {
          "title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\"",
          "score": 0.002048480746626145
        },
        {
          "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
          "score": 0.0019476361600320822
        },
        {
          "title": "Autoformalization with Large Language Models",
          "score": 0.0019051437598204613
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 0.0018409021410540683
        },
        {
          "title": "A Contrastive Framework for Neural Text Generation",
          "score": 0.0018185398378996775
        },
        {
          "title": "Contrastive Search Is What You Need For Neural Text Generation",
          "score": 0.0016211071888055167
        }
      ],
      "by_authority": [
        {
          "title": "Training language models to follow instructions with human feedback",
          "score": 0.10479205743881029
        },
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 0.09425187017669936
        },
        {
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
          "score": 0.0776663669846473
        },
        {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "score": 0.059197826250290804
        },
        {
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
          "score": 0.038462622150874586
        },
        {
          "title": "Large Language Models are Zero-Shot Reasoners",
          "score": 0.034596122235898294
        },
        {
          "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
          "score": 0.028977009078390084
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 0.020164719615866394
        },
        {
          "title": "Solving Quantitative Reasoning Problems with Language Models",
          "score": 0.01392845117734739
        },
        {
          "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",
          "score": 0.011868188221971698
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 0.010696599031658472
        },
        {
          "title": "PaLM 2 Technical Report",
          "score": 0.008226326290949352
        },
        {
          "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
          "score": 0.007465360404767012
        },
        {
          "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
          "score": 0.006511900787159605
        },
        {
          "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
          "score": 0.006184362287671235
        },
        {
          "title": "Qwen Technical Report",
          "score": 0.005687863824812615
        },
        {
          "title": "Towards Reasoning in Large Language Models: A Survey",
          "score": 0.005567414283323361
        },
        {
          "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
          "score": 0.005453901638013549
        },
        {
          "title": "Red Teaming Language Models with Language Models",
          "score": 0.0053900946951124205
        },
        {
          "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
          "score": 0.005344693700464302
        }
      ]
    }
  },
  {
    "window_start": 2023,
    "window_end": 2025,
    "paper_count": 4766,
    "edge_count": 6128,
    "dbcv_score": 0.4489389482264777,
    "optimal_params": {
      "n_neighbors": 15,
      "min_cluster_size": 25,
      "min_samples": 15
    },
    "topics": [
      {
        "cluster_id": 2,
        "size": 619,
        "keywords": [
          "language",
          "natural",
          "processing",
          "models",
          "large",
          "medical",
          "review",
          "clinical",
          "using",
          "health",
          "healthcare",
          "learning",
          "chatgpt",
          "applications",
          "intelligence"
        ],
        "confidence": 0.9805535270110135
      },
      {
        "cluster_id": 15,
        "size": 276,
        "keywords": [
          "language",
          "chatgpt",
          "ai",
          "education",
          "natural",
          "learning",
          "processing",
          "intelligence",
          "artificial",
          "models",
          "large",
          "review",
          "using",
          "chatbot",
          "generative"
        ],
        "confidence": 0.9231795362201275
      },
      {
        "cluster_id": 19,
        "size": 225,
        "keywords": [
          "language",
          "models",
          "large",
          "translation",
          "languages",
          "multilingual",
          "processing",
          "natural",
          "machine",
          "low",
          "model",
          "resource",
          "arabic",
          "survey",
          "based"
        ],
        "confidence": 0.8142350972628384
      },
      {
        "cluster_id": 23,
        "size": 217,
        "keywords": [
          "language",
          "image",
          "models",
          "visual",
          "vision",
          "multimodal",
          "text",
          "generation",
          "learning",
          "video",
          "large",
          "understanding",
          "natural",
          "3d",
          "answering"
        ],
        "confidence": 0.9401624534673454
      },
      {
        "cluster_id": 8,
        "size": 174,
        "keywords": [
          "language",
          "code",
          "models",
          "large",
          "generation",
          "software",
          "natural",
          "llm",
          "using",
          "requirements",
          "engineering",
          "llms",
          "based",
          "study",
          "processing"
        ],
        "confidence": 0.6170390358654497
      },
      {
        "cluster_id": 5,
        "size": 164,
        "keywords": [
          "language",
          "models",
          "large",
          "attacks",
          "privacy",
          "survey",
          "adversarial",
          "security",
          "llm",
          "model",
          "backdoor",
          "learning",
          "based",
          "detection",
          "natural"
        ],
        "confidence": 0.9451592430071555
      },
      {
        "cluster_id": 29,
        "size": 160,
        "keywords": [
          "language",
          "large",
          "models",
          "efficient",
          "tuning",
          "model",
          "fine",
          "llm",
          "inference",
          "parameter",
          "survey",
          "efficiency",
          "based",
          "acceleration",
          "llms"
        ],
        "confidence": 0.917389289863763
      },
      {
        "cluster_id": 3,
        "size": 159,
        "keywords": [
          "language",
          "models",
          "protein",
          "learning",
          "large",
          "model",
          "based",
          "using",
          "natural",
          "drug",
          "prediction",
          "design",
          "molecular",
          "processing",
          "deep"
        ],
        "confidence": 0.9104430590396401
      },
      {
        "cluster_id": 10,
        "size": 142,
        "keywords": [
          "intelligence",
          "ai",
          "artificial",
          "language",
          "natural",
          "processing",
          "review",
          "management",
          "learning",
          "using",
          "analysis",
          "machine",
          "role",
          "challenges",
          "financial"
        ],
        "confidence": 0.6226572972166532
      },
      {
        "cluster_id": 14,
        "size": 135,
        "keywords": [
          "analysis",
          "sentiment",
          "language",
          "learning",
          "based",
          "natural",
          "processing",
          "deep",
          "using",
          "classification",
          "text",
          "techniques",
          "models",
          "model",
          "social"
        ],
        "confidence": 0.6987099079515492
      },
      {
        "cluster_id": 22,
        "size": 116,
        "keywords": [
          "language",
          "graph",
          "knowledge",
          "models",
          "large",
          "graphs",
          "extraction",
          "information",
          "question",
          "answering",
          "model",
          "reasoning",
          "text",
          "based",
          "llms"
        ],
        "confidence": 0.715102762590685
      },
      {
        "cluster_id": 26,
        "size": 95,
        "keywords": [
          "language",
          "large",
          "learning",
          "robot",
          "model",
          "models",
          "survey",
          "robotic",
          "vision",
          "autonomous",
          "reinforcement",
          "manipulation",
          "human",
          "natural",
          "using"
        ],
        "confidence": 0.668222709338197
      },
      {
        "cluster_id": 24,
        "size": 88,
        "keywords": [
          "language",
          "reasoning",
          "models",
          "large",
          "natural",
          "logical",
          "survey",
          "explanations",
          "causal",
          "symbolic",
          "model",
          "mathematical",
          "llms",
          "inference",
          "thought"
        ],
        "confidence": 0.8724185583579949
      },
      {
        "cluster_id": 6,
        "size": 79,
        "keywords": [
          "speech",
          "language",
          "audio",
          "models",
          "large",
          "generation",
          "natural",
          "text",
          "model",
          "survey",
          "using",
          "processing",
          "synthesis",
          "recognition",
          "learning"
        ],
        "confidence": 0.6349471911383826
      },
      {
        "cluster_id": 12,
        "size": 78,
        "keywords": [
          "detection",
          "language",
          "processing",
          "using",
          "learning",
          "natural",
          "news",
          "fake",
          "based",
          "social",
          "model",
          "media",
          "deep",
          "hate",
          "machine"
        ],
        "confidence": 0.8166748282180027
      },
      {
        "cluster_id": 25,
        "size": 73,
        "keywords": [
          "language",
          "models",
          "large",
          "survey",
          "evaluation",
          "model",
          "challenges",
          "llm",
          "generation",
          "nlp",
          "llms",
          "evaluating",
          "long",
          "benchmark",
          "efficient"
        ],
        "confidence": 0.9811424422053999
      },
      {
        "cluster_id": 7,
        "size": 58,
        "keywords": [
          "language",
          "models",
          "bias",
          "large",
          "fairness",
          "gender",
          "social",
          "natural",
          "mitigating",
          "biases",
          "model",
          "llms",
          "processing",
          "survey",
          "measuring"
        ],
        "confidence": 0.7422945208926447
      },
      {
        "cluster_id": 11,
        "size": 57,
        "keywords": [
          "language",
          "dialogue",
          "conversational",
          "models",
          "large",
          "creativity",
          "natural",
          "conversation",
          "multi",
          "llms",
          "using",
          "based",
          "ai",
          "processing",
          "llm"
        ],
        "confidence": 0.7823264019885807
      },
      {
        "cluster_id": 31,
        "size": 57,
        "keywords": [
          "time",
          "series",
          "forecasting",
          "transformer",
          "survey",
          "models",
          "transformers",
          "large",
          "model",
          "learning",
          "language",
          "temporal",
          "prediction",
          "based",
          "deep"
        ],
        "confidence": 0.8728326320176847
      },
      {
        "cluster_id": 18,
        "size": 54,
        "keywords": [
          "sql",
          "language",
          "data",
          "large",
          "text",
          "models",
          "natural",
          "visualization",
          "database",
          "based",
          "survey",
          "tabular",
          "model",
          "ai",
          "querying"
        ],
        "confidence": 0.8263951031650891
      },
      {
        "cluster_id": 27,
        "size": 53,
        "keywords": [
          "language",
          "agents",
          "large",
          "models",
          "llm",
          "planning",
          "based",
          "agent",
          "model",
          "multi",
          "collaboration",
          "tasks",
          "reasoning",
          "autonomous",
          "framework"
        ],
        "confidence": 0.9624679304534014
      },
      {
        "cluster_id": 17,
        "size": 50,
        "keywords": [
          "recommendation",
          "language",
          "large",
          "recommender",
          "models",
          "survey",
          "systems",
          "based",
          "model",
          "personalized",
          "generation",
          "generative",
          "llm",
          "chatgpt",
          "user"
        ],
        "confidence": 0.7507154332712765
      },
      {
        "cluster_id": 0,
        "size": 48,
        "keywords": [
          "language",
          "natural",
          "processing",
          "using",
          "learning",
          "analysis",
          "deep",
          "machine",
          "prediction",
          "based",
          "text",
          "sentiment",
          "construction",
          "data",
          "intelligence"
        ],
        "confidence": 0.7736190418948966
      },
      {
        "cluster_id": 20,
        "size": 42,
        "keywords": [
          "retrieval",
          "augmented",
          "generation",
          "language",
          "models",
          "survey",
          "large",
          "question",
          "answering",
          "rag",
          "information",
          "generative",
          "framework",
          "evaluation",
          "multi"
        ],
        "confidence": 0.8390696960257904
      },
      {
        "cluster_id": 9,
        "size": 40,
        "keywords": [
          "language",
          "models",
          "brain",
          "human",
          "large",
          "neural",
          "processing",
          "natural",
          "word",
          "model",
          "comprehension",
          "semantic",
          "eeg",
          "encoding",
          "representations"
        ],
        "confidence": 0.8260861575898643
      },
      {
        "cluster_id": 13,
        "size": 38,
        "keywords": [
          "emotion",
          "language",
          "personality",
          "models",
          "recognition",
          "large",
          "based",
          "emotions",
          "analysis",
          "natural",
          "speech",
          "text",
          "chatgpt",
          "emotional",
          "traits"
        ],
        "confidence": 0.9773498402568199
      },
      {
        "cluster_id": 4,
        "size": 36,
        "keywords": [
          "language",
          "large",
          "models",
          "hallucination",
          "hallucinations",
          "llms",
          "survey",
          "based",
          "mitigating",
          "detecting",
          "speech",
          "open",
          "vision",
          "generation",
          "knowledge"
        ],
        "confidence": 0.8260319184188227
      },
      {
        "cluster_id": 16,
        "size": 35,
        "keywords": [
          "summarization",
          "text",
          "language",
          "abstractive",
          "using",
          "survey",
          "models",
          "review",
          "based",
          "learning",
          "automatic",
          "natural",
          "techniques",
          "news",
          "narrative"
        ],
        "confidence": 0.9084740817767857
      },
      {
        "cluster_id": 21,
        "size": 32,
        "keywords": [
          "language",
          "legal",
          "natural",
          "processing",
          "large",
          "models",
          "domain",
          "law",
          "text",
          "survey",
          "case",
          "nlp",
          "reasoning",
          "leveraging",
          "knowledge"
        ],
        "confidence": 0.9443845600110493
      },
      {
        "cluster_id": 32,
        "size": 30,
        "keywords": [
          "learning",
          "deep",
          "models",
          "neural",
          "model",
          "networks",
          "language",
          "space",
          "dynamics",
          "transformers",
          "data",
          "transformer",
          "scaling",
          "oscillatory",
          "generative"
        ],
        "confidence": 0.9976281157910167
      },
      {
        "cluster_id": 1,
        "size": 30,
        "keywords": [
          "language",
          "processing",
          "natural",
          "models",
          "survey",
          "large",
          "learning",
          "deep",
          "challenges",
          "using",
          "based",
          "detection",
          "news",
          "framework",
          "overview"
        ],
        "confidence": 0.9315145410431802
      },
      {
        "cluster_id": 30,
        "size": 28,
        "keywords": [
          "transformers",
          "transformer",
          "efficient",
          "attention",
          "long",
          "context",
          "sequence",
          "modeling",
          "linear",
          "approximation",
          "review",
          "performance",
          "models",
          "range",
          "length"
        ],
        "confidence": 0.9979283865768168
      },
      {
        "cluster_id": 28,
        "size": 28,
        "keywords": [
          "neural",
          "networks",
          "review",
          "learning",
          "deep",
          "convolutional",
          "survey",
          "graph",
          "comprehensive",
          "network",
          "based",
          "architectures",
          "applications",
          "advances",
          "detection"
        ],
        "confidence": 0.9961993838200891
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 12954
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 3800
        },
        {
          "title": "DINOv2: Learning Robust Visual Features without Supervision",
          "score": 3256
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 2659
        },
        {
          "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
          "score": 2098
        },
        {
          "title": "Qwen Technical Report",
          "score": 1756
        },
        {
          "title": "A Survey on Evaluation of Large Language Models",
          "score": 1619
        },
        {
          "title": "A Survey on Large Language Model based Autonomous Agents",
          "score": 1225
        },
        {
          "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
          "score": 1224
        },
        {
          "title": "PaLM 2 Technical Report",
          "score": 1178
        },
        {
          "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
          "score": 1155
        },
        {
          "title": "Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation",
          "score": 943
        },
        {
          "title": "Sentiment Analysis",
          "score": 830
        },
        {
          "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
          "score": 816
        },
        {
          "title": "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development",
          "score": 774
        },
        {
          "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
          "score": 760
        },
        {
          "title": "Evaluating Object Hallucination in Large Vision-Language Models",
          "score": 755
        },
        {
          "title": "StarCoder: may the source be with you!",
          "score": 747
        },
        {
          "title": "Abstractive Text Summarization Using GAN",
          "score": 702
        },
        {
          "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
          "score": 692
        }
      ],
      "by_pagerank": [
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 0.06583267588889086
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 0.011350041522185913
        },
        {
          "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
          "score": 0.011149494833498008
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 0.005918025188487031
        },
        {
          "title": "PaLM 2 Technical Report",
          "score": 0.0049551589401314764
        },
        {
          "title": "Pretraining Language Models with Human Preferences",
          "score": 0.004720704916584174
        },
        {
          "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
          "score": 0.0036984040708287888
        },
        {
          "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
          "score": 0.003485647402555354
        },
        {
          "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
          "score": 0.0034536868977731974
        },
        {
          "title": "Qwen Technical Report",
          "score": 0.003445911801224939
        },
        {
          "title": "A Survey on Evaluation of Large Language Models",
          "score": 0.003374004038899332
        },
        {
          "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
          "score": 0.003137231158866907
        },
        {
          "title": "Large Language Models",
          "score": 0.003044397580720357
        },
        {
          "title": "Scaling Transformer to 1M tokens and beyond with RMT",
          "score": 0.002613211672283791
        },
        {
          "title": "RWKV: Reinventing RNNs for the Transformer Era",
          "score": 0.0025710786898258675
        },
        {
          "title": "Augmented Language Models: a Survey",
          "score": 0.002443901236221786
        },
        {
          "title": "StarCoder: may the source be with you!",
          "score": 0.0023531023289971417
        },
        {
          "title": "Natural Language Processing in the Legal Domain",
          "score": 0.002232407970103365
        },
        {
          "title": "A Survey on Large Language Model based Autonomous Agents",
          "score": 0.0021514004167457294
        },
        {
          "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
          "score": 0.0020210861285935144
        }
      ],
      "by_authority": [
        {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "score": 0.3033823908163801
        },
        {
          "title": "A Survey of Large Language Models",
          "score": 0.04109488942884325
        },
        {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "score": 0.016894151905533634
        },
        {
          "title": "Qwen Technical Report",
          "score": 0.015641583085980966
        },
        {
          "title": "PaLM 2 Technical Report",
          "score": 0.014877721504532015
        },
        {
          "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
          "score": 0.01043698881905489
        },
        {
          "title": "One Fits All: Power General Time Series Analysis by Pretrained LM",
          "score": 0.009582990765784628
        },
        {
          "title": "StarCoder: may the source be with you!",
          "score": 0.00889128820644364
        },
        {
          "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
          "score": 0.00866102082780334
        },
        {
          "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
          "score": 0.008648973000745235
        },
        {
          "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge",
          "score": 0.008447006730030775
        },
        {
          "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
          "score": 0.007931975384894157
        },
        {
          "title": "A Survey on Evaluation of Large Language Models",
          "score": 0.007698488697999809
        },
        {
          "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
          "score": 0.006780599421134146
        },
        {
          "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
          "score": 0.006725085648753328
        },
        {
          "title": "A Survey on Large Language Model based Autonomous Agents",
          "score": 0.00589284221735051
        },
        {
          "title": "Augmented Language Models: a Survey",
          "score": 0.005800884577187354
        },
        {
          "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models",
          "score": 0.005694097570244175
        },
        {
          "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance",
          "score": 0.005609827554920815
        },
        {
          "title": "Language is All a Graph Needs",
          "score": 0.0054839806805447285
        }
      ]
    }
  },
  {
    "window_start": 2024,
    "window_end": 2026,
    "paper_count": 2760,
    "edge_count": 898,
    "dbcv_score": 0.4316315074849698,
    "optimal_params": {
      "n_neighbors": 10,
      "min_cluster_size": 15,
      "min_samples": 15
    },
    "topics": [
      {
        "cluster_id": 3,
        "size": 314,
        "keywords": [
          "language",
          "processing",
          "natural",
          "models",
          "review",
          "large",
          "medical",
          "healthcare",
          "clinical",
          "using",
          "learning",
          "systematic",
          "applications",
          "health",
          "machine"
        ],
        "confidence": 0.9664228918549984
      },
      {
        "cluster_id": 36,
        "size": 128,
        "keywords": [
          "language",
          "models",
          "large",
          "languages",
          "multilingual",
          "translation",
          "natural",
          "processing",
          "low",
          "resource",
          "machine",
          "arabic",
          "survey",
          "model",
          "llm"
        ],
        "confidence": 0.9245150290904816
      },
      {
        "cluster_id": 2,
        "size": 110,
        "keywords": [
          "language",
          "protein",
          "models",
          "processing",
          "natural",
          "large",
          "model",
          "learning",
          "quantum",
          "based",
          "prediction",
          "using",
          "drug",
          "design",
          "foundation"
        ],
        "confidence": 0.7304508476291179
      },
      {
        "cluster_id": 7,
        "size": 108,
        "keywords": [
          "language",
          "models",
          "large",
          "attacks",
          "llm",
          "privacy",
          "survey",
          "detection",
          "security",
          "model",
          "backdoor",
          "llms",
          "attack",
          "learning",
          "ai"
        ],
        "confidence": 0.9124202739631386
      },
      {
        "cluster_id": 9,
        "size": 98,
        "keywords": [
          "language",
          "ai",
          "education",
          "learning",
          "natural",
          "processing",
          "chatbot",
          "large",
          "models",
          "chatbots",
          "chatgpt",
          "using",
          "intelligence",
          "artificial",
          "opportunities"
        ],
        "confidence": 0.6048353824991053
      },
      {
        "cluster_id": 12,
        "size": 91,
        "keywords": [
          "ai",
          "intelligence",
          "artificial",
          "driven",
          "management",
          "language",
          "challenges",
          "enhancing",
          "sustainable",
          "review",
          "processing",
          "analysis",
          "applications",
          "services",
          "natural"
        ],
        "confidence": 0.6683209302755102
      },
      {
        "cluster_id": 26,
        "size": 84,
        "keywords": [
          "language",
          "large",
          "models",
          "tuning",
          "fine",
          "efficient",
          "llm",
          "model",
          "inference",
          "llms",
          "low",
          "rank",
          "parameter",
          "pruning",
          "decoding"
        ],
        "confidence": 0.9667062884304135
      },
      {
        "cluster_id": 15,
        "size": 82,
        "keywords": [
          "language",
          "code",
          "generation",
          "llm",
          "models",
          "large",
          "llms",
          "software",
          "study",
          "model",
          "based",
          "engineering",
          "natural",
          "verification",
          "programming"
        ],
        "confidence": 0.7376365387068743
      },
      {
        "cluster_id": 31,
        "size": 64,
        "keywords": [
          "language",
          "models",
          "learning",
          "robot",
          "large",
          "model",
          "manipulation",
          "vision",
          "robotic",
          "reward",
          "guided",
          "llm",
          "preference",
          "action",
          "human"
        ],
        "confidence": 0.758184751656197
      },
      {
        "cluster_id": 28,
        "size": 57,
        "keywords": [
          "analysis",
          "sentiment",
          "learning",
          "deep",
          "based",
          "language",
          "natural",
          "processing",
          "machine",
          "classification",
          "using",
          "techniques",
          "text",
          "social",
          "model"
        ],
        "confidence": 0.8472453158727652
      },
      {
        "cluster_id": 35,
        "size": 50,
        "keywords": [
          "language",
          "large",
          "models",
          "text",
          "survey",
          "generation",
          "summarization",
          "llms",
          "evaluation",
          "challenges",
          "based",
          "long",
          "llm",
          "systematic",
          "natural"
        ],
        "confidence": 0.958054172830625
      },
      {
        "cluster_id": 27,
        "size": 49,
        "keywords": [
          "agents",
          "language",
          "llm",
          "agent",
          "large",
          "based",
          "models",
          "multi",
          "planning",
          "framework",
          "model",
          "llms",
          "tasks",
          "autonomous",
          "plan"
        ],
        "confidence": 0.876468268410807
      },
      {
        "cluster_id": 33,
        "size": 48,
        "keywords": [
          "retrieval",
          "augmented",
          "generation",
          "language",
          "models",
          "survey",
          "large",
          "rag",
          "evaluation",
          "answering",
          "question",
          "information",
          "processing",
          "natural",
          "multi"
        ],
        "confidence": 0.7104423913899834
      },
      {
        "cluster_id": 20,
        "size": 46,
        "keywords": [
          "image",
          "generation",
          "visual",
          "language",
          "text",
          "video",
          "multimodal",
          "editing",
          "model",
          "learning",
          "temporal",
          "llms",
          "vision",
          "evaluating",
          "models"
        ],
        "confidence": 0.8865829788527463
      },
      {
        "cluster_id": 18,
        "size": 44,
        "keywords": [
          "time",
          "series",
          "forecasting",
          "prediction",
          "transformer",
          "models",
          "based",
          "temporal",
          "model",
          "long",
          "traffic",
          "learning",
          "term",
          "power",
          "survey"
        ],
        "confidence": 0.9160009705599639
      },
      {
        "cluster_id": 24,
        "size": 44,
        "keywords": [
          "knowledge",
          "graph",
          "language",
          "large",
          "graphs",
          "models",
          "reasoning",
          "enhanced",
          "augmented",
          "based",
          "survey",
          "model",
          "question",
          "answering",
          "construction"
        ],
        "confidence": 0.7461349931214305
      },
      {
        "cluster_id": 29,
        "size": 39,
        "keywords": [
          "detection",
          "language",
          "news",
          "fake",
          "learning",
          "processing",
          "based",
          "natural",
          "using",
          "deep",
          "social",
          "approach",
          "media",
          "machine",
          "hate"
        ],
        "confidence": 0.6416200874605726
      },
      {
        "cluster_id": 5,
        "size": 39,
        "keywords": [
          "speech",
          "language",
          "models",
          "audio",
          "large",
          "evaluation",
          "model",
          "synthesis",
          "survey",
          "generation",
          "omni",
          "mamba",
          "natural",
          "technical",
          "report"
        ],
        "confidence": 0.7538638496815967
      },
      {
        "cluster_id": 30,
        "size": 36,
        "keywords": [
          "reasoning",
          "language",
          "models",
          "large",
          "symbolic",
          "explanations",
          "natural",
          "llms",
          "neuro",
          "logical",
          "ai",
          "time",
          "grounded",
          "llm",
          "learning"
        ],
        "confidence": 0.8181249720818711
      },
      {
        "cluster_id": 8,
        "size": 34,
        "keywords": [
          "language",
          "models",
          "large",
          "brain",
          "model",
          "neural",
          "natural",
          "processing",
          "eeg",
          "semantic",
          "scale",
          "neurons",
          "llms",
          "networks",
          "word"
        ],
        "confidence": 0.8184685883957862
      },
      {
        "cluster_id": 0,
        "size": 29,
        "keywords": [
          "language",
          "processing",
          "natural",
          "using",
          "analysis",
          "learning",
          "text",
          "neural",
          "assessment",
          "construction",
          "prediction",
          "machine",
          "networks",
          "mining",
          "building"
        ],
        "confidence": 0.750689718763214
      },
      {
        "cluster_id": 23,
        "size": 22,
        "keywords": [
          "recommendation",
          "language",
          "models",
          "large",
          "based",
          "graph",
          "recommender",
          "survey",
          "model",
          "systems",
          "llm",
          "user",
          "personalized",
          "using",
          "job"
        ],
        "confidence": 0.9184082076401179
      },
      {
        "cluster_id": 17,
        "size": 22,
        "keywords": [
          "language",
          "models",
          "survey",
          "visual",
          "large",
          "multimodal",
          "question",
          "answering",
          "vision",
          "natural",
          "comprehensive",
          "evaluation",
          "understanding",
          "model",
          "knowledge"
        ],
        "confidence": 0.9755973583132178
      },
      {
        "cluster_id": 6,
        "size": 21,
        "keywords": [
          "language",
          "models",
          "large",
          "bias",
          "llms",
          "fairness",
          "model",
          "detecting",
          "mitigating",
          "gender",
          "evaluation",
          "testing",
          "inference",
          "debiasing",
          "stop"
        ],
        "confidence": 0.9409657712804735
      },
      {
        "cluster_id": 1,
        "size": 21,
        "keywords": [
          "language",
          "processing",
          "natural",
          "models",
          "large",
          "survey",
          "based",
          "challenges",
          "deep",
          "overview",
          "learning",
          "future",
          "advancements",
          "finance",
          "applications"
        ],
        "confidence": 0.9102109293456903
      },
      {
        "cluster_id": 13,
        "size": 21,
        "keywords": [
          "legal",
          "language",
          "natural",
          "processing",
          "large",
          "models",
          "documents",
          "reasoning",
          "survey",
          "knowledge",
          "nlp",
          "challenges",
          "compliance",
          "quality",
          "text"
        ],
        "confidence": 0.8770089543423409
      },
      {
        "cluster_id": 14,
        "size": 20,
        "keywords": [
          "wireless",
          "models",
          "large",
          "language",
          "llm",
          "learning",
          "networks",
          "model",
          "6g",
          "edge",
          "foundation",
          "distributed",
          "inference",
          "modal",
          "multi"
        ],
        "confidence": 0.9236139072951948
      },
      {
        "cluster_id": 16,
        "size": 20,
        "keywords": [
          "vision",
          "transformer",
          "image",
          "survey",
          "rwkv",
          "mamba",
          "diffusion",
          "review",
          "natural",
          "based",
          "models",
          "transformers",
          "sensing",
          "remote",
          "language"
        ],
        "confidence": 0.9852176173819741
      },
      {
        "cluster_id": 22,
        "size": 19,
        "keywords": [
          "sql",
          "text",
          "language",
          "database",
          "based",
          "linking",
          "large",
          "models",
          "search",
          "schema",
          "llm",
          "generation",
          "survey",
          "natural",
          "systems"
        ],
        "confidence": 0.9157789999802792
      },
      {
        "cluster_id": 34,
        "size": 18,
        "keywords": [
          "models",
          "language",
          "large",
          "survey",
          "era",
          "small",
          "evaluation",
          "roadmap",
          "science",
          "llms",
          "llm",
          "benchmarking",
          "model",
          "rise",
          "potential"
        ],
        "confidence": 0.994983963360891
      },
      {
        "cluster_id": 25,
        "size": 18,
        "keywords": [
          "attention",
          "learning",
          "bias",
          "transformers",
          "optimization",
          "token",
          "neural",
          "linear",
          "sparse",
          "implicit",
          "prediction",
          "geometry",
          "networks",
          "language",
          "data"
        ],
        "confidence": 0.9968341167253048
      },
      {
        "cluster_id": 4,
        "size": 18,
        "keywords": [
          "language",
          "large",
          "hallucination",
          "models",
          "hallucinations",
          "llms",
          "based",
          "mitigating",
          "speech",
          "internal",
          "states",
          "automatic",
          "detecting",
          "multi",
          "layers"
        ],
        "confidence": 0.9557690937132287
      },
      {
        "cluster_id": 19,
        "size": 18,
        "keywords": [
          "generation",
          "language",
          "3d",
          "scene",
          "large",
          "models",
          "cad",
          "multimodal",
          "shape",
          "llms",
          "llm4cad",
          "computer",
          "design",
          "aided",
          "end"
        ],
        "confidence": 0.9986528370931175
      },
      {
        "cluster_id": 21,
        "size": 18,
        "keywords": [
          "emotion",
          "speech",
          "emotional",
          "recognition",
          "language",
          "models",
          "text",
          "emo",
          "instruction",
          "tuning",
          "large",
          "analysis",
          "emotions",
          "face",
          "understanding"
        ],
        "confidence": 0.9941747040832882
      },
      {
        "cluster_id": 10,
        "size": 17,
        "keywords": [
          "dialogue",
          "conversational",
          "models",
          "language",
          "real",
          "turn",
          "conversation",
          "duplex",
          "search",
          "llms",
          "time",
          "generation",
          "spoken",
          "multi",
          "processing"
        ],
        "confidence": 0.9940381480604896
      },
      {
        "cluster_id": 11,
        "size": 17,
        "keywords": [
          "artificial",
          "intelligence",
          "generative",
          "language",
          "applications",
          "future",
          "review",
          "ai",
          "exploring",
          "processing",
          "natural",
          "analysis",
          "perspectives",
          "recent",
          "advances"
        ],
        "confidence": 0.9873566950626824
      },
      {
        "cluster_id": 32,
        "size": 17,
        "keywords": [
          "prompt",
          "language",
          "large",
          "models",
          "optimization",
          "engineering",
          "survey",
          "learning",
          "nlp",
          "methods",
          "prompting",
          "natural",
          "tasks",
          "domain",
          "knowledge"
        ],
        "confidence": 0.9925954967673202
      }
    ],
    "top_papers": {
      "by_citation": [
        {
          "title": "Intelligent Clinical Documentation: Harnessing Generative AI for Patient-Centric Clinical Note Generation",
          "score": 943
        },
        {
          "title": "Sentiment Analysis",
          "score": 830
        },
        {
          "title": "Abstractive Text Summarization Using GAN",
          "score": 702
        },
        {
          "title": "GPT-4 passes the bar exam",
          "score": 400
        },
        {
          "title": "Large Language Models: A Survey",
          "score": 394
        },
        {
          "title": "OLMo: Accelerating the Science of Language Models",
          "score": 388
        },
        {
          "title": "AI-Driven Proactive Cloud Application Data Access Security",
          "score": 216
        },
        {
          "title": "Statistical mechanics of deep learning",
          "score": 215
        },
        {
          "title": "SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation",
          "score": 210
        },
        {
          "title": "TrustLLM: Trustworthiness in Large Language Models",
          "score": 205
        },
        {
          "title": "PMC-LLaMA: toward building open-source language models for medicine",
          "score": 181
        },
        {
          "title": "A Survey on Large Language Models for Code Generation",
          "score": 177
        },
        {
          "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
          "score": 169
        },
        {
          "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
          "score": 168
        },
        {
          "title": "Autoencoders and their applications in machine learning: a survey",
          "score": 149
        },
        {
          "title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation",
          "score": 149
        },
        {
          "title": "Transformative Potential of AI in Healthcare: Definitions, Applications, and Navigating the Ethical Landscape and Public Perspectives",
          "score": 144
        },
        {
          "title": "Qwen2-Audio Technical Report",
          "score": 136
        },
        {
          "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications.",
          "score": 135
        },
        {
          "title": "Security and Privacy Challenges of Large Language Models: A Survey",
          "score": 125
        }
      ],
      "by_pagerank": [
        {
          "title": "TrustLLM: Trustworthiness in Large Language Models",
          "score": 0.00524828659739278
        },
        {
          "title": "OLMo: Accelerating the Science of Language Models",
          "score": 0.0036282066119888295
        },
        {
          "title": "Implicit Optimization Bias of Next-token Prediction in Linear Models",
          "score": 0.0032469245267765855
        },
        {
          "title": "Large Language Models: A Survey",
          "score": 0.003217484088553004
        },
        {
          "title": "Evaluating Text-to-Visual Generation with Image-to-Text Generation",
          "score": 0.003160305743007164
        },
        {
          "title": "GenAI-Bench: Evaluating and Improving Compositional Text-to-Visual Generation",
          "score": 0.002952010708990978
        },
        {
          "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model",
          "score": 0.0026755572952251025
        },
        {
          "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
          "score": 0.001925129975626458
        },
        {
          "title": "Efficiency Optimization of Large-Scale Language Models Based on Deep Learning in Natural Language Processing Tasks",
          "score": 0.001883534860465934
        },
        {
          "title": "Mechanics of Next Token Prediction with Self-Attention",
          "score": 0.0018267003437178165
        },
        {
          "title": "Qwen2-Audio Technical Report",
          "score": 0.0016991727617972812
        },
        {
          "title": "Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations",
          "score": 0.001695722241109399
        },
        {
          "title": "PMC-LLaMA: toward building open-source language models for medicine",
          "score": 0.0016452522210621844
        },
        {
          "title": "ShieldGPT: An LLM-based Framework for DDoS Mitigation",
          "score": 0.001593537870093023
        },
        {
          "title": "CAMEx: Curvature-aware Merging of Experts",
          "score": 0.0015398073924943292
        },
        {
          "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
          "score": 0.0015398073924943292
        },
        {
          "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF",
          "score": 0.0015398073924943292
        },
        {
          "title": "MoLEx: Mixture of Layer Experts for Finetuning with Sparse Upcycling",
          "score": 0.0015398073924943292
        },
        {
          "title": "Word-specific tonal realizations in Mandarin",
          "score": 0.0015398073924943292
        },
        {
          "title": "Time and thyme again: Connecting English spoken word duration to models of the mental lexicon",
          "score": 0.0015398073924943292
        }
      ],
      "by_authority": [
        {
          "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
          "score": 0.47820060753676413
        },
        {
          "title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs",
          "score": 0.44237627485106507
        },
        {
          "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
          "score": 0.05427178885816634
        },
        {
          "title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models",
          "score": 0.009784584963187012
        },
        {
          "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks",
          "score": 0.00284503592853255
        },
        {
          "title": "Security and Privacy Challenges of Large Language Models: A Survey",
          "score": 0.001485592050635929
        },
        {
          "title": "EdgeLLM: Fast On-Device LLM Inference With Speculative Decoding",
          "score": 0.0011468979526752926
        },
        {
          "title": "EdgeShard: Efficient LLM Inference via Collaborative Edge Computing",
          "score": 0.0007916987749295648
        },
        {
          "title": "A Survey on Mixture of Experts in Large Language Models",
          "score": 0.0007148862833563144
        },
        {
          "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
          "score": 0.0006552333864752062
        },
        {
          "title": "PMC-LLaMA: toward building open-source language models for medicine",
          "score": 0.0006418207131003949
        },
        {
          "title": "Mobile-LLaMA: Instruction Fine-Tuning Open-Source LLM for Network Analysis in 5G Networks",
          "score": 0.000627163595453557
        },
        {
          "title": "Harnessing LLMs for API Interactions: A Framework for Classification and Synthetic Data Generation",
          "score": 0.0006264297605104957
        },
        {
          "title": "A Survey on Large Language Models for Communication, Network, and Service Management: Application Insights, Challenges, and Future Directions",
          "score": 0.0006139797337080617
        },
        {
          "title": "Personalized Wireless Federated Learning for Large Language Models",
          "score": 0.0006139797337080617
        },
        {
          "title": "Generative AI Meets Semantic Communication: Evolution and Revolution of Communication Tasks",
          "score": 0.0006139797337080617
        },
        {
          "title": "Distributed Foundation Models for Multi-Modal Learning in 6G Wireless Networks",
          "score": 0.0005677504482860644
        },
        {
          "title": "Multilingual Brain Surgeon: Large Language Models Can Be Compressed Leaving No Language behind",
          "score": 0.0005511695243925239
        },
        {
          "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model",
          "score": 0.000417506619362208
        },
        {
          "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
          "score": 0.0002577634854964429
        }
      ]
    }
  }
]