{
  "window": "2020_2022",
  "num_clusters": 12,
  "cluster_details": {
    "0": [
      {
        "id": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
        "title": "Language Models are Few-Shot Learners"
      },
      {
        "id": "9ffcb3624f2637b5d0fe28c61ec8472293cfebc7",
        "title": "All the News That’s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation"
      },
      {
        "id": "ad5970584754cc7a1d91c95ab84a1e210258183a",
        "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"
      },
      {
        "id": "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d",
        "title": "Experience Grounds Language"
      },
      {
        "id": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
        "title": "Scaling Laws for Neural Language Models"
      },
      {
        "id": "933b37b21e9d61139660088adb032ff3fdf56d86",
        "title": "Learning Video Representations from Large Language Models"
      },
      {
        "id": "0e1f80a3f52c9051026656f00e31c0b9c1428d7a",
        "title": "Can language models automate data wrangling?"
      },
      {
        "id": "7821e7639ffaeea175422f35fae2eb1c095ed1a6",
        "title": "Protein Language Models and Structure Prediction: Connection and Progression"
      },
      {
        "id": "e541fb54b8b9f2b4d8253f678a28830cd4f52d86",
        "title": "A Survey of Text Representation Methods and Their Genealogy"
      },
      {
        "id": "a8b896d5dd327ac808bcbcf4e4ed324a4615c356",
        "title": "Adaptation Approaches for Nearest Neighbor Language Models"
      },
      {
        "id": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "id": "2c6ac935c826002976722ca8d3319f691975687e",
        "title": "Self-conditioned Embedding Diffusion for Text Generation"
      },
      {
        "id": "af53c3903d8d14e278700363e44e74f3c1b0991f",
        "title": "Rationalizing predictions by adversarial information calibration"
      },
      {
        "id": "d2aa9772a914b2fd7f193182f44c25b1e1a96107",
        "title": "LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails"
      },
      {
        "id": "59e0ef773d2383875c6711f136e7159af06dcdbb",
        "title": "Language Detoxification with Attribute-Discriminative Latent Space"
      },
      {
        "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "id": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "id": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "id": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "id": "814a4f680b9ba6baba23b93499f4b48af1a27678",
        "title": "Measuring Massive Multitask Language Understanding"
      },
      {
        "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "id": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "id": "490d8006851b1562cfd9ec1f057471f2868289d1",
        "title": "Rethinking with Retrieval: Faithful Large Language Model Inference"
      },
      {
        "id": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "id": "cb8374f90c7b61f07b257377b4ce5e6d917c0df3",
        "title": "Privacy-Preserving Models for Legal Natural Language Processing"
      },
      {
        "id": "29642e7371f0dd814bfb6d4591c78b83b0dc8b43",
        "title": "Natural language processing in toxicology: Delineating adverse outcome pathways and guiding the application of new approach methodologies"
      },
      {
        "id": "5581bf85386737bd3378eec68189759a05280bea",
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic"
      },
      {
        "id": "61ddf932488405ab1c7b275460d2b3c5dfa274a0",
        "title": "Fixing Model Bugs with Natural Language Patches"
      },
      {
        "id": "18b3ab9763ed3c4633ee68aa6dd75f6377837553",
        "title": "Natural Language Deduction with Incomplete Information"
      },
      {
        "id": "fc5631cdd08722f51e0ce4b718de0f081ae73603",
        "title": "DANLI: Deliberative Agent for Following Natural Language Instructions"
      },
      {
        "id": "e383ee065bb7b9ef798d5aff9db794691f4c0e38",
        "title": "Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook"
      },
      {
        "id": "e9bc29cfcfbea4d137652d10715a9c9389349a90",
        "title": "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"
      },
      {
        "id": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
      },
      {
        "id": "db4ab91d5675c37795e719e997a2827d3d83cd45",
        "title": "Towards Reasoning in Large Language Models: A Survey"
      },
      {
        "id": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
        "title": "Language Models of Code are Few-Shot Commonsense Learners"
      },
      {
        "id": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
        "title": "Large Language Models are Better Reasoners with Self-Verification"
      },
      {
        "id": "2abed82162c47a0cc32cd62afcf46b0745541017",
        "title": "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"
      },
      {
        "id": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
        "title": "Rationale-Augmented Ensembles in Language Models"
      },
      {
        "id": "e37310ccc5f368355c19b4de45826278aeaff280",
        "title": "Understanding HTML with Large Language Models"
      },
      {
        "id": "0b0c87602818160e75ff54b48ef154d0ca27dd45",
        "title": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations"
      },
      {
        "id": "551b05734eb2181c4ca009a411144e8447ed1606",
        "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis"
      },
      {
        "id": "b4c80344081d8c548fc4d68868b8262182a146fa",
        "title": "Structured information extraction from complex scientific text with fine-tuned large language models"
      },
      {
        "id": "72c53ffacd4ad86391dd70d3b18c2b9e80ba2956",
        "title": "TransPolymer: a Transformer-based language model for polymer property predictions"
      },
      {
        "id": "2a7ae3e98357569c41424dacd60c62d3df78a0db",
        "title": "Limitations of Language Models in Arithmetic and Symbolic Induction"
      },
      {
        "id": "763125fd2befe605b009cdd8d7ee8c8c694bc9e5",
        "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models"
      },
      {
        "id": "f7704c835ecf7f14dcbaea750f63d45102ff79c4",
        "title": "Predicting dementia from spontaneous speech using large language models"
      },
      {
        "id": "ccfa7a251644aafc7f85ca66c6652adfcf93429c",
        "title": "From Word Embeddings to Pre-Trained Language Models: A State-of-the-Art Walkthrough"
      },
      {
        "id": "d4de516ab5544ec78a4bc638a1d272e551b95bdf",
        "title": "Augmenting interpretable models with large language models during training"
      },
      {
        "id": "c2d0b6abc49aa4749bec53990934a86378aac9d6",
        "title": "A Measure-Theoretic Characterization of Tight Language Models"
      },
      {
        "id": "6d4a12ea469ff08634eeb24c47b265a7dca2fce2",
        "title": "PADL: Language-Directed Physics-Based Character Control"
      },
      {
        "id": "0caf0fa48aced139c0d8214be4a795d9576b9990",
        "title": "Natural Test Generation for Precise Testing of Question Answering Software"
      },
      {
        "id": "656cbdb55b510f9fe462a3993edbd5783e2c2cd4",
        "title": "Material transformers: deep learning language models for generative materials design"
      },
      {
        "id": "1139a20f41f8686f9d4e8b32550fe7bc8dc9ca13",
        "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost"
      },
      {
        "id": "c7175ad54baf30168ce6c366350d21a08e17a91e",
        "title": "I can’t believe there’s no images! : Learning Visual Tasks Using Only Language Supervision"
      },
      {
        "id": "22fbef2bfef213a7619ee4f307e8f42d1888e638",
        "title": "LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation"
      },
      {
        "id": "d651691e42d42423f0b13a0ee7dfc67087d87379",
        "title": "'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers"
      },
      {
        "id": "ec30bbb6ac76dd861fd23a71c65008ce509d22ad",
        "title": "Understanding models understanding language"
      },
      {
        "id": "04f87baf7d1b3eb303a52a8a66c8189f396dd114",
        "title": "Application of Pretrained Large Language Models in Embodied Artificial Intelligence"
      },
      {
        "id": "f557f3a32d309373e7d31bb93ca1b80b4a6e39e7",
        "title": "Symbolic Math Reasoning with Language Models"
      },
      {
        "id": "dfd461850a228b555487b14ede991adbf85c93aa",
        "title": "Collectively encoding protein properties enriches protein language models"
      },
      {
        "id": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "id": "e342165a614588878ad0f4bc9bacf3905df34d08",
        "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications"
      },
      {
        "id": "0d0dbfb1b315a43216020abaf74d289456198219",
        "title": "MaPLe: Multi-modal Prompt Learning"
      },
      {
        "id": "60c8d0619481eaafdd1189af610d0e636271fed5",
        "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"
      },
      {
        "id": "af1c871282ec122869d03f5420ef5d9143358a91",
        "title": "Visual Programming: Compositional visual reasoning without training"
      },
      {
        "id": "25425e299101b13ec2872417a14f961f4f8aa18e",
        "title": "VIMA: General Robot Manipulation with Multimodal Prompts"
      },
      {
        "id": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b",
        "title": "In-context Examples Selection for Machine Translation"
      },
      {
        "id": "86d507f4c395d966cd0fb8c72a87777876a08850",
        "title": "Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives"
      },
      {
        "id": "ca086f4c09cf8de705830ac2b70951737fab93ca",
        "title": "A Review of Sparse Expert Models in Deep Learning"
      },
      {
        "id": "744ee04b08e04ad216bd586ac74bbc1fed0dea8f",
        "title": "Transformers in Remote Sensing: A Survey"
      },
      {
        "id": "2dbec38fe353ab0e495ad09263389dbc9260824d",
        "title": "A Survey of Deep Learning for Mathematical Reasoning"
      },
      {
        "id": "2677645b0f96c8c055b83c904d531cfe22b2e623",
        "title": "GPT-3-Driven Pedagogical Agents to Train Children’s Curious Question-Asking Skills"
      },
      {
        "id": "ec27f85979899a4193a8ec3b932ddb677c59be62",
        "title": "Legal Prompt Engineering for Multilingual Legal Judgement Prediction"
      },
      {
        "id": "a1186d7d9a9ef258c76afef1177e4f348061a537",
        "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers"
      },
      {
        "id": "84fd742b380c9518a313c3e0a5d3b92447d25fda",
        "title": "Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets"
      },
      {
        "id": "22b58dce1a13382418b8372bbd50ed3b2533f899",
        "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"
      },
      {
        "id": "5697a0ede5425954d48daa6e1893dc87bd7d8be7",
        "title": "Contrastive Search Is What You Need For Neural Text Generation"
      },
      {
        "id": "19be53b21a022f7578e073709836c35ff52bec43",
        "title": "Mining legal arguments in court decisions"
      },
      {
        "id": "cf27dba8f270e57088f83b206234327eb5a7a1ee",
        "title": "Visuals to Text: A Comprehensive Review on Automatic Image Captioning"
      },
      {
        "id": "ae441f7305dc2cd58c708528b3ecee3501cc5c46",
        "title": "Plansformer: Generating Symbolic Plans using Transformers"
      },
      {
        "id": "ca2ea26b851fea6914a65b233b7daf8f32e38073",
        "title": "CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation"
      },
      {
        "id": "569ba1ef5db91bc5ecb49c8beb2a14633f4e3159",
        "title": "COCOA"
      },
      {
        "id": "154a5d37e44fac0a5bd6cacb8fdee0e8b7b3bff2",
        "title": "Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain"
      },
      {
        "id": "836ca61c0226fd5f763335ad4c13acc784251343",
        "title": "Towards a Unified View on Visual Parameter-Efficient Transfer Learning"
      },
      {
        "id": "060cee8411181e8151ab1e3212b81528accd9b8b",
        "title": "On Transforming Reinforcement Learning With Transformers: The Development Trajectory"
      },
      {
        "id": "b5880664772bdfdea9cba0be37290921dd10effa",
        "title": "Legal Judgment Prediction: A Survey of the State of the Art"
      },
      {
        "id": "7b604cd12bfd735f16d2097357b3d6ca584d53a1",
        "title": "Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning"
      },
      {
        "id": "17c26a709f02e7e865f284225e3f56d19fc1843c",
        "title": "Gesture2Vec: Clustering Gestures using Representation Learning Methods for Co-speech Gesture Generation"
      },
      {
        "id": "8e2930ac8ae9a758b367513cccb7d562f354afea",
        "title": "Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production"
      },
      {
        "id": "d4bcc5c1bb5ae63878b7801f7f3170fd8b22a347",
        "title": "Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer"
      },
      {
        "id": "bcf272c12c55362a92741b1a0b2ac76f066466f9",
        "title": "ClassActionPrediction: A Challenging Benchmark for Legal Judgment Prediction of Class Action Cases in the US"
      },
      {
        "id": "60cc0b1a573e75732d54ac688b84b7bd4a021b89",
        "title": "NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations On-the-Fly"
      },
      {
        "id": "567067ad6c2f7b53eaf967a27effb109d235d87a",
        "title": "The Moral Foundations Reddit Corpus"
      },
      {
        "id": "d1feb79f63ea52839f4a784fbd7d60bb73dd98dd",
        "title": "ComFact: A Benchmark for Linking Contextual Commonsense Knowledge"
      },
      {
        "id": "fa4188bcc6728b580699f714a8a6fe5fc60d7dfe",
        "title": "Rethinking data-driven networking with foundation models: challenges and opportunities"
      },
      {
        "id": "0e5e6c9460ed9fd695064daf7868fbb57c9a0f24",
        "title": "Human vs. GPT-3: The challenges of extracting emotions from child responses"
      },
      {
        "id": "f240b39aa5aa30b3e4f7a8e68d681a13a9a72054",
        "title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\""
      },
      {
        "id": "5b9798dcecb4894e22335a2e5d6980b3b92c6c5f",
        "title": "UGIF: UI Grounded Instruction Following"
      },
      {
        "id": "940d32e2f5ac604393e8a9ef9195f53e00fc20ad",
        "title": "Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding"
      },
      {
        "id": "5b0f4c1d8ac2a0795a3da5e1186add532c47dbe3",
        "title": "Automatic Detection of Machine Generated Texts: Need More Tokens"
      },
      {
        "id": "66d3d8ba9419fe8927cd8452eda62e5bf3c00f98",
        "title": "On the Vulnerabilities of Text-to-SQL Models"
      },
      {
        "id": "108ea619cfef2c53b36ace97cef6830944c1d802",
        "title": "Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool"
      },
      {
        "id": "6ac1fccf1e04487d439ee598f51c03ddac5144ca",
        "title": "N-Grammer: Augmenting Transformers with latent n-grams"
      },
      {
        "id": "8747b5e9d19d4cf6eab9b93d8a434595e4a7e11f",
        "title": "Traditional Machine Learning and Deep Learning-based Text Classification for Turkish Law Documents using Transformers and Domain Adaptation"
      },
      {
        "id": "b173aed9f741cdf21c2cabd40ae4dfff223dbd9d",
        "title": "A natural language fMRI dataset for voxelwise encoding models"
      },
      {
        "id": "721acfc94c91c685646b2944a39e220e4ba71b29",
        "title": "Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations"
      },
      {
        "id": "91deaf9d324c8feafc189da0da03e60a60287bca",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      },
      {
        "id": "c305ab1bdba79442bec72ec7f5c5ee7c49c2a566",
        "title": "Visual Language Maps for Robot Navigation"
      },
      {
        "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
        "title": "Ask Me Anything: A simple strategy for prompting language models"
      },
      {
        "id": "dcb31b98ec58f3fff9f94f148e2952595f017fd9",
        "title": "ProtGPT2 is a deep unsupervised language model for protein design"
      },
      {
        "id": "c067664a45dce31411b3052c635c044ad4587db4",
        "title": "Generating Diverse Code Explanations using the GPT-3 Large Language Model"
      },
      {
        "id": "ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b",
        "title": "CLIPPO: Image-and-Language Understanding from Pixels Only"
      },
      {
        "id": "d19bae780d5fe93e4d007e325c278598ec7f9ea4",
        "title": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE"
      },
      {
        "id": "5d8fd04c436367b18b35e28332ee8e452a477f3f",
        "title": "Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study"
      },
      {
        "id": "0ab77793e1f654c1d46179fcfad9de5566ed2995",
        "title": "A single-cell gene expression language model"
      },
      {
        "id": "9cb5f39cc6b6645f18b922e312f8193f14d8359a",
        "title": "EW-Tune: A Framework for Privately Fine-Tuning Large Language Models with Differential Privacy"
      },
      {
        "id": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "id": "c8d594f09413b1555970f43e68847c211235d60f",
        "title": "Prompting GPT-3 To Be Reliable"
      },
      {
        "id": "70b98d90767345b15e0569082c0e4ac661279b5d",
        "title": "Is GPT-3 a Good Data Annotator?"
      },
      {
        "id": "b37d57edf4a84da158ab8d77921d4aa39faceb32",
        "title": "FP8 Formats for Deep Learning"
      },
      {
        "id": "c822486b8f1dcbef3b96b136c85d48d0dc580f31",
        "title": "Audio Retrieval with WavText5K and CLAP Training"
      },
      {
        "id": "288b24f16fe7341c91def471120fa23233e34acc",
        "title": "Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors"
      },
      {
        "id": "591627746d0f8c3b642b7c9415bbc8af66e24a0e",
        "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation"
      },
      {
        "id": "7694f004c67840d7f098b3612d4b3dabd915c116",
        "title": "Executing your Commands via Motion Diffusion in Latent Space"
      },
      {
        "id": "30477855d76058a9b542cabea3058aad1a837d51",
        "title": "A Case for Business Process-Specific Foundation Models"
      },
      {
        "id": "49aec6fb44ab52181960512a6067eded0ce4182b",
        "title": "Benchmarking Long-tail Generalization with Likelihood Splits"
      },
      {
        "id": "3e5698bcb675eceaa8bc462a38a6a6ea91ddfb1a",
        "title": "The death of the short-form physics essay in the coming AI revolution"
      },
      {
        "id": "bc145083eadcd03fc4567ab52b51bd30392ca1f9",
        "title": "How persuasive is AI-generated argumentation? An analysis of the quality of an argumentative text produced by the GPT-3 AI text generator"
      },
      {
        "id": "951016af892f5ecde12b2acd9374b92b66afb796",
        "title": "AI art in architecture"
      },
      {
        "id": "b7430ea60368aceabbaeeba328167e0eac0639f2",
        "title": "Is Neuro-Symbolic AI Meeting its Promise in Natural Language Processing? A Structured Review"
      },
      {
        "id": "a84c39c299e0e01219b42a74593cb50f1b8fd2bc",
        "title": "Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing"
      },
      {
        "id": "802a5d24c78f713e282b003d99b4afd924bd7568",
        "title": "A Survey on Dynamic Neural Networks for Natural Language Processing"
      },
      {
        "id": "508ecc80dc2c74da8a1c61006a53bfde6d88d00e",
        "title": "Making humanoid robots teaching assistants by using natural language processing (NLP) cloud-based services"
      },
      {
        "id": "14d0cc870105c2e7029fce5676387eb75f42a65d",
        "title": "Robust Quantification of Gender Disparity in Pre-Modern English Literature using Natural Language Processing"
      },
      {
        "id": "df602516e28a9ef0ef665ed0aef551984d8d770d",
        "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)"
      },
      {
        "id": "3b9b1aba877ecd3f7e508cbc78a41b623349902b",
        "title": "Translation between Molecules and Natural Language"
      },
      {
        "id": "5e8d3c2dc0fc53949794fc00600e25558c4a2441",
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation"
      },
      {
        "id": "47e15941c8b157873c8264e4bf50318d1ba5cd18",
        "title": "Natural Language to Code Translation with Execution"
      },
      {
        "id": "196cc546041cb6db167784f632037f0a1dcf4a79",
        "title": "Generating Natural Language Proofs with Verifier-Guided Search"
      },
      {
        "id": "e5aa2a1e36a2c68fa4aa59afdb8b6e1c419f547c",
        "title": "Natural Language Deduction through Search over Statement Compositions"
      },
      {
        "id": "d97867578a50f824a19168c024e138e3ca482746",
        "title": "Deep Lexical Hypothesis: Identifying personality structure in natural language"
      },
      {
        "id": "01a621bb49dc6d6c348806d37b5fd8d157bc106d",
        "title": "Tracked-Vehicle Retrieval by Natural Language Descriptions With Domain Adaptive Knowledge"
      },
      {
        "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "id": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "id": "7cbc2a7843411a1768ab762930707af0a3c33a19",
        "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
      },
      {
        "id": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "id": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
        "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"
      },
      {
        "id": "23c265ba884b92ecbd9d18641078d964697e4590",
        "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"
      },
      {
        "id": "24ed74ed29c057cba8b52fff4edd2c0d7f408716",
        "title": "VLP: A Survey on Vision-language Pre-training"
      },
      {
        "id": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5",
        "title": "Autoformalization with Large Language Models"
      },
      {
        "id": "b92628d13e8d090d042232fe6ae0b8998634b893",
        "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"
      },
      {
        "id": "b66e776502b8484a09f8759c8dc3737ebe714f34",
        "title": "Controllable protein design with language models"
      },
      {
        "id": "5eeb828685e44ca5b8ebafb34a9fa4d51c9186df",
        "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models"
      },
      {
        "id": "bbf3451c9c4eb6d8d84314b584afb0fadd01ffba",
        "title": "A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction"
      },
      {
        "id": "795b31fd57612718ab2074614df152da6e692db5",
        "title": "Prompt Tuning for Discriminative Pre-trained Language Models"
      },
      {
        "id": "3b239d232ebb0fdb0515f41fd439e54ed4e8f86a",
        "title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large Models"
      },
      {
        "id": "bd3d8a17988add79205f697465bbd6c2ef59e621",
        "title": "Prompt–RSVQA: Prompting visual context to a language model for Remote Sensing Visual Question Answering"
      },
      {
        "id": "f8f2b17083c10f730b711a938e2bb5da992086e7",
        "title": "AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models"
      },
      {
        "id": "4962ce1242ff905b71e199964eafea3d2ad9688d",
        "title": "A Domain-adaptive Pre-training Approach for Language Bias Detection in News"
      },
      {
        "id": "1415479215dcfec5e2ee0d33f1e1565ae2c65bb9",
        "title": "Examining Scaling and Transfer of Language Model Architectures for Machine Translation"
      },
      {
        "id": "49c7599fad96dbb571f7b2e9d6b6c733b4bf6d6d",
        "title": "Transcormer: Transformer for Sentence Scoring with Sliding Language Modeling"
      },
      {
        "id": "5d79d41b74999a286de2621ce732a928727efb3f",
        "title": "Leveraging pre-trained language models for conversational information seeking from text"
      },
      {
        "id": "979eb5c97c49d7979447ed684500895a24d75ac4",
        "title": "The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"
      },
      {
        "id": "01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8",
        "title": "Behavior Transformers: Cloning k modes with one stone"
      },
      {
        "id": "c1ffa773f29e567e13b00495767f8c42e2bb44b2",
        "title": "A Contrastive Framework for Neural Text Generation"
      },
      {
        "id": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
        "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"
      },
      {
        "id": "fe26607ca95c0d1d9005810b4ad12845ee69e9cf",
        "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem"
      },
      {
        "id": "8a8e2c23fd7179d981cbefbbc4844494e7255c53",
        "title": "GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry"
      },
      {
        "id": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17",
        "title": "Designing Effective Sparse Expert Models"
      },
      {
        "id": "00debf63dafa966221c6e572f7705a813d704ff1",
        "title": "K-LITE: Learning Transferable Visual Models with External Knowledge"
      },
      {
        "id": "fcc7a44c19a553998dc471031652531c71a832aa",
        "title": "A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and Challenges"
      },
      {
        "id": "0265144c696bf9371a0a63ece590dd2403ee71be",
        "title": "When Do Flat Minima Optimizers Work?"
      },
      {
        "id": "315ae27770cf0eea905875a61e639878629ae3b1",
        "title": "Self-Supervised Learning of Brain Dynamics from Broad Neuroimaging Data"
      },
      {
        "id": "77abefb3f7f46939af46ea472fcd8c0ad93fda35",
        "title": "Crystal twins: self-supervised learning for crystalline material property prediction"
      },
      {
        "id": "a80f2102e5de3ead1b9689b440503f49383ddc94",
        "title": "Is a Question Decomposition Unit All We Need?"
      },
      {
        "id": "aec212c099e8a711367582806d620f7b0867ce91",
        "title": "DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation"
      },
      {
        "id": "0f72e329d3b0f1cfe388d102ef5fec0677ac7558",
        "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics"
      },
      {
        "id": "81b234a1e6da7bc8131e8585a9455dca5dd68754",
        "title": "Transkimmer: Transformer Learns to Layer-wise Skim"
      },
      {
        "id": "7107d06366b48b3593c8128ed2ca67e0b413628c",
        "title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"
      },
      {
        "id": "9038f40c43e7d62d8f1dc4819093083090911f7a",
        "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning"
      },
      {
        "id": "932b6353204e56f20917edadda2fa636ace21090",
        "title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks"
      },
      {
        "id": "f12515166c1ccc3301746fd00ffbbd1852d09ad6",
        "title": "vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training"
      },
      {
        "id": "a0532a7ab0cd6f327977d2bb122abd6c12f7a0df",
        "title": "DSS-TRM: deep spatial–spectral transformer for hyperspectral image classification"
      },
      {
        "id": "c963c505ffc4cc8b33315eb967784d0a466b3910",
        "title": "ProQA: Structural Prompt-based Pre-training for Unified Question Answering"
      },
      {
        "id": "fd9ec4bf9cd83c3d0e38470407427a8deeb8d547",
        "title": "PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map"
      },
      {
        "id": "59d68464fdefa57d42ba461cfa7da3004b13be63",
        "title": "Modern Question Answering Datasets and Benchmarks: A Survey"
      },
      {
        "id": "84df90baa7e2a84c5a7abec69231b5db7208c9dc",
        "title": "Example-based Hypernetworks for Out-of-Distribution Generalization"
      },
      {
        "id": "ed7f77fc20193f67bbd0a3aa4a80427abd6197d4",
        "title": "End-to-End Pedestrian Trajectory Forecasting with Transformer Network"
      },
      {
        "id": "e0180b6ca96283ef8498307f7fd0c6a227081d21",
        "title": "Predicting Issue Types with seBERT"
      },
      {
        "id": "2b5721b1a73a42d64d8bd3235752d1f3a787378b",
        "title": "DevFormer: A Symmetric Transformer for Context-Aware Device Placement"
      },
      {
        "id": "0ff05c98be6015695871bd8216c35e572a9d2d95",
        "title": "Effects of Similarity Score Functions in Attention Mechanisms on the Performance of Neural Question Answering Systems"
      },
      {
        "id": "02993aa3024a7c16a7ffc94edbb974f4ffde95f6",
        "title": "Detecting Text Formality: A Study of Text Classification Approaches"
      },
      {
        "id": "32d7ad711eb8357ded5ca9fe715342dedb9f4237",
        "title": "Text- and author-dependent moral foundations classification"
      },
      {
        "id": "6d8f5366a04ed1955bb62759369c938f05da7f27",
        "title": "Open-Ended Reinforcement Learning with Neural Reward Functions"
      },
      {
        "id": "1a8682490a9d7ffa7cdcf3d13b958a8758fbf43d",
        "title": "Detecting explicit lyrics: a case study in Italian music"
      },
      {
        "id": "bc64190d42d9dc34077b6a096d9053bb88deaa3a",
        "title": "NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks"
      },
      {
        "id": "4a3553941825e7c46eb052e7c3c9fc3e6de895b1",
        "title": "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers"
      },
      {
        "id": "9b2339cfe4640841078073dfa1c1c60e84eeeaa2",
        "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language"
      },
      {
        "id": "3f35e2b369cd975224675016cf44db0b450c0231",
        "title": "Understanding English as a Foreign Language Students’ Idea Generation Strategies for Creative Writing With Natural Language Generation Tools"
      },
      {
        "id": "d48b29889241551e1ee6622fa78c3fa4159255dd",
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"
      },
      {
        "id": "b9b220b485d2add79118ffdc2aaa148b67fa53ef",
        "title": "Pre-Trained Language Models for Interactive Decision-Making"
      },
      {
        "id": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20",
        "title": "Wordcraft: Story Writing With Large Language Models"
      },
      {
        "id": "15ac70d077bb735eed4a8502ce49aa7782c803fd",
        "title": "What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data"
      },
      {
        "id": "5bc61019771a9fe2a12cc41bad1d9ae4222a152c",
        "title": "PromptMaker: Prompt-based Prototyping with Large Language Models"
      },
      {
        "id": "47e135ef718baf6a3a86a1167b67ed96f7932ca4",
        "title": "LEBP - Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents"
      },
      {
        "id": "15af0909d4426dabe9f64af752f0298a9a29fa8b",
        "title": "Self-Training Vision Language BERTs With a Unified Conditional Model"
      },
      {
        "id": "a8a1440b72b70d1372d3f51ab119a5bba70b4d92",
        "title": "CALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations"
      },
      {
        "id": "93324dfb8b02f43edb4122d08ccc6b90d3a6b577",
        "title": "Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining"
      },
      {
        "id": "58b59d0a10917580e377dae4996be0ebdb176875",
        "title": "AvatarCLIP"
      },
      {
        "id": "202967f77c4384bce80eaf2fa5737259008267d3",
        "title": "Learning to Merge Tokens in Vision Transformers"
      },
      {
        "id": "e98799e709dc93a8ea721dd6b3e1398104797050",
        "title": "Cracking the code: Co-coding with AI in creative programming education"
      },
      {
        "id": "54020e5fe48ebb250f27d744e20a63cac2988a84",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "id": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
        "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"
      },
      {
        "id": "760f807406272b5ede591f19241824f2d17c319a",
        "title": "Multi-Task Learning in Natural Language Processing: An Overview"
      },
      {
        "id": "11907f691e9b7fc32a492e1de676a4b788add155",
        "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain"
      },
      {
        "id": "7cf0d71014b6a1a5397d37481c91efbb1a5660ea",
        "title": "A Comparison of Natural Language Processing and Machine Learning Methods for Phishing Email Detection"
      },
      {
        "id": "52db8674337e5d86dcb96d013734befc8c3d4581",
        "title": "Large Language Models are not Models of Natural Language: they are Corpus Models."
      },
      {
        "id": "21174b47cca9679d3425db4545bd3fac285e36f9",
        "title": "Event Log Construction from Customer Service Conversations Using Natural Language Inference"
      },
      {
        "id": "75c864fc2e384ce299a4031e31c48f9ebc0a9842",
        "title": "Natural Language for Human-Robot Collaboration: Problems Beyond Language Grounding"
      },
      {
        "id": "96ea07447d2f9adefe03852a878517a2a6d45b96",
        "title": "Learning to Prompt for Vision-Language Models"
      },
      {
        "id": "638e198424f81570441a91bafda16d96f53d3c72",
        "title": "SLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Natural Speech"
      },
      {
        "id": "80d0116d77beeded0c23cf48946d9d10d4faee14",
        "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"
      },
      {
        "id": "319b84be7a843250bc81d7086f79a4126d550277",
        "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
      },
      {
        "id": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
        "title": "Primer: Searching for Efficient Transformers for Language Modeling"
      },
      {
        "id": "c16d35a6d36a3688f222e1f7ebb858da67f5da7d",
        "title": "BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models"
      },
      {
        "id": "c2a79e2a65b721d4de5f6d4806323174b9f8f393",
        "title": "Towards Zero-Label Language Learning"
      },
      {
        "id": "20a01009d38d083a49e01ef46005363135453661",
        "title": "The great Transformer: Examining the role of large language models in the political economy of AI"
      },
      {
        "id": "a3184d40d390793232c99c89b57b8f65c16320b2",
        "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
      },
      {
        "id": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79",
        "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"
      },
      {
        "id": "17aa05693a9d3dcbb0ffc7a05be59ef621c4d0d4",
        "title": "PatentNet: multi-label classification of patent documents using deep learning based language understanding"
      },
      {
        "id": "a0b777b25cdf0fc992568ca52a5c7bebf1ee987f",
        "title": "Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research"
      },
      {
        "id": "cb416e2d8bd72203bc73c1005251dcb12b153d20",
        "title": "Language Representation Models: An Overview"
      },
      {
        "id": "0d232524732c133e428e8867de69a48886509a7e",
        "title": "Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning"
      },
      {
        "id": "3d46e008543b353b51407fb805edbd1da23dfe6c",
        "title": "Language Models as a Knowledge Source for Cognitive Agents"
      },
      {
        "id": "677f0b5c01c16565a6c802ad7d253928fa6ae8ec",
        "title": "MultiModal Language Modelling on Knowledge Graphs for Deep Video Understanding"
      },
      {
        "id": "5dcc65466e4dd1f400823f424d720422a65cd7a5",
        "title": "Technical Language Supervision for Intelligent Fault Diagnosis in Process Industry"
      },
      {
        "id": "614dd2ae6db71cd5b2a6069407d3e0705ab2c19c",
        "title": "OpenPrompt: An Open-source Framework for Prompt-learning"
      },
      {
        "id": "77805d75199e7b9e580b4827f56a069ba0ddd13f",
        "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model"
      },
      {
        "id": "cbf98ebe967e0f3f3236e7932f37013b98244e94",
        "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"
      },
      {
        "id": "c215987b9fb31c2152773368102b9e45f75181a1",
        "title": "End-to-End Referring Video Object Segmentation with Multimodal Transformers"
      },
      {
        "id": "dca4d9abbc82e57dfa52f932e893d467a63e0682",
        "title": "Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation"
      },
      {
        "id": "4698fc4712f0212c8a3810fd67b41ee8b8896aba",
        "title": "Generate & Rank: A Multi-task Framework for Math Word Problems"
      },
      {
        "id": "1cbb3d96242c3f47c3f40aada33616d0f5c07737",
        "title": "Inductive Biases and Variable Creation in Self-Attention Mechanisms"
      },
      {
        "id": "e3b400db8dfe913d7865d8ded26a621e1902b830",
        "title": "TRS: Transformers for Remote Sensing Scene Classification"
      },
      {
        "id": "60c498956cb5737c4964aaca0b920592bd7f5689",
        "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search"
      },
      {
        "id": "04833b92c9002f241b8f8b956d018759eebc85b3",
        "title": "Tailor: Generating and Perturbing Text with Semantic Controls"
      },
      {
        "id": "a702ceeabc4c2e959513747f7ed2f5c29f7dbfcd",
        "title": "Collective intelligence for deep learning: A survey of recent developments"
      },
      {
        "id": "865bc9d894a55f3e77ea29f11691f8bf550571d5",
        "title": "Emerging trends: A gentle introduction to fine-tuning"
      },
      {
        "id": "6dc8693674a105c6daca5200141c50362e3044fc",
        "title": "Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images"
      },
      {
        "id": "42188859cdc5089f75cbbe5479793a67320b617c",
        "title": "RSVQA Meets Bigearthnet: A New, Large-Scale, Visual Question Answering Dataset for Remote Sensing"
      },
      {
        "id": "e853ef5d79b5584f4476e82c7a6b736c53a18bfc",
        "title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting"
      },
      {
        "id": "8cf3a454556060d6e9aa86dbabf221bd10bf9759",
        "title": "On the Effectiveness of Transfer Learning for Code Search"
      },
      {
        "id": "82edbb92d0f6952224c5aa0aff264b44b8fb4e98",
        "title": "Toward Foundation Models for Earth Monitoring: Proposal for a Climate Change Benchmark"
      },
      {
        "id": "9fc493346c6673e9b1ded18580322b3e79032274",
        "title": "OVANA: An Approach to Analyze and Improve the Information Quality of Vulnerability Databases"
      },
      {
        "id": "d7af5edf573116367cadd048b213a4f63e8367c9",
        "title": "How to transfer algorithmic reasoning knowledge to learn new algorithms?"
      },
      {
        "id": "f8cdb94c0446f3170cbb32ad6019b21839c4b237",
        "title": "Words to Matter: De novo Architected Materials Design Using Transformer Neural Networks"
      },
      {
        "id": "1667f299ecabc56206967b14caa06dd3350b6c87",
        "title": "Serf: Towards better training of deep neural networks using log-Softplus ERror activation Function"
      },
      {
        "id": "3299a03c49bc1a0a5f396ae6c6a659fe2b4ee3b0",
        "title": "Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management"
      },
      {
        "id": "2ba1a332484d30b86871a30b4fe85e7e9e7266cb",
        "title": "Capitalization and punctuation restoration: a survey"
      },
      {
        "id": "a0e6eeae8755260b8e0092e83b0249ab4f80d964",
        "title": "Creating User Interface Mock-ups from High-Level Text Descriptions with Deep-Learning Models"
      },
      {
        "id": "e9c0378ae58fb48c776605a855dd52abcaa9aa27",
        "title": "DaCy: A Unified Framework for Danish NLP"
      },
      {
        "id": "9b1263b047b13276f03030670da8175677102c74",
        "title": "Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering"
      },
      {
        "id": "0293c0ab2871f42e46a3a2e9e77196ad544326ff",
        "title": "The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification"
      },
      {
        "id": "065715bf35d1dba5bf06f59f7e1e8390b93e6adf",
        "title": "Evaluation of contextual embeddings on less-resourced languages"
      },
      {
        "id": "1c4bb3f8b2683cbd9c0d1230132364d16c39fb89",
        "title": "Using Transfer Learning to contextually Optimize Optical Character Recognition (OCR) output and perform new Feature Extraction on a digitized cultural and historical dataset"
      },
      {
        "id": "2bb0e6001b72290724fae5f716eabca4347b1df6",
        "title": "Tools for disambiguating RFCs"
      },
      {
        "id": "5436193122dff271796bca07df7cecb7a8d6dea6",
        "title": "Natural language-guided programming"
      },
      {
        "id": "88e8801e4daf404d3d40f1648ef29faeb8e6d58a",
        "title": "Blended Diffusion for Text-driven Editing of Natural Images"
      },
      {
        "id": "738e3e0623054da29dc57fc6aee5e6711867c4e8",
        "title": "CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation"
      },
      {
        "id": "76a2b197b5427ffd1d3470c6d3ea026588eb5d0a",
        "title": "CRIS: CLIP-Driven Referring Image Segmentation"
      },
      {
        "id": "a2412fdebd53bd25476f834ae2b8aa8cb44cb1e1",
        "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design"
      },
      {
        "id": "5ef4e013ce7bf0c19873c83ffd6898ce33ffd542",
        "title": "AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summarization"
      },
      {
        "id": "38dcd087f0c2801fcd75c0906a0e12258087ce8b",
        "title": "FairyTailor: A Multimodal Generative Framework for Storytelling"
      },
      {
        "id": "1c83f3f9789df43bf937ae2618721e2da83dcc06",
        "title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning"
      },
      {
        "id": "fe9d978f7718474e9613bac114c398614f09be71",
        "title": "Sinkformers: Transformers with Doubly Stochastic Attention"
      },
      {
        "id": "ffcd58f453f207d48075627da011f62782334c8f",
        "title": "Go Wider Instead of Deeper"
      },
      {
        "id": "879eaab2275a364549809560b42f0fef357ebbce",
        "title": "BERT: A Review of Applications in Natural Language Processing and Understanding"
      },
      {
        "id": "00213d44e03dae916860c0512025b5f96c3ee231",
        "title": "Data augmentation in natural language processing: a novel text generation approach for long and short text classifiers"
      },
      {
        "id": "8473e8a6534130418e05977e91fc874dcd5e56de",
        "title": "Context-Sensitive Visualization of Deep Learning Natural Language Processing Models"
      },
      {
        "id": "86ff29da65c3fd7f80a49c6cfcda9818480b83dc",
        "title": "Draw Me a Flower: Processing and Grounding Abstraction in Natural Language"
      },
      {
        "id": "911b7539e964782670e555930b291de16fa971c5",
        "title": "Flexible Generation of Natural Language Deductions"
      },
      {
        "id": "8be99c2d0802d6222e233dd67d2927c75a0bed24",
        "title": "Towards Accurate Visual and Natural Language-Based Vehicle Retrieval Systems"
      },
      {
        "id": "55b17a76d8b9f0a9adb5c116450a2cfd2844448c",
        "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation"
      },
      {
        "id": "f5d1dbdaa6c8a44f388f3f7fe538403baefc1252",
        "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do"
      },
      {
        "id": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
        "title": "PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"
      },
      {
        "id": "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5",
        "title": "Pretrained Language Models for Text Generation: A Survey"
      },
      {
        "id": "3be3146737c4b83cab1b754beafa738c1f941bf0",
        "title": "Innovative Bert-Based Reranking Language Models for Speech Recognition"
      },
      {
        "id": "b3e0e149669d21582b3bb6add9124aa4fb3f90dd",
        "title": "ParaLaw Nets - Cross-lingual Sentence-level Pretraining for Legal Text Processing"
      },
      {
        "id": "138d78692f72d44fa49629ce6d2b01d83add3f89",
        "title": "Exploring the Data Efficiency of Cross-Lingual Post-Training in Pretrained Language Models"
      },
      {
        "id": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "id": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
        "title": "Scaling Vision with Sparse Mixture of Experts"
      },
      {
        "id": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"
      },
      {
        "id": "3dcfa05a1c162e6cab927c5b08d0444f7b6691f4",
        "title": "Probing Classifiers: Promises, Shortcomings, and Advances"
      },
      {
        "id": "f8b5340b221ac2ac7c2067f28032c00636e6fc14",
        "title": "Vision Transformers for Remote Sensing Image Classification"
      },
      {
        "id": "add5f3f820b393e7ce5ed467814253824ecc484b",
        "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions"
      },
      {
        "id": "ef03f3910685dda98279aa2bcefbeada3362a626",
        "title": "When does pretraining help?: assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings"
      },
      {
        "id": "0c97903e7d85c05bcd2a7e26fc2a4a47998f5dde",
        "title": "Plagiarism in the age of massive Generative Pre-trained Transformers (GPT-3)"
      },
      {
        "id": "d4f6ef636e16b001986b541aa2afc76eed42ae34",
        "title": "Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery"
      },
      {
        "id": "7072db6eddb85ecd2c117365d91bd694760f726e",
        "title": "Position Information in Transformers: An Overview"
      },
      {
        "id": "a8f2c28e6721beb39e3696d78dd7da93596e3778",
        "title": "A Survey of Textual Emotion Recognition and Its Challenges"
      },
      {
        "id": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d",
        "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"
      },
      {
        "id": "09a4d9ab6a305ecf04d8a2e8f4d979f1bfbe1dc3",
        "title": "URLTran: Improving Phishing URL Detection Using Transformers"
      },
      {
        "id": "d77ad36898a623b2e82d96a0b8d9920204ca60bf",
        "title": "Ungoliant: An Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus"
      },
      {
        "id": "36a2c27ffa72c05c2a17dc90b7c54e492b88ba01",
        "title": "Few-shot Conformal Prediction with Auxiliary Tasks"
      },
      {
        "id": "afa72122ba06b6a694c21cf67d82620662e4917c",
        "title": "A full-stack search technique for domain optimized deep learning accelerators"
      },
      {
        "id": "ce9a7d4652192a4f32cd10cff211b47b2a2d9817",
        "title": "Accommodating Transformer onto FPGA: Coupling the Balanced Model Compression and FPGA-Implementation Optimization"
      },
      {
        "id": "04b40daa1ca74bdbb578beb314bf662538ecd18e",
        "title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders"
      },
      {
        "id": "c7f977f556d2060238fdc1286d057d46958afaf9",
        "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations"
      },
      {
        "id": "d8df456f790381f4ddb388be24a546625bd75ee2",
        "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"
      },
      {
        "id": "e4720dfdbc2d0da5db0ddf04cdab3a531c74680f",
        "title": "Text analysis using deep neural networks in digital humanities and information science"
      },
      {
        "id": "0216e5538eeb93c3adea7a6e01bd87709b277b0a",
        "title": "Spatiotemporal Transformer for Video-based Person Re-identification"
      },
      {
        "id": "cec62e7a1ad6b6aa7dcab0e27a7d9c44ce6e486e",
        "title": "InferBERT: A Transformer-Based Causal Inference Framework for Enhancing Pharmacovigilance"
      },
      {
        "id": "98adf45ce7a9b6bc5424b641ca22724fb770e529",
        "title": "Understanding Emails and Drafting Responses - An Approach Using GPT-3"
      },
      {
        "id": "bf459cca52f336de2afe61e6e6348571c73943c1",
        "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices"
      },
      {
        "id": "d13a0c8d49cb268d8d245925baee0316c1fe1875",
        "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"
      },
      {
        "id": "ae3d6eb676079d67068cbef2dcac7bcd220e3803",
        "title": "Knowledge-driven Natural Language Understanding of English Text and its Applications"
      },
      {
        "id": "46dfe93b284c929e7340e3f59303f25ad1a9df88",
        "title": "Generating Syntactically Controlled Paraphrases without Using Annotated Parallel Pairs"
      },
      {
        "id": "71fab1ce3c66998ba681ab378484be77690327a9",
        "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge"
      },
      {
        "id": "7d9a3b94f78827952b078c664b0da1c02e1c2ee3",
        "title": "What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?"
      },
      {
        "id": "44a422a2514c1cd6828423b5edce53d0dbdabd73",
        "title": "Structure-inducing pre-training"
      },
      {
        "id": "7e58442e384ffc41c42c179ee3c41ea45c23f682",
        "title": "Natural Language Processing with Python and spaCy: A Practical Introduction"
      },
      {
        "id": "a2c44f0b729740c5e0aadff833f8031919cf75a8",
        "title": "A Study of Pre-trained Language Models in Natural Language Processing"
      },
      {
        "id": "09bfe057c9285577242636950c6835b8731a07fb",
        "title": "Multi-task learning for natural language processing in the 2020s: where are we going?"
      },
      {
        "id": "0b4a8b1c98bd13ce0a6281bb1af3761f7f887235",
        "title": "Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements"
      },
      {
        "id": "15f002dde348b82817fa2a59e7ed56e6e3ec6972",
        "title": "Augmented Natural Language for Generative Sequence Labeling"
      },
      {
        "id": "7ca4abace88db259faed67686ed7bba02b46eb82",
        "title": "Language-Conditioned Imitation Learning for Robot Manipulation Tasks"
      },
      {
        "id": "dec2f6d3215de9aa2d87d358b7933fb21eeb3bc0",
        "title": "Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs"
      },
      {
        "id": "48745e3485f84cc5a2dab8e1ce41de0a38afb490",
        "title": "Efficient Transformer-based Large Scale Language Representations using Hardware-friendly Block Structured Pruning"
      },
      {
        "id": "3c7250dc37a3e3f110fe19884343ee61988f70b6",
        "title": "Misspelling Correction with Pre-trained Contextual Language Model"
      },
      {
        "id": "5aed438fecf702ead9c0cddfa9a80bd00cff0138",
        "title": "SSE-PT: Sequential Recommendation Via Personalized Transformer"
      },
      {
        "id": "9438bc5626b2d9a771cecc7a41ecabf6639db53c",
        "title": "Automatic Detection of Machine Generated Text: A Critical Survey"
      },
      {
        "id": "68a439902fd7d8005222f75d354a41fba6d60741",
        "title": "Efficient sparse collective communication and its application to accelerate distributed deep learning"
      },
      {
        "id": "55c4a747855c74210919c45f7899e1f79e4c97f5",
        "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data"
      },
      {
        "id": "24ed85ad966823868c1694a19385d01c6ad71008",
        "title": "A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving"
      },
      {
        "id": "6b6e3ad8df96cd409e6798a65150cb5ba80a4c13",
        "title": "Inferring experimental procedures from text-based representations of chemical reactions"
      },
      {
        "id": "06439c29150efb6441bfc928dfa5c0e0967edd9b",
        "title": "Exploring BERT’s sensitivity to lexical cues using tests from semantic priming"
      },
      {
        "id": "c0f709acf38eb27702b0fbce1215db0ebaa2de2b",
        "title": "SMYRF: Efficient Attention using Asymmetric Clustering"
      },
      {
        "id": "3938fe72ccfe4fe92387258874cb1cbe66194d4f",
        "title": "Point to the Expression: Solving Algebraic Word Problems Using the Expression-Pointer Transformer Model"
      },
      {
        "id": "db8fdf93aaf27cfd93fa0a41d78ce11b61a3a6dc",
        "title": "The Depth-to-Width Interplay in Self-Attention."
      },
      {
        "id": "d752424bca533b07d22d1821eed236abcde0fd9a",
        "title": "Semi-automated protocol disambiguation and code generation"
      },
      {
        "id": "1b188f604f52f5809711a31d3b505e7cbc82a455",
        "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification"
      },
      {
        "id": "b0f7aac40fd8623c5cbbb40fa488d3a546818f48",
        "title": "SEMANTIC NETWORKS FOR ENGINEERING DESIGN: A SURVEY"
      },
      {
        "id": "3250b43f23d67fe5fd3df1245daf0343da212d82",
        "title": "Improving Constituency Parsing with Span Attention"
      },
      {
        "id": "9453056463f37c572f50d129273df1f3cba2583f",
        "title": "A Conversational Digital Assistant for Intelligent Process Automation"
      },
      {
        "id": "dc3877e3058bc9c854e57770e596acf188d9b57e",
        "title": "Unit Test Case Generation with Transformers"
      },
      {
        "id": "bb6ae805a00f57c7d5316b18c83d9eead9ae0177",
        "title": "Natural Language Processing Applications in Business"
      },
      {
        "id": "14b65a86c82e38fce0eb3506e0d4084ad5cdb583",
        "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
      },
      {
        "id": "fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea",
        "title": "A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications"
      },
      {
        "id": "ae94c9499e53cdbd2676601839afa85dfaf36a5f",
        "title": "Transformer Networks for Trajectory Forecasting"
      },
      {
        "id": "9b529fe170823f95509585d5aa39fa01a43558fd",
        "title": "How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence"
      },
      {
        "id": "52f47e781852a77abedada48cfa971b24c919dde",
        "title": "Calibration of Pre-trained Transformers"
      },
      {
        "id": "11342d45911ee8a7c9e3a94117ce774ad7036172",
        "title": "Neural Unsupervised Domain Adaptation in NLP—A Survey"
      },
      {
        "id": "5290d7921f0266c8b50b79fc8a0b7d22868f4f60",
        "title": "The Cost of Training NLP Models: A Concise Overview"
      },
      {
        "id": "7e0f91e51ee372939c96714c7919dde6dc756849",
        "title": "Improving Image Captioning with Better Use of Caption"
      },
      {
        "id": "1f55b5790e4645e701b5b76be4b47573bed6c448",
        "title": "Paraphrase Generation with Latent Bag of Words"
      },
      {
        "id": "c014f8bc3b521453a93a13bb2c90700fcf462738",
        "title": "Limits to Depth Efficiencies of Self-Attention"
      },
      {
        "id": "39283c3d6262b24bd61c88038353f3ed0145b6e4",
        "title": "Self-Attention with Cross-Lingual Position Representation"
      },
      {
        "id": "719af99501c8b2435cec86547afbca6b224bb2ea",
        "title": "Semi-Supervised Learning on Meta Structure: Multi-Task Tagging and Parsing in Low-Resource Scenarios"
      },
      {
        "id": "4bce0e394c2bfdcbfbe5910a7740a653af3284d6",
        "title": "AQuA: ASP-Based Visual Question Answering"
      },
      {
        "id": "782a50a48ba5d32839631254285d989bfadfd193",
        "title": "Interpretable Entity Representations through Large-Scale Typing"
      },
      {
        "id": "8492975a32595878cff9ae645bf54d133c24b066",
        "title": "Spoken Document Retrieval Leveraging Bert-Based Modeling and Query Reformulation"
      },
      {
        "id": "0c5480ba0e07da4995006bf7959e6a2e7b3e279d",
        "title": "Neural Syntactic Preordering for Controlled Paraphrase Generation"
      }
    ],
    "6": [
      {
        "id": "d47a682723f710395454687319bb55635e653105",
        "title": "Language (Technology) is Power: A Critical Survey of “Bias” in NLP"
      },
      {
        "id": "babeda48b10a4d638252118f2238d05a06f4ec55",
        "title": "StereoSet: Measuring stereotypical bias in pretrained language models"
      },
      {
        "id": "3cd98a010b36832fc2bd8368cd4f34c72cd0ac6f",
        "title": "A Survey of Active Learning for Natural Language Processing"
      },
      {
        "id": "bf46423b4ff56e9d92acaad0717774c6fe7013b2",
        "title": "Differential Privacy in Natural Language Processing The Story So Far"
      },
      {
        "id": "dad36d9b8d51e7ae14150a48a76bde0a8eb50eac",
        "title": "Fairness in Deep Learning: A Survey on Vision and Language Research"
      },
      {
        "id": "c6629429d064b2ed3117e40d5558e21376aef337",
        "title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models"
      },
      {
        "id": "b22a61395dd47f74c3c05c9e801deaec3f01f482",
        "title": "Human-Machine Collaboration Approaches to Build a Dialogue Dataset for Hate Speech Countering"
      },
      {
        "id": "39880b887c19ae80d71643b37d6fc89aba8ec0c4",
        "title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation"
      },
      {
        "id": "a45ff2a8b18abc850b267cf0ec6e391dba9138a5",
        "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness"
      },
      {
        "id": "c8f0db126728b4486715d67324e37527e4e13570",
        "title": "Large scale analysis of gender bias and sexism in song lyrics"
      },
      {
        "id": "9654e273dd15aa448c4c7e11fbaa3afe32603e34",
        "title": "TweetNERD - End to End Entity Linking Benchmark for Tweets"
      },
      {
        "id": "6b251abe70615f4c3a34a67cfa776a914367ad6e",
        "title": "How to keep text private? A systematic review of deep learning methods for privacy-preserving natural language processing"
      },
      {
        "id": "7335fbc509da851b9d141a40e6463b5e82dea01c",
        "title": "A Survey on Bias and Fairness in Natural Language Processing"
      },
      {
        "id": "53ed38af7df82242e0d7b13c19c814762de75bca",
        "title": "Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language"
      },
      {
        "id": "d9424371662717c8981eef3d501d7ce59c66ce77",
        "title": "On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations"
      },
      {
        "id": "0607b299284cb44eaee0aedd95db3c88b00ff944",
        "title": "Gender Bias in Masked Language Models for Multiple Languages"
      },
      {
        "id": "5bfff0955b511ce4ecb906c67cbe323b60b5c6d3",
        "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias"
      },
      {
        "id": "b2563d102456d5140ecb4111e7f08481f720d9a4",
        "title": "Theories of “Gender” in NLP Bias Research"
      },
      {
        "id": "20e500b69f81bba7f55e7e22215bd92528ac7d05",
        "title": "LTP: A New Active Learning Strategy for CRF-Based Named Entity Recognition"
      },
      {
        "id": "c90a14b54ed56c975fda929418342cf373925e4e",
        "title": "Entity-aware Transformers for Entity Search"
      },
      {
        "id": "9784bf6b4a0b16aff1817c760f48b16ff1e97bea",
        "title": "When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning"
      },
      {
        "id": "34c7761a61ab10c538d5d8f816175caaefadbbe8",
        "title": "StereoKG: Data-Driven Knowledge Graph Construction For Cultural Knowledge and Stereotypes"
      },
      {
        "id": "7418816f3b27326f8b46a60b30ad5b3c4c7c59d9",
        "title": "Quantifying Gender Bias in Consumer Culture"
      },
      {
        "id": "130d432ccbc836380a212bea618f84ff094a6a52",
        "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"
      },
      {
        "id": "04ec406caebff60e226695c921f0af1b29162c5f",
        "title": "A Survey on Gender Bias in Natural Language Processing"
      },
      {
        "id": "f0110d0f7e938038bc1305c4c243b2d631d32c39",
        "title": "Federated Learning Meets Natural Language Processing: A Survey"
      },
      {
        "id": "89d17b41448ab85a13931eb72482eb7017063199",
        "title": "Gender Bias and Under-Representation in Natural Language Processing Across Human Languages"
      },
      {
        "id": "fb1810f403288c40291fbe65fdfa4adf55c466e7",
        "title": "Your fairness may vary: Pretrained language model fairness in toxic text classification"
      },
      {
        "id": "67ad491b16bf77e9a54a8b8b1dc23dadc5545467",
        "title": "Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models"
      },
      {
        "id": "beb4f0ef465212c5eae59e85dc838d3ba47dbacc",
        "title": "On Measures of Biases and Harms in NLP"
      },
      {
        "id": "e7b1e34113ccf40eebee5dac5357246eda585152",
        "title": "TEM: High Utility Metric Differential Privacy on Text"
      },
      {
        "id": "89e5ffd5ce92011e234f0b0ea0d6f2e43647b463",
        "title": "Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens"
      },
      {
        "id": "b21e1ffeae34d878e21699d142f37b200688ddbf",
        "title": "Identification of Bias Against People with Disabilities in Sentiment Analysis and Toxicity Detection Models"
      },
      {
        "id": "058e1d5faa5499c35a5e651dead828785bccfd03",
        "title": "Gender Bias in Text: Origin, Taxonomy, and Implications"
      },
      {
        "id": "a55c399bbb0382650459da59fc545f2dd275012b",
        "title": "FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks"
      },
      {
        "id": "0ad8f1d3fd138fa1c254289edefd86f6c87f92d0",
        "title": "The Gender Gap Tracker: Using Natural Language Processing to measure gender bias in media"
      },
      {
        "id": "437727b6c00a5eb4944600091f66f41626d1002d",
        "title": "Unmasking the Mask - Evaluating Social Biases in Masked Language Models"
      },
      {
        "id": "6fefb9bdce7c70f473574edc3ded714680030d2f",
        "title": "On the Importance of Effectively Adapting Pretrained Language Models for Active Learning"
      },
      {
        "id": "494b06f4c12952abe582e91e94513814a68738de",
        "title": "Towards generalisable hate speech detection: a review on obstacles and solutions"
      },
      {
        "id": "c0e6cd2ec3bc9eb46c7d45bb708854da3327339e",
        "title": "A Survey on Bias in Deep NLP"
      },
      {
        "id": "497d29459a894ac38a48ed58753976ccbf2aa433",
        "title": "Intrinsic Bias Metrics Do Not Correlate with Application Bias"
      },
      {
        "id": "3df5343edf4da012e39a5ed79b6ceb723f38b2bf",
        "title": "Kleister: Key Information Extraction Datasets Involving Long Documents with Complex Layouts"
      },
      {
        "id": "7707d54987b4ac7a7b94d8b932e3757a92a6a559",
        "title": "BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems"
      },
      {
        "id": "194d1089b730e4a1d15a60a0144b50771909c3c5",
        "title": "Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates"
      },
      {
        "id": "0d2f279be0e985cf875e57ee7af45b80de81962b",
        "title": "Multimodal Hate Speech Detection in Greek Social Media"
      },
      {
        "id": "26b7eacd6aaff6c2bd1beac40b96597fb1d29a1e",
        "title": "Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources"
      },
      {
        "id": "4bd0e72a6e41e34ae4d8d01f72aee0a3ce2d646a",
        "title": "Going Full-TILT Boogie on Document Understanding with Text-Image-Layout Transformer"
      },
      {
        "id": "ad113aa8aedb6a352720a54bcd3018ef2364b69c",
        "title": "Content Analysis of Textbooks via Natural Language Processing: Findings on Gender, Race, and Ethnicity in Texas U.S. History Textbooks"
      },
      {
        "id": "065c51b1d5306788a5ecf88c949210b1ba1369d0",
        "title": "Empirical Studies of Institutional Federated Learning For Natural Language Processing"
      },
      {
        "id": "59faa64d2681b6257ca5a87bc28d7d535b8c8809",
        "title": "Situated Data, Situated Systems: A Methodology to Engage with Power Relations in Natural Language Processing Research"
      },
      {
        "id": "9e3b63b402e1f983c88f488ebdefe15c23751349",
        "title": "Enhanced Privacy and Data Protection using Natural Language Processing and Artificial Intelligence"
      },
      {
        "id": "19803adec3b97fb2e3c8097f17bf33fabf311795",
        "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach"
      },
      {
        "id": "2cf2d1491f72f198ae9990971cf2846e9fe51141",
        "title": "Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective"
      },
      {
        "id": "8e990e779cd70b70e316b129f334edfb8aa89af1",
        "title": "Differentially Private Language Models Benefit from Public Pre-training"
      },
      {
        "id": "71a85e735a3686bef8cce3725ae5ba82e2cabb1b",
        "title": "Underspecification Presents Challenges for Credibility in Modern Machine Learning"
      },
      {
        "id": "c41655809f991fa0553cbb8e9a66f184a6cdc154",
        "title": "Resources and benchmark corpora for hate speech detection: a systematic review"
      },
      {
        "id": "5f5e9366983b53d4a753627d1144daa8e890e02f",
        "title": "Metamorphic Testing and Certified Mitigation of Fairness Violations in NLP Models"
      },
      {
        "id": "f0f81bda8974900a46d19ac9882cdeaa3dccf458",
        "title": "Visualizing Transformers for NLP: A Brief Survey"
      },
      {
        "id": "acc5a82962b6b80a51d0aa125be110a662d331c4",
        "title": "Detecting and understanding moral biases in news"
      },
      {
        "id": "9a33f92315803d5f280eff026746f1665777a28f",
        "title": "LOGAN: Local Group Bias Detection by Clustering"
      },
      {
        "id": "3432ae8405d6e35a0a987adbb8327bffeb6eef80",
        "title": "Domain Adversarial Fine-Tuning as an Effective Regularizer"
      },
      {
        "id": "3313d6c1425d96f68c6107f640c8c385a0e5aec5",
        "title": "Astraea: Grammar-Based Fairness Testing"
      },
      {
        "id": "ee81584f7e426b891765fe122655fe466b00a072",
        "title": "SecureNLP: A System for Multi-Party Privacy-Preserving Natural Language Processing"
      },
      {
        "id": "baf60d13c98916b77b09bc525ede1cd610ed1db5",
        "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping"
      },
      {
        "id": "b3c73de96640ee858f83c3f0eda2a3d15d59b847",
        "title": "Privacy Risks of General-Purpose Language Models"
      },
      {
        "id": "7ae4b59702e6f82c74299acb5a9e721b135c6bce",
        "title": "Selecting Informative Contexts Improves Language Model Fine-tuning"
      },
      {
        "id": "868c3a07ca240ad66d953bfb5094ad962f1cdca4",
        "title": "Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias"
      },
      {
        "id": "9b1927cff86419aadf12b14bd3e9cff1fe1d3242",
        "title": "REL: An Entity Linker Standing on the Shoulders of Giants"
      },
      {
        "id": "eb1602ecba96beadeb7d2f05e1b57fa6b339fc69",
        "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?"
      },
      {
        "id": "99fe63245121726cbab65847718ec10d5ab9355e",
        "title": "Bias in word embeddings"
      },
      {
        "id": "e3e9d2bdcc3fefab7c294196c8b2e149727376ed",
        "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer"
      },
      {
        "id": "7dc21b6c7c02708e4de6c6c260b4439b46fbd086",
        "title": "Improving BERT Fine-Tuning via Self-Ensemble and Self-Distillation"
      },
      {
        "id": "1d29e42bda72365f4eea9522f769d9aa83281d32",
        "title": "Quantifying Gender Bias in Different Corpora"
      },
      {
        "id": "3a129c8d67ddbf521cc5c8ec85e1225ed06262da",
        "title": "Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning"
      },
      {
        "id": "e2d2f64b3bb200c2c3db5ddc367b06311c369341",
        "title": "Kleister: A novel task for Information Extraction involving Long Documents with Complex Layout"
      },
      {
        "id": "37ad7b9dd64ba6f52e007c562868e1f51226c427",
        "title": "Privacy-Preserving Deep Learning NLP Models for Cancer Registries"
      },
      {
        "id": "0b09edd6be4e55167f055c00def29e6b221d8ee6",
        "title": "Federated pretraining and fine tuning of BERT using clinical notes from multiple silos"
      },
      {
        "id": "307ed22ce979f1a6c5d4afd996d53cd022217f57",
        "title": "deb2viz: Debiasing gender in word embedding data using subspace visualization"
      }
    ],
    "4": [
      {
        "id": "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31",
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
      },
      {
        "id": "832fff14d2ed50eb7969c4c4b976c35776548f56",
        "title": "REALM: Retrieval-Augmented Language Model Pre-Training"
      },
      {
        "id": "80376bdec5f534be78ba82821f540590ebce5559",
        "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"
      },
      {
        "id": "3a07a87090a061ca41dd30ac8398a9a5d9d39826",
        "title": "Dense Text Retrieval Based on Pretrained Language Models: A Survey"
      },
      {
        "id": "529e997e0d9730c25ad4347502da7e5a753274b8",
        "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference"
      },
      {
        "id": "214619b8ea2d05d019291a5fd7674da85ed3a3b7",
        "title": "INTERACTION: A Generative XAI Framework for Natural Language Inference Explanations"
      },
      {
        "id": "ee8de585183763ff64cb3c81ecda2fc75fa81507",
        "title": "Large Language Models with Controllable Working Memory"
      },
      {
        "id": "a26623d52d24e03044a158cddad931ec5ab7304c",
        "title": "A Survey of Knowledge Enhanced Pre-Trained Language Models"
      },
      {
        "id": "9bf75110ea0923bbed49256b5491f1ec284019ec",
        "title": "From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"
      },
      {
        "id": "1833ad08d52a454a50490fed91181fc7e2cc397a",
        "title": "DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning"
      },
      {
        "id": "4790b411835089fc593b154d3c661f5ccfc05ae6",
        "title": "Explainability of Text Processing and Retrieval Methods: A Critical Survey"
      },
      {
        "id": "285d13bf3cbe6a8a0f164f584d84f8b74067271f",
        "title": "Towards Faithful Model Explanation in NLP: A Survey"
      },
      {
        "id": "a814b76e589ef27e3f4af379d319d02d2110faa1",
        "title": "Recent Advances in Text-to-SQL: A Survey of What We Have and What We Expect"
      },
      {
        "id": "f7fd184eaa573205dff97d86c836f3038143e87a",
        "title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks"
      },
      {
        "id": "bceebd434d7502dcd87004ec7313c2eea2c512fc",
        "title": "A Survey for Efficient Open Domain Question Answering"
      },
      {
        "id": "4e042409c4dc5210787fcd149485a59be3eef0f6",
        "title": "Lecture Notes on Neural Information Retrieval"
      },
      {
        "id": "94b34ad657bcfc9f1a8ed1ab1c3144aae9980901",
        "title": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval"
      },
      {
        "id": "7e942e813e3fb814f2d0b997ae849548a9e40638",
        "title": "Towards Generalizable and Robust Text-to-SQL Parsing"
      },
      {
        "id": "dfb22e1bfecafb4e37e6b3c0f4fd547669ced6ea",
        "title": "FeatureCut: An Adaptive Data Augmentation for Automated Audio Captioning"
      },
      {
        "id": "025d4b73c1d0f267ca39fa9649413d88ed3cc5bc",
        "title": "Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning"
      },
      {
        "id": "907e12e46eba463883442b02dd91a302cd55acd1",
        "title": "A Survey of Multi-task Learning in Natural Language Processing: Regarding Task Relatedness and Training Methods"
      },
      {
        "id": "d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3",
        "title": "Fine-tuned Language Models are Continual Learners"
      },
      {
        "id": "97f456643712e9618edd7465676c62af3c8ae690",
        "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models"
      },
      {
        "id": "336672adf959e5eb4a2a189c976a2847a68f85c5",
        "title": "ANNA”:\" Enhanced Language Representation for Question Answering"
      },
      {
        "id": "cc73da69eb495a122ed24bc680bf9e2f1a420e0c",
        "title": "A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications"
      },
      {
        "id": "55e31baa3ae5f32fb5e695761892319e26dbc639",
        "title": "Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data"
      },
      {
        "id": "b20a5cd0068c6ffdc9614d018389fe28f8deab45",
        "title": "Negative Sampling for Contrastive Representation Learning: A Review"
      },
      {
        "id": "1e482e72b0837a7ed4c0a7370a9af3e2ba2eaf29",
        "title": "Modeling Beats and Downbeats with a Time-Frequency Transformer"
      },
      {
        "id": "69183f1d1edd9bea96c74999ea78303f2a1cd5c8",
        "title": "MetaAID: A Flexible Framework for Developing Metaverse Applications via AI Technology and Human Editing"
      },
      {
        "id": "15031e6a94f9d6d19f74740c224a5523ec64d975",
        "title": "Improving Biomedical Information Retrieval with Neural Retrievers"
      },
      {
        "id": "72a0736627b2a8448dc653c0d1e4c80960c929b5",
        "title": "Leveraging Pre-trained BERT for Audio Captioning"
      },
      {
        "id": "011c2b5a08c32e57f7c55539a1944bc733f83aa3",
        "title": "On Continual Model Refinement in Out-of-Distribution Data Streams"
      },
      {
        "id": "daaa79be8d02cd3f748b6a3465fc1f09b5068880",
        "title": "Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models"
      },
      {
        "id": "37e2c7c89325a1a4685a46ff830fe7ecca8f1f80",
        "title": "Learning to Scaffold: Optimizing Model Explanations for Teaching"
      },
      {
        "id": "5773e14b945abdfac831c74292a1c7a116c498e7",
        "title": "A survey on improving NLP models with human explanations"
      },
      {
        "id": "3db46a625b7e58d2e762668791e1ad6c4d614519",
        "title": "Beyond the Status Quo: A Contemporary Survey of Advances and Challenges in Audio Captioning"
      },
      {
        "id": "423372879d2bc1b0884ed8d58c3549988d695d75",
        "title": "Fast Semantic Matching via Flexible Contextualized Interaction"
      },
      {
        "id": "cdaa895850578d495c0837d70b811dc53241f4c8",
        "title": "E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning"
      },
      {
        "id": "588c15ef53d38c7c0266eddd235d64df68d6a973",
        "title": "Separate What You Describe: Language-Queried Audio Source Separation"
      },
      {
        "id": "d71915cf609dd0e785da481fcdab826c38611243",
        "title": "HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing"
      },
      {
        "id": "d5784fd3ac7e06ec030abb8f7787faa9279c1a50",
        "title": "Interpreting Deep Learning Models in Natural Language Processing: A Review"
      },
      {
        "id": "379e9d5bba8a617b3114bd5b562b14aa6abc5282",
        "title": "Natural SQL: Making SQL Easier to Infer from Natural Language Specifications"
      },
      {
        "id": "58ad7dd2bba99329bc41363f9741aa01c18e2546",
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning"
      },
      {
        "id": "290867638c5ca520de5c48aa4336f196d426c226",
        "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey"
      },
      {
        "id": "e55391a9406245584b3e5b3225dad2e171b9a06b",
        "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"
      },
      {
        "id": "48a3184b25a90d6864326aec7950af6aee60ef49",
        "title": "A Survey of Knowledge Enhanced Pre-trained Language Models"
      },
      {
        "id": "1cef73714d8ad89e442a9635fcd3061c61067638",
        "title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking"
      },
      {
        "id": "c397d0e17ced17e72aa3fc0df645eeabcabc32de",
        "title": "Efficient Training of Audio Transformers with Patchout"
      },
      {
        "id": "8f4bc7e92526faeb65fabd60e5d8c86392fce414",
        "title": "MT3: Multi-Task Multitrack Music Transcription"
      },
      {
        "id": "4097ae9aca6444fd7536bfbed1e62560521b70d3",
        "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval"
      },
      {
        "id": "032f7bc2878a3f02922003b9359b70c2c6532c0e",
        "title": "SpecTNT: a Time-Frequency Transformer for Music Audio"
      },
      {
        "id": "8c24ef76d0a15bae316d1b9e6ab526ea5af93530",
        "title": "Graph Neural Networks: Methods, Applications, and Opportunities"
      },
      {
        "id": "f5a3dbc0518df5ca1b6333ae93244dde7f793736",
        "title": "Block-Skim: Efficient Question Answering for Transformer"
      },
      {
        "id": "9c58df84d6eddd4830a430dc5e164429cf89a79a",
        "title": "Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Semantic Information"
      },
      {
        "id": "cd84f79105746e8eadb63b9beef21ef3f9c02766",
        "title": "Diverse Audio Captioning Via Adversarial Training"
      },
      {
        "id": "311e48e1c4a0dcd65a6699376ffc85a24a333a56",
        "title": "Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience"
      },
      {
        "id": "84a326563c8679e6173c13adcf21bbd0bdc2aa04",
        "title": "Adversarial Domain Adaptation for Cross-lingual Information Retrieval with Multilingual BERT"
      },
      {
        "id": "00ac7891f1024244aa902e14f681a5a7fcd40228",
        "title": "Can Deep Neural Networks Predict Data Correlations from Column Names?"
      },
      {
        "id": "97fcbad1088e219621b72ef928b2e3824c46bbd7",
        "title": "Local Interpretations for Explainable Natural Language Processing: A Survey"
      },
      {
        "id": "7ef33fd5b0ef0c4de42cf0afdc9f7dfb0f430b20",
        "title": "From Natural Language Processing to Neural Databases"
      },
      {
        "id": "962aa5b847f1692af058bd14fc0e8c3f0a0fee73",
        "title": "Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing"
      },
      {
        "id": "ebdb02f9d0f31d3a13070f61fa07fe2fa11cb531",
        "title": "Model Explainability in Deep Learning Based Natural Language Processing"
      },
      {
        "id": "de2d64e0c66c2f1d4e68fc2528fe07eebb4ada57",
        "title": "Summary of Research Methods on Pre-Training Models of Natural Language Processing"
      },
      {
        "id": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
        "title": "Measuring and Improving Consistency in Pretrained Language Models"
      },
      {
        "id": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
        "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"
      },
      {
        "id": "e259ee075998eedc0b0c91c17769bf9dffeba46f",
        "title": "Graph Self-Supervised Learning: A Survey"
      },
      {
        "id": "31852f9fc732c0868af12d631c72693702d80521",
        "title": "Text Data Augmentation for Deep Learning"
      },
      {
        "id": "db9296eaa252231e24d066e8413bf29fb058ee45",
        "title": "Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering"
      },
      {
        "id": "2d00798b8a7d979c925901e9faa5fe4360030ca2",
        "title": "Self-Supervised Learning on Graphs: Contrastive, Generative, or Predictive"
      },
      {
        "id": "d88c1255876b62fb5f5a8b292098ca430710a540",
        "title": "The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures"
      },
      {
        "id": "cefd3993db4d065b95ab8f105452fb728c02b60e",
        "title": "Can We Automate Scientific Reviewing?"
      },
      {
        "id": "5cdd2cc5ef826a10785d3c6aeb56f1ea5fc0075a",
        "title": "B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval"
      },
      {
        "id": "c426ce2819990b2de233f93c11940cfb0161b836",
        "title": "Audio Transformers: Transformer Architectures For Large Scale Audio Understanding. Adieu Convolutions"
      },
      {
        "id": "d8e8e35bf4cf8821ade2d58b34d9ae23a9b08ab2",
        "title": "LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching"
      },
      {
        "id": "becc21ab34a7858e9bec469c9329ddacf39472fa",
        "title": "Dynamic Hybrid Relation Exploration Network for Cross-Domain Context-Dependent Semantic Parsing"
      },
      {
        "id": "16529f7194bf7faee8a4e43fd54aefeb8730f236",
        "title": "Database reasoning over text"
      },
      {
        "id": "8fc27b2f4c118c50e208a563835fd5e52a522980",
        "title": "Building Interpretable Interaction Trees for Deep NLP Models"
      },
      {
        "id": "b3c914c8fc3eb3620ed1406289ae4f1ca2c617b0",
        "title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification"
      },
      {
        "id": "d64192da0b6d43c91c902105c089b18b76f9fe77",
        "title": "Dynamic Hybrid Relation Network for Cross-Domain Context-Dependent Semantic Parsing"
      },
      {
        "id": "3d0853eb0429fff9a6ff04fd46ea221e4f84fbf2",
        "title": "The Case for NLP-Enhanced Database Tuning: Towards Tuning Tools that \"Read the Manual\""
      },
      {
        "id": "155ba0a6a126e997661c65cef8a7d504657d28be",
        "title": "Knowledge-Grounded Self-Rationalization via Extractive and Natural Language Explanations"
      },
      {
        "id": "c114db5f1c38cbe6797bc74ef98072cac71f6cc6",
        "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"
      },
      {
        "id": "373bc164d7b552f8782988e7da6b0d00092a20b0",
        "title": "Continual Lifelong Learning in Natural Language Processing: A Survey"
      },
      {
        "id": "a381826827df23f11c0dc600e1d7445fe4fa7e7c",
        "title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables"
      },
      {
        "id": "c0c11be3de5d102b9cd6de1ff7a413a8ea007b92",
        "title": "ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning"
      },
      {
        "id": "b1e6aa78db5478be5eaa47697382241c2b7aab1f",
        "title": "Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models"
      },
      {
        "id": "54b804fd5511a431bbeb49accd21152296dcad67",
        "title": "E-BERT: A Phrase and Product Knowledge Enhanced Language Model for E-commerce"
      },
      {
        "id": "4f0a8cad6d6a8d0397ad1bd35acce6458aa7164c",
        "title": "Contrastive Representation Learning: A Framework and Review"
      },
      {
        "id": "2b4bc49a3b23229a060609380752666b24b435fb",
        "title": "Distilling Knowledge from Reader to Retriever for Question Answering"
      },
      {
        "id": "c845494445f3bfa01d8245a4759b144e27aa3788",
        "title": "A Survey of Knowledge-enhanced Text Generation"
      },
      {
        "id": "343e06bae852f74a98573e798b501f6003bcb1c0",
        "title": "Measuring Association Between Labels and Free-Text Rationales"
      },
      {
        "id": "fae68ee703880a95afa9c2029822b904dd55f9ba",
        "title": "SHAP values for Explaining CNN-based Text Classification Models"
      },
      {
        "id": "28709eca90cac4102518fce9d9b4982ea949d20a",
        "title": "Survey of Neural Text Representation Models"
      },
      {
        "id": "c2f5fda4d843a382eb19ef87f99e1ecec8489b86",
        "title": "Cold-start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks"
      },
      {
        "id": "a20712b1b9779ee43ce143a19b3f67f0cacbbf57",
        "title": "Neural Databases"
      },
      {
        "id": "b360427d0991143013da6a208ccf28bcc8028fab",
        "title": "Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training"
      },
      {
        "id": "210cf704dddaa922e4eafe634dbabf707d6683bc",
        "title": "LIMIT-BERT : Linguistics Informed Multi-Task BERT"
      },
      {
        "id": "b589bce24ae76b72de831b2ce15dc8668b8d30b8",
        "title": "Convolution-deconvolution word embedding: An end-to-end multi-prototype fusion embedding method for natural language processing"
      },
      {
        "id": "706f756b71f0bf51fc78d98f52c358b1a3aeef8e",
        "title": "Self-Supervised Learning: Generative or Contrastive"
      },
      {
        "id": "91fb815361fdbf80ff15ce4d783a41846bd99232",
        "title": "GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training"
      },
      {
        "id": "b2a839e3ee68e81b863b73ee08c6626c94477fef",
        "title": "WT5?! Training Text-to-Text Models to Explain their Predictions"
      },
      {
        "id": "e092ecf56fcca38d0cd6fe9e1e6b11c380f6c286",
        "title": "A Survey on Contextual Embeddings"
      },
      {
        "id": "3fa8d2a9e9a9cf3ee9626424a157888580dcfaba",
        "title": "A Survey of Deep Learning Techniques for Neural Machine Translation"
      },
      {
        "id": "1657220981714a6c312b364dbb51d604521f894e",
        "title": "Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection"
      },
      {
        "id": "429becc14890854bf1394a870b074ca8ea6734bd",
        "title": "Transformer-based End-to-End Question Generation"
      },
      {
        "id": "d9ec170160c69df9863f390d06bfefc213d0836f",
        "title": "Document Summarization with VHTM: Variational Hierarchical Topic-Aware Mechanism"
      },
      {
        "id": "60b8ad6177230ad5402af409a6edb5af441baeb4",
        "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT"
      }
    ],
    "2": [
      {
        "id": "2ffcf8352223c95ae8cef4daaec995525ecc926b",
        "title": "Adversarial Training for Large Neural Language Models"
      },
      {
        "id": "1c299218d103234f45113f7f927cc0cc6aaefb9f",
        "title": "An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding"
      },
      {
        "id": "35f4bc71e41ff70bbc752955c1b00e0501bc2cf7",
        "title": "Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings"
      },
      {
        "id": "3bc5a1fd0a746d544be88879f64da60c21251083",
        "title": "A survey on clinical natural language processing in the United Kingdom from 2007 to 2022"
      },
      {
        "id": "6ab6e6f62323132e299fc6717ad0f5ca000414d5",
        "title": "Natural language processing in clinical neuroscience and psychiatry: A review"
      },
      {
        "id": "366651973ed638551708be264b44927b5567ab4d",
        "title": "Improving Methods of Identifying Anaphylaxis for Medical Product Safety Surveillance Using Natural Language Processing and Machine Learning"
      },
      {
        "id": "c9a7a2e1d852016d22d9330167ed3764cb95a6ef",
        "title": "Natural Language Processing Challenges and Issues: A Literature Review"
      },
      {
        "id": "6fa1afda228f2cab27b5b9afd09253cbe7634e44",
        "title": "Hospital-wide natural language processing summarising the health data of 1 million patients"
      },
      {
        "id": "8af69a903a14657ec96b93c4b6e139771beec106",
        "title": "On the Explainability of Natural Language Processing Deep Models"
      },
      {
        "id": "a28f9187b8c213182bdcbfaeaa4d4f04ae6366dd",
        "title": "Review of Natural Language Processing in Pharmacology"
      },
      {
        "id": "5a51efbbc17e14dbdca5669e1ef14cc9335afb26",
        "title": "Analysis of the Development Trend of Sports Research in China and Taiwan Using Natural Language Processing"
      },
      {
        "id": "9ab6727fade0fa3f7a121801069eae9cc1ab6f50",
        "title": "DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing"
      },
      {
        "id": "7d0abebf379383afbf9b7b3f8ab89561d1aa7596",
        "title": "MaterialBERT for natural language processing of materials science texts"
      },
      {
        "id": "40fe011402ff452b0b134d8c5179ef52db986f33",
        "title": "Clinical Application of Detecting COVID-19 Risks: A Natural Language Processing Approach"
      },
      {
        "id": "2cce05dcae43fe3fb5a251f500a455cafe0be0e8",
        "title": "Natural Language Interfaces to Data"
      },
      {
        "id": "b0b9cbddb4f564f657b0473a9fbc8995bb0d4564",
        "title": "A comparative study of pre-trained language models for named entity recognition in clinical trial eligibility criteria from multiple corpora"
      },
      {
        "id": "38e995753f222ba7d198d6d54033058de84a77f1",
        "title": "MedJEx: A Medical Jargon Extraction Model with Wiki’s Hyperlink Span and Contextualized Masked Language Model Score"
      },
      {
        "id": "fa51ee96b3e2597e656b69352ea6fb5bcebd19ad",
        "title": "A Novel Approach for Emotion Detection and Sentiment Analysis for Low Resource Urdu Language Based on CNN-LSTM"
      },
      {
        "id": "fe214e738e6dcde26ff375ac7007979808ab554d",
        "title": "Don’t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling"
      },
      {
        "id": "3a021e1acc7588897df3f58e3ad928122846122f",
        "title": "Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning"
      },
      {
        "id": "44279244407a64431810f982be6d0c7da4429dd7",
        "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"
      },
      {
        "id": "22374e8aa39c3f39b7f578b3fd04d969df93854d",
        "title": "AttentionSiteDTI: an interpretable graph-based model for drug-target interaction prediction using NLP sentence-level relation classification"
      },
      {
        "id": "29005c5f2a37d8f8c3b4db62e04a2dffd7d9187c",
        "title": "Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches"
      },
      {
        "id": "15ecdf371d691c6ba0378f3c43bc8ead70c2d501",
        "title": "CSL: A Large-scale Chinese Scientific Literature Dataset"
      },
      {
        "id": "115d7d411f221295b483534eb7faef651f18451c",
        "title": "Summarizing Patients’ Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models"
      },
      {
        "id": "4374a11aad9d51138990735aea0764a710a29f92",
        "title": "The Leaf Clinical Trials Corpus: a new resource for query generation from clinical trial eligibility criteria"
      },
      {
        "id": "cbeb03802b00024bf3d43b63814241e5261abe93",
        "title": "Relaxed Attention for Transformer Models"
      },
      {
        "id": "310c2f0ac40821283a8ccee7ef44792982a6dc72",
        "title": "A survey of contrastive learning in NLP"
      },
      {
        "id": "f48543f11a1235a5ca0383ce1d191e5acb9f62a3",
        "title": "Explainable Slot Type Attentions to Improve Joint Intent Detection and Slot Filling"
      },
      {
        "id": "c205cea82ec5db88be8f466d5b3823e37cb8f341",
        "title": "Feature-Level Debiased Natural Language Understanding"
      },
      {
        "id": "685904e57ae81e95bd8b54a126b1278a44085ade",
        "title": "Investigating Fairness Disparities in Peer Review: A Language Model Enhanced Approach"
      },
      {
        "id": "06862ac31ee1d331a08716d6109c4bf6e5bc6bac",
        "title": "From Black Boxes to Conversations: Incorporating XAI in a Conversational Agent"
      },
      {
        "id": "96a47bd554cc17d661251c393e3335ef7d19fd70",
        "title": "Detection and moderation of detrimental content on social media platforms: current status and future directions"
      },
      {
        "id": "c4cc399f654e51da8c46e6cca651deb2ba2bd0a0",
        "title": "Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data"
      },
      {
        "id": "3b2ee6dc0895acc12314cb95d3cfbee11c714fa8",
        "title": "A survey on extremism analysis using natural language processing: definitions, literature review, trends and challenges"
      },
      {
        "id": "6a3a75561c627c118778e4d056c080cd70056d21",
        "title": "Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding"
      },
      {
        "id": "f5b0d7722f41d3507887eb28690a6c7cddd719c0",
        "title": "Psychosis Relapse Prediction Leveraging Electronic Health Records Data and Natural Language Processing Enrichment Methods"
      },
      {
        "id": "a1aeb5442e31276b197696f49b3243112a4049ce",
        "title": "Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing"
      },
      {
        "id": "1cfe67bad95a16bc249941b829d113d830031cf5",
        "title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets"
      },
      {
        "id": "332dc8b2ca9d49fad607c7282f3360bb2a9aacf3",
        "title": "A large language model for electronic health records"
      },
      {
        "id": "0db5207510819b9956849eb84bfe8703f8f3688d",
        "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model"
      },
      {
        "id": "0ca80cd04d2f485cc29f690befde3f7f532d6ea3",
        "title": "Label Anchored Contrastive Learning for Language Understanding"
      },
      {
        "id": "a09cf9f20efd8b09f46418a6acfbbcc2835cd936",
        "title": "OneRel: Joint Entity and Relation Extraction with One Module in One Step"
      },
      {
        "id": "22a77ab4b79b43c69ce25a272de480b2a025c6a4",
        "title": "BERN2: an advanced neural biomedical named entity recognition and normalization tool"
      },
      {
        "id": "b52958224f11c5a3940ef44228f0008ed3df66b0",
        "title": "Deep Sentiment Analysis Using CNN-LSTM Architecture of English and Roman Urdu Text Shared in Social Media"
      },
      {
        "id": "f30e95be411456a709e7cb9a8b3a3e557bd0356a",
        "title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences"
      },
      {
        "id": "9d11c43c7870920d3a2aa45ddf84e543dbd4fb98",
        "title": "Automated clinical coding: what, why, and where we are?"
      },
      {
        "id": "ff5c0e3e23a79fbcca95aa6dba1ec7ba71baf204",
        "title": "Unsupervised Sentence Representation via Contrastive Learning with Mixing Negatives"
      },
      {
        "id": "11500f6a5a5f00969b7600a8047e0c08ece4d9d2",
        "title": "PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings"
      },
      {
        "id": "fb5273032ab997dddd1b2917ed41808fd66fb909",
        "title": "MCSE: Multimodal Contrastive Learning of Sentence Embeddings"
      },
      {
        "id": "8cb68e36cf9121f0ec122bf6ded5f1bebf8ad90d",
        "title": "Transformer-Based Graph Convolutional Network for Sentiment Analysis"
      },
      {
        "id": "eadba0531ba299d2f66fcff0fa07ab37e4ce5d7d",
        "title": "Just Rank: Rethinking Evaluation with Word and Sentence Similarities"
      },
      {
        "id": "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
        "title": "Leveraging Locality in Abstractive Text Summarization"
      },
      {
        "id": "4732499ab45dbcb092631050d596bbf4533ee869",
        "title": "BIOS: An Algorithmically Generated Biomedical Knowledge Graph"
      },
      {
        "id": "82bef8b5fc633475bc5278c1dd98f180f1a5d471",
        "title": "Ontology-driven and weakly supervised rare disease identification from clinical notes"
      },
      {
        "id": "dc1402d05c6a18843d1fc6b31f1bd4fcdaa8ad30",
        "title": "Benchmarking for biomedical natural language processing tasks with a domain specific ALBERT"
      },
      {
        "id": "a9c5e23c5559bfc4d95dd166c1ed29fa026bbf2e",
        "title": "Fine-tuning large neural language models for biomedical natural language processing"
      },
      {
        "id": "891137d590020214ff703d443cb960a50254ab94",
        "title": "Prediction of 30-Day Readmission After Stroke Using Machine Learning and Natural Language Processing"
      },
      {
        "id": "1deab15a5a30c1da962804527c1139f9d38c5f5b",
        "title": "A Scoping Review of Publicly Available Language Tasks in Clinical Natural Language Processing"
      },
      {
        "id": "5572fd371a3ac287fb0d68f90f4a3f8574c9547b",
        "title": "Listening to Mental Health Crisis Needs at Scale: Using Natural Language Processing to Understand and Evaluate a Mental Health Crisis Text Messaging Service"
      },
      {
        "id": "3926f776c27c1ca63b9433b162993fa7089697f0",
        "title": "Explainability for Natural Language Processing"
      },
      {
        "id": "d639c05ac81060554b26c1ebbbff73641a077da3",
        "title": "Machine learning‐enabled multitrust audit of stroke comorbidities using natural language processing"
      },
      {
        "id": "8b954e1654c6b759a957fd11e66c111e6105fb3f",
        "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"
      },
      {
        "id": "b146be9e80c66a6e062a1525693311fac65ae19e",
        "title": "MatSciBERT: A materials domain language model for text mining and information extraction"
      },
      {
        "id": "b15469d0ab3dc3a9dec037d761817b3fe546bed6",
        "title": "Pre-trained Language Models in Biomedical Domain: A Systematic Survey"
      },
      {
        "id": "ae9e743c6ca4ac7e33c72cf140c69aa434eb3b19",
        "title": "RoBERTuito: a pre-trained language model for social media text in Spanish"
      },
      {
        "id": "93537c66ffb6977649444a5325ae56965a2266cf",
        "title": "Early prediction of diagnostic-related groups and estimation of hospital cost by processing clinical notes"
      },
      {
        "id": "a01d7465a1361f44787c36f8f4c80ef4f1e44853",
        "title": "Deep learning with language models improves named entity recognition for PharmaCoNER"
      },
      {
        "id": "6a9d593274e0ae2cdaab85dfd5a90aa33db649f5",
        "title": "A Hybrid CNN-LSTM: A Deep Learning Approach for Consumer Sentiment Analysis Using Qualitative User-Generated Contents"
      },
      {
        "id": "ef2d0ba37da024817d59160a432e7ad586ad5f6a",
        "title": "Attention-Based CNN and Bi-LSTM Model Based on TF-IDF and GloVe Word Embedding for Sentiment Analysis"
      },
      {
        "id": "ea48a830ed7ace758677944f7957348d332b78c8",
        "title": "Chest ImaGenome Dataset for Clinical Reasoning"
      },
      {
        "id": "2607dce6dcb9043ca9cae67e25e6a24411f08c0b",
        "title": "BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation"
      },
      {
        "id": "1982899ca875375227be5c131249cf3107bb9560",
        "title": "Virtual Augmentation Supported Contrastive Learning of Sentence Representations"
      },
      {
        "id": "f64e0f5a1af8f49a8d67b7c9d0bce8f95a2b176a",
        "title": "Sentimental Analysis of Movie Reviews using Soft Voting Ensemble-based Machine Learning"
      },
      {
        "id": "ba53ea9e88486b8a0eda94e1cd84f6b0c33dffe9",
        "title": "SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations"
      },
      {
        "id": "25b3aacaa9b98f0e0d41588cf0e36efe4fb7dc43",
        "title": "BERTax: taxonomic classification of DNA sequences with Deep Neural Networks"
      },
      {
        "id": "2a9a8102602d23954e0e8dd457209e5ef9e4b052",
        "title": "Deployment of a Free-Text Analytics Platform at a UK National Health Service Research Hospital: CogStack at University College London Hospitals"
      },
      {
        "id": "023719b69c1722e35ad4d06e2efe130f630334f0",
        "title": "Simple Contrastive Representation Adversarial Learning for NLP Tasks"
      },
      {
        "id": "e2d328f98a6249a5e86c06c1934360e3c055141d",
        "title": "Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation"
      },
      {
        "id": "b42d20ec9580ebd76860890a1d7a7fdcc742677e",
        "title": "Modeling Protein Using Large-scale Pretrain Language Model"
      },
      {
        "id": "42ba421092c4272a747b5321a27f988fd7b0d7f3",
        "title": "Adversarial Reinforced Instruction Attacker for Robust Vision-Language Navigation"
      },
      {
        "id": "059524838ac1db807ecf3ec4145e69d9b9976ee4",
        "title": "A natural language processing approach for identifying temporal disease onset information from mental healthcare text"
      },
      {
        "id": "76f3d302537f3a46167472d9ffaaac0c7fe9fbee",
        "title": "Looking through glass: Knowledge discovery from materials science literature using natural language processing"
      },
      {
        "id": "10175b9bea1f16dce439200da93f6ab3c76e1a86",
        "title": "A survey on extremism analysis using Natural Language Processing"
      },
      {
        "id": "01730636fe12bd3c15597e9439aba9b0b27ac150",
        "title": "A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned, and Perspectives"
      },
      {
        "id": "58fe64beb45b18f63cbc001849a0dee3e4e60482",
        "title": "Improving Biomedical Pretrained Language Models with Knowledge"
      },
      {
        "id": "1991ea2ec85113cadf38faea840f4b5cf73ae0c7",
        "title": "ELECTRAMed: a new pre-trained language representation model for biomedical NLP"
      },
      {
        "id": "5a7981e317557acde0c96e7346068cad10782f5c",
        "title": "Sentiment Analysis Using Language Models: A Study"
      },
      {
        "id": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
      },
      {
        "id": "077c713bccd9d2c7fde68d4cbde06ab0f07a6855",
        "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"
      },
      {
        "id": "a2fd50aa4dff5e04ed8535d84550da8bff316208",
        "title": "Whitening Sentence Representations for Better Semantics and Faster Retrieval"
      },
      {
        "id": "c8559021289f08eaf8cf2294e406bc1c6b506d19",
        "title": "Recent advances in deep learning based dialogue systems: a systematic survey"
      },
      {
        "id": "0b24f27d920bd1d23c8f6a1ab2b603c5c14f0a36",
        "title": "Deep Learning applications for COVID-19"
      },
      {
        "id": "474c63c4238e830ae395cad69bb4533e69731ff3",
        "title": "Limitations of Transformers on Clinical Text Classification"
      },
      {
        "id": "0021b3beb2ee0906b425eef7c0f453623c1c6a03",
        "title": "Certified Robustness to Word Substitution Attack with Differential Privacy"
      },
      {
        "id": "1dd525b5af40e613ae1665cf15a193b5ef23431b",
        "title": "Improving BERT Model Using Contrastive Learning for Biomedical Relation Extraction"
      },
      {
        "id": "ab151c1ca0479b677003ef200018b93e983aa0ec",
        "title": "Learning to Remove: Towards Isotropic Pre-trained BERT Embedding"
      },
      {
        "id": "cd287288c9fbaa97cbd3f4a070e9552592626fa5",
        "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision"
      },
      {
        "id": "dbcff24e72e8360f2026018a5cde646f369767cb",
        "title": "Not All Attention Is All You Need"
      },
      {
        "id": "65cc7839ee6e595382bb45b15968c565746d1d94",
        "title": "Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System"
      },
      {
        "id": "fde0b404220505f1df0f0bde84ee24fe6e12a354",
        "title": "Improving Text-to-SQL with Schema Dependency Learning"
      },
      {
        "id": "b668738dde6c770a19f6f7441dd1748d82032d48",
        "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports"
      },
      {
        "id": "a2f38d03fd363e920494ad65a5f0ad8bd18cd60b",
        "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"
      },
      {
        "id": "829e36b23f7b42e109f84b5b761052498b291962",
        "title": "A Survey of the State of Explainable AI for Natural Language Processing"
      },
      {
        "id": "39ab363020278d614a8d3c7e4fb46a795b40ef9b",
        "title": "Using Natural Language Processing on Electronic Health Records to Enhance Detection and Prediction of Psychosis Risk."
      },
      {
        "id": "e8f4b7de50f3f3ea50eb5ab362923ab2dc1d5c1e",
        "title": "Multi-domain Clinical Natural Language Processing with MedCAT: the Medical Concept Annotation Toolkit"
      },
      {
        "id": "8487a7dd9321a6cb2aae16451ace766d89a125ee",
        "title": "Advanced natural language processing technique to predict patient disposition based on emergency triage notes"
      },
      {
        "id": "b81e0154125e5865ce026e73c59c36564ed50f5d",
        "title": "Applied natural language processing in mental health big data"
      },
      {
        "id": "176fac7bcfa9c764b586295e7a8b8b5c8ba4c526",
        "title": "Family History Extraction From Synthetic Clinical Narratives Using Natural Language Processing: Overview and Evaluation of a Challenge Data Set and Solutions for the 2019 National NLP Clinical Challenges (n2c2)/Open Health Natural Language Processing (OHNLP) Competition"
      },
      {
        "id": "44bedf3f438d081202b12577de63841ab0111e5d",
        "title": "Prediction of severe chest injury using natural language processing from the electronic health record."
      },
      {
        "id": "926329e8c699c7310a42c4090a5190870a05c8a5",
        "title": "Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference"
      },
      {
        "id": "470735385073e6b378717a886dcc8f1014e69e8a",
        "title": "A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-Art Word Representation Language Models"
      },
      {
        "id": "03935e520c612ac9f137d9e9ef388e0c08568b60",
        "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus"
      },
      {
        "id": "f2861a7c7155c50e2efab3bdec1a491a1b786b03",
        "title": "BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition"
      },
      {
        "id": "fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7",
        "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"
      },
      {
        "id": "02f3c052a9cf675a6f033eac56c9dacb0a10ea28",
        "title": "A Survey on Contrastive Self-supervised Learning"
      },
      {
        "id": "23849edbce90489b29264367c855b7eb3e2275b5",
        "title": "An Empirical Investigation towards Efficient Multi-Domain Language Model Pre-training"
      },
      {
        "id": "8edf070ee55db69f06b43fb46b055182837598f7",
        "title": "On the Sentence Embeddings from BERT for Semantic Textual Similarity"
      },
      {
        "id": "e1e438381418751f16b6813b00bd62a8949f8374",
        "title": "Joint Entity and Relation Extraction With Set Prediction Networks"
      },
      {
        "id": "734f85727161f27bc7b295f0140a905363202d3f",
        "title": "Learning from others' mistakes: Avoiding dataset biases without modeling them"
      },
      {
        "id": "0fc2b08409def9f7ef6beff8ff2155c0dcff2345",
        "title": "A Relation-Specific Attention Network for Joint Entity and Relation Extraction"
      },
      {
        "id": "85b063ab7990c2a54ab2f009988cd1af4b84df6d",
        "title": "Adversarial Machine Learning in Wireless Communications Using RF Data: A Review"
      },
      {
        "id": "14bfe1af57cad759b0b2d771e941d8e8602060cc",
        "title": "Detecting White Supremacist Hate Speech Using Domain Specific Word Embedding With Deep Learning and BERT"
      },
      {
        "id": "06af86469540794522169e80361aff89d6d9535e",
        "title": "Summarizing Medical Conversations via Identifying Important Utterances"
      },
      {
        "id": "f1e1ea1138d8d18f965b0e0cb5bacb150d4f115d",
        "title": "Attention as Relation: Learning Supervised Multi-head Self-Attention for Relation Extraction"
      },
      {
        "id": "c7af54975379ef2e50d530219843b2cdc754a1a0",
        "title": "Emotion Detection in Roman Urdu Text using Machine Learning"
      },
      {
        "id": "d38c6e29c800b11fc60c0db6776f949a8ebd4aaf",
        "title": "Towards Improved Deep Contextual Embedding for the identification of Irony and Sarcasm"
      },
      {
        "id": "deb5dcb8d09213837b2b87008585783e73302c42",
        "title": "Sequence Generation with Mixed Representations"
      },
      {
        "id": "5a11bd4e678fcb05cb8f5d30c45877fb58bdd3b3",
        "title": "A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation"
      },
      {
        "id": "9e90c17ef40404b79ad0f12d9b9c94656f12dfcd",
        "title": "Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation"
      },
      {
        "id": "126fb7df6bcab2b70000dfe5b940ada63ae1ba6a",
        "title": "COVID-Twitter-BERT: A natural language processing model to analyse COVID-19 content on Twitter"
      },
      {
        "id": "db528269ef800727245c0fcb35b692d29c1ccdc9",
        "title": "Natural language processing (NLP) in management research: A literature review"
      },
      {
        "id": "f8fe7acad3d38b05b3de94421b13a83e8ac0b93a",
        "title": "Generation and evaluation of artificial mental health records for Natural Language Processing"
      },
      {
        "id": "13f7f75be4d81af3cd4d886b5beab6660eeafba5",
        "title": "Prediction of general medical admission length of stay with natural language processing and deep learning: a pilot study"
      },
      {
        "id": "cecfdcb28735860b791060b865bc762849911da0",
        "title": "Risk of mortality and cardiopulmonary arrest in critical patients presenting to the emergency department using machine learning and natural language processing"
      },
      {
        "id": "5b015296730273921889e54a0a31e3b173017026",
        "title": "TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue"
      },
      {
        "id": "ad98f7d494fc30380fc4a857a3efa2f8ec03e704",
        "title": "State of the Art and Open Challenges in Natural Language Interfaces to Data"
      },
      {
        "id": "7fed15cc79332f83b7bfe920c02a9c954322ddcc",
        "title": "Improving Neural Language Generation with Spectrum Control"
      },
      {
        "id": "a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031",
        "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers"
      },
      {
        "id": "5d4de0fa45aeddc31142e6a24666d06ed7923f1e",
        "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"
      },
      {
        "id": "32d281a1e7a0a2d4e2b3f34e0f71780c987e1374",
        "title": "DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations"
      },
      {
        "id": "d86c90698bef82f7a80ef2163f89a5b2a525b3d5",
        "title": "Sentiment Analysis Based on Deep Learning: A Comparative Study"
      },
      {
        "id": "1df0af901ff0faf12a2a890fcce66a71f6bea605",
        "title": "Broad-coverage biomedical relation extraction with SemRep"
      },
      {
        "id": "a58c97f8421ad97da4a08c8d45b8e355ab7de2ad",
        "title": "Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble"
      },
      {
        "id": "616e0f73229c118e1e50a1d9e868a198915cecd8",
        "title": "Testing Contextualized Word Embeddings to Improve NER in Spanish Clinical Case Narratives"
      },
      {
        "id": "1296715c88e599f61d94ffdc646d552a3daddc38",
        "title": "Comparison of rule-based and neural network models for negation detection in radiology reports"
      },
      {
        "id": "40c1847ae9a1d4305bb6dc72e51d083a9fdf3ff9",
        "title": "Multi-branch Attentive Transformer"
      }
    ],
    "8": [
      {
        "id": "97f08c1ae8ca5ddf5948c66bfbbc0546ac154807",
        "title": "Pretrained Transformers Improve Out-of-Distribution Robustness"
      },
      {
        "id": "6079285d8a68a2e80107188126dc496abd0c1900",
        "title": "ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications"
      },
      {
        "id": "484b4e96428a7d3ab46330a15b14278ca7bd68ca",
        "title": "GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation"
      },
      {
        "id": "41d75ee68c26f1ed0ac42e2567e570aae96f8a4a",
        "title": "SOLD: Sinhala Offensive Language Dataset"
      },
      {
        "id": "85d08a213e9533c515601451cd78f971e547b1ae",
        "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization"
      },
      {
        "id": "bd307b92cd72e85c3dbecc9a999f820815adfc63",
        "title": "Enhancing Detection of Arabic Social Spam Using Data Augmentation and Machine Learning"
      },
      {
        "id": "d1d8660ef2e5e608e25d0a484d70e3497fc5c051",
        "title": "Exploring Representation-level Augmentation for Code Search"
      },
      {
        "id": "4178159a23f9e07ee05a78aa7ddd921cf094c65a",
        "title": "GradMask: Gradient-Guided Token Masking for Textual Adversarial Example Detection"
      },
      {
        "id": "eb39dda2df56270599f2a28bc6433c84c1704949",
        "title": "Extracted BERT Model Leaks More Information than You Think!"
      },
      {
        "id": "8267e6c9bf8d10013eae45916fc2861a9a09fffa",
        "title": "To Softmax, or not to Softmax: that is the question when applying Active Learning for Transformer Models"
      },
      {
        "id": "3159bd7af555c9612de2e59ea82d100d4140f667",
        "title": "Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions"
      },
      {
        "id": "d3e4553f0a1fd465ae358701f1bdc2e8265308d6",
        "title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing"
      },
      {
        "id": "c860735c03b94aa5ab0aa0e62a232ba1bca41f2e",
        "title": "Resources for Turkish natural language processing: A critical survey"
      },
      {
        "id": "53fb6cd45fc455ec460cab526ab47c0085092115",
        "title": "Wearable Sensor-Based Human Activity Recognition with Transformer Model"
      },
      {
        "id": "c9944f7da8aa77e64455ebea7ec488074931bbbf",
        "title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation"
      },
      {
        "id": "ebd1e9d5f26fa817469c10bd0bb9cb5207143aeb",
        "title": "Evaluation Gaps in Machine Learning Practice"
      },
      {
        "id": "3b55412f2d173504417590012676e5c7eb27de6c",
        "title": "Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness"
      },
      {
        "id": "ee5a743129e5785b92aff156a947ca8c6beabbbc",
        "title": "Robust Conversational Agents against Imperceptible Toxicity Triggers"
      },
      {
        "id": "1a066314e819d34aa4a836ef59a730ff1a11a1b0",
        "title": "An Empirical Study on Explanations in Out-of-Domain Settings"
      },
      {
        "id": "dcd8e4e17e829640fa8ea4cca9ee5b79f433cda7",
        "title": "Phrase-level Textual Adversarial Attack with Label Preservation"
      },
      {
        "id": "43dfcb27404d53a9dfaf80020e028a1599d88639",
        "title": "Text Transformations in Contrastive Self-Supervised Learning: A Review"
      },
      {
        "id": "28beacbb4cca9cfa63fb444b73343ec89b624563",
        "title": "A Two-Step Approach to Leverage Contextual Data: Speech Recognition in Air-Traffic Communications"
      },
      {
        "id": "cddf40e579a596d0110b260313adf43470617c4c",
        "title": "Datasets: A Community Library for Natural Language Processing"
      },
      {
        "id": "eeec016e6cb725efce680d8172c1fcaf58727a7e",
        "title": "Data Augmentation Approaches in Natural Language Processing: A Survey"
      },
      {
        "id": "a5881560968963d0c845c468a273261fde0b7248",
        "title": "Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing"
      },
      {
        "id": "f659031ceb7bbdcb7b0690742f35e2924fd1ed75",
        "title": "Towards Robustness Against Natural Language Word Substitutions"
      },
      {
        "id": "89daa253cfd707958b1539ec4d8ea9664e8ceb7d",
        "title": "NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation"
      },
      {
        "id": "3b451fa663704f927e1ec602d7c0845a9826922d",
        "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations"
      },
      {
        "id": "99c766f0d71130e7db1290520e48f86cedcebfc4",
        "title": "SMAT: An Attention-Based Deep Learning Solution to the Automation of Schema Matching"
      },
      {
        "id": "96fd89de07a69dd2dc94d71f884e64174c5974e2",
        "title": "Interpreting the Robustness of Neural NLP Models to Textual Perturbations"
      },
      {
        "id": "78a0d6c9899be720ec8052a8a593885bdd6e7cce",
        "title": "Data Augmentation for Low-Resource Named Entity Recognition Using Backtranslation"
      },
      {
        "id": "c1526ec26d67a4976eae17548e6c3296adc6f5ac",
        "title": "kFolden: k-Fold Ensemble for Out-Of-Distribution Detection"
      },
      {
        "id": "8436897e713c2242d6291df9a6a33c1544d4dd39",
        "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"
      },
      {
        "id": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
        "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?"
      },
      {
        "id": "9606c85e44e421f7eb822c9f7130610d6d9d8ad9",
        "title": "Deep Learning Transformer Architecture for Named-Entity Recognition on Low-Resourced Languages: State of the art results"
      },
      {
        "id": "35bbb7e34f709384591ab822a353b4b664dd5c90",
        "title": "Token-modification adversarial attacks for natural language processing: A survey"
      },
      {
        "id": "0f71a4fa9736ae916e6aef53045f6be4c901b0ff",
        "title": "Reliability Testing for Natural Language Processing Systems"
      },
      {
        "id": "fe9cd5bcca161289b0e3da0f49114dcccf62eaa1",
        "title": "Token-Aware Virtual Adversarial Training in Natural Language Understanding"
      },
      {
        "id": "9b54941de1e21826ecc28b32730ac3f69991ede4",
        "title": "Robustness Gym: Unifying the NLP Evaluation Landscape"
      },
      {
        "id": "cbc1e8bbfe98f94c0d13d111b824cf603b62712c",
        "title": "Bad Characters: Imperceptible NLP Attacks"
      },
      {
        "id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
        "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!"
      },
      {
        "id": "d6357b1c61611f744acbae69484acd7f21c89dff",
        "title": "Substructure Substitution: Structured Data Augmentation for NLP"
      },
      {
        "id": "546daf45b98dcaa1c7b438ec04b2a8b744f279d7",
        "title": "Certified Robustness to Programmable Transformations in LSTMs"
      },
      {
        "id": "aba19f5ae0747dd1d02137ca596a74e70514a246",
        "title": "Low-Resource Named Entity Recognition via the Pre-Training Model"
      },
      {
        "id": "2ef79342ff22661cd7bc18833049085e6b3501c4",
        "title": "Generating Natural Language Attacks in a Hard Label Black Box Setting"
      },
      {
        "id": "f6b9ff8cfc60241008e3748efcf05cbe91037aa0",
        "title": "Bi-LSTM Model to Increase Accuracy in Text Classification: Combining Word2vec CNN and Attention Mechanism"
      },
      {
        "id": "472cd41fa2ba2e520706f232cae12db4a7b5e60a",
        "title": "Contextualized Perturbation for Textual Adversarial Attack"
      },
      {
        "id": "bdbb944a84b8cdec8d120d2d2535995e335d0174",
        "title": "An Analysis of Simple Data Augmentation for Named Entity Recognition"
      },
      {
        "id": "3aba6c5f59080c24301121bfc3a1445fc292476b",
        "title": "Mixup-Transformer: Dynamic Data Augmentation for NLP Tasks"
      },
      {
        "id": "c299a4083443bea26188567979f20b8305554c0b",
        "title": "GenAug: Data Augmentation for Finetuning Text Generators"
      },
      {
        "id": "9b9a6d6a698cce777929ecc65c9fc5d09b2232ac",
        "title": "T3: Tree-Autoencoder Constrained Adversarial Text Generation for Targeted Attack"
      },
      {
        "id": "adc9533f3d4c3b41b21c10a948a6118018df2a5a",
        "title": "Searching for a Search Method: Benchmarking Search Algorithms for Generating NLP Adversarial Examples"
      },
      {
        "id": "7d96eaaa71a9556ab3b0c04c691af0b27b769d03",
        "title": "HateGAN: Adversarial Generative-Based Data Augmentation for Hate Speech Detection"
      },
      {
        "id": "2a8a2ab581f2e89c9a66e1b353346e1bb86ee6f6",
        "title": "Mixup-Transfomer: Dynamic Data Augmentation for NLP Tasks"
      },
      {
        "id": "eb2f1480eca6195fe8dbab5de8bbac8749e9be58",
        "title": "RODA: Reverse Operation Based Data Augmentation for Solving Math Word Problems"
      },
      {
        "id": "885275636fcde952a41f5272af72bc956b106f3d",
        "title": "Text Data Augmentation: Towards better detection of spear-phishing emails"
      },
      {
        "id": "9210dc01a1586e3452bf944624a2269ad9010f2f",
        "title": "A Text Classification Survey: From Shallow to Deep Learning"
      },
      {
        "id": "88338c58701f34503c7af77e34f19d9a5cd66313",
        "title": "Adversarial Attacks on Deep-learning Models in Natural Language Processing"
      },
      {
        "id": "bf9a8fb50aca26774ebf4815db2d8712e2c5830c",
        "title": "TextAttack: A Framework for Adversarial Attacks in Natural Language Processing"
      },
      {
        "id": "d9dd2be773cb7de6d7cf61e601144453c680dc53",
        "title": "Data and Representation for Turkish Natural Language Inference"
      },
      {
        "id": "c75d95b4bb5c8d8e0bb2e9cc8f3dc97c7080fe0f",
        "title": "Massive Choice, Ample Tasks (MaChAmp): A Toolkit for Multi-task Learning in NLP"
      },
      {
        "id": "2d374cdc65c047e5e860e9e5147775a81216f23e",
        "title": "Viability of Neural Networks for Core Technologies for Resource-Scarce Languages"
      },
      {
        "id": "13cbb4fa75192275dcb4d929c07841ca505c2bda",
        "title": "Lexical Normalization for Code-switched Data and its Effect on POS Tagging"
      }
    ],
    "3": [
      {
        "id": "8771679aac0e90371340bd8c657317f5be113e81",
        "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"
      },
      {
        "id": "276a00173085d75232dee025104cc27ad23eaf75",
        "title": "A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks"
      },
      {
        "id": "0b40cdf63f651f89904e5e1c79ab7225b0a666ff",
        "title": "Continual Learning of Natural Language Processing Tasks: A Survey"
      },
      {
        "id": "a8c09c41f39d798dc4201eeec1452fe617e428df",
        "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing"
      },
      {
        "id": "baf3b64df9dd05385abcfd11d964f15c9684cc4d",
        "title": "Analyzing and Defending against Membership Inference Attacks in Natural Language Processing Classification"
      },
      {
        "id": "3f6243097a58e386aea1215fed4f372dee07a100",
        "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
      },
      {
        "id": "37e8151365c93578e6d645b27377ebb0414b22ed",
        "title": "A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models"
      },
      {
        "id": "5aa7bdcae38076b80229c0a024f5b656ac6607af",
        "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter"
      },
      {
        "id": "200ef1cde362aafbf598a2b5a1c5f35504ca2289",
        "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design"
      },
      {
        "id": "86891d00499eebe86d3f1e39143d412addf2652b",
        "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"
      },
      {
        "id": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
        "title": "ViA: A Novel Vision-Transformer Accelerator Based on FPGA"
      },
      {
        "id": "3b97687f80cf2ed753789429ba6d2e53667a0dd7",
        "title": "On the effectiveness of compact biomedical transformers"
      },
      {
        "id": "52eab50953077c78991a9c045483664086fe37c7",
        "title": "PipeEdge: Pipeline Parallelism for Large-Scale Model Inference on Heterogeneous Edge Devices"
      },
      {
        "id": "a4d45c1f04822fe24a6ddd3f9018bd701e5a7933",
        "title": "Demystifying BERT: System Design Implications"
      },
      {
        "id": "f1557a3638b164c632660a8bd4186076a96c7bf1",
        "title": "Bebert: Efficient And Robust Binary Ensemble Bert"
      },
      {
        "id": "f806d16e3efd0f3f5b63491a6f6d4a7334df4cee",
        "title": "TCB: Accelerating Transformer Inference Services with Request Concatenation"
      },
      {
        "id": "17a8bd6a5763f6607863348ce1757ac2ad3417fd",
        "title": "Accelerating Transformer Networks through Recomposing Softmax Layers"
      },
      {
        "id": "f3e70aba917d3adc5a01387f8e76da856c1b55aa",
        "title": "Hardware Acceleration of Transformer Networks using FPGAs"
      },
      {
        "id": "43b7437ed33a29d3d90239ad66f325a465ff7e91",
        "title": "Meta Learning for Natural Language Processing: A Survey"
      },
      {
        "id": "6da9a81b75e7ad02867860753d1aa276673a3a77",
        "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"
      },
      {
        "id": "b77cb53e9a0d9fbe77b4b6d0982ed6fc9ad6f1a8",
        "title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language"
      },
      {
        "id": "05d70085d1b580b2369942410ae77c48d1eeacca",
        "title": "Exploring Extreme Parameter Compression for Pre-trained Language Models"
      },
      {
        "id": "d90ea09d6bb1e17cb07c7e6edb325a755ed1a493",
        "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models"
      },
      {
        "id": "f019c76f90f1a822351d76d317339f4c7bb8da06",
        "title": "Knowledge Distillation of Russian Language Models with Reduction of Vocabulary"
      },
      {
        "id": "edfea69d9cd1a4d40f4d879aa36f93ad7d26a659",
        "title": "QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization"
      },
      {
        "id": "00df5cf0d83c48657d453ab8083d8805a67f744f",
        "title": "Measuring the Carbon Intensity of AI in Cloud Instances"
      },
      {
        "id": "7711774cf9f1d8683daf853b9e5773158b064980",
        "title": "Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications"
      },
      {
        "id": "fb2307f7ce7c6868429ee3ee15d6eaf311ecba5c",
        "title": "BiBERT: Accurate Fully Binarized BERT"
      },
      {
        "id": "76d40153acfbb35a7eb8272a4215854cafa10e78",
        "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"
      },
      {
        "id": "df434c1289f3c7243b585cb9982afac3c5bf0439",
        "title": "MoEBERT: from BERT to Mixture-of-Experts via Importance-Guided Adaptation"
      },
      {
        "id": "d4bf2cb0dba48f54614f79595587f06bdbfb2e4c",
        "title": "Efficient Fine-Tuning of BERT Models on the Edge"
      },
      {
        "id": "a71d57029884e60735f5672d33794e29d0ae3dea",
        "title": "Mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models"
      },
      {
        "id": "cf8f5baabeba69ddca9f8b65e05d467b50253e0a",
        "title": "Winning the Lottery Ahead of Time: Efficient Early Network Pruning"
      },
      {
        "id": "557c3e1234a5d8fbde234c03776d12f6c3a67f8e",
        "title": "Federated Split BERT for Heterogeneous Text Classification"
      },
      {
        "id": "258d3488a9784c6829c278b31b202657b45c0655",
        "title": "SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression"
      },
      {
        "id": "bbd5eb0924ec07a83dbc99151a09f463300c0001",
        "title": "Accelerating attention through gradient-based learned runtime pruning"
      },
      {
        "id": "8dd49f94b213834e580eeea18098f402b1be1226",
        "title": "Adversarial Data Augmentation for Task-Specific Knowledge Distillation of Pre-trained Transformers"
      },
      {
        "id": "73c722148ed4a5301dc75ae291b647a1915b8ecd",
        "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"
      },
      {
        "id": "e2502731ae5d3c07d736f2ec27abcf80d5238167",
        "title": "Differentially Private Model Compression"
      },
      {
        "id": "e2b29af251adc9628053966ea2b9cb45b4cdbb4b",
        "title": "REx: Data-Free Residual Quantization Error Expansion"
      },
      {
        "id": "08460ecff91b8a54358b9c1709d7dc6a77417f62",
        "title": "Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing"
      },
      {
        "id": "511e2fe08d8ce170736b59d386a3301a6d6e57b0",
        "title": "NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM"
      },
      {
        "id": "9202a718ce05395b6e17d5301e3a2e8b1021f31b",
        "title": "Prune Once for All: Sparse Pre-Trained Language Models"
      },
      {
        "id": "ef18db2a18ac61e72783a613328842ce86ef00bf",
        "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"
      },
      {
        "id": "574072ae4556649de1c59d8f284955730f5a71a0",
        "title": "A Short Study on Compressing Decoder-Based Language Models"
      },
      {
        "id": "08c2e7812ff224db1c877b4d14730d6288d529aa",
        "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression"
      },
      {
        "id": "38f683ec0b9fda2069c0b2cb7ee1c71035915723",
        "title": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation"
      },
      {
        "id": "b97c3c370401dc34d2adbeb24f34de5180a14be6",
        "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"
      },
      {
        "id": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
        "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
      },
      {
        "id": "37187ceb6008d49e1758bab0d4f86bf39aa175cf",
        "title": "A Survey on Green Deep Learning"
      },
      {
        "id": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2",
        "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"
      },
      {
        "id": "8d911c2be6b68771cc1dae24fd3c5c5dc5261e81",
        "title": "Towards Efficient NLP: A Standard Evaluation and A Strong Baseline"
      },
      {
        "id": "c67b1a62b868a758791c88d5465c7b6d53510fc3",
        "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"
      },
      {
        "id": "b6f616e9305e59c9dc7ccf33c311ede47584caf6",
        "title": "Kronecker Decomposition for GPT Compression"
      },
      {
        "id": "b7b3343b45c785ccab1c94beecc28ea91e041685",
        "title": "E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs"
      },
      {
        "id": "217913c84a4bdbe5cee3630d70480fda8d44bfb0",
        "title": "Magic Pyramid: Accelerating Inference with Early Exiting and Token Pruning"
      },
      {
        "id": "c2848530890acd7549fe7cfec7d4fc5a80c2feb7",
        "title": "Distilling the Knowledge of Romanian BERTs Using Multiple Teachers"
      },
      {
        "id": "9f840be023309cc957dc741dce85dfc6b1a3b486",
        "title": "NPE: An FPGA-based Overlay Processor for Natural Language Processing"
      },
      {
        "id": "7a320541d7772bedc9b7f537f6bd459675675bb0",
        "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing"
      },
      {
        "id": "5b89435357806e89cd428c56ce98aeb930b0df5c",
        "title": "Variance-reduced First-order Meta-learning for Natural Language Processing Tasks"
      },
      {
        "id": "a7ca9df39318fb12b5fcd99664fa48538bdb0bb4",
        "title": "Greedy-layer pruning: Speeding up transformer models for natural language processing"
      },
      {
        "id": "0a9b7f2ff0e25f4eecd946bae4ad2f8c2d53b562",
        "title": "Automated essay scoring using efficient transformer-based language models"
      },
      {
        "id": "54b75720b82f2af779e11910985faa9eb343a345",
        "title": "Distilling Large Language Models into Tiny and Effective Students using pQRNN"
      },
      {
        "id": "093253653cd0b55970c390d77b75137c4095dc29",
        "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
      },
      {
        "id": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
        "title": "I-BERT: Integer-only BERT Quantization"
      },
      {
        "id": "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"
      },
      {
        "id": "b080ba53a471348e7e76234decdf14e730fea7db",
        "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers"
      },
      {
        "id": "d6eeb0ca9e2f34f2427866aa864d364ec78e6049",
        "title": "Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning"
      },
      {
        "id": "dd0a27aa2285bc64798fa76944400ab6d9ce3025",
        "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"
      },
      {
        "id": "40235eded15f44c8c4a7f48468adcc7df4e171fb",
        "title": "Rethinking Network Pruning – under the Pre-train and Fine-tune Paradigm"
      },
      {
        "id": "50de6d069125f556dedb7cdf058071e66673f992",
        "title": "Attention mechanisms and deep learning for machine vision: A survey of the state of the art"
      },
      {
        "id": "0789d0f6f3459b7689486ebc80ce39373dd8cc17",
        "title": "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation"
      },
      {
        "id": "0fe8b49369d70a2be473435a82b01544704b3c9f",
        "title": "Evolving Attention with Residual Convolutions"
      },
      {
        "id": "2a0ae7182b13789056e13dc1887904c923a92675",
        "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization"
      },
      {
        "id": "27a9bdedcc2c3c0bc9116d7cd56fb72744b6a26a",
        "title": "Clustering Word Embeddings with Self-Organizing Maps. Application on LaRoSeDa - A Large Romanian Sentiment Data Set"
      },
      {
        "id": "82ab2f4a51d37c9dca79251a684c8507a156aff7",
        "title": "Structured Sparsity Inducing Adaptive Optimizers for Deep Learning"
      },
      {
        "id": "9a0ce4d9b12337c1c4b65e56cab5b87dcc5f5aa2",
        "title": "XtremeDistilTransformers: Task Transfer for Task-agnostic Distillation"
      },
      {
        "id": "d73d8edbc804b7848b4a5a1a11b6d33f9f42fa94",
        "title": "Elbert: Fast Albert with Confidence-Window Based Early Exit"
      },
      {
        "id": "3623a39bfbbefff942a2f370d76dd18fbc1d9139",
        "title": "Demystifying BERT: Implications for Accelerator Design"
      },
      {
        "id": "069a05c0ae7ac49ad7eb8e0a0744c212c58bd863",
        "title": "Meta-learning for Few-shot Natural Language Processing: A Survey"
      },
      {
        "id": "e2d38543bd3cf813c63df336b21b003156ed48a8",
        "title": "Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start"
      },
      {
        "id": "4c3b044cc98def3defc3d562e5c0d811f7b0d200",
        "title": "LRC-BERT: Latent-representation Contrastive Knowledge Distillation for Natural Language Understanding"
      },
      {
        "id": "73e0f38ab49b19b86321016b773e15f1d02e3a72",
        "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and Head Pruning"
      },
      {
        "id": "e71aed7a0680c8fc09733f1dcd0cd3f6bb9cb7aa",
        "title": "The Lottery Ticket Hypothesis for Pre-trained BERT Networks"
      },
      {
        "id": "097210dc65924f8ce59523faf444e635523dc714",
        "title": "TernaryBERT: Distillation-aware Ultra-low Bit BERT"
      },
      {
        "id": "7c6c31412c5dad22543bb71e31620e8868d644a3",
        "title": "FTRANS: energy-efficient acceleration of transformers using FPGA"
      },
      {
        "id": "3af8a493cf756f9fe72623204a11e378a9cd71a5",
        "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference"
      },
      {
        "id": "9baab08fbe37369856688b2abe5b3c90cce1682c",
        "title": "Compression of Deep Learning Models for Text: A Survey"
      },
      {
        "id": "b6451cfb71be72f8f9e0f5d2f529fea231adb382",
        "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer"
      },
      {
        "id": "8b0a0f6d1cd6f3aa9b54be45d5127bb016a98171",
        "title": "The birth of Romanian BERT"
      },
      {
        "id": "5585c7fcbda5d94e946fe9091860e2e574927ed8",
        "title": "Efficient Meta Lifelong-Learning with Limited Memory"
      },
      {
        "id": "52c8c417771a582c77fef22bfc38a0db835212b4",
        "title": "Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking"
      },
      {
        "id": "218ddb2d8182d197dd2cff40220179d7867c3589",
        "title": "RoBERT – A Romanian BERT Model"
      },
      {
        "id": "8fd0b70cfd6bbdaef9fbe1073afb3920cb61f80b",
        "title": "BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover’s Distance"
      },
      {
        "id": "24c5242f71af8795021270f52030534e587dfc1e",
        "title": "Tensorized Embedding Layers"
      },
      {
        "id": "0b98f8ec299de3358c5dfc0d842529b5aee0e97c",
        "title": "Progressively Stacking 2.0: A Multi-stage Layerwise Training Method for BERT Training Speedup"
      },
      {
        "id": "ffb0bbe26f1cbc0ab22ef34784248f2dcd3a5e5c",
        "title": "Finding Fast Transformers: One-Shot Neural Architecture Search by Component Composition"
      },
      {
        "id": "4ed811133638ceed1f1cd22e2725649ef24cb5fe",
        "title": "EasyTransfer: A Simple and Scalable Deep Transfer Learning Platform for NLP Applications"
      },
      {
        "id": "8e844acf10d6e0efd0bd4744ec77f52186873efd",
        "title": "Undivided Attention: Are Intermediate Layers Necessary for BERT?"
      },
      {
        "id": "ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7",
        "title": "HAT: Hardware-Aware Transformers for Efficient Natural Language Processing"
      },
      {
        "id": "501a8b86428563539667e8117cd8409674ef97c3",
        "title": "TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing"
      },
      {
        "id": "2573af4e13d9a5dddb257d22cd38a600528d9a8b",
        "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"
      },
      {
        "id": "66f0f35fc78bdf2af9de46093d49a428970cde2e",
        "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning"
      },
      {
        "id": "4c7346ae1126dd853f4802962d26f844ceb82940",
        "title": "Compressing Language Models using Doped Kronecker Products"
      },
      {
        "id": "1c332cfa211400fc6f56983fb01a6692046116dd",
        "title": "DynaBERT: Dynamic BERT with Adaptive Width and Depth"
      },
      {
        "id": "8af925f4edf45131b5b6fed8aa655089d58692fa",
        "title": "Lite Transformer with Long-Short Range Attention"
      },
      {
        "id": "d9b824dbecbe3a1f0b1489f9e4521a532a63818d",
        "title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning"
      },
      {
        "id": "fcf1b4473a0af1f3ebc0fd556ee30c9309ff6345",
        "title": "SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training"
      },
      {
        "id": "0171ad4cc87cc7db25b4ec3169e293eed9a13b39",
        "title": "Training with Quantization Noise for Extreme Model Compression"
      },
      {
        "id": "d3c6c635b9cfd8890c7244d3db4be53d45944963",
        "title": "A^3: Accelerating Attention Mechanisms in Neural Networks with Approximation"
      },
      {
        "id": "738215a396f6eee1709c6b521a6199769f0ce674",
        "title": "Compressing Large-Scale Transformer-Based Models: A Case Study on BERT"
      },
      {
        "id": "2b9955bc08fc5f4ddba73082ddabcfaabdbb4416",
        "title": "Poor Man's BERT: Smaller and Faster Transformer Models"
      },
      {
        "id": "14216c91c7d02e58717204f04131107778a84e7b",
        "title": "Multi-Head Attention: Collaborate Instead of Concatenate"
      },
      {
        "id": "54d4ff8d536b292149a4fa017c22349cf4e54ce4",
        "title": "AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural Architecture Search"
      },
      {
        "id": "9c5a239b75bade55c830b164e2fadc424e879137",
        "title": "XtremeDistil: Multi-stage Distillation for Massive Multilingual Models"
      },
      {
        "id": "a81674f480dba239e12c80910528cae5d3a28e97",
        "title": "schuBERT: Optimizing Elements of BERT"
      },
      {
        "id": "1b0c8b26affd13e10ace5770e85478d60dcc368e",
        "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy Efficient Inference"
      }
    ],
    "1": [
      {
        "id": "495da6f19baa09c6db3697d839e10432cdc25934",
        "title": "Multilingual Denoising Pre-training for Neural Machine Translation"
      },
      {
        "id": "8ae9a17c87a4518b513e860683a0ef7824be994d",
        "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"
      },
      {
        "id": "8db5adadc0ab39f95a26a2eb6499d340d6c5ea21",
        "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation"
      },
      {
        "id": "df44833f374eea3537960b9e2d0c5482ccd27272",
        "title": "Event Extraction: A Survey"
      },
      {
        "id": "30c0cdc414f68211d5d0514df027cec22e005174",
        "title": "A Survey on In-context Learning"
      },
      {
        "id": "a25e9cc63e7660b2dc0659adc5319131d3a1e0ab",
        "title": "Deciphering microbial gene function using natural language processing"
      },
      {
        "id": "5bc52da51fe82c6de07979b830502a7afbf86843",
        "title": "RastrOS Project: Natural Language Processing contributions to the development of an eye-tracking corpus with predictability norms for Brazilian Portuguese"
      },
      {
        "id": "a7410cfe1a5b966d113a6ef5a2d9022f409d40df",
        "title": "Building Natural Language Processing Applications with EasyNLP"
      },
      {
        "id": "17dcfef70619c0423e0527f0c9d90f4858125f5f",
        "title": "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding"
      },
      {
        "id": "356633ba77851f4a9f8e2ba713ca83a4ef2c140e",
        "title": "JamPatoisNLI: A Jamaican Patois Natural Language Inference Dataset"
      },
      {
        "id": "76d688dac9817f2f401e136d1e831c0ec9f9b9f5",
        "title": "Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training"
      },
      {
        "id": "72fbd9a613d4fad4542c8dda785a74cc36db6d5c",
        "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks"
      },
      {
        "id": "5d3cf0909ba206cd6bc2e86610f77ca25d9b2d1c",
        "title": "Finding Skill Neurons in Pre-trained Transformer-based Language Models"
      },
      {
        "id": "00552729b09cf9430c0289c9c43965b63918cdae",
        "title": "LERT: A Linguistically-motivated Pre-trained Language Model"
      },
      {
        "id": "a593ce5d2a93f3113be7717d08d1ba8e62fd7ddf",
        "title": "Benchmarking Language Models for Code Syntax Understanding"
      },
      {
        "id": "1d52755458a0fae5b5abd23c1ebb7c66926d2bd5",
        "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking"
      },
      {
        "id": "d357705dd0189e5398cdf1e8129127b66df06d6e",
        "title": "Effectiveness of French Language Models on Abstractive Dialogue Summarization Task"
      },
      {
        "id": "f3a6115e5fb2237df938976e005468f0b18da797",
        "title": "The Stack: 3 TB of permissively licensed source code"
      },
      {
        "id": "df1cc92fba512ce7d28d1d608ea19f18cda185ca",
        "title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence"
      },
      {
        "id": "808e9ce4e86e79098edea7f00b5b91663b87a5e6",
        "title": "A taxonomy and review of generalization research in NLP"
      },
      {
        "id": "03c19ddaa26068f23e27ba94b10f08160e87f668",
        "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources"
      },
      {
        "id": "44ebfdb670007b3949507be0d1a1fca93bc3d5d5",
        "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges"
      },
      {
        "id": "180da92f3ba98e12c548dbd5c1d68b066e9b926b",
        "title": "Vision Transformers in Medical Imaging: A Review"
      },
      {
        "id": "bd7d7f7d79df4c628494ee4d6ce19cd7dbebb367",
        "title": "Is this Change the Answer to that Problem?: Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness"
      },
      {
        "id": "46d64582890afc296c134c0ba69137bb1d7fbba7",
        "title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator"
      },
      {
        "id": "ea2fb89403ea1cd6af000e761e2f72eb7c150607",
        "title": "TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion"
      },
      {
        "id": "f4f43a23274cdf74ac017f877f7bd9cb1283b9c8",
        "title": "A transformer-based IDE plugin for vulnerability detection"
      },
      {
        "id": "1daf323d06fc68feba72d8826ea3372ef50aee14",
        "title": "“It’s Not Just Hate”: A Multi-Dimensional Perspective on Detecting Harmful Speech Online"
      },
      {
        "id": "21c56b7677c930138dc32a7197e4ecf7d2ea6512",
        "title": "Automatic Terminology Extraction and Ranking for Feature Modeling"
      },
      {
        "id": "ac5225708efd250d217424ba27885e90f186160d",
        "title": "Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words"
      },
      {
        "id": "13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8",
        "title": "MarianCG: a code generation transformer model inspired by machine translation"
      },
      {
        "id": "4e0526421da87d88627fef66e9e84ed559fff249",
        "title": "TweetNLP: Cutting-Edge Natural Language Processing for Social Media"
      },
      {
        "id": "2369e8614d9c854dd5d4d68b284ac266988871bf",
        "title": "Resume Classification System using Natural Language Processing and Machine Learning Techniques"
      },
      {
        "id": "cd4d15e3706b28e07f95d38efe785a29c5f06fb6",
        "title": "Boamente: A Natural Language Processing-Based Digital Phenotyping Tool for Smart Monitoring of Suicidal Ideation"
      },
      {
        "id": "c442b4c1a6030b315f8714726239123b05685066",
        "title": "EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing"
      },
      {
        "id": "6a2d4eccd8c0679afa3f9c83725ee5e84dbd378f",
        "title": "Automatic Software Engineering Position Resume Screening using Natural Language Processing, Word Matching, Character Positioning, and Regex"
      },
      {
        "id": "a2cd34174ffd3822ec04d00f261ee1690e1318b1",
        "title": "An Extensive Study on Pretrained Models for Natural Language Processing Based on Transformers"
      },
      {
        "id": "b6adc972f4bdbbb4dc7143713d16a3408a71ef7e",
        "title": "Automated Customer Complaint Processing for Water Utilities Based on Natural Language Processing—Case Study of a Dutch Water Utility"
      },
      {
        "id": "06ef1f9e1a760df942548ab38ea7e49e6145c6bd",
        "title": "Deep Learning, Natural Language Processing, and Explainable Artificial Intelligence in the Biomedical Domain"
      },
      {
        "id": "a5ae31b1ff040bd5c3d4d1b2ffca1efd3f8948d6",
        "title": "Intelligent Scoring of English Composition by Machine Learning from the Perspective of Natural Language Processing"
      },
      {
        "id": "7028c98fda13aad5e73a027354fb7cb7f43d5412",
        "title": "Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge"
      },
      {
        "id": "7e5500ec95b28b8e59b03599c59f4d0b65e67094",
        "title": "Current Approaches and Applications in Natural Language Processing"
      },
      {
        "id": "fc14091bbd7d1eb2c9f23ff9c75a4222f2604143",
        "title": "Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning"
      },
      {
        "id": "80b2b006ed2f26ec3ddc91e303dc9861fb456a26",
        "title": "On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules"
      },
      {
        "id": "1b45949be1b203f096d6451d88cf91174c620be7",
        "title": "From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems"
      },
      {
        "id": "5669f85449f35352eda1f6c82163f1f82ff1f68c",
        "title": "Extract Aspect-based Financial Opinion Using Natural Language Inference"
      },
      {
        "id": "5d1c711c1d5b5f8399938af5e3df904680ce24a7",
        "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks"
      },
      {
        "id": "bb7e46f316d319f9819c3554c99995ef8361ae9c",
        "title": "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"
      },
      {
        "id": "9a4953a8c0df1d7d7b11c8e1747eb156caa9fbcb",
        "title": "PERT: Pre-training BERT with Permuted Language Model"
      },
      {
        "id": "a3628013db1e9da171128a43ac9a6fa895845f73",
        "title": "Multi-task Pre-training Language Model for Semantic Network Completion"
      },
      {
        "id": "3fa0959ed06206b81f6d2125c19e1a5958250e6d",
        "title": "Threats to Pre-trained Language Models: Survey and Taxonomy"
      },
      {
        "id": "55b9a2ade0a49e9cf10b71528d69dfee4e826025",
        "title": "Cedille: A large autoregressive French language model"
      },
      {
        "id": "e475d7c3b10d548e59590902474ff99206a732f3",
        "title": "The Dark Side of the Language: Pre-trained Transformers in the DarkNet"
      },
      {
        "id": "d7ef7e6e7e78c982771b9c2d762c17132767153b",
        "title": "Making Adversarially-Trained Language Models Forget with Model Retraining: A Case Study on Hate Speech Detection"
      },
      {
        "id": "f59859bddd6f91f15894cb709cd46d2c437539b6",
        "title": "Overcoming Language Disparity in Online Content Classification with Multimodal Learning"
      },
      {
        "id": "2f72dbc05a112739538d042a6b30cf8c223c2d78",
        "title": "Multi-Task Text Classification using Graph Convolutional Networks for Large-Scale Low Resource Language"
      },
      {
        "id": "7e3081b0d698f8abf16dee626d782f3339482fe7",
        "title": "An Assessment of the Impact of OCR Noise on Language Models"
      },
      {
        "id": "ed4087f6e8d77452810979f58246c5b2ad846cf8",
        "title": "Transformers in Time Series: A Survey"
      },
      {
        "id": "598231eb906b183f7a2a408ef4536127e11e3de9",
        "title": "Challenges and Strategies in Cross-Cultural NLP"
      },
      {
        "id": "11f64ec047782cada21d50efea1e0dc5843675f6",
        "title": "NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages"
      },
      {
        "id": "3eae4612f4d0968f411c48e91bdf2af561d871d3",
        "title": "Parallel Instance Query Network for Named Entity Recognition"
      },
      {
        "id": "e8ad4d82f23f88c17f401c785e24e00841719f49",
        "title": "Using Transfer Learning for Code-Related Tasks"
      },
      {
        "id": "0dac0e73dc0d6f0ebbbd45ea2e3bc60d437200e1",
        "title": "CIRCLE: continual repair across programming languages"
      },
      {
        "id": "e72ce8bd009e1c97fbd896321efe5a63b3d95c34",
        "title": "PTM4Tag: Sharpening Tag Recommendation of Stack Overflow Posts with Pre-trained Models"
      },
      {
        "id": "8bb9db78b4413b92cdeeae9e24e955aab9c87ae1",
        "title": "A Survey of Adversarial Defences and Robustness in NLP"
      },
      {
        "id": "1b57b772528d6aacd705ddddbf7a61595ff2c86b",
        "title": "BERT-Based Transfer-Learning Approach for Nested Named-Entity Recognition Using Joint Labeling"
      },
      {
        "id": "1c81225b4146297173e034a03fcb4c74b4e52e0f",
        "title": "Active Few-Shot Learning with FASL"
      },
      {
        "id": "7e15af5f6b3a794167bc4dc194768b348becc5d3",
        "title": "Improving Tokenisation by Alternative Treatment of Spaces"
      },
      {
        "id": "d98c29264e73f785ab8be2145ae81495e6775c21",
        "title": "ZeroBERTo: Leveraging Zero-Shot Text Classification by Topic Modeling"
      },
      {
        "id": "267dcb61f72f48dadd60a3a770493c0c9f70be65",
        "title": "Evaluation of Transfer Learning for Polish with a Text-to-Text Model"
      },
      {
        "id": "0a0a47ff85527ee9461eb5796703b65a95944fc4",
        "title": "Social Media Sentiment Analysis for Cryptocurrency Market Prediction"
      },
      {
        "id": "3c1855cf61d00282c7a41addb416bb4d9206064b",
        "title": "Beyond the Benchmarks: Toward Human-Like Lexical Representations"
      },
      {
        "id": "ffecaa278e8fd6997030adca52c852c1bbbaf3f2",
        "title": "Towards Debiasing Translation Artifacts"
      },
      {
        "id": "8ce8d0a34759f0d94abfc11b4478cc97ad6ed162",
        "title": "BanglaNLG and BanglaT5: Benchmarks and Resources for Evaluating Low-Resource Natural Language Generation in Bangla"
      },
      {
        "id": "c10d0353d8a169d23351f1dbec8cdfdd8c62a60e",
        "title": "Context-Tuning: Learning Contextualized Prompts for Natural Language Generation"
      },
      {
        "id": "c7cbbbdba2c942180675c2a69035e506d9378679",
        "title": "Using Pre-Trained Models to Boost Code Review Automation"
      },
      {
        "id": "564992c12097a87c279a90a1459eb72bbd007090",
        "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation"
      },
      {
        "id": "546a91d0957a2c34e083a9d9c3d8784aeac980ae",
        "title": "PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization"
      },
      {
        "id": "9aacdbc8b04fa63e6fe93f62a737a11c613f08fb",
        "title": "Recent Advances in Neural Text Generation: A Task-Agnostic Survey"
      },
      {
        "id": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
        "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
      },
      {
        "id": "6c761cfdb031701072582e434d8f64d436255da6",
        "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"
      },
      {
        "id": "8a9fd97dafd664b8fd1ae4ed5ca5071adf9f20ba",
        "title": "Paradigm Shift in Natural Language Processing"
      },
      {
        "id": "46d846aa7df7b5c0cd4b129591b5754272701eae",
        "title": "On Transferability of Prompt Tuning for Natural Language Processing"
      },
      {
        "id": "39afb79bc3c3eb37ff244357d29f3055e43047b6",
        "title": "LegalNLP - Natural Language Processing methods for the Brazilian Legal Language"
      },
      {
        "id": "3389f38e5c231488cdff4e8d3ae8ab5bfd19c71c",
        "title": "Resume Validation and Filtration using Natural Language Processing"
      },
      {
        "id": "5e86c1adbe5b7cfccf2201e1c34400c819cdcdab",
        "title": "The King is Naked: on the Notion of Robustness for Natural Language Processing"
      },
      {
        "id": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904",
        "title": "Few-shot Learning with Multilingual Generative Language Models"
      },
      {
        "id": "9b56086e420ecb216f85d408a25264f640e46705",
        "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners"
      },
      {
        "id": "d095f9ffcb5905bf0858ad1769d3d90e2e8737e2",
        "title": "Jigsaw: Large Language Models meet Program Synthesis"
      },
      {
        "id": "42fc019b2668c9d9d984154d4c57f6c6d5a91619",
        "title": "Language Models are Few-shot Multilingual Learners"
      },
      {
        "id": "1081df12b1500f48718ad9cdf3b0b90f44a31fc9",
        "title": "Curriculum learning for language modeling"
      },
      {
        "id": "5075a0df5fc7ad5f1399450498044627ebe7a9f9",
        "title": "Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining"
      },
      {
        "id": "234c0e26e011c67fe4e6af2925439eb922104eff",
        "title": "JavaBERT: Training a Transformer-Based Model for the Java Programming Language"
      },
      {
        "id": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"
      },
      {
        "id": "d617f51833860dc50d202af7f80be71304b2e994",
        "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
      },
      {
        "id": "13783f9d98b47a5b525bc3bec01d02088d5b1ef7",
        "title": "Assessing Generalizability of CodeBERT"
      },
      {
        "id": "77dbebb3ebf48b3e6bf1e420e4da55c95320a521",
        "title": "Hierarchical Heterogeneous Graph Representation Learning for Short Text Classification"
      },
      {
        "id": "36ecc1e815820915c52a30700e7ee5350723f971",
        "title": "On cross-lingual retrieval with multilingual text encoders"
      },
      {
        "id": "72d49acb1483c29eb6eb6ce27a1b5e85d2d424e9",
        "title": "Ternion: An Autonomous Model for Fake News Detection"
      },
      {
        "id": "a1040e3d1aeb952b8a1d940e68f8632c34d58421",
        "title": "LOT: A Story-Centric Benchmark for Evaluating Chinese Long Text Understanding and Generation"
      },
      {
        "id": "f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0",
        "title": "Why don’t people use character-level machine translation?"
      },
      {
        "id": "a41856aa69a9b06ab37f0ce5dc4cc861f7099292",
        "title": "Validation on machine reading comprehension software without annotated labels: a property-based method"
      },
      {
        "id": "d85e994a2f993a65b15865784fee9d68776343d9",
        "title": "Overview of the First Shared Task on Automatic Minuting (AutoMin) at Interspeech 2021"
      },
      {
        "id": "12de79e60271e2bd8aed4efc98d4a0b67fe62ade",
        "title": "An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering"
      },
      {
        "id": "e3b100e5a8847c150985af5ccc838220b2d30e35",
        "title": "AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters"
      },
      {
        "id": "79244bdbe17a1dd01e2ee2435a6b667c86539727",
        "title": "Application of Sequence Embedding in Protein Sequence-Based Predictions"
      },
      {
        "id": "158a17902c71b49f5f82dbdfaeaf04ee70ba1385",
        "title": "Sequence-to-Sequence Lexical Normalization with Multilingual Transformers"
      },
      {
        "id": "d87647784c12517d31964cc508d5b8423cc24f50",
        "title": "Integrating Approaches to Word Representation"
      },
      {
        "id": "0e8d71db17c07378ccee228a198abcd425b0cf21",
        "title": "Indian Legal NLP Benchmarks : A Survey"
      },
      {
        "id": "2fab75cfd8394de70bca365572bc5bb04a1b1eb5",
        "title": "DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding"
      },
      {
        "id": "2569a7309142e40815cf556b6417059df9abbda8",
        "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark"
      },
      {
        "id": "dfb4c9396e99a4eb7abfe3798e83ec56b4ccdb9c",
        "title": "Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers"
      },
      {
        "id": "d29036946152bddf950fec7a08c2828a8a8f902e",
        "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems"
      },
      {
        "id": "fe0b1f6194b490f6bbc41c716a58901c1049ccd8",
        "title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation"
      },
      {
        "id": "642e280df732665249315d6c144871f0e2ceeae6",
        "title": "NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands"
      },
      {
        "id": "49a77a36a0a60d29aa7838ac49a055a69658b195",
        "title": "K-PLUG: Knowledge-injected Pre-trained Language Model for Natural Language Understanding and Generation in E-Commerce"
      },
      {
        "id": "b58d8579ece27a60432e667bfbdb750590fa65d9",
        "title": "True Few-Shot Learning with Language Models"
      },
      {
        "id": "64a1dbdd7653eaca25c78e87335ee156b6f6959e",
        "title": "Constrained Language Models Yield Few-Shot Semantic Parsers"
      },
      {
        "id": "bbfdcbfee1762d48cae9db8637f21ea3c234ba30",
        "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation"
      },
      {
        "id": "5bfb0cc16b871c75e32a6a9d54dd7db225260e04",
        "title": "CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing"
      },
      {
        "id": "dc70b180329a6ffead5e48093fb5a551955047c4",
        "title": "HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish"
      },
      {
        "id": "cf5e670a79847d9be0eb185fb372d99d30d4d98f",
        "title": "Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains"
      },
      {
        "id": "857a091532c3641071cb7e411980eb6454a000ae",
        "title": "The Evolution of Language Models Applied to Emotion Analysis of Arabic Tweets"
      },
      {
        "id": "c4a24f0a480b15254b84b1f5620a6c1fc34f4c72",
        "title": "Probing Multilingual Language Models for Discourse"
      },
      {
        "id": "bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e",
        "title": "Transformer models for text-based emotion detection: a review of BERT-based approaches"
      },
      {
        "id": "2b9762e91305986ac8a2d624d0a69521304405f3",
        "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"
      },
      {
        "id": "c2c7233293d55f201fe5b496234bed1914eea70e",
        "title": "Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"
      },
      {
        "id": "0ab855d5a81fdcb33a884465df6598570a1d0a21",
        "title": "Locate and Label: A Two-stage Identifier for Nested Named Entity Recognition"
      },
      {
        "id": "e79d1206292bc5e67ba19737d87d4b2ea4a37105",
        "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"
      },
      {
        "id": "6563251e69e4378c189d0a0c94d8d19508d552c8",
        "title": "MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"
      },
      {
        "id": "c1272cf10f827bd9b8e43cc06b0d5831c59de52e",
        "title": "Challenges of Hate Speech Detection in Social Media"
      },
      {
        "id": "e403faa971e2c750999a3d10bef8b01dd3e85f7b",
        "title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains"
      },
      {
        "id": "3cae37de3e8c7b6f1f4d272fd0a7a837e3f8b8bc",
        "title": "A Sequence-to-Set Network for Nested Named Entity Recognition"
      },
      {
        "id": "ac006d13d98d80c5bde2a6f9a3d41af56bf20153",
        "title": "Deep Learning for Android Malware Defenses: A Systematic Literature Review"
      },
      {
        "id": "5e08011d3ef194650a1471caa7996c2186788a6b",
        "title": "Learned Embeddings from Deep Learning to Visualize and Predict Protein Sets"
      },
      {
        "id": "64902a5077ee68011cd467398dbb66511e8e891a",
        "title": "It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning"
      },
      {
        "id": "7038a6b7070d2ffff88e8b12d43a1e185d739884",
        "title": "Representation learning applications in biological sequence analysis"
      },
      {
        "id": "26e3d58181724f9ef77973ff0f65bac06e499fec",
        "title": "ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation"
      },
      {
        "id": "f386368c0868793b90d7e03b50010af320c21722",
        "title": "deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search"
      },
      {
        "id": "d61387def94f4a21125b143c6db909466574f306",
        "title": "Zero-shot Label-Aware Event Trigger and Argument Classification"
      },
      {
        "id": "0f2f9cbb1b3ab8c5718d64359052d5e71f2d0dc7",
        "title": "Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods"
      },
      {
        "id": "44725678a7e2b48fdcf6ad3b86d1a96dec736a9e",
        "title": "Evaluating pre-trained models for user feedback analysis in software engineering: a study on classification of app-reviews"
      },
      {
        "id": "8d0ebf8d6f6a94a0d85257c010ffe9ba1c162eb0",
        "title": "A Systematic Review of Machine Learning Algorithms in Cyberbullying Detection: Future Directions and Challenges"
      },
      {
        "id": "ff24b920d7a70e209c21f27274c22f8a0d5f6a7f",
        "title": "Pretraining model for biological sequence data"
      },
      {
        "id": "9fca3388b1a7065478eb4992c9217040fcaac982",
        "title": "Sentence Extraction-Based Machine Reading Comprehension for Vietnamese"
      },
      {
        "id": "6fb369c5776cc17273ff74f46c641000e51dca2e",
        "title": "Discriminative Self-training for Punctuation Prediction"
      },
      {
        "id": "e77c8f93bf92bc9198c3b8b981d223bf56aa707f",
        "title": "BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource Language Understanding Evaluation in Bangla"
      },
      {
        "id": "111dbe14083359ab39886790632e7f1421732a8a",
        "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"
      },
      {
        "id": "a07ff593c396078ee91e797d6f03a8c0517a401c",
        "title": "Generating novel protein sequences using Gibbs sampling of masked language models"
      },
      {
        "id": "8c4f89a9ac30cf94186916be1bfaa02dbfb3600d",
        "title": "CoDesc: A Large Code–Description Parallel Dataset"
      },
      {
        "id": "db233bac71c35e222a78a7e730e14b336bf9915e",
        "title": "Towards Automating Code Review Activities"
      },
      {
        "id": "455cdafd55a5b5ddefa029bf97801327e142646d",
        "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios"
      },
      {
        "id": "26da56580fcf8fc6dd5644ede6575bdbbd7caf1a",
        "title": "IndoNLU: Benchmark and Resources for Evaluating Indonesian Natural Language Understanding"
      },
      {
        "id": "1c6970dc9d4da9f5e94399e344fe8ba901d8fe81",
        "title": "PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"
      },
      {
        "id": "1841cf23c65ff2f27f21ba0d2268c3445f20332f",
        "title": "Few-Shot Text Generation with Natural Language Instructions"
      },
      {
        "id": "ca9b4fc03ad3ea4680ab2204ecf215f333c616a4",
        "title": "ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing"
      },
      {
        "id": "b68b2e81ae2de647394ec05ee62ecf108bf2b50a",
        "title": "Eliciting Knowledge from Language Models Using Automatically Generated Prompts"
      },
      {
        "id": "9392f11732c22a1de9db34cf4d8c20d54f544658",
        "title": "Learning the language of viral evolution and escape"
      },
      {
        "id": "5fe78eb0f142902237df11cb67c455787a759172",
        "title": "GLGE: A New General Language Generation Evaluation Benchmark"
      },
      {
        "id": "59c0076b3d814588e320820b95563965733d1875",
        "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"
      },
      {
        "id": "9ef33af1b2ebda2f2edd6c1394f314d7ac2f00f2",
        "title": "TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification"
      },
      {
        "id": "1d95011355628f7aef068ab1914198e43258c530",
        "title": "BERTimbau: Pretrained BERT Models for Brazilian Portuguese"
      },
      {
        "id": "bba5e0ad118f323ff6c72536e2b5e1fb1b9c1ff9",
        "title": "Be More with Less: Hypergraph Attention Networks for Inductive Text Classification"
      },
      {
        "id": "5235570f12dad580ee92156c50d00a4184cb599c",
        "title": "A Survey on Text Classification: From Shallow to Deep Learning"
      },
      {
        "id": "b252328704c5a7d9cebd8e29b0210f3bc2f214a2",
        "title": "Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?"
      },
      {
        "id": "146adb863cf47182080e0c7f0ef6499ef568122b",
        "title": "Text Graph Transformer for Document Classification"
      },
      {
        "id": "9eda9bbb3317452646e23f8341e5a4e5c7e73c58",
        "title": "Do Response Selection Models Really Know What's Next? Utterance Manipulation Strategies for Multi-turn Response Selection"
      },
      {
        "id": "e15982656e43cb880aee0388a9daf06a5f3d6464",
        "title": "A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning"
      },
      {
        "id": "c47611ecff497c01e8bebc1da2ffba4e9b4d9de9",
        "title": "A Survey on Deep Learning and Explainability for Automatic Report Generation from Medical Images"
      },
      {
        "id": "5b2ec7a534cab750c6b00fe491500681ae3b1527",
        "title": "PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data"
      },
      {
        "id": "68f2d74f82ec544433f3936dbbf6b6f5255fee10",
        "title": "An Empirical Study of Tokenization Strategies for Various Korean NLP Tasks"
      },
      {
        "id": "930eaa5486f1e8a5947fa5d7ac48ecfa6f3c42ea",
        "title": "A comparative review on deep learning models for text classification"
      },
      {
        "id": "5eea6a9de39c41715e105f5943ac0fcb98fa245c",
        "title": "Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base"
      },
      {
        "id": "5dfcc1f19a22c3bc081f2ff4410eb1efc7061838",
        "title": "Nested Named Entity Recognition with Partially-Observed TreeCRFs"
      },
      {
        "id": "335a05c16fcbc7a2c2bf5221299de608b08030f0",
        "title": "From Hero to Zéroe: A Benchmark of Low-Level Adversarial Attacks"
      },
      {
        "id": "c968e8dc442102b38b134b1afadc7cc78fc5b5fb",
        "title": "Interpretable Multi-dataset Evaluation for Named Entity Recognition"
      },
      {
        "id": "218762e0983562209b7374405c9e467fac630118",
        "title": "Fully Convolutional Network Bootstrapped by Word Encoding and Embedding for Activity Recognition in Smart Homes"
      },
      {
        "id": "c7c201d81538bf68ff373238597fd2991a0dca44",
        "title": "Profile Prediction: An Alignment-Based Pre-Training Task for Protein Sequence Models"
      },
      {
        "id": "cb6deacd399fb335f89d4308adbec3bab11d1e30",
        "title": "Text classification based on gated recurrent unit combines with support vector machine"
      },
      {
        "id": "d4277569f7238db3d39cc7780e755a4f77558ac1",
        "title": "Bangla Documents Classification using Transformer Based Deep Learning Models"
      },
      {
        "id": "70b0c85638d195dbde56cbedc94ae4363b272b58",
        "title": "A Pre-Training Technique to Localize Medical BERT and to Enhance Biomedical BERT"
      },
      {
        "id": "3deb38dae20efacab76dc93767c02098741e749f",
        "title": "DeNERT-KG: Named Entity and Relation Extraction Model Using DQN, Knowledge Graph, and BERT"
      },
      {
        "id": "3bcb17559ce96eb20fa79af8194f4af0380d194a",
        "title": "Pre-trained models for natural language processing: A survey"
      },
      {
        "id": "7b9b756ab509cb9f52dbac95e3e901d571f0784f",
        "title": "A Survey of the Usages of Deep Learning for Natural Language Processing"
      },
      {
        "id": "d16ab5c19ed33a263b6412ac41a4ea1f068d254a",
        "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing"
      },
      {
        "id": "77b91d7607518994d04f75119db4138b23e2eb87",
        "title": "Natural Language Processing Advancements By Deep Learning: A Survey"
      },
      {
        "id": "4f4202aac8c900efc79ec534f5f3b10b07bb45f5",
        "title": "Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data"
      },
      {
        "id": "18318b10e7c2dd4ad292208f4399eb1d4dca5768",
        "title": "CLUE: A Chinese Language Understanding Evaluation Benchmark"
      },
      {
        "id": "b0b0dddb8310e01b9407a21674c2d33a23a6e967",
        "title": "Byte Pair Encoding is Suboptimal for Language Model Pretraining"
      },
      {
        "id": "406afe68e789fcb0d7e3d24ebea65b53d206f740",
        "title": "Exploring Software Naturalness through Neural Language Models"
      },
      {
        "id": "7cf8510d5905bd8a63f1e098e05ab591d689e0fd",
        "title": "Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction"
      },
      {
        "id": "10467a1466aeec246ac0a577bfc311ec4de110de",
        "title": "Alternating Language Modeling for Cross-Lingual Pre-Training"
      },
      {
        "id": "aa9c6d43b36a55b34c2e9207355d355fd94691af",
        "title": "Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond"
      },
      {
        "id": "82453548b97f78ab2cdb9a8626ff858db9ce5a82",
        "title": "Pre-training Polish Transformer-based Language Models at Scale"
      },
      {
        "id": "df56748cd4f52a58973b4ac52c0bf9156c5f52f0",
        "title": "Unsupervised Translation of Programming Languages"
      },
      {
        "id": "84286c4a614c198593d0e19623cdce318416f212",
        "title": "Named Entity Recognition as Dependency Parsing"
      },
      {
        "id": "d97e7561fa7710213ccd4f8128044ea6849be377",
        "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning"
      },
      {
        "id": "297ad41c0e7264e67ae078921e2a57436293ce72",
        "title": "XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation"
      },
      {
        "id": "23ce8950b9360158c04ab0c1dcf9a73022b60673",
        "title": "Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks"
      },
      {
        "id": "6602100baa3399b5d3a390d6281a7caadb626ea6",
        "title": "Structure-Augmented Text Representation Learning for Efficient Knowledge Graph Completion"
      },
      {
        "id": "56d1003fd02346e93354ab55cd204485c268512a",
        "title": "Compositional Explanations of Neurons"
      },
      {
        "id": "085b360d3c08aaf997f45a78e27f2629f5625205",
        "title": "Translation Artifacts in Cross-lingual Transfer Learning"
      },
      {
        "id": "186f69beeed6ee9b697010863f3a72ba6754a53a",
        "title": "Boundary Enhanced Neural Span Classification for Nested Named Entity Recognition"
      },
      {
        "id": "7907d06a0a8cd2d25480422944c88f66db950d3d",
        "title": "FQuAD: French Question Answering Dataset"
      },
      {
        "id": "12aa5c0c9fa3077f41f153cdccfa2aacf8940be3",
        "title": "From static to dynamic word representations: a survey"
      },
      {
        "id": "76a211ec3b4b96f37038c3993c81597cb1ea7a4a",
        "title": "Morphological Word Segmentation on Agglutinative Languages for Neural Machine Translation"
      },
      {
        "id": "3ae0dc0ac6c45d8fbb40c034f8c29bcbb87652c5",
        "title": "Measuring the Impact of Readability Features in Fake News Detection"
      },
      {
        "id": "2e808fa44bd52e7234f04f9fb8819ff840608586",
        "title": "Morfessor EM+Prune: Improved Subword Segmentation with Expectation Maximization and Pruning"
      },
      {
        "id": "65dbb886849e44d8375ba5ee815dae581d332daf",
        "title": "Banner: A Cost-Sensitive Contextualized Model for Bangla Named Entity Recognition"
      },
      {
        "id": "939bf05b764f5409becc8fe8910d5a39bfa58cde",
        "title": "Q-Bully: A Reinforcement Learning based Cyberbullying Detection Framework"
      },
      {
        "id": "725d5acdbdf0a11677f785a16e1722b92c55a47f",
        "title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization"
      },
      {
        "id": "0d38ca7f3b631e731d6275497cb0230b27610508",
        "title": "SDN2GO: An Integrated Deep Learning Model for Protein Function Prediction"
      }
    ],
    "5": [
      {
        "id": "f1f78bbb3e146874501e3c52b56cff6abf731842",
        "title": "Deep representation learning: Fundamentals, Perspectives, Applications, and Open Challenges"
      },
      {
        "id": "ad7ddcc14984caae308c397f1a589aae75d4ab71",
        "title": "Training data-efficient image transformers & distillation through attention"
      },
      {
        "id": "980d1c3bf9d1a3c0ce68567e0efc1a72f203f12c",
        "title": "Global memory transformer for processing long documents"
      },
      {
        "id": "a7d0e2afdeb11172d444e12489266ebf495ed221",
        "title": "What is Wrong with Language Models that Can Not Tell a Story?"
      },
      {
        "id": "6ed5ff2912cf2d94d21414b1a15d0de86aa00c2f",
        "title": "A survey: object detection methods from CNN to transformer"
      },
      {
        "id": "5152b9391762aefc532f85a801093bd38a6688c6",
        "title": "Hierarchical Graph Transformer with Adaptive Node Sampling"
      },
      {
        "id": "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "title": "Efficient Long-Text Understanding with Short-Text Models"
      },
      {
        "id": "94a96f64bd93ad91642fa04da09bb709a26ac277",
        "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting"
      },
      {
        "id": "741a7faf9dbefd418cda878c61c5b839ecc02977",
        "title": "A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective"
      },
      {
        "id": "661e8d555c4424b5953f17434f2ba910bfcf3afe",
        "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer"
      },
      {
        "id": "9b8a94f3b3ca2a598645970a4dfe248fff20d9b8",
        "title": "Enhance the Visual Representation via Discrete Adversarial Training"
      },
      {
        "id": "ac40f3197f77397fdf07a73f788056efc99028df",
        "title": "3D Vision with Transformers: A Survey"
      },
      {
        "id": "f461669f4e6e843af6478ddcb805093511f649d2",
        "title": "Rethinking Attention Mechanism in Time Series Classification"
      },
      {
        "id": "cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6",
        "title": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI"
      },
      {
        "id": "ecb89bc79a30569f5fae75c8735779f08a58550e",
        "title": "Predicting Future Laboratory Fault Friction Through Deep Learning Transformer Models"
      },
      {
        "id": "1350574ef040fa5313687c601e34720d5ebc4bf4",
        "title": "Vision Transformers: State of the Art and Research Challenges"
      },
      {
        "id": "fa0872e45bf6c0fee528c86120f52490142efba6",
        "title": "Exploring Stochastic Autoregressive Image Modeling for Visual Representation"
      },
      {
        "id": "1d589e172084a1451398f0d6c30592baa8224732",
        "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization"
      },
      {
        "id": "3e5b7f0b64c60ca0bb5ef547e8e3dcc2a568ab75",
        "title": "MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting"
      },
      {
        "id": "e0d9d80dd0ef1a9996feac61d3315f44ec31aca3",
        "title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers"
      },
      {
        "id": "d96db566031b59b476b872320a8c7bbe17f54611",
        "title": "Understanding the Failure of Batch Normalization for Transformers in NLP"
      },
      {
        "id": "bbfe80a347aab7e5d3cb82c6d37e13393fdf8a78",
        "title": "ASiT: Local-Global Audio Spectrogram Vision Transformer for Event Classification"
      },
      {
        "id": "936b00941c6f11ef4102ba21664fbd2fef15dcac",
        "title": "Link of Transformers in CV and NLP: A Brief Survey"
      },
      {
        "id": "6281c40c66febca1d8003bcc6fdfd2189b30c38f",
        "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"
      },
      {
        "id": "f7410f535bda5b5a9888512fb954193245c1d0b2",
        "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images"
      },
      {
        "id": "a601e71127d95bdae30fd818d2a0cc34b80b13f7",
        "title": "Masked Autoencoders for Point Cloud Self-supervised Learning"
      },
      {
        "id": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "title": "cosFormer: Rethinking Softmax in Attention"
      },
      {
        "id": "226fcbe55235d873bedb2fcf5b981bd5ec860d4f",
        "title": "SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer"
      },
      {
        "id": "bdacaef16d45e1e7826bf5191637d436d5a98451",
        "title": "Transformers in Time-Series Analysis: A Tutorial"
      },
      {
        "id": "75c642ebdcfbd4c16d6c3161130b72ff9af5c311",
        "title": "Three things everyone should know about Vision Transformers"
      },
      {
        "id": "5eeb80dc67590422db64ca95ec0aded24799cfb6",
        "title": "Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse"
      },
      {
        "id": "258e214522232a479b103d358969edf5a25d61ad",
        "title": "A Data-scalable Transformer for Medical Image Segmentation: Architecture, Model Efficiency, and Benchmark"
      },
      {
        "id": "022b3d5f684dd6e1b74e0455b5b78a3986c8b69f",
        "title": "Transformers in 3D Point Clouds: A Survey"
      },
      {
        "id": "41b6cc4acedea461646ea85426f4f750a753a33b",
        "title": "A Survey on Attention Mechanisms for Medical Applications: are we Moving Toward Better Algorithms?"
      },
      {
        "id": "a9c72a0aedd209c2f565d09fd46dd27c78585a91",
        "title": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives"
      },
      {
        "id": "fdf23f096e4721b18041c5ccc0a984ebfae72f01",
        "title": "DCT-Former: Efficient Self-Attention with Discrete Cosine Transform"
      },
      {
        "id": "786077a54eee75d0fd74b8565f91b9386a6344cd",
        "title": "Hierarchical Graph Convolutional Networks for Structured Long Document Classification"
      },
      {
        "id": "9690e7056c972e298c6696fa4b1f485b82ded433",
        "title": "FashionVLP: Vision Language Transformer for Fashion Retrieval with Feedback"
      },
      {
        "id": "3ccc6b6f84fec578c215cab35ec06ad384040ea5",
        "title": "Multi-Modal Dynamic Graph Transformer for Visual Grounding"
      },
      {
        "id": "0a1ff1d4102d94a50f8862f60bc2ac21f36ad592",
        "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts"
      },
      {
        "id": "43fae0a7af211d91557d115d2f82e3c46d8bf022",
        "title": "Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation"
      },
      {
        "id": "73d64ecbe3e846394444dab6c5e89ba33e5daa49",
        "title": "Memory transformer with hierarchical attention for long document processing"
      },
      {
        "id": "454304628bf10f02aba1c2cfc95891e94d09208e",
        "title": "Graph Neural Networks with Learnable Structural and Positional Representations"
      },
      {
        "id": "a2a0fc5014423d6283c80511b9272d941109ece2",
        "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation"
      },
      {
        "id": "bd9ab344da99022cbbbfd3f5c9c82a0b21c60ad9",
        "title": "nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer"
      },
      {
        "id": "a9c214e846188adb645021cd7b1964b8ea1fef6f",
        "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"
      },
      {
        "id": "333212e246fb65f7c9d43862021e78f007c48449",
        "title": "A Survey of Visual Transformers"
      },
      {
        "id": "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "title": "SOFT: Softmax-free Transformer with Linear Complexity"
      },
      {
        "id": "5e2180e4ce9d218cccb1c78a93a863d5f967d907",
        "title": "Transformers in computational visual media: A survey"
      },
      {
        "id": "07df27f2c7ecacca99a581efbf4be7a0b3b3ba8b",
        "title": "Gophormer: Ego-Graph Transformer for Node Classification"
      },
      {
        "id": "7e0c7fdad758482375cb89a110b2f5ad4bee57dd",
        "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization"
      },
      {
        "id": "1a8c0bc4520f9836d36e1e09fa8df6b54b4b363e",
        "title": "A Statutory Article Retrieval Dataset in French"
      },
      {
        "id": "48af9b314181b04edcc0b7224ffe4689036b755f",
        "title": "Improving Transformers with Probabilistic Attention Keys"
      },
      {
        "id": "ee09751fab987e60d8d005ef20fe57c5defc408e",
        "title": "MalBERT: Malware Detection using Bidirectional Encoder Representations from Transformers*"
      },
      {
        "id": "203b965e5c9eb1e1c521ec66f82b036335c7cd4d",
        "title": "Shifted Chunk Transformer for Spatio-Temporal Representational Learning"
      },
      {
        "id": "e0cbbca02b332f398c6639b3bea0613f79166220",
        "title": "ABC: Attention with Bounded-memory Control"
      },
      {
        "id": "9583d94fe6297acb3f287329dff6fe09b1c5c4c2",
        "title": "KeyBLD: Selecting Key Blocks with Local Pre-ranking for Long Document Information Retrieval"
      },
      {
        "id": "61df7b79b7c1df8557d9f6a1642a6315cad40d7d",
        "title": "Make A Long Image Short: Adaptive Token Length for Vision Transformers"
      },
      {
        "id": "e1f7478294fe01ce271cdef9ba93f4c675d92dc9",
        "title": "Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama"
      },
      {
        "id": "722ad6ac92286507437b31486f47987d6ece05c9",
        "title": "BEiT: BERT Pre-Training of Image Transformers"
      },
      {
        "id": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "title": "Transformers in Vision: A Survey"
      },
      {
        "id": "7519a1e9e7371df79bd8a21cee871feb0ec597a5",
        "title": "UNETR: Transformers for 3D Medical Image Segmentation"
      },
      {
        "id": "147164a3905f41a7a5a10f732d086a621c9c5862",
        "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer"
      },
      {
        "id": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "title": "Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention"
      },
      {
        "id": "7fff8018bf625447df837c2fda5c58a705fbc038",
        "title": "XCiT: Cross-Covariance Image Transformers"
      },
      {
        "id": "7507603da9711c0e43e29f3094f33d9337e7dfbd",
        "title": "3D Human Pose Estimation with Spatial and Temporal Transformers"
      },
      {
        "id": "2984ab83ade26639c3a82d29628d0d9e4abbebb0",
        "title": "Incorporating Convolution Designs into Visual Transformers"
      },
      {
        "id": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
        "title": "Do Transformers Really Perform Bad for Graph Representation?"
      },
      {
        "id": "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"
      },
      {
        "id": "2e8149dafb864ec3675087c99bf5572fcf4eb170",
        "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"
      },
      {
        "id": "36b9d0f8610a82fd25854889d9327a04da4ff8fd",
        "title": "MST: Masked Self-Supervised Transformer for Visual Representation"
      },
      {
        "id": "166e98317ed9c4687e71bef55a6800431e00b8fa",
        "title": "SiT: Self-supervised vIsion Transformer"
      },
      {
        "id": "8e27d8069035cb22a8c1c50d3971b4a61f0143f4",
        "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting"
      },
      {
        "id": "f4299e47a76d5d7cf1638ba347d9848903ef5a60",
        "title": "Gated Transformer Networks for Multivariate Time Series Classification"
      },
      {
        "id": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"
      },
      {
        "id": "f1937a51c1bbb4b5971179f72d4753ccc206bb88",
        "title": "Dual Transformer for Point Cloud Analysis"
      },
      {
        "id": "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "title": "Finetuning Pretrained Transformers into RNNs"
      },
      {
        "id": "1f668aebd03b5150c2c2fddae48f4a65cb4a80a8",
        "title": "Container: Context Aggregation Network"
      },
      {
        "id": "80bb30e65c36545f7dcaae8fa9f1e66e550d4731",
        "title": "A survey on deep reinforcement learning for audio-based applications"
      },
      {
        "id": "f4566761fe39c4b5273d696d9bc3f4195c9325bb",
        "title": "Long-Span Summarization via Local Attention and Content Selection"
      },
      {
        "id": "fbcbe5a222786f38a1c69c3487b4edf8ca469934",
        "title": "Fully Transformer Networks for Semantic Image Segmentation"
      },
      {
        "id": "ea6beaaa274cf4e8a906690951e6867ff87ded05",
        "title": "HeBERT & HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis and Emotion Recognition"
      },
      {
        "id": "7303d35fd15ac87e9c3f7dc7b8b9ec05cccef95b",
        "title": "Point Cloud Learning with Transformer"
      },
      {
        "id": "f98f32e05fffb50437b556412127a92108d1d0b2",
        "title": "Transformers: \"The End of History\" for NLP?"
      },
      {
        "id": "7fa6c0b5fd534ecf214b634f68a85a60d3b3191f",
        "title": "NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series Forecasting"
      },
      {
        "id": "9d312b231b42c6fe4ff4259a86b28da3d5cb2d86",
        "title": "A method Based on an Attention Mechanism to Measure the Similarity of two Sentences"
      },
      {
        "id": "38d5e7774e79861315e043dc2dd764d051516d74",
        "title": "Language Through a Prism: A Spectral Approach for Multiscale Language Representations"
      },
      {
        "id": "d40c77c010c8dbef6142903a02f2a73a85012d5d",
        "title": "A Survey on Vision Transformer"
      },
      {
        "id": "ff50b46b4e1cc0fd9beb832fc3468785b635a824",
        "title": "PCT: Point cloud transformer"
      },
      {
        "id": "7e5709d81558d3ef4265de29ea75931afeb1f2dd",
        "title": "Efficient Transformers: A Survey"
      },
      {
        "id": "849b88ddc8f8cabc6d4246479b275a1ee65d0647",
        "title": "A Generalization of Transformer Networks to Graphs"
      },
      {
        "id": "c13a8f9edb933e60c7a989244aee56283a54ce37",
        "title": "UP-DETR: Unsupervised Pre-training for Object Detection with Transformers"
      },
      {
        "id": "b1330ac569550ee40afef26d3f989e5bad24d974",
        "title": "Deep Learning for Text Style Transfer: A Survey"
      },
      {
        "id": "d2e54b3a596a1dce0def9d035dfe1fb7c0c6142a",
        "title": "Toward Transformer-Based Object Detection"
      },
      {
        "id": "c36571ac50808c75fa8a5d37f1041af22e89e6ee",
        "title": "GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training"
      },
      {
        "id": "d99e71e1191d4814b725fa71beb0ca67f3282df6",
        "title": "100,000 Podcasts: A Spoken English Document Corpus"
      },
      {
        "id": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
        "title": "Linformer: Self-Attention with Linear Complexity"
      },
      {
        "id": "b45d656ac8cc2e940609580cf291ee76ffcac20a",
        "title": "On Layer Normalization in the Transformer Architecture"
      },
      {
        "id": "d27669c82faf78ea08cceaa0a171b540cccc304d",
        "title": "ETC: Encoding Long and Structured Inputs in Transformers"
      },
      {
        "id": "28307bc149a74cfcae657f782f1c7630b6f4acce",
        "title": "Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task"
      },
      {
        "id": "b9d4d87ea78099da5ba3c4351b0889dc203a9ba2",
        "title": "PowerNorm: Rethinking Batch Normalization in Transformers"
      },
      {
        "id": "168fc3525f7b97695a97b04e257ee9bd1e832acb",
        "title": "Memory Transformer"
      },
      {
        "id": "baed71eed57ad462f3ab138d4b1700a738cd5414",
        "title": "ETC: Encoding Long and Structured Data in Transformers"
      },
      {
        "id": "63857190aaf5aab1d94b54bb257b7b03b8cb5a50",
        "title": "GMAT: Global Memory Augmentation for Transformers"
      },
      {
        "id": "97c5f41275b3dbacec17a6c7340724616713b7e2",
        "title": "Style-transfer and Paraphrase: Looking for a Sensible Semantic Similarity Metric"
      }
    ],
    "7": [
      {
        "id": "a36e9583a0212c11020eefa1d9574512ad9964a4",
        "title": "A pre-trained BERT for Korean medical natural language processing"
      },
      {
        "id": "308b0117b5fb147ab0b9d1c20ed242809170bd6d",
        "title": "Analyzing patient experiences using natural language processing: development and validation of the artificial intelligence patient reported experience measure (AI-PREM)"
      },
      {
        "id": "5e2ebd08d2d683ab4061503b6165e21d9a6cf613",
        "title": "Identification of Preanesthetic History Elements by a Natural Language Processing Engine"
      },
      {
        "id": "1f38f4537f90825f3df6c2b4bc4e42691157b543",
        "title": "Leveraging natural language processing and geospatial time series model to analyze COVID-19 vaccination sentiment dynamics on Tweets"
      },
      {
        "id": "7a51a0d1a8fb0a516a0d20e27d44f82c9fa0f31c",
        "title": "Natural Language Processing for Improved Characterization of COVID-19 Symptoms: Observational Study of 350,000 Patients in a Large Integrated Health Care System"
      },
      {
        "id": "f4a8f86ce62995eccf4db80d37cd4e7952960452",
        "title": "The Role of Natural Language Processing during the COVID-19 Pandemic: Health Applications, Opportunities, and Challenges"
      },
      {
        "id": "f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3",
        "title": "A review on Natural Language Processing Models for COVID-19 research"
      },
      {
        "id": "b6fce90925c977ab1f98f4db9487596c9595f97b",
        "title": "Learning structures of the French clinical language: development and validation of word embedding models using 21 million clinical reports from electronic health records"
      },
      {
        "id": "8e02d13ef468751c2730a2e7c109c2bd070235f8",
        "title": "Automated Rule-Based Data Cleaning Using NLP"
      },
      {
        "id": "033909a3c80f17a38f0bb757ed9c63424b12053a",
        "title": "SynKB: Semantic Search for Synthetic Procedures"
      },
      {
        "id": "b4354536b8259c147301cdc4468aa8fa67432437",
        "title": "How can natural language processing help model informed drug development?: a review"
      },
      {
        "id": "301c6b4117580efba62549c87687ac4bcbe701f2",
        "title": "Natural Language Processing Methods and Bipolar Disorder: Scoping Review"
      },
      {
        "id": "76b09bf664e10ae3a181ced7f0e1a186e6977a20",
        "title": "A smile is all you need: predicting limiting activity coefficients from SMILES with natural language processing"
      },
      {
        "id": "e09860c6b7d882c866b357cf4b5c0ac96b9d4b17",
        "title": "Assessment of medical management in Coronary Type 2 Diabetic patients with previous percutaneous coronary intervention in Spain: A retrospective analysis of electronic health records using Natural Language Processing"
      },
      {
        "id": "5d7db4bf4f5b8f45b6abc48fac41eec3a78ba4c0",
        "title": "Development and evaluation of an interoperable natural language processing system for identifying pneumonia across clinical settings of care and institutions"
      },
      {
        "id": "8a5044ecdd26f3d8510d8705e442ca8498a87c70",
        "title": "A Complete Process of Text Classification System Using State-of-the-Art NLP Models"
      },
      {
        "id": "a4af31815e1efdbbeacc091c62f78ed4401f2fae",
        "title": "Natural Language Processing for Smart Healthcare"
      },
      {
        "id": "c723d5e21e6c0dff278acfa2f847ea9fa46d4cff",
        "title": "Assessing the Performance of Clinical Natural Language Processing Systems: Development of an Evaluation Methodology"
      },
      {
        "id": "67691afe29958fefe70cc903dc78856f63ade678",
        "title": "Natural Language Processing as an Emerging Tool to Detect Late-Life Depression"
      },
      {
        "id": "f5e38f2a2150ff5733e151976641516f369fd637",
        "title": "Natural language processing enabling COVID-19 predictive analytics to support data-driven patient advising and pooled testing"
      },
      {
        "id": "f28f61ae2a8339f496991e7cbc69ff91b3d6d67e",
        "title": "Using natural language processing to understand, facilitate and maintain continuity in patient experience across transitions of care"
      },
      {
        "id": "b528ea72af504f57517e6e4bb5badf8a4d4791aa",
        "title": "ReHouSED: A novel measurement of Veteran housing stability using natural language processing"
      },
      {
        "id": "b5efb15307ea5d6dd9963105a5fe03f95253a1a1",
        "title": "Language Processing Model Construction and Simulation Based on Hybrid CNN and LSTM"
      },
      {
        "id": "cc375e2130cae2bc1f390a6c84a0258bbf9971a8",
        "title": "Applying natural language processing and machine learning techniques to patient experience feedback: a systematic review"
      },
      {
        "id": "ed57d8fea73b2c480aca5e3696316554b07a284c",
        "title": "Deep Learning-Based Natural Language Processing for Screening Psychiatric Patients"
      },
      {
        "id": "4fc735c4d1a5b75519ead032e9208c0e5c2689b5",
        "title": "Natural Language Processing and Machine Learning for Detection of Respiratory Illness by Chest CT Imaging and Tracking of COVID-19 Pandemic in the US"
      },
      {
        "id": "8becf144134e36245e4680a9f166d7582d4fda51",
        "title": "Evaluation of Natural Language Processing for the Identification of Crohn Disease–Related Variables in Spanish Electronic Health Records: A Validation Study for the PREMONITION-CD Project"
      },
      {
        "id": "5598a06c3d3fcd416adcb9537e52c033e754666f",
        "title": "Monitoring COVID-19 on Social Media: Development of an End-to-End Natural Language Processing Pipeline Using a Novel Triage and Diagnosis Approach"
      },
      {
        "id": "07644c0aedf88eec0e848a3329cd924754689a79",
        "title": "Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python"
      },
      {
        "id": "4101fd75aaf775fe659f5c68a1bbf5d1865838ef",
        "title": "Transmol: repurposing a language model for molecular generation"
      },
      {
        "id": "2f04196496122924a6ee0dfa89374130b9f7a539",
        "title": "Web Search Engine Misinformation Notifier Extension (SEMiNExt): A Machine Learning Based Approach during COVID-19 Pandemic"
      },
      {
        "id": "8005e5bee6da81d44602e788100f7050728d0fdf",
        "title": "Generating Synthetic Training Data for Supervised De-Identification of Electronic Health Records"
      },
      {
        "id": "285831411cfb23f9d3cd8956e6bba000189f85af",
        "title": "Artificial Intelligence in the fight against Covid-19"
      },
      {
        "id": "964301fe5fd0d09fb261346238bd3b19c9f6ab94",
        "title": "Banking news-events representation and classification with a novel hybrid model using DistilBERT and rule-based features"
      },
      {
        "id": "a38010318f08d86ae93f0593039c27feececb9f8",
        "title": "Benchmarking Effectiveness and Efficiency of Deep Learning Models for Semantic Textual Similarity in the Clinical Domain: Validation Study"
      },
      {
        "id": "d619e3534b12a12269483336273a47b5b5a538df",
        "title": "Natural Language Processing Reveals Vulnerable Mental Health Support Groups and Heightened Health Anxiety on Reddit During COVID-19: Observational Study"
      },
      {
        "id": "2aec57a5c9698ea7518767489c135c395ea1a7a7",
        "title": "Clinical Characteristics and Prognostic Factors for Intensive Care Unit Admission of Patients With COVID-19: Retrospective Study Using Machine Learning and Natural Language Processing"
      },
      {
        "id": "b03683ddf7917fbb075533231f5d456492feece5",
        "title": "Unmasking the conversation on masks: Natural language processing for topical sentiment analysis of COVID-19 Twitter discourse"
      },
      {
        "id": "494885502d712ebff9d45010a7f23159725ae9cc",
        "title": "Natural Language Processing for Rapid Response to Emergent Diseases: Case Study of Calcium Channel Blockers and Hypertension in the COVID-19 Pandemic"
      },
      {
        "id": "f923c8e30650e2e1df99f54d00c89eccf1f33649",
        "title": "Prediction of Stroke Outcome Using Natural Language Processing-Based Machine Learning of Radiology Report of Brain MRI"
      },
      {
        "id": "f78f7c9e32c401c96966881d45cfc7ce6fc7e8f6",
        "title": "Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP)"
      },
      {
        "id": "b16d9f414aab0d5d00eb2fae0f20b846011be726",
        "title": "Identification of Adverse Drug Event–Related Japanese Articles: Natural Language Processing Analysis"
      },
      {
        "id": "4b3ee579f82457421db9d75147571ab51f95cf6c",
        "title": "Artificial Intelligence in the Battle against Coronavirus (COVID-19): A Survey and Future Research Directions"
      },
      {
        "id": "3d09912d7630b11dcb36af59eff64474ea281d80",
        "title": "The 2019 n2c2/OHNLP Track on Clinical Semantic Textual Similarity: Overview"
      },
      {
        "id": "24f0c288d51ff334ef69868f5ad56c5f67f2038d",
        "title": "COVID-19 SignSym: a fast adaptation of a general clinical NLP tool to identify and normalize COVID-19 signs and symptoms to OMOP common data model"
      },
      {
        "id": "16cca900a850eee1b832937281bb08c84beb86ff",
        "title": "Identification of Semantically Similar Sentences in Clinical Notes: Iterative Intermediate Training Using Multi-Task Learning"
      },
      {
        "id": "63afca040c4cc4aecb9cac5c7c7358e1b0d599d7",
        "title": "Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis"
      },
      {
        "id": "1d00f609e8cbac2f7a67a04e13adf82a096ea689",
        "title": "Predicting Chemical Properties using Self-Attention Multi-task Learning based on SMILES Representation"
      },
      {
        "id": "184583c02f11f95e92bd85f60b4017e15657547e",
        "title": "Natural Language Processing to Extract Meaningful Information from Patient Experience Feedback"
      },
      {
        "id": "c20adaf3c7d193c5bc2ea2b6697e85656e296d74",
        "title": "A Natural Language Processing System for National COVID-19 Surveillance in the US Department of Veterans Affairs"
      },
      {
        "id": "c20dd209bfe5fd9d5935a69ae00f3be8530b40e9",
        "title": "Exploring Chemical Space using Natural Language Processing Methodologies for Drug Discovery"
      },
      {
        "id": "ac8c0c400f30d3c32def1c404025460b727b26ff",
        "title": "An Industrial Approach to Using Artificial Intelligence and Natural Language Processing for Accelerated Document Preparation in Drug Development"
      },
      {
        "id": "65ac8e344171c51adf6511f013310bb2a7e17be1",
        "title": "How to automatically turn patient experience free-text responses into actionable insights: a natural language programming (NLP) approach"
      },
      {
        "id": "5406e153957dd7a165264da6e6e5d81251997404",
        "title": "State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis"
      },
      {
        "id": "82b08336a874e37408ec3c0dbb1014661a3eaaad",
        "title": "Characterizing the Propagation of Situational Information in Social Media During COVID-19 Epidemic: A Case Study on Weibo"
      },
      {
        "id": "07581e49aff7231159c9405195872d3a154733be",
        "title": "Clinical Text Data in Machine Learning: Systematic Review"
      },
      {
        "id": "4e80898c8c79c0bc770f29ac51ccd283b5e0d9de",
        "title": "Predicting COVID-19 in China Using Hybrid AI Model"
      },
      {
        "id": "3809fc1545f9876efd3cf8737662e2f88c609788",
        "title": "Neural Entity Linking: A Survey of Models based on Deep Learning"
      },
      {
        "id": "02809fc23aecf33e3ed95b83d1d03b54fb5c3d0a",
        "title": "An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining"
      },
      {
        "id": "15cbcf124c3b385b605005c37b0da384ba1d1e02",
        "title": "A Smart Chatbot Architecture based NLP and Machine Learning for Health Care Assistance"
      },
      {
        "id": "76912f2d895788189c301286d486b31e5b99a33d",
        "title": "Exploring Transformer Text Generation for Medical Dataset Augmentation"
      },
      {
        "id": "4c1111aadf4452f40268df582cfad1334c526841",
        "title": "Distributed representation and one-hot representation fusion with gated network for clinical semantic textual similarity"
      },
      {
        "id": "3ee7be4353a04ef3ac77913e651f0770b46e7977",
        "title": "Development of a System for Postmarketing Population Pharmacokinetic and Pharmacodynamic Studies Using Real‐World Data From Electronic Health Records"
      },
      {
        "id": "950850e22e42201f152d90dc6f53d53e39d37657",
        "title": "Deep Sentiment Classification and Topic Discovery on Novel Coronavirus or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network Approach"
      }
    ],
    "9": [
      {
        "id": "083909126ac03227be6c0afc21b58cf5acf4d5e7",
        "title": "Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record"
      },
      {
        "id": "b246d524606b2bc6fb8e9093f45c9614f293156e",
        "title": "Discovering social determinants of health from case reports using natural language processing: algorithmic development and validation"
      },
      {
        "id": "e332a20f199bd97217fcfebf9752bb7b25d79584",
        "title": "Towards Natural Language-Based Visualization Authoring"
      },
      {
        "id": "7e6279fcbfbc19c3c40b816812a91510e79bd8bd",
        "title": "Facilitating Conversational Interaction in Natural Language Interfaces for Visualization"
      },
      {
        "id": "1d18f579554549f4784306917674a4bb8322c167",
        "title": "PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English"
      },
      {
        "id": "e5f5a058c26b87dc94b8b0c35e9395bac578969c",
        "title": "Information extraction from electronic medical documents: state of the art and future research directions"
      },
      {
        "id": "a96a7069bd35ac4a2473651a92cccabacce50dfd",
        "title": "Eeny, meeny, miny, moe. How to choose data for morphological inflection."
      },
      {
        "id": "4f451ba06c4c9effd6c4ac0bae222495501a6200",
        "title": "Innovations in Neural Data-to-text Generation"
      },
      {
        "id": "3761612bfd5064f55dc1bf6157f73f9142225167",
        "title": "Frame Detection in German Political Discourses: How Far Can We Go Without Large-Scale Manual Corpus Annotation?"
      },
      {
        "id": "a346971a2e65e1e2f9f749ec7944023d1b6ad2be",
        "title": "Classifying social determinants of health from unstructured electronic health records using deep learning-based natural language processing"
      },
      {
        "id": "77f06e3d1dfac471322392bd60b9d048152280a4",
        "title": "Experimental Standards for Deep Learning in Natural Language Processing Research"
      },
      {
        "id": "b15472739a0a20f7cfa8294122a07e7cf5e2c945",
        "title": "UNLT: Urdu Natural Language Toolkit"
      },
      {
        "id": "042764240d8047b7df96fd6e3519c17076250b9b",
        "title": "Morphological Processing of Low-Resource Languages: Where We Are and What’s Next"
      },
      {
        "id": "dab09b61ca05ff57b1bc6937fa7eee96720098e8",
        "title": "Multilingual Open Text Release 1: Public Domain News in 44 Languages"
      },
      {
        "id": "ed86431b0bbde0121cbe81b49fd5ac9ffcd6bf16",
        "title": "Interactive Data Analysis with Next-step Natural Language Query Recommendation"
      },
      {
        "id": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
        "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text"
      },
      {
        "id": "7b3963d9ba52bfccd40ac1dba060e7a46a8f581c",
        "title": "Extracting social determinants of health from electronic health records using natural language processing: a systematic review"
      },
      {
        "id": "cb0de2de79533d4faada3d745f43702eb89d1a60",
        "title": "Reusable Templates and Guides For Documenting Datasets and Models for Natural Language Processing and Generation: A Case Study of the HuggingFace and GEM Data and Model Cards"
      },
      {
        "id": "096618aba19bce1d917df488a04891ab239cffdd",
        "title": "Towards Natural Language Interfaces for Data Visualization: A Survey"
      },
      {
        "id": "98bb0a6a7944afe38dd2ac2505dd8057bb507577",
        "title": "nvBench: A Large-Scale Synthesized Dataset for Cross-Domain Natural Language to Visualization Task"
      },
      {
        "id": "294a3bbfaa5382c7b475ebd283aac7ad02e2d075",
        "title": "A Differentiable Language Model Adversarial Attack on Text Classifiers"
      },
      {
        "id": "2406b1ae0be691f235a824e5a366842c633665d1",
        "title": "Unifying Privacy Policy Detection"
      },
      {
        "id": "bdaa46edaf6596a3c67cd23d5f140d3ab0fb85e1",
        "title": "On the Difficulty of Translating Free-Order Case-Marking Languages"
      },
      {
        "id": "df2f4e150765ce15cd7956266d8d958b37676201",
        "title": "Natural Language to Visualization by Neural Machine Translation"
      },
      {
        "id": "82451ff3ac88da3c206f75abcbfe7389a4d89268",
        "title": "Talk2Data: A Natural Language Interface for Exploratory Visual Analysis via Question Decomposition"
      },
      {
        "id": "e7735be4f0fc427515fd7206ebc714356b97e71a",
        "title": "The Benchmark Lottery"
      },
      {
        "id": "b53c386b7c65af80905dc05a9b27e98e03324739",
        "title": "Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing"
      },
      {
        "id": "70791eb3e2b87f361f8977eafd78f8d3e1b5affb",
        "title": "BNLP: Natural language processing toolkit for Bengali language"
      },
      {
        "id": "dc551a804c50cc9f1952926ae4a6365db61d8fc4",
        "title": "A survey on extraction of causal relations from natural language text"
      },
      {
        "id": "6803adc7d8b891be652d18815f830f7a42a0f5b5",
        "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets"
      },
      {
        "id": "a0f788f6de0fb83d623c875a98120e3f347f70d1",
        "title": "Biomedical and clinical English model packages for the Stanza Python NLP library"
      },
      {
        "id": "a0fcced57c26ff5dad30e7a404a84ea26844764e",
        "title": "A clinical trials corpus annotated with UMLS entities to enhance the access to evidence-based medicine"
      },
      {
        "id": "f9ff9dfab3a1e6b78ef625ad0b1618dbca19f78f",
        "title": "Named Entity Recognition of Traditional Chinese Medicine Patents Based on BiLSTM-CRF"
      },
      {
        "id": "acf67a1ac768cfda4cb25ff6f927cc5de813b2d3",
        "title": "The Human Evaluation Datasheet 1.0: A Template for Recording Details of Human Evaluation Experiments in NLP"
      },
      {
        "id": "33fa38d80ca900dafe708747f062b75f6e4abd96",
        "title": "Us vs. Them: A Dataset of Populist Attitudes, News Bias and Emotions"
      },
      {
        "id": "408032f4a72d58e6733140df571bafab5c1e334d",
        "title": "Cross-lingual alignments of ELMo contextual embeddings"
      },
      {
        "id": "9522f69a033fbd5ad200a775c983e05b5cab5e95",
        "title": "Gamified crowdsourcing for idiom corpora construction"
      },
      {
        "id": "8b20173b98914f36302389e4c761c334fe867dcd",
        "title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts"
      },
      {
        "id": "581e386b5971429cb60d3594076dc365c7a54827",
        "title": "Extracting Family History Information From Electronic Health Records: Natural Language Processing Analysis"
      },
      {
        "id": "8e8c9e907f9be0c098d90eec387da88dd0111238",
        "title": "Targeting the Benchmark: On Methodology in Current Natural Language Processing Research"
      },
      {
        "id": "9d0130a8f9677311cdc7a1e353fe586afe2ccd58",
        "title": "Story Analysis Using Natural Language Processing and Interactive Dashboards"
      },
      {
        "id": "47a2723639bd29591472c64e3c47cbffad4eecdf",
        "title": "Extending Drag-and-Drop Actions-Based Model-to-Model Transformations with Natural Language Processing"
      },
      {
        "id": "22735a48750b1f3de43b53d44c94d8642d19d103",
        "title": "Sentence, Phrase, and Triple Annotations to Build a Knowledge Graph of Natural Language Processing Contributions—A Trial Dataset"
      },
      {
        "id": "5593676873d799a4727123a2cbffb231d3b4eb80",
        "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries"
      },
      {
        "id": "a778f025f2bfdd628d622bff50dbccde7e3a7192",
        "title": "KLPT – Kurdish Language Processing Toolkit"
      },
      {
        "id": "09bf4d005cb334e38bc28c5ad2d4f21d3a1d13cc",
        "title": "Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus"
      },
      {
        "id": "40330055346d9ace82a8cdefd9d8cd757cf276d1",
        "title": "N-LTP: A Open-source Neural Chinese Language Technology Platform with Pretrained Models"
      },
      {
        "id": "4d1367141eb8210a981428f3a7fdbdbc9db3e248",
        "title": "A Tokenization System for the Kurdish Language"
      },
      {
        "id": "07eb06b8d8480f130cc28734b022cadcafe336d6",
        "title": "Privacy Policies over Time: Curation and Analysis of a Million-Document Dataset"
      },
      {
        "id": "0bd100685a492501d3f5f82a383edb3b5ded0f06",
        "title": "GREEK-BERT: The Greeks visiting Sesame Street"
      },
      {
        "id": "7b81b5a3bd6214ed813fa5f427e77e78142ee91b",
        "title": "The Pragmatics behind Politics: Modelling Metaphor, Framing and Emotion in Political Discourse"
      },
      {
        "id": "c8752767677efc3a3514dc52a7ac1973b08fb774",
        "title": "Crosscast: Adding Visuals to Audio Travel Podcasts"
      },
      {
        "id": "c30cbf22b946ac094fa4b543bad08bbbba7500ed",
        "title": "IGT2P: From Interlinear Glossed Texts to Paradigms"
      },
      {
        "id": "d3acb7e3ed1156d9c876a5866435e3e7b0213a22",
        "title": "Leveraging Multilingual News Websites for Building a Kurdish Parallel Corpus"
      },
      {
        "id": "3888cc7d41469be1adba7f942606336539262860",
        "title": "Enhancing deep neural networks with morphological information"
      },
      {
        "id": "641a9749fe546a02bbab9a86bfc91492db1c3bc5",
        "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"
      },
      {
        "id": "5c2117f244d38f6b6c6b04aaa2e4f122f7a883e7",
        "title": "What are We Depressed about When We Talk about COVID19: Mental Health Analysis on Tweets Using Natural Language Processing"
      },
      {
        "id": "654e2c9b5c1602e5fb0b746a25cd5b3e29063332",
        "title": "NLPContributions: An Annotation Scheme for Machine Reading of Scholarly Contributions in Natural Language Processing Literature"
      },
      {
        "id": "b044395ed89de33b43d304c008d3c5a7727d423d",
        "title": "SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection"
      },
      {
        "id": "ebeed3d81649ab67e6220d6db0c2c361cbc20784",
        "title": "Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy Policies"
      },
      {
        "id": "9b7c59f69324dadc7cd3fef4071448a7f76ae97b",
        "title": "MorphAGram, Evaluation and Framework for Unsupervised Morphological Segmentation"
      },
      {
        "id": "3bf903f6806a268d57920e5714ba6a05a1f55fa1",
        "title": "Unsupervised Morphological Paradigm Completion"
      },
      {
        "id": "e52220473fe88febf6c2336c1cc8c1d1c05c6f13",
        "title": "Interweaving Multimodal Interaction With Flexible Unit Visualizations for Data Exploration"
      }
    ],
    "11": [
      {
        "id": "31133c1964c7dfd696e6c3651e0b6653f873c750",
        "title": "A Survey on NLP Resources, Tools, and Techniques for Marathi Language Processing"
      },
      {
        "id": "60d4da04889eb33268ebf58b3b8be730d2877ceb",
        "title": "RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use"
      },
      {
        "id": "85fdfa8cf883017b6613602c6558846951ec134d",
        "title": "A Medical Information Extraction Workbench to Process German Clinical Text"
      },
      {
        "id": "9da92d387cbc09c9b68e30a52ff472a83f4283d0",
        "title": "Critical assessment of transformer-based AI models for German clinical notes"
      },
      {
        "id": "b7ed145a0c2e39772cdc5f1e94b73ad8e49c6387",
        "title": "MonkeyPox2022Tweets: A Large-Scale Twitter Dataset on the 2022 Monkeypox Outbreak, Findings from Analysis of Tweets, and Open Research Questions"
      },
      {
        "id": "154624f1d15472cacbc8c4eeb55126e903d061e7",
        "title": "Detection of Depression Severity Using Bengali Social Media Posts on Mental Health: Study Using Natural Language Processing Techniques"
      },
      {
        "id": "645a317c9305207e95d03b5756a65e7e850f32d5",
        "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus"
      },
      {
        "id": "114324581055b8f33fb9f4f16a52f1db95c69046",
        "title": "A Fine-Tuned BERT-Based Transfer Learning Approach for Text Classification"
      },
      {
        "id": "46258ab268e3d940e72ca9370899924f76e7ef38",
        "title": "Part of speech tagging: a systematic review of deep learning and machine learning approaches"
      },
      {
        "id": "96fb0630238edd4a40e3497236bce711cf66b3be",
        "title": "Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi"
      },
      {
        "id": "aa14eb5c8eb4021b073b473b0c0e2e60eb1f14f5",
        "title": "GERNERMED++: Transfer Learning in German Medical NLP"
      },
      {
        "id": "25f102a11fbfa6663d56664ac87ad789ce7bd3c4",
        "title": "Comparison of Pretraining Models and Strategies for Health-Related Social Media Text Classification"
      },
      {
        "id": "78e8400bdffb9901826f16cd6472661064be37fd",
        "title": "A Computational Look at Oral History Archives"
      },
      {
        "id": "04bac77744499c79650daeffa37d0f641e17de7f",
        "title": "FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning"
      },
      {
        "id": "b94eb745b8b7376294aa492b3aa91dc3f7421047",
        "title": "A Trade-off between ML and DL Techniques in Natural Language Processing"
      },
      {
        "id": "bb8d2aa9ef06ecc830828dee92bfdeb46e14e25f",
        "title": "L3CubeMahaSent: A Marathi Tweet-based Sentiment Analysis Dataset"
      },
      {
        "id": "f76a2774f2b3189f80a998869d5d3fb28eb2b092",
        "title": "Experimental Evaluation of Deep Learning models for Marathi Text Classification"
      },
      {
        "id": "b01b962620a1627dbdd0d202eda9e7185fa8f2e0",
        "title": "Highly accurate classification of chest radiographic reports using a deep learning natural language model pre-trained on 3.8 million text reports"
      },
      {
        "id": "f0520b991c1a16449a95ff98771d3cf86ed71428",
        "title": "GottBERT: a pure German Language Model"
      },
      {
        "id": "0d965ed237a3b4592ecefdb618c29f63adedff76",
        "title": "Towards Debiasing Sentence Representations"
      },
      {
        "id": "22dc2a13b77c221c52a4bff9c8561052cab7e1f8",
        "title": "GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines"
      },
      {
        "id": "1d0b7260eb159cb91e5d232bb656afe9f4c5a20f",
        "title": "Evaluating Bias In Dutch Word Embeddings"
      },
      {
        "id": "3e83d34b61584df960d900290a9b5038968817c1",
        "title": "Assessment of DistilBERT performance on Named Entity Recognition task for the detection of Protected Health Information and medical concepts"
      },
      {
        "id": "ff7d8f21439ff1c723255adca1253402d482ae80",
        "title": "Part-of-Speech Tagging via Deep Neural Networks for Northern-Ethiopic Languages"
      },
      {
        "id": "c85b3b15845b87a1af9eddcc55191d647d3b1393",
        "title": "Nepali POS Tagging Using Deep Learning Approaches"
      },
      {
        "id": "bdd0272a648a52f28550e4ec8580bb091182bbc3",
        "title": "Deep Learning Techniques for Part of Speech Tagging by Natural Language Processing"
      },
      {
        "id": "634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b",
        "title": "RobBERT: a Dutch RoBERTa-based Language Model"
      },
      {
        "id": "20895098dedfb656f333834319db729df7a9e0fa",
        "title": "Intra-Processing Methods for Debiasing Neural Networks"
      },
      {
        "id": "f4e7e312e1e2b462936534fd566fb4a16e0d29d2",
        "title": "Text classification models for the automatic detection of nonmedical prescription medication use from social media"
      },
      {
        "id": "309f8ec8a6999e291b2ae87bc06fe5e7313a29f1",
        "title": "Parts-of-Speech tagging for Malayalam using deep learning techniques"
      },
      {
        "id": "6e0004931905f649754b64a947dc05db5473cff5",
        "title": "Machine Learning Approaches for Amharic Parts-of-speech Tagging"
      },
      {
        "id": "211890e5c88c4221d93db665c41a7c30b3a3ac6b",
        "title": "Implementation of Automated Bengali Parts of Speech Tagger: An Approach Using Deep Learning Algorithm"
      }
    ],
    "10": [
      {
        "id": "f9d01f8ab34bb707afdee382350f7ce23534c2d9",
        "title": "AraLegal-BERT: A pretrained language model for Arabic Legal text"
      },
      {
        "id": "ae936e99875c5598f80c5ddcb0609bdc6946e97a",
        "title": "Towards improving e-commerce customer review analysis for sentiment detection"
      },
      {
        "id": "f32adfa48111ccee0845bc22c5e3c17988d36e8b",
        "title": "Arabic Aspect-Based Sentiment Classification Using Seq2Seq Dialect Normalization and Transformers"
      },
      {
        "id": "fa7a651ecd90e64c0cc309417b96891044b32a06",
        "title": "Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets"
      },
      {
        "id": "61d71388025347124a2bbd09804c09bff7023347",
        "title": "Transformers for Tabular Data Representation: A Tutorial on Models and Applications"
      },
      {
        "id": "f127606e987a6c7bb8851bfd3dc8172fe4925687",
        "title": "ANEC: An Amharic Named Entity Corpus and Transformer Based Recognizer"
      },
      {
        "id": "bfb33894af77576cc6975b25f93a247633119b9f",
        "title": "Transformer-Based Language Models for Software Vulnerability Detection"
      },
      {
        "id": "61304013c1e2babe302fc065ec027156268218d5",
        "title": "TCE at Qur’an QA 2022: Arabic Language Question Answering Over Holy Qur’an Using a Post-Processed Ensemble of BERT-based Models"
      },
      {
        "id": "212ad044aec57eec03984ef70bc6eb628825f599",
        "title": "Transfer Learning for Sentiment Analysis Using BERT Based Supervised Fine-Tuning"
      },
      {
        "id": "8583e6f2dd9347c23078e55e48f86754df11dcd7",
        "title": "Multimodal Hate Speech Detection from Bengali Memes and Texts"
      },
      {
        "id": "15c5cb27b470e953323a6f763fd85d1697b3ea9d",
        "title": "The design, construction and evaluation of annotated Arabic cyberbullying corpus"
      },
      {
        "id": "3b66811896a6f96cec8e9eef9d83c410b14be482",
        "title": "CS-UM6P at SemEval-2022 Task 6: Transformer-based Models for Intended Sarcasm Detection in English and Arabic"
      },
      {
        "id": "de914efd2b095686f2cfbc743d40ca49693c99e2",
        "title": "A New Dataset for Topic-Based Paragraph Classification in Genocide-Related Court Transcripts"
      },
      {
        "id": "1d2d8449d9f390724abab01de12deade1c5807ce",
        "title": "Towards Responsible Natural Language Annotation for the Varieties of Arabic"
      },
      {
        "id": "fb5855e68b029f1391bea0cc4d8b1ca1e7335bb2",
        "title": "BERT Models for Arabic Text Classification: A Systematic Review"
      },
      {
        "id": "8b430ae5af9d7991cb3e698b2b30296fdf43dd15",
        "title": "Five sources of bias in natural language processing"
      },
      {
        "id": "a50ee01a66aa92b664a78b2a0628ac346f53e31d",
        "title": "ClimateBert: A Pretrained Language Model for Climate-Related Text"
      },
      {
        "id": "3979141ef17f9bd2b8a5c290e33775f90a14154c",
        "title": "DziriBERT: a Pre-trained Language Model for the Algerian Dialect"
      },
      {
        "id": "c4e98b8e4482b166750e4a211a99632f08778c34",
        "title": "Sentiment Analysis for Thai Language in Hotel Domain Using Machine Learning Algorithms"
      },
      {
        "id": "2743e66939b30c43affb3c9e31f20cfac2109045",
        "title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks"
      },
      {
        "id": "afa135f0c733fceb045599a2190833a810c2cfe0",
        "title": "A Convolutional Stacked Bidirectional LSTM with a Multiplicative Attention Mechanism for Aspect Category and Sentiment Detection"
      },
      {
        "id": "300e65aec7bda43e0849be3dd67acfde018228b5",
        "title": "The Arabic Parallel Gender Corpus 2.0: Extensions and Analyses"
      },
      {
        "id": "541502d193616b7d3c5fba2fb252a8d86698b99f",
        "title": "Pre-trained Transformer-Based Approach for Arabic Question Answering : A Comparative Study"
      },
      {
        "id": "d1d3dde91e3e73ccfbeb176f3af565a4507be077",
        "title": "AI-Based Misogyny Detection from Arabic Levantine Twitter Tweets"
      },
      {
        "id": "7f5d5e032011ec4f3f6b2a414ba5a5c0a8b696f6",
        "title": "Security Vulnerability Detection Using Deep Learning Natural Language Processing"
      },
      {
        "id": "ddf9a64f822244e8fa42e88142a28d8fa8e9ec0a",
        "title": "A Survey of Offensive Language Detection for the Arabic Language"
      },
      {
        "id": "983921bd0ccaee71df7580ce13dd0d53dba5f368",
        "title": "Empathetic BERT2BERT Conversational Model: Learning Arabic Language Generation with Little Data"
      },
      {
        "id": "3f7623f5c6d62c7dd45b1e592c5e03b7f32450c9",
        "title": "The Modern Greek Language on the Social Web: A Survey of Data Sets and Mining Applications"
      },
      {
        "id": "98bb75dcb7dfe8e675781fe2008170e8f00a5dee",
        "title": "FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information"
      },
      {
        "id": "552dd5274a7caeeb0052f6827811b798b71b8bff",
        "title": "Arabic Fake News Detection: Comparative Study of Neural Networks and Transformer-Based Approaches"
      },
      {
        "id": "6e2a57b46a96694e6dfd3c65796c877dd0094f3d",
        "title": "QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus"
      },
      {
        "id": "ae67be32398f6100bff7d47e5eefb8155e7e2412",
        "title": "Intelligent Detection of False Information in Arabic Tweets Utilizing Hybrid Harris Hawks Based Feature Selection and Machine Learning Models"
      },
      {
        "id": "68713fdc2b2e7182548c8d7d3a247bddb79c51b4",
        "title": "Domain Adaptation for Arabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding"
      },
      {
        "id": "724cc225a0171bcde03e06e83b64bc21d9aa5460",
        "title": "Combining Context-Free and Contextualized Representations for Arabic Sarcasm Detection and Sentiment Identification"
      },
      {
        "id": "980fe68a95a38779e87126492a4cedf084f42f73",
        "title": "BERT Multilingual and Capsule Network for Arabic Sentiment Analysis"
      },
      {
        "id": "618a84ea5d9ac23c6a93961ba798154026754ddd",
        "title": "A panoramic survey of natural language processing in the Arab world"
      },
      {
        "id": "659c7a93a05b96dc803bb86e5522369bdeb173fa",
        "title": "Analyzing Sustainability Reports Using Natural Language Processing"
      },
      {
        "id": "523d9b25339be038d7c58f99e5dfb713c3f36be4",
        "title": "ARBML: Democritizing Arabic Natural Language Processing Tools"
      },
      {
        "id": "4bff523f34599564e2909731267c27b1ce5e4d9b",
        "title": "Comparison of Deep Learning Models and Various Text Pre-Processing Techniques for the Toxic Comments Classification"
      },
      {
        "id": "1e4cda8be54999ced1324777fa462a85e2c9746c",
        "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic"
      },
      {
        "id": "a554a0aae55be5597de8f6ece0a4dd0bd5a0e5f4",
        "title": "A Survey on Text Classification: From Traditional to Deep Learning"
      },
      {
        "id": "802b32fdd960961371136fdd8b2ffabd3db30701",
        "title": "Deep learning CNN–LSTM framework for Arabic sentiment analysis using textual information shared in social networks"
      },
      {
        "id": "b1b848860dc03366665fdcafc96a8c096d8c80ef",
        "title": "Load What You Need: Smaller Versions of Mutlilingual BERT"
      },
      {
        "id": "2f4898cd30ac6758cc510d3a99346e8ec78a4c52",
        "title": "Combining BERT with Static Word Embeddings for Categorizing Social Media"
      },
      {
        "id": "086e6733e5fda70bbce0c7545bd06d5634918a60",
        "title": "CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims"
      },
      {
        "id": "995ec006ac98a697ea38bd4eea8c1f3170a8adb4",
        "title": "CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing"
      },
      {
        "id": "f436151792ca141055dc4db8f9aece826a26a447",
        "title": "Virtual learning communities (VLCs) rethinking: influence on behavior modification—bullying detection through machine learning and natural language processing"
      },
      {
        "id": "1359d2ef45f1550941e22bf046026c89f6edf315",
        "title": "AraBERT: Transformer-based Model for Arabic Language Understanding"
      },
      {
        "id": "be158d5ab493b2f2dae736a2ca92afcd66ed5be4",
        "title": "ParsBERT: Transformer-based Model for Persian Language Understanding"
      },
      {
        "id": "6551f742b825561d26242ca8a646ba0e33fb109f",
        "title": "What the [MASK]? Making Sense of Language-Specific BERT Models"
      },
      {
        "id": "407ebb66dfe7d36a9d74168ce5ee4343e141a546",
        "title": "Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network"
      },
      {
        "id": "309527c1320b3673796e7df0c88a7832946572f3",
        "title": "Understanding the perception of COVID-19 policies by mining a multilanguage Twitter dataset"
      },
      {
        "id": "19de5f5ba7d4d3d38e53deeb9b2cde54e9c0d66a",
        "title": "Intelligent detection of hate speech in Arabic social network: A machine learning approach"
      },
      {
        "id": "52347be1d7fc47815e624cd896b8942beb62c774",
        "title": "Evidence Inference 2.0: More Data, Better Models"
      },
      {
        "id": "93928395177b28ffb5ecf4030bad61136d17bb08",
        "title": "Beheshti-NER: Persian named entity recognition Using BERT"
      },
      {
        "id": "31c21da64f5f416d0f6e5505debd13776f81e8d6",
        "title": "Task-Optimized Word Embeddings for Text Classification Representations"
      }
    ]
  },
  "cluster_keywords": {
    "0": [
      "trained",
      "ai",
      "attention",
      "phishing",
      "lingual"
    ],
    "6": [
      "privacy",
      "deep",
      "speech",
      "sentiment",
      "corpora"
    ],
    "4": [
      "knowledge",
      "parsing",
      "deep",
      "bert",
      "supervised"
    ],
    "2": [
      "health",
      "biomedical",
      "supervised",
      "patients",
      "semantics"
    ],
    "8": [
      "augmentation",
      "robust",
      "speech",
      "training",
      "languages"
    ],
    "3": [
      "bert",
      "compression",
      "accelerator",
      "trained",
      "scalable"
    ],
    "1": [
      "translation",
      "dialogue",
      "encoder",
      "supervised",
      "generative"
    ],
    "5": [
      "vision",
      "visual",
      "networks",
      "supervised",
      "graphnorm"
    ],
    "7": [
      "covid",
      "pandemic",
      "patients",
      "coronary",
      "lstm"
    ],
    "9": [
      "morphological",
      "english",
      "nlpcontributions",
      "documenting",
      "talk2data"
    ],
    "11": [
      "german",
      "dutch",
      "classification",
      "health",
      "clinical"
    ],
    "10": [
      "dialect",
      "classification",
      "transformer",
      "speech",
      "persian"
    ]
  }
}