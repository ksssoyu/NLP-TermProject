{
  "window": "2021_2023",
  "num_clusters": 10,
  "cluster_details": {
    "1": [
      {
        "id": "7f6cdece764b56262e910a801f8c133e6c27b7ab",
        "title": "Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation"
      },
      {
        "id": "88bddfb7d1e0462be8fe99fdbd71c658140cb17b",
        "title": "From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"
      },
      {
        "id": "e86a0320e4e22b5a4dc673fed10268cdce520526",
        "title": "Cook2LTL: Translating Cooking Recipes to LTL Formulae using Large Language Models"
      },
      {
        "id": "9cbbb250a565228ba328038ee7944b89cff53e84",
        "title": "Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning"
      },
      {
        "id": "051a7d88d82734bb143341815b5cceb1f9014f54",
        "title": "Human Motion Generation: A Survey"
      },
      {
        "id": "68fd6cc9b41291d625b41761149016be6485c0b3",
        "title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey"
      },
      {
        "id": "271de6816367398ff5c3619e3b061b414388cd8c",
        "title": "Information Flow Control in Machine Learning through Modular Model Architecture"
      },
      {
        "id": "8e9b649f6223caec508d55b0e6574a454c637a26",
        "title": "Revisiting the Role of Language Priors in Vision-Language Models"
      },
      {
        "id": "4ca824e792f3a2c777ffa5896a2e7cdf11b9518d",
        "title": "A Survey of Diffusion Models in Natural Language Processing"
      },
      {
        "id": "ac47bd3b512301371fc87c68416befce6589912e",
        "title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning"
      },
      {
        "id": "1b492746ee3a304a13950cad1a59861b9ee44645",
        "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?"
      },
      {
        "id": "cff26bda86237d113ed01c812ad8bedd0afbe070",
        "title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"
      },
      {
        "id": "094490a5035c6c151b45d0ad5961186ab0eeb5b4",
        "title": "Diffusion Models in NLP: A Survey"
      },
      {
        "id": "2c6ac935c826002976722ca8d3319f691975687e",
        "title": "Self-conditioned Embedding Diffusion for Text Generation"
      },
      {
        "id": "f0b8479d2d09dda3a07aa6fc2149dbfcd0f4608b",
        "title": "DiffusER: Discrete Diffusion via Edit-based Reconstruction"
      },
      {
        "id": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "id": "c3382fd533b9dd7f8ed7ba7766159079bc1d3935",
        "title": "BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"
      },
      {
        "id": "329960d5f4287077cdc258d49a66bdf277c1ee29",
        "title": "Natural Language Supervision For General-Purpose Audio Representations"
      },
      {
        "id": "ec592e12f45e20819afe203164bbbd0de8990510",
        "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis"
      },
      {
        "id": "338d8f3b199abcebc85f34016b0162ab3a9d5310",
        "title": "A Survey on Model Compression for Large Language Models"
      },
      {
        "id": "7d970b5e773aaa317d3b522eefae77943c6847d0",
        "title": "Are Natural Domain Foundation Models Useful for Medical Image Classification?"
      },
      {
        "id": "914a0f5e7eb98842f220a5082dba4f9382086f27",
        "title": "Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation"
      },
      {
        "id": "76f676cfdb9129e881f1e15f78ae373dee4d6d9d",
        "title": "Survey of Social Bias in Vision-Language Models"
      },
      {
        "id": "cc25dc73b0a479094744b2cbf24e84d0b109e4f4",
        "title": "REAL: Resilience and Adaptation using Large Language Models on Autonomous Aerial Robots"
      },
      {
        "id": "86fd2a793a26f3ba9e6082b01eedf401fe613776",
        "title": "Language Integration in Remote Sensing: Tasks, datasets, and future directions"
      },
      {
        "id": "debee48621164a721b5928729897a89682222654",
        "title": "Contextual Prompt Learning for Vision-Language Understanding"
      },
      {
        "id": "b24488203f61e413fe2497957a33780e149b6feb",
        "title": "WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model"
      },
      {
        "id": "2a3da709f2ed0d594f1fa6efce2a8cbba20980a2",
        "title": "Transferable Models for Bioacoustics with Human Language Supervision"
      },
      {
        "id": "1b90e9e9734bed6b379ae87d688cb3b887baf597",
        "title": "Objaverse-XL: A Universe of 10M+ 3D Objects"
      },
      {
        "id": "83c48aa341850af478247e3b34ba1ee1db9f1236",
        "title": "Meta-Transformer: A Unified Framework for Multimodal Learning"
      },
      {
        "id": "ce9c0935c074a0ca8769f13fd4e8651cee263112",
        "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning"
      },
      {
        "id": "6140211405f9917ded519da50f00eee989eabd7f",
        "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis"
      },
      {
        "id": "cbe16171a8cf3dc5488084ee40bbe5866bdfe3f9",
        "title": "Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders"
      },
      {
        "id": "85f163f55f0aa42664c0393325e56dfea05fe08e",
        "title": "HiLM-D: Enhancing MLLMs with Multi-scale High-Resolution Details for Autonomous Driving"
      },
      {
        "id": "6eb3dd2b64db74f9743802acbb9875bdffb9d246",
        "title": "Text-guided Foundation Model Adaptation for Pathological Image Classification"
      },
      {
        "id": "9e99648c5d4d9ce4fba73007291bbd3f804c83ea",
        "title": "ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation"
      },
      {
        "id": "47997f6fb2eb5bcc59a0266d11d07f6c08233d65",
        "title": "A Survey on Transferability of Adversarial Examples across Deep Neural Networks"
      },
      {
        "id": "be974844cd1e5a441fcfebcff62f72e48af46f63",
        "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges"
      },
      {
        "id": "c4cb9f0145cebb169fbdf235fb1f76e1e5b82e72",
        "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings"
      },
      {
        "id": "804356bb7e8cc7b8735cb4a3cf37d5a4de315742",
        "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs"
      },
      {
        "id": "5311db0b04b95fa43b886387fb1f484055638660",
        "title": "Modal-aware Visual Prompting for Incomplete Multi-modal Brain Tumor Segmentation"
      },
      {
        "id": "f57fae177e4e6e0a2b306b7b6bef899821ce9545",
        "title": "Instruct Me More! Random Prompting for Visual In-Context Learning"
      },
      {
        "id": "99b0c3a18050889c591e1db6d51ca01298638437",
        "title": "Transformers in Reinforcement Learning: A Survey"
      },
      {
        "id": "91e3906550821c4624146e6e87db36c3296e773a",
        "title": "Applications of Large Scale Foundation Models for Autonomous Driving"
      },
      {
        "id": "0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3",
        "title": "Text-to-Image Generation for Abstract Concepts"
      },
      {
        "id": "fb1708cb3af4315b3e46ba832047a7ff03bf83cd",
        "title": "Deep learning and knowledge graph for image/video captioning: A review of datasets, evaluation metrics, and methods"
      },
      {
        "id": "6d122357d5d94f88a65f6d8bf83a95ec4869a12d",
        "title": "DEVELOPMENT OF A QUESTION ANSWERING CHATBOT FOR BLOCKCHAIN DOMAIN"
      },
      {
        "id": "9c93f65f5bffad8362537b61310babbc5f1af3ac",
        "title": "DeepSA: a deep-learning driven predictor of compound synthesis accessibility"
      },
      {
        "id": "644134880884afb8d4b0ec174d7bea6cd204072c",
        "title": "Multimodal Bias: Assessing Gender Bias in Computer Vision Models with NLP Techniques"
      },
      {
        "id": "abca18159e0836406a648414cd4275715f3cc12e",
        "title": "Fast Sampling via Discrete Non-Markov Diffusion Models with Predetermined Transition Time"
      },
      {
        "id": "5b97c7ba8dc762d917af5a486ca76e352c8f2a33",
        "title": "Assessment of the E3C corpus for the recognition of disorders in clinical texts"
      },
      {
        "id": "04353fca556a75a6ad96862398d684772000026f",
        "title": "Inversion-Free Image Editing with Natural Language"
      },
      {
        "id": "03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f",
        "title": "Describing Differences in Image Sets with Natural Language"
      },
      {
        "id": "42fb38cba6e078a1c5c2d98694dd55613ebca150",
        "title": "LangSplat: 3D Language Gaussian Splatting"
      },
      {
        "id": "6c64ddd2190909de2c680dd18abc9b92e80c39f9",
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action"
      },
      {
        "id": "d3367dc9a7a1d7ae70a06eadc02b2430f4529f7c",
        "title": "Large Language Models for Robotics: A Survey"
      },
      {
        "id": "84d99893ee24fc825e359598d44d602c45c4865e",
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"
      },
      {
        "id": "d39182113cd4176ead48027b4fc05fe06ec6aaca",
        "title": "Language Models as Black-Box Optimizers for Vision-Language Models"
      },
      {
        "id": "21701a348d3a0a695f3c9fef11fdc8d872e844d5",
        "title": "TypeFly: Flying Drones with Large Language Model"
      },
      {
        "id": "a71678abb2e5e3aae7b3f2a08a5bffc87036d8f1",
        "title": "Language-driven Scene Synthesis using Multi-conditional Diffusion Model"
      },
      {
        "id": "25905977ce577f1e637ed9be53695fde8888ca09",
        "title": "Instruct2Attack: Language-Guided Semantic Adversarial Attacks"
      },
      {
        "id": "5bcb0153dd0840113eb27d4d6f753414ef656a03",
        "title": "Emu Edit: Precise Image Editing via Recognition and Generation Tasks"
      },
      {
        "id": "55367fbade73f96181ffcf52169d0471d4c014a2",
        "title": "GraphText: Graph Reasoning in Text Space"
      },
      {
        "id": "2dbaf84d12e40c9dcedb577c2be7a68faf5715d7",
        "title": "Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation"
      },
      {
        "id": "819f477065088220a6f706cd9ef76dbcb4b4c134",
        "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists"
      },
      {
        "id": "2d9310132cfe9046f4b61f6e90a5ef92c6e0ba71",
        "title": "BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys"
      },
      {
        "id": "0c474b03e8fd385b08a40df22934c9d9b180ffb7",
        "title": "De-Diffusion Makes Text a Strong Cross-Modal Interface"
      },
      {
        "id": "1b9314b5b12f4c20eb899e03fa32e2d7e020c5bc",
        "title": "Performance of emergency triage prediction of an open access natural language processing based chatbot application (ChatGPT): A preliminary, scenario-based cross-sectional study"
      },
      {
        "id": "f6c494234a818b2aec286b257ee6117f2894bcf7",
        "title": "CLAP Learning Audio Concepts from Natural Language Supervision"
      },
      {
        "id": "3ad346ae7af5c30964c4916dbcee798f72e1bdb7",
        "title": "Translating Natural Language to Planning Goals with Large-Language Models"
      },
      {
        "id": "51d4dcd726f5be80062bfef4416bb8ef3438a039",
        "title": "FindVehicle and VehicleFinder: a NER dataset for natural language-based vehicle retrieval and a keyword-based cross-modal vehicle retrieval system"
      },
      {
        "id": "a2f6a812924a1112c434510975eddaeb2b22523a",
        "title": "Tracked-Vehicle Retrieval by Natural Language Descriptions with Multi-Contextual Adaptive Knowledge"
      },
      {
        "id": "72a2cff51bb9a87bbe4fc41325f5a4afc82a0366",
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models"
      },
      {
        "id": "ccb82158429d82a53b541649d351382267e1e89d",
        "title": "Exploring the Implications of ChatGPT for Language Learning in Higher Education"
      },
      {
        "id": "afc5092a4116f27b4c64733c7815cd662bab78f7",
        "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"
      },
      {
        "id": "5aa2ff41c82ac5e7444abd65f014955c0fbb5209",
        "title": "CLIPPING: Distilling CLIP-Based Models with a Student Base for Video-Language Retrieval"
      },
      {
        "id": "a8680b3419f3cbe6650f72b1023aed0ad0becb9e",
        "title": "Chain of Thought Prompt Tuning in Vision Language Models"
      },
      {
        "id": "047e3812854a86b2a2e113219fa956eda860ce24",
        "title": "Introspective Tips: Large Language Model for In-Context Decision Making"
      },
      {
        "id": "2a9f1e923ccd7aa8f9411471275bef251a1466dd",
        "title": "Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving"
      },
      {
        "id": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
        "title": "DINOv2: Learning Robust Visual Features without Supervision"
      },
      {
        "id": "5814bd146b37e13115af4330caf3a751159a156f",
        "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"
      },
      {
        "id": "39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f",
        "title": "GPT (Generative Pre-Trained Transformer)— A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions"
      },
      {
        "id": "746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7",
        "title": "Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"
      },
      {
        "id": "e701e4c02a32da186d25b08373ada12d83b73b3d",
        "title": "Scaling Robot Learning with Semantically Imagined Experience"
      },
      {
        "id": "5fce7d9442b06cab91174fb68ba52ff6bdaa29cc",
        "title": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks"
      },
      {
        "id": "5f61ddc37476acf3741b0bfe5fcb59639cadbb86",
        "title": "What Makes Good Examples for Visual In-Context Learning?"
      },
      {
        "id": "948e8cfae92c2004f2dd5c9316f5972f8baaea21",
        "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"
      },
      {
        "id": "a8a934fdad1b07be978bc28795e848c4fa2ff549",
        "title": "DiffuRec: A Diffusion Model for Sequential Recommendation"
      },
      {
        "id": "ed1353d705eeabc0e916caba5fbae890eefe4f84",
        "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training"
      },
      {
        "id": "0d230fe2af4fe4fd2257ba2a10cfeb91126e27a3",
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation"
      },
      {
        "id": "122f2d18a7b621576254ec12079941ecf0632360",
        "title": "GEO-Bench: Toward Foundation Models for Earth Monitoring"
      },
      {
        "id": "789b91e432d7d669b82bc94f4877f7ecf3505c27",
        "title": "Efficient Multimodal Fusion via Interactive Prompting"
      },
      {
        "id": "83daad52d4cc7706b0aa08bf9b90dfa2efe97876",
        "title": "Changes to Captions: An Attentive Network for Remote Sensing Change Captioning"
      },
      {
        "id": "8c88c693d5dc2802d3b76b85740e1f04fdaaf801",
        "title": "Large sequence models for sequential decision-making: a survey"
      },
      {
        "id": "5fa10872ef8037853ff7c8baf5f77fb55a918eca",
        "title": "Diffusion Models for Non-autoregressive Text Generation: A Survey"
      },
      {
        "id": "d9f9d3450d6a3a8c1f79b9d289313c3e27972b10",
        "title": "Towards In-context Scene Understanding"
      },
      {
        "id": "23aa67f41bf301a81a1dc737d710e9e612a23883",
        "title": "A semi-supervised short text sentiment classification method based on improved Bert model from unlabelled data"
      },
      {
        "id": "9dee1aceb09f7d4c22fdbaf49d238e1502effd1b",
        "title": "Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought"
      },
      {
        "id": "cd633e08340e13ca25e9b663ef0b5366b3e032be",
        "title": "A Multi-Modal Context Reasoning Approach for Conditional Inference on Joint Textual and Visual Clues"
      },
      {
        "id": "205d2ed0906440f07a0275d7d6a63bced60951fc",
        "title": "InstructVid2Vid: Controllable Video Editing with Natural Language Instructions"
      },
      {
        "id": "db77bf2378a2ac764984f5e5bd81f68e994c3323",
        "title": "Learning to Imagine: Visually-Augmented Natural Language Generation"
      },
      {
        "id": "7df705d1f31508e570483f82aeeb83cf146d2098",
        "title": "Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation"
      },
      {
        "id": "268b9ea1fa06a24dc12f96b16f60a52ccea352ae",
        "title": "Diagnosing and Rectifying Vision Models using Language"
      },
      {
        "id": "05aea4a4646cb971bb253ced42e8935034a60b57",
        "title": "MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images"
      },
      {
        "id": "18c499092df491b0eb4eed3a1c0266b67a93ae13",
        "title": "CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning"
      },
      {
        "id": "83bfe9264c46d656a0d1c7dd86cb2ccd254d9d60",
        "title": "ERRA: An Embodied Representation and Reasoning Architecture for Long-Horizon Language-Conditioned Manipulation Tasks"
      },
      {
        "id": "6c2e438068e101d31af55add9f6ac6cac7159bc8",
        "title": "Practical and ethical challenges of large language models in education: A systematic scoping review"
      },
      {
        "id": "314047a5aad780af9efef7ebd4a41e6995666543",
        "title": "Key-Locked Rank One Editing for Text-to-Image Personalization"
      },
      {
        "id": "b44e302e87fdb84cb299f1ba5fdb89f0d1b309b6",
        "title": "Guided Motion Diffusion for Controllable Human Motion Synthesis"
      },
      {
        "id": "390eabbedf9a25d82eefd01a484b1f81fdd801df",
        "title": "InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions"
      },
      {
        "id": "dc135dabef805c7271f53ec4b212bdf8996cfd9d",
        "title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers"
      },
      {
        "id": "6d7534a41fc933f4f6a99e039f585dc57a370a29",
        "title": "ADAPT: Action-aware Driving Caption Transformer"
      },
      {
        "id": "1f898d66acabff511a3871b82799aa73c0055402",
        "title": "A Reparameterized Discrete Diffusion Model for Text Generation"
      },
      {
        "id": "83d4b22d803ae856cf6b308482bd504fa151d39e",
        "title": "Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation"
      },
      {
        "id": "d115238c6ee8fcdd635247f871d25732b457d1d3",
        "title": "Zero-shot spatial layout conditioning for text-to-image diffusion models"
      },
      {
        "id": "67cdecbcfed07b9a29d9e2a92da684604383afd7",
        "title": "TESS: Text-to-Text Self-Conditioned Simplex Diffusion"
      },
      {
        "id": "119d3beca449efd9096d58674cf01a99c793a9a7",
        "title": "Towards Open-World Interactive Disambiguation for Robotic Grasping"
      },
      {
        "id": "734e47fe233f0b7aa2e9c4ac582b35bf60773cf8",
        "title": "Localizing Moments in Long Video Via Multimodal Guidance"
      },
      {
        "id": "3599a236f285af48782fc30b1341d13ec7320735",
        "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"
      },
      {
        "id": "fc5631cdd08722f51e0ce4b718de0f081ae73603",
        "title": "DANLI: Deliberative Agent for Following Natural Language Instructions"
      },
      {
        "id": "e9bc29cfcfbea4d137652d10715a9c9389349a90",
        "title": "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"
      },
      {
        "id": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
      },
      {
        "id": "0b0c87602818160e75ff54b48ef154d0ca27dd45",
        "title": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations"
      },
      {
        "id": "6d4a12ea469ff08634eeb24c47b265a7dca2fce2",
        "title": "PADL: Language-Directed Physics-Based Character Control"
      },
      {
        "id": "c7175ad54baf30168ce6c366350d21a08e17a91e",
        "title": "I can’t believe there’s no images! : Learning Visual Tasks Using Only Language Supervision"
      },
      {
        "id": "22fbef2bfef213a7619ee4f307e8f42d1888e638",
        "title": "LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation"
      },
      {
        "id": "dad36d9b8d51e7ae14150a48a76bde0a8eb50eac",
        "title": "Fairness in Deep Learning: A Survey on Vision and Language Research"
      },
      {
        "id": "04f87baf7d1b3eb303a52a8a66c8189f396dd114",
        "title": "Application of Pretrained Large Language Models in Embodied Artificial Intelligence"
      },
      {
        "id": "c6629429d064b2ed3117e40d5558e21376aef337",
        "title": "HERB: Measuring Hierarchical Regional Bias in Pre-trained Language Models"
      },
      {
        "id": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "id": "e342165a614588878ad0f4bc9bacf3905df34d08",
        "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications"
      },
      {
        "id": "0d0dbfb1b315a43216020abaf74d289456198219",
        "title": "MaPLe: Multi-modal Prompt Learning"
      },
      {
        "id": "60c8d0619481eaafdd1189af610d0e636271fed5",
        "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"
      },
      {
        "id": "af1c871282ec122869d03f5420ef5d9143358a91",
        "title": "Visual Programming: Compositional visual reasoning without training"
      },
      {
        "id": "25425e299101b13ec2872417a14f961f4f8aa18e",
        "title": "VIMA: General Robot Manipulation with Multimodal Prompts"
      },
      {
        "id": "a1186d7d9a9ef258c76afef1177e4f348061a537",
        "title": "SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers"
      },
      {
        "id": "84fd742b380c9518a313c3e0a5d3b92447d25fda",
        "title": "Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets"
      },
      {
        "id": "569ba1ef5db91bc5ecb49c8beb2a14633f4e3159",
        "title": "COCOA"
      },
      {
        "id": "154a5d37e44fac0a5bd6cacb8fdee0e8b7b3bff2",
        "title": "Consecutive Pretraining: A Knowledge Transfer Learning Strategy with Relevant Unlabeled Data for Remote Sensing Domain"
      },
      {
        "id": "836ca61c0226fd5f763335ad4c13acc784251343",
        "title": "Towards a Unified View on Visual Parameter-Efficient Transfer Learning"
      },
      {
        "id": "060cee8411181e8151ab1e3212b81528accd9b8b",
        "title": "On Transforming Reinforcement Learning With Transformers: The Development Trajectory"
      },
      {
        "id": "7b604cd12bfd735f16d2097357b3d6ca584d53a1",
        "title": "Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning"
      },
      {
        "id": "39880b887c19ae80d71643b37d6fc89aba8ec0c4",
        "title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation"
      },
      {
        "id": "d4bcc5c1bb5ae63878b7801f7f3170fd8b22a347",
        "title": "Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer"
      },
      {
        "id": "f5c8b9331e1eb3ba556dd55ecf1d143ddf8781b7",
        "title": "Open-ended remote sensing visual question answering with transformers"
      },
      {
        "id": "721acfc94c91c685646b2944a39e220e4ba71b29",
        "title": "Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations"
      },
      {
        "id": "91deaf9d324c8feafc189da0da03e60a60287bca",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      },
      {
        "id": "c305ab1bdba79442bec72ec7f5c5ee7c49c2a566",
        "title": "Visual Language Maps for Robot Navigation"
      },
      {
        "id": "ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b",
        "title": "CLIPPO: Image-and-Language Understanding from Pixels Only"
      },
      {
        "id": "5d8fd04c436367b18b35e28332ee8e452a477f3f",
        "title": "Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study"
      },
      {
        "id": "d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
        "title": "Meta-Reinforcement Learning via Language Instructions"
      },
      {
        "id": "c822486b8f1dcbef3b96b136c85d48d0dc580f31",
        "title": "Audio Retrieval with WavText5K and CLAP Training"
      },
      {
        "id": "288b24f16fe7341c91def471120fa23233e34acc",
        "title": "Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors"
      },
      {
        "id": "591627746d0f8c3b642b7c9415bbc8af66e24a0e",
        "title": "Visualize Before You Write: Imagination-Guided Open-Ended Text Generation"
      },
      {
        "id": "7694f004c67840d7f098b3612d4b3dabd915c116",
        "title": "Executing your Commands via Motion Diffusion in Latent Space"
      },
      {
        "id": "30477855d76058a9b542cabea3058aad1a837d51",
        "title": "A Case for Business Process-Specific Foundation Models"
      },
      {
        "id": "951016af892f5ecde12b2acd9374b92b66afb796",
        "title": "AI art in architecture"
      },
      {
        "id": "7335fbc509da851b9d141a40e6463b5e82dea01c",
        "title": "A Survey on Bias and Fairness in Natural Language Processing"
      },
      {
        "id": "a1aeb5442e31276b197696f49b3243112a4049ce",
        "title": "Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing"
      },
      {
        "id": "3b9b1aba877ecd3f7e508cbc78a41b623349902b",
        "title": "Translation between Molecules and Natural Language"
      },
      {
        "id": "53ed38af7df82242e0d7b13c19c814762de75bca",
        "title": "Informativeness and Invariance: Two Perspectives on Spurious Correlations in Natural Language"
      },
      {
        "id": "01a621bb49dc6d6c348806d37b5fd8d157bc106d",
        "title": "Tracked-Vehicle Retrieval by Natural Language Descriptions With Domain Adaptive Knowledge"
      },
      {
        "id": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
        "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"
      },
      {
        "id": "24ed74ed29c057cba8b52fff4edd2c0d7f408716",
        "title": "VLP: A Survey on Vision-language Pre-training"
      },
      {
        "id": "4f1d598f919aae55c3cbbc425ef1514a54e2b8cd",
        "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions"
      },
      {
        "id": "66ee488cf3dad5bb83804124367460edddd3c271",
        "title": "Vision-and-Language Pretrained Models: A Survey"
      },
      {
        "id": "0607b299284cb44eaee0aedd95db3c88b00ff944",
        "title": "Gender Bias in Masked Language Models for Multiple Languages"
      },
      {
        "id": "97f456643712e9618edd7465676c62af3c8ae690",
        "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models"
      },
      {
        "id": "3b239d232ebb0fdb0515f41fd439e54ed4e8f86a",
        "title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large Models"
      },
      {
        "id": "bd3d8a17988add79205f697465bbd6c2ef59e621",
        "title": "Prompt–RSVQA: Prompting visual context to a language model for Remote Sensing Visual Question Answering"
      },
      {
        "id": "01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8",
        "title": "Behavior Transformers: Cloning k modes with one stone"
      },
      {
        "id": "fe26607ca95c0d1d9005810b4ad12845ee69e9cf",
        "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem"
      },
      {
        "id": "22a77ab4b79b43c69ce25a272de480b2a025c6a4",
        "title": "BERN2: an advanced neural biomedical named entity recognition and normalization tool"
      },
      {
        "id": "00debf63dafa966221c6e572f7705a813d704ff1",
        "title": "K-LITE: Learning Transferable Visual Models with External Knowledge"
      },
      {
        "id": "fa21a215468e881820266d1df362340987bc3fa8",
        "title": "Diversify and Disambiguate: Learning From Underspecified Data"
      },
      {
        "id": "0265144c696bf9371a0a63ece590dd2403ee71be",
        "title": "When Do Flat Minima Optimizers Work?"
      },
      {
        "id": "0f72e329d3b0f1cfe388d102ef5fec0677ac7558",
        "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics"
      },
      {
        "id": "fd9ec4bf9cd83c3d0e38470407427a8deeb8d547",
        "title": "PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map"
      },
      {
        "id": "49c82c3fe90bb3eadf7f1303e904598dc0d2e679",
        "title": "On the Application of Sentence Transformers to Automatic Short Answer Grading in Blended Assessment"
      },
      {
        "id": "bc64190d42d9dc34077b6a096d9053bb88deaa3a",
        "title": "NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks"
      },
      {
        "id": "b9b220b485d2add79118ffdc2aaa148b67fa53ef",
        "title": "Pre-Trained Language Models for Interactive Decision-Making"
      },
      {
        "id": "15ac70d077bb735eed4a8502ce49aa7782c803fd",
        "title": "What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data"
      },
      {
        "id": "47e135ef718baf6a3a86a1167b67ed96f7932ca4",
        "title": "LEBP - Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents"
      },
      {
        "id": "a8a1440b72b70d1372d3f51ab119a5bba70b4d92",
        "title": "CALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations"
      },
      {
        "id": "93324dfb8b02f43edb4122d08ccc6b90d3a6b577",
        "title": "Weakly Supervised Video Moment Localization with Contrastive Negative Sample Mining"
      },
      {
        "id": "58b59d0a10917580e377dae4996be0ebdb176875",
        "title": "AvatarCLIP"
      },
      {
        "id": "efb3b4550b7308ddeb2f382d8c1439e6ec7a99ec",
        "title": "Interactive Robotic Grasping with Attribute-Guided Disambiguation"
      },
      {
        "id": "54020e5fe48ebb250f27d744e20a63cac2988a84",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "id": "130d432ccbc836380a212bea618f84ff094a6a52",
        "title": "Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"
      },
      {
        "id": "2b365afbc7ac4a2f2437f0fb2d1ece87a5ae958b",
        "title": "Survey of Pre-trained Models for Natural Language Processing"
      },
      {
        "id": "96ea07447d2f9adefe03852a878517a2a6d45b96",
        "title": "Learning to Prompt for Vision-Language Models"
      },
      {
        "id": "fc7c20644d32542e11531e551900f305fc0953ad",
        "title": "SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation"
      },
      {
        "id": "67ad491b16bf77e9a54a8b8b1dc23dadc5545467",
        "title": "Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models"
      },
      {
        "id": "0d232524732c133e428e8867de69a48886509a7e",
        "title": "Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning"
      },
      {
        "id": "677f0b5c01c16565a6c802ad7d253928fa6ae8ec",
        "title": "MultiModal Language Modelling on Knowledge Graphs for Deep Video Understanding"
      },
      {
        "id": "5dcc65466e4dd1f400823f424d720422a65cd7a5",
        "title": "Technical Language Supervision for Intelligent Fault Diagnosis in Process Industry"
      },
      {
        "id": "216d093cb2ad81bf55c21dbce2217f2b9032e67b",
        "title": "Just Train Twice: Improving Group Robustness without Training Group Information"
      },
      {
        "id": "b90c090f7928a78d85d952737488be1ef8587ae5",
        "title": "A Literature Survey of Recent Advances in Chatbots"
      },
      {
        "id": "60c498956cb5737c4964aaca0b920592bd7f5689",
        "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search"
      },
      {
        "id": "ea48a830ed7ace758677944f7957348d332b78c8",
        "title": "Chest ImaGenome Dataset for Clinical Reasoning"
      },
      {
        "id": "a702ceeabc4c2e959513747f7ed2f5c29f7dbfcd",
        "title": "Collective intelligence for deep learning: A survey of recent developments"
      },
      {
        "id": "42188859cdc5089f75cbbe5479793a67320b617c",
        "title": "RSVQA Meets Bigearthnet: A New, Large-Scale, Visual Question Answering Dataset for Remote Sensing"
      },
      {
        "id": "82edbb92d0f6952224c5aa0aff264b44b8fb4e98",
        "title": "Toward Foundation Models for Earth Monitoring: Proposal for a Climate Change Benchmark"
      },
      {
        "id": "f8cdb94c0446f3170cbb32ad6019b21839c4b237",
        "title": "Words to Matter: De novo Architected Materials Design Using Transformer Neural Networks"
      },
      {
        "id": "b1aa394d8d03cd17ee3d3626cbb98b2e4f6b1933",
        "title": "How to find a good image-text embedding for remote sensing visual question answering?"
      },
      {
        "id": "88e8801e4daf404d3d40f1648ef29faeb8e6d58a",
        "title": "Blended Diffusion for Text-driven Editing of Natural Images"
      },
      {
        "id": "738e3e0623054da29dc57fc6aee5e6711867c4e8",
        "title": "CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation"
      },
      {
        "id": "76a2b197b5427ffd1d3470c6d3ea026588eb5d0a",
        "title": "CRIS: CLIP-Driven Referring Image Segmentation"
      },
      {
        "id": "5aebb2f01a1d13a89587af77b2554ff22ae1c6c9",
        "title": "INVIGORATE: Interactive Visual Grounding and Grasping in Clutter"
      },
      {
        "id": "38dcd087f0c2801fcd75c0906a0e12258087ce8b",
        "title": "FairyTailor: A Multimodal Generative Framework for Storytelling"
      },
      {
        "id": "1c83f3f9789df43bf937ae2618721e2da83dcc06",
        "title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning"
      },
      {
        "id": "8be99c2d0802d6222e233dd67d2927c75a0bed24",
        "title": "Towards Accurate Visual and Natural Language-Based Vehicle Retrieval Systems"
      },
      {
        "id": "55b17a76d8b9f0a9adb5c116450a2cfd2844448c",
        "title": "ImaginE: An Imagination-Based Automatic Evaluation Metric for Natural Language Generation"
      },
      {
        "id": "f5d1dbdaa6c8a44f388f3f7fe538403baefc1252",
        "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do"
      },
      {
        "id": "437727b6c00a5eb4944600091f66f41626d1002d",
        "title": "Unmasking the Mask - Evaluating Social Biases in Masked Language Models"
      },
      {
        "id": "9b3c525e1f22601b21df0e96cddebeaa89ac9a2b",
        "title": "POP-ON: Prediction of Process Using One-Way Language Model Based on NLP Approach"
      },
      {
        "id": "b249fe4e5e2bada6655ce5d61e7f50da5d471cb4",
        "title": "Domain Generalization: A Survey"
      },
      {
        "id": "f864d4d2267abba15eb43db54f58286aef78292b",
        "title": "Offline Reinforcement Learning as One Big Sequence Modeling Problem"
      },
      {
        "id": "bd66f8a4e883ddf8a337dabdad88bc12b72d7c0e",
        "title": "Transformer models for text-based emotion detection: a review of BERT-based approaches"
      },
      {
        "id": "add5f3f820b393e7ce5ed467814253824ecc484b",
        "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions"
      },
      {
        "id": "c0e6cd2ec3bc9eb46c7d45bb708854da3327339e",
        "title": "A Survey on Bias in Deep NLP"
      },
      {
        "id": "6bda005a727969356a012e177cd600e7fdf80c96",
        "title": "A Review on Explainability in Multimodal Deep Neural Nets"
      },
      {
        "id": "c1272cf10f827bd9b8e43cc06b0d5831c59de52e",
        "title": "Challenges of Hate Speech Detection in Social Media"
      },
      {
        "id": "040432550b54d03777c4a2b79ec44f5ea135053f",
        "title": "The digital scribe in clinical practice: a scoping review and research agenda"
      },
      {
        "id": "3943dca7b29580cb3c4e1e002afb54c566a69368",
        "title": "Cross-modal Adversarial Reprogramming"
      },
      {
        "id": "c0eea2237a1bfb280c9f733b14157751c20f42d4",
        "title": "Evaluating Pretrained Transformer-based Models for COVID-19 Fake News Detection"
      },
      {
        "id": "b3c914c8fc3eb3620ed1406289ae4f1ca2c617b0",
        "title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification"
      },
      {
        "id": "8d0ebf8d6f6a94a0d85257c010ffe9ba1c162eb0",
        "title": "A Systematic Review of Machine Learning Algorithms in Cyberbullying Detection: Future Directions and Challenges"
      },
      {
        "id": "98adf45ce7a9b6bc5424b641ca22724fb770e529",
        "title": "Understanding Emails and Drafting Responses - An Approach Using GPT-3"
      },
      {
        "id": "f13bbe25e59fef5a209091b6192841367b2a87a4",
        "title": "MusCaps: Generating Captions for Music Audio"
      }
    ],
    "0": [
      {
        "id": "a987c21afbfb3bd746d114e248202c074b1c40ca",
        "title": "Large Language Models for Mathematicians"
      },
      {
        "id": "936f7f0fa77efcd322805b93a8d74c48a4108290",
        "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"
      },
      {
        "id": "088617a8862cfa372a62070916e88a5f10e7690b",
        "title": "Assessing Translation Capabilities of Large Language Models involving English and Indian Languages"
      },
      {
        "id": "ec67d5f0e236f23c6b48b926f9e25db52194dd71",
        "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey"
      },
      {
        "id": "64809140c38ce9d4bee3a30c59da9a1decd88b97",
        "title": "Large Language Model-Driven Classroom Flipping: Empowering Student-Centric Peer Questioning with Flipped Interaction"
      },
      {
        "id": "bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3",
        "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"
      },
      {
        "id": "813987d484a9eea03e95e677707fd011947a4154",
        "title": "First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models"
      },
      {
        "id": "afb2d837f337a60785a9f9a50f590f11f37876b0",
        "title": "From natural language to simulations: applying AI to automate simulation modelling of logistics systems"
      },
      {
        "id": "95240dda409e28acccdc5cf619ad0c036cf4292d",
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
      },
      {
        "id": "a419e1d74cfc2b5ff400963476bda5c6ae66e172",
        "title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"
      },
      {
        "id": "5b038c1a93967072cc76689fd805e756f804cc42",
        "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"
      },
      {
        "id": "64384525318f934fa085de86badc24403487d38a",
        "title": "Tokenizer Choice For LLM Training: Negligible or Crucial?"
      },
      {
        "id": "d558f604981ed9911532baac17425b294799b528",
        "title": "Demystifying Embedding Spaces using Large Language Models"
      },
      {
        "id": "a2bf0f9d41c550bc7cf69706b322f9c9838d3b00",
        "title": "Large Language Models for Test-Free Fault Localization"
      },
      {
        "id": "c182bcd5f37f37fea9f3dad856dc381e0f19578a",
        "title": "Augmenting LLMs with Knowledge: A survey on hallucination prevention"
      },
      {
        "id": "5432b77bfb1dced97c5b1fc684b0fa7d0d84c424",
        "title": "Large Language Models in Finance: A Survey"
      },
      {
        "id": "3613299c54bbea66dd6db1b00573f7ade021a5a9",
        "title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback"
      },
      {
        "id": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "id": "1d266cde7ac654e2a55ff52988f134ff66d17218",
        "title": "Examining the Potential of Generative Language Models for Aviation Safety Analysis: Case Study and Insights Using the Aviation Safety Reporting System (ASRS)"
      },
      {
        "id": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e",
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"
      },
      {
        "id": "0b1131a4ef51cf93da8ba9565e68f68c9ff5e792",
        "title": "The now and future of ChatGPT and GPT in psychiatry"
      },
      {
        "id": "4b0faf4968f9b927d6dce946226feb69a82a6766",
        "title": "A User-Centered Security Evaluation of Copilot"
      },
      {
        "id": "9fec5cf2f06e6fd8c5e6f6028226082d1ecec5b7",
        "title": "Extrapolating Large Language Models to Non-English by Aligning Languages"
      },
      {
        "id": "09f79daf280659e1cc254ca9d454ac2bacad7620",
        "title": "Evaluating and Explaining Large Language Models for Code Using Syntactic Structures"
      },
      {
        "id": "ff6f2b9e56ee0f3f26bcbdc5079678c059fe24e3",
        "title": "A Theory for Emergence of Complex Skills in Language Models"
      },
      {
        "id": "98032f95e274db30570727fb7196c15e325fb35a",
        "title": "Three Bricks to Consolidate Watermarks for Large Language Models"
      },
      {
        "id": "8439616aada33a889f4f52d6a9a3d7f6c5a30b79",
        "title": "Assessing How Large Language Models Can Be Integrated with or Used for Blockchain Technology: Overview and Illustrative Case Study"
      },
      {
        "id": "3e664adb009dce373129a3563e4b2cb08731bc76",
        "title": "PolyLM: An Open Source Polyglot Large Language Model"
      },
      {
        "id": "99de79d26873100e7cb2791adf87aed130d5012a",
        "title": "Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game"
      },
      {
        "id": "b7c94daa083c2e919d192cb7130836440d9c9a02",
        "title": "GPT-3 vs Object Oriented Programming Assignments: An Experience Report"
      },
      {
        "id": "aff0fe00ed7892d1185e8c5b66d318d3892abe6e",
        "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health"
      },
      {
        "id": "3dcf2db20082b480c6c091eea025465cc4fe57a6",
        "title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap"
      },
      {
        "id": "8071b53a46b0eb368741afcd90c3f93d95f56fdc",
        "title": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review"
      },
      {
        "id": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
        "title": "Faith and Fate: Limits of Transformers on Compositionality"
      },
      {
        "id": "af067c6f1c12941625e4c3b49b002c7c7c0b2542",
        "title": "BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model"
      },
      {
        "id": "0fbf7ea1a3bd1754ed9aa12ed25906b731ece589",
        "title": "Training Data Extraction From Pre-trained Language Models: A Survey"
      },
      {
        "id": "f8f6942be75d102a14c6441e0bb31ac7c59235a4",
        "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models"
      },
      {
        "id": "879a7f5abdb7ab803d48172d4f0830965f989d46",
        "title": "Language Model Tokenizers Introduce Unfairness Between Languages"
      },
      {
        "id": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
        "title": "StarCoder: may the source be with you!"
      },
      {
        "id": "8cf819f6ee33909484ece40d79944c9c37f01a89",
        "title": "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development"
      },
      {
        "id": "b3c1fad1f5f8f0213b6d3f3458fa86205a3434f7",
        "title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models"
      },
      {
        "id": "dfd8944d39b378489b878d6e105d040fa0e524db",
        "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis"
      },
      {
        "id": "16d83e930a4dab2d49f5d276838ddce79df3f787",
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"
      },
      {
        "id": "4de290467d903b9977e31b3d4084006789bd6ebd",
        "title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era"
      },
      {
        "id": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "id": "285dae5c2f2ef55c70971094a1ddd45afe720eee",
        "title": "Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense"
      },
      {
        "id": "7d46073cce9f5ed8d931b804cfaf327201dd4e26",
        "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models"
      },
      {
        "id": "a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5",
        "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"
      },
      {
        "id": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "id": "dccd176071e62d3c82af3710956256cbcf4df9b4",
        "title": "Toward a Theory of Causation for Interpreting Neural Code Models"
      },
      {
        "id": "933b37b21e9d61139660088adb032ff3fdf56d86",
        "title": "Learning Video Representations from Large Language Models"
      },
      {
        "id": "a8b896d5dd327ac808bcbcf4e4ed324a4615c356",
        "title": "Adaptation Approaches for Nearest Neighbor Language Models"
      },
      {
        "id": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "id": "a8ca46b171467ceb2d7652fbfb67fe701ad86092",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      },
      {
        "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "id": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
        "title": "Evaluating Large Language Models Trained on Code"
      },
      {
        "id": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "id": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "id": "4deeb8ea9fd60858bde847775170168e1ede883e",
        "title": "keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM"
      },
      {
        "id": "919215590b3301f1154deebd1b64722dd3746b8f",
        "title": "Can ChatGPT’s Responses Boost Traditional Natural Language Processing?"
      },
      {
        "id": "99e470e72d74bebb31a08ca9d4cd6eca3db6ca7d",
        "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing"
      },
      {
        "id": "e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43",
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions"
      },
      {
        "id": "888728745dbb769e29ed475d4f7661eebe1a71cf",
        "title": "A Survey on Evaluation of Large Language Models"
      },
      {
        "id": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "id": "bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7",
        "title": "Bias and Fairness in Large Language Models: A Survey"
      },
      {
        "id": "ca31b8584b6c022ef15ddfe994fe361e002b7729",
        "title": "A Comprehensive Overview of Large Language Models"
      },
      {
        "id": "f43b8a87a96f8abc2467b90538b643a6061416e9",
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision"
      },
      {
        "id": "007c3d9b8dab341d2c77c4ee764fd921f7f14956",
        "title": "Adapted large language models can outperform medical experts in clinical text summarization."
      },
      {
        "id": "0b220041eb83c23b7b10d32a5d08c0309d528071",
        "title": "Large Language Models for Information Retrieval: A Survey"
      },
      {
        "id": "b8b8d5655df1c6a71bbb713387863e34cc055332",
        "title": "Detecting Language Model Attacks with Perplexity"
      },
      {
        "id": "8d65b5940a4dbed8c18e02ca35e3a9d7a14ea76b",
        "title": "Language is All a Graph Needs"
      },
      {
        "id": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
        "title": "Large Language Models on Graphs: A Comprehensive Survey"
      },
      {
        "id": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
        "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers"
      },
      {
        "id": "83de13bd492b9e72c314e308f0d77014154a6a74",
        "title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment"
      },
      {
        "id": "00e18c603e60d861c4e99c541e4d65ef442d5945",
        "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory"
      },
      {
        "id": "942130a875ccfe55a4c60c27c636f693e25cb13d",
        "title": "Personality Traits in Large Language Models"
      },
      {
        "id": "eb3e6dc2476c5a377b7c3d1489c014712a17a376",
        "title": "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA"
      },
      {
        "id": "33112b58e3eb4a6506fa537d892dc6742c5e794d",
        "title": "Large language models in health care: Development, applications, and challenges"
      },
      {
        "id": "daaa7d4ffb9265226e4baadd2db9a01aa7b2f6fb",
        "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts"
      },
      {
        "id": "78b1601c013769294a1927d43e50dfa81d6af75f",
        "title": "LLMs4OL: Large Language Models for Ontology Learning"
      },
      {
        "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
        "title": "CLadder: Assessing Causal Reasoning in Language Models"
      },
      {
        "id": "dcf87f11e245b76437c2f551c1ff6a7842585811",
        "title": "ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning"
      },
      {
        "id": "fac468032e0c38ea10dfb95ba6cdeac51a473050",
        "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models"
      },
      {
        "id": "09826f769cef899388909d9f4cfaa335429c41a4",
        "title": "Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations"
      },
      {
        "id": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e",
        "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce"
      },
      {
        "id": "0c8a630657a2cf5dea41472a9b5e20544ce2bd56",
        "title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks"
      },
      {
        "id": "bd09391fbd124dc0c0a6be5d0ab2eb5d9c43fbac",
        "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets"
      },
      {
        "id": "1153b05709be3dd0dc5e5d4d4ee8f631995ad768",
        "title": "CAT-LM Training Language Models on Aligned Code And Tests"
      },
      {
        "id": "94dd5c30b052e29039c20e8cbd4214bb2302740a",
        "title": "Composite Backdoor Attacks Against Large Language Models"
      },
      {
        "id": "4a806670b0aeb2b55c1efce5ae294e34ac9c676b",
        "title": "On the application of Large Language Models for language teaching and assessment technology"
      },
      {
        "id": "8c9b8ba4a44b9736ea9db94f11c3227d5bb91a09",
        "title": "Do Large Language Models Know about Facts?"
      },
      {
        "id": "49408c5e1ac75854f1580e561384df2be870d559",
        "title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases"
      },
      {
        "id": "7c93bc4173c9b4d2a71866e8189c8e35cdb00b34",
        "title": "Large Language Models for Code Analysis: Do LLMs Really Do Their Job?"
      },
      {
        "id": "00a67af3b7dc785b4813b61d232cc76b4fb2b189",
        "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning"
      },
      {
        "id": "660db1e60311d7d2a24f39d0c96fb12b69ccdcef",
        "title": "Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models"
      },
      {
        "id": "ad44f2392df3c6f2c45d21fe1dbd8ec17003530d",
        "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models"
      },
      {
        "id": "462041e29f2e4e8d78b3214ee1a286865bc68721",
        "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences"
      },
      {
        "id": "62b4e06f5249d22e4a153ec4a2dc934c6a014372",
        "title": "OWL: A Large Language Model for IT Operations"
      },
      {
        "id": "c94471a213c1b160e9c3f0f85f92a0e8ef9af8ea",
        "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation"
      },
      {
        "id": "bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f",
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"
      },
      {
        "id": "9a2f47777b99a92effb4e998b7082e1e92ae13bc",
        "title": "Improving Language Plasticity via Pretraining with Active Forgetting"
      },
      {
        "id": "75059feaaec7dd0a8810d8f4ec6985f5d00b73d5",
        "title": "A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"
      },
      {
        "id": "72273f7a050529fc71c7d45c0256d2b9754f56bb",
        "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration"
      },
      {
        "id": "3b88526a0f0337e3a6b632b4af8fd0882eb4b470",
        "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models"
      },
      {
        "id": "4118c8bca76b4bfafc379d80bf91455df88614b6",
        "title": "Aligning Language Models with Human Preferences via a Bayesian Approach"
      },
      {
        "id": "8d34de18ce1c2345e3fa1bff786a2410c2783e6a",
        "title": "FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking"
      },
      {
        "id": "dd0612ce863f64b0f69d0d9f708c52e829f6f859",
        "title": "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"
      },
      {
        "id": "f131b342e3aede46d24afc9b9055a94cceb0936a",
        "title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction"
      },
      {
        "id": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
        "title": "Metacognitive Prompting Improves Understanding in Large Language Models"
      },
      {
        "id": "4ae7c4decd1df71c466f19d66d69b555945098c4",
        "title": "Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data"
      },
      {
        "id": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
        "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"
      },
      {
        "id": "a86d3692af34ce70fd8caaa319009f85a88fdb4d",
        "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models"
      },
      {
        "id": "4014253368133c01bfc0383660c518d11afccad2",
        "title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration"
      },
      {
        "id": "00e527ece4db4857bb7a03bd924341eb2b5000fa",
        "title": "HPC-GPT: Integrating Large Language Model for High-Performance Computing"
      },
      {
        "id": "d0edff5402edeab721b7681643e1ff7c2354de4a",
        "title": "Leveraging pre-trained language models for mining microbiome-disease relationships"
      },
      {
        "id": "0bedef4949d3f408530ccb53ec795c31b358b4b6",
        "title": "Generating Multiple Choice Questions for Computing Courses Using Large Language Models"
      },
      {
        "id": "1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35",
        "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese"
      },
      {
        "id": "1146d40d3d01427a008a20530269667b8989750c",
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation"
      },
      {
        "id": "27ae43f78463d6417cd97276375a7e864af7dc60",
        "title": "TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety"
      },
      {
        "id": "08400a12db21fe4d4f844fec57844e74ac09c9ba",
        "title": "Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction"
      },
      {
        "id": "840fb73f48ab761b48b041ea2b9fe4d92d0eca57",
        "title": "Contextual Refinement of Translations: Large Language Models for Sentence and Document-Level Post-Editing"
      },
      {
        "id": "f1278443e9df54711b8f55548961862337c193a6",
        "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization"
      },
      {
        "id": "b74be6891cd7f6d81e346852494c08528dbffdc4",
        "title": "TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine"
      },
      {
        "id": "5fb8dd5e31bf423d456ae580e494b406b4403bbf",
        "title": "A Large Language Model Approach to Educational Survey Feedback Analysis"
      },
      {
        "id": "476de9654c840a080276c4b8b5dadfdbe25c663f",
        "title": "Construction contract risk identification based on knowledge-augmented language model"
      },
      {
        "id": "0ae7bc864a66feae9e5a189a9ac24cf4c19aa0dd",
        "title": "Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"
      },
      {
        "id": "be7dbac2bcaed4cd034a7371004a011933e1bdca",
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model"
      },
      {
        "id": "537335d9aad0ddbaef93e7f88b0db096671ef6ec",
        "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"
      },
      {
        "id": "fd42031baa3fe8690a00767c2fdf52dbcf945713",
        "title": "Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking"
      },
      {
        "id": "a94b1989594987de5daa19503929f5f34ed6bc47",
        "title": "Large Language Models for Fuzzing Parsers (Registered Report)"
      },
      {
        "id": "b411c8f98865565f54642af1a5c010bde6beaedc",
        "title": "Exploring Language Models: A Comprehensive Survey and Analysis"
      },
      {
        "id": "4d3179b1fde10d6e4bf044454d37fbbc6591ba53",
        "title": "Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction"
      },
      {
        "id": "a9eb336485e148d0a3f5010693d7752facba2875",
        "title": "CFGPT: Chinese Financial Assistant with Large Language Model"
      },
      {
        "id": "1a9fba77d6f4f8ceb754fbc7d4f02574f4ab3ea5",
        "title": "Ophtha-LLaMA2: A Large Language Model for Ophthalmology"
      },
      {
        "id": "8d9d724e387079743e719b7f1af257c120eae51e",
        "title": "Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention"
      },
      {
        "id": "1366b07120580eaf1badde105b9361806e8f9629",
        "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models"
      },
      {
        "id": "220178fb7f488c6925f4607dc58b599afa6ee967",
        "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction"
      },
      {
        "id": "211b499ac24a45c3edb5b4ca019fb3e28ffe2329",
        "title": "Exploring Large Language Models for Code Explanation"
      },
      {
        "id": "ff2eecb21972eb287064f98db1a4487c62bd7566",
        "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models"
      },
      {
        "id": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
        "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems"
      },
      {
        "id": "a3509cef906a4517238c1764676cf637efcd1d5e",
        "title": "Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models"
      },
      {
        "id": "553d85e202fdbdd4101673b9205135b8eb94811d",
        "title": "Recovering from Privacy-Preserving Masking with Large Language Models"
      },
      {
        "id": "2883358201b604b86b4d11fa5d00653e244bedac",
        "title": "Procedural Text Mining with Large Language Models"
      },
      {
        "id": "61672e29c47e73cc317425c2842fedba0a52438a",
        "title": "PersianLLaMA: Towards Building First Persian Large Language Model"
      },
      {
        "id": "836b9658eb81f321de90423b6259b07a398ca79b",
        "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way"
      },
      {
        "id": "43189527d68e612a911680c7039dddba4f030985",
        "title": "AI-Powered Software Testing: The Impact of Large Language Models on Testing Methodologies"
      },
      {
        "id": "1477a634e05a07d7d73f2b7a9f7b9003ad7e6858",
        "title": "Are Large Language Models Good Fact Checkers: A Preliminary Study"
      },
      {
        "id": "9a2ee8c33deef36c2dbc7af5d2a8a7cc7c87d882",
        "title": "Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task"
      },
      {
        "id": "f221eccdd96122a42c5e65532373e6974b30c20c",
        "title": "Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills"
      },
      {
        "id": "d0ffb09a00b67365efb9e217c3fd45d804733810",
        "title": "Large Language Models are biased to overestimate profoundness"
      },
      {
        "id": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "id": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      },
      {
        "id": "cd2f4aaf98bb1e020cff310000c8049d3460c54e",
        "title": "NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark"
      },
      {
        "id": "9fcdbfdf28245010c875ce85502351fe05c04b49",
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View"
      },
      {
        "id": "83ac79bb8e8695fb3c3c024be74790d862adea74",
        "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"
      },
      {
        "id": "f80e7b5baa6adf0cd5dddb5ba973fed9d8a216cf",
        "title": "Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education"
      },
      {
        "id": "f5cc6df1a7d00a5ed93bee3db2e5df00c338a936",
        "title": "LLMs Accelerate Annotation for Medical Information Extraction"
      },
      {
        "id": "d1b11a8f4e06eb377febc01ec3f4e4eb3345becf",
        "title": "Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study"
      },
      {
        "id": "c47ba62dd18c70aafa04a8e04e105f624090217c",
        "title": "LLM Based Generation of Item-Description for Recommendation System"
      },
      {
        "id": "3c55aa582a2d682eb53e9237589234854e0b92d8",
        "title": "Empirical Study of Zero-Shot NER with ChatGPT"
      },
      {
        "id": "b3eed5317463c994f98de7b5b8e4558aa0b623b4",
        "title": "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT"
      },
      {
        "id": "7c9bb230946cf48a7b9de97fd0281f42fbc51d31",
        "title": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting"
      },
      {
        "id": "58219d9826f9ddde448c73e7ecc690111f5698f4",
        "title": "ReAcTable: Enhancing ReAct for Table Question Answering"
      },
      {
        "id": "4726d1dc54851db99c29180127d840bd19f20afc",
        "title": "Positional Description Matters for Transformers Arithmetic"
      },
      {
        "id": "bfeda6c7aa7899a80adb01894555b09d24756a59",
        "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"
      },
      {
        "id": "7c629ca898fb8c87dd363a16d43f29f6ed44dfa8",
        "title": "Generative AI Text Classification using Ensemble LLM Approaches"
      },
      {
        "id": "c1284ee1ddf29955a1a02bdc45abdaac63745017",
        "title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method"
      },
      {
        "id": "c3c20340fb3b4c4db30291e3b29a1a7b557d102d",
        "title": "Application of ChatGPT in Routine Diagnostic Pathology: Promises, Pitfalls, and Potential Future Directions"
      },
      {
        "id": "cfd7550d6c25f4771549726b289665c8aa5ae1be",
        "title": "Thread of Thought Unraveling Chaotic Contexts"
      },
      {
        "id": "ae87028a650705a02f2f8e7bbba9f2c5718ddb68",
        "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey"
      },
      {
        "id": "644a0aa82f58b7f387fb1b731f0932c84c2f200f",
        "title": "Exploring the Boundaries of GPT-4 in Radiology"
      },
      {
        "id": "401510c31725ea9277a4d2e049682578a0134e1c",
        "title": "Distilled GPT for source code summarization"
      },
      {
        "id": "18b75ea107ed166d7120c12c162b94f02e20b417",
        "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks"
      },
      {
        "id": "d2af7f63861ad683b061b508316624615bff162d",
        "title": "Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations"
      },
      {
        "id": "37d7fd90943b76657ff88d030a9d28677914160f",
        "title": "Investigating ChatGPT’s Potential to Assist in Requirements Elicitation Processes"
      },
      {
        "id": "2967ab775f6cdabc6ab59010734f352dd3ebc8d6",
        "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction"
      },
      {
        "id": "880aa839e9275808573f221988944323bf40ddab",
        "title": "Bias and Fairness in Chatbots: An Overview"
      },
      {
        "id": "e06595ebb2fe4d73fe42e566b57d7a109df75615",
        "title": "Instruction-following Evaluation through Verbalizer Manipulation"
      },
      {
        "id": "4016fe2e7916b349f58d53f2e8a756bccb9c147a",
        "title": "From Turing to Transformers: A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models"
      },
      {
        "id": "c02cded20074fff4310d7bc943d0b8bfff305d58",
        "title": "Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench"
      },
      {
        "id": "247509f5658374678830582dddfdb6bc4d8af25e",
        "title": "Is ChatGPT a Good Personality Recognizer? A Preliminary Study"
      },
      {
        "id": "afa9f128435501d0f2c3ae524a0a7698d0bc3d21",
        "title": "Fast Quantum Algorithm for Attention Computation"
      },
      {
        "id": "4f249487c670263700df7b2269cdb92a265bc21f",
        "title": "Effective Distillation of Table-based Reasoning Ability from LLMs"
      },
      {
        "id": "10eb81d069f3654fa234f769852173a9ddadfabd",
        "title": "Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges"
      },
      {
        "id": "f7432217b3ce63f3e3c5d231c69103a8d1feeaa3",
        "title": "Distributional Semantics"
      },
      {
        "id": "0fb61be60088e80e565b84f44e49ba30630b6126",
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"
      },
      {
        "id": "647d64679501b9c161d39f9872d255b8dec95def",
        "title": "DUnE: Dataset for Unified Editing"
      },
      {
        "id": "ec2c330301fa8a9f4b6357d9ca630bf5bcd50996",
        "title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams"
      },
      {
        "id": "e3015bd4740b1eb5be667481344e3b83ebfdd556",
        "title": "Portrayal: Leveraging NLP and Visualization for Analyzing Fictional Characters"
      },
      {
        "id": "b5e44d73ccaae49540f10c72cae0d902d6170754",
        "title": "Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety"
      },
      {
        "id": "d76389f4585aef5c8176ff27e5f93b22e7d5dfbf",
        "title": "Vec-Tok Speech: Speech Vectorization and Tokenization for Neural Speech Generation"
      },
      {
        "id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
        "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning"
      },
      {
        "id": "557b10adfad3b9963f18f7438935d8c86109691e",
        "title": "M2C: Towards Automatic Multimodal Manga Complement"
      },
      {
        "id": "a97123a21f27ebd0e4bd0f6c4def42c87813fd64",
        "title": "Is ChatGPT a Good Multi-Party Conversation Solver?"
      },
      {
        "id": "599c24276441b9255b523eec917231b8b49a620e",
        "title": "Plansformer Tool: Demonstrating Generation of Symbolic Plans Using Transformers"
      },
      {
        "id": "cb1b43d25f18ad37d7ca1110dcdb87287b4186c6",
        "title": "An Overview Of Temporal Commonsense Reasoning and Acquisition"
      },
      {
        "id": "ae10d1462e63fd361550546d2470510e9a93fe75",
        "title": "Crosslingual Retrieval Augmented In-context Learning for Bangla"
      },
      {
        "id": "acd7adcdb914fb2e18a19792af47e59e78cab3f0",
        "title": "Overview of the PromptCBLUE Shared Task in CHIP2023"
      },
      {
        "id": "ba198cf0fd16b173fb07736e91fc75e17198bf69",
        "title": "Exploring the Effectiveness of GPT-3 in Translating Specialized Religious Text from Arabic to English: A Comparative Study with Human Translation"
      },
      {
        "id": "6ad7b84eba57e9d506444d604e6719d1b419dca0",
        "title": "AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction"
      },
      {
        "id": "23cc6b2ed88872fcd3767cf054100e8eddcdb0a1",
        "title": "CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks"
      },
      {
        "id": "5402c22369d0190d0a002b7a1222d403edae010a",
        "title": "Defining a New NLP Playground"
      },
      {
        "id": "94d878ba7eeba4abc4d5e42b4c2c4c98d4e575ce",
        "title": "Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning"
      },
      {
        "id": "3ca44971c0be92b25a9905972a6d1cbe730061cb",
        "title": "Natural Language to Code: How Far Are We?"
      },
      {
        "id": "a7cb000aa86eb035cdb527349885191fe52baf86",
        "title": "Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation"
      },
      {
        "id": "4c6fb350e7769cb730a15c62927b6e9b563d0157",
        "title": "DARWIN Series: Domain Specific Large Language Models for Natural Science"
      },
      {
        "id": "28c6ac721f54544162865f41c5692e70d61bccab",
        "title": "A Survey on Large Language Model based Autonomous Agents"
      },
      {
        "id": "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
      },
      {
        "id": "8aab972b0c3a3d581536b0d74339794809dc1a64",
        "title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models"
      },
      {
        "id": "529ff7d6441d244212cf2becafd12a7e67ac56d9",
        "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"
      },
      {
        "id": "958530b2a6267fd0c251a7c82e0267c21dca9cdf",
        "title": "Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing"
      },
      {
        "id": "65d7663b60d95f98e6281ecc4da9c7a975119b91",
        "title": "GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT"
      },
      {
        "id": "bab5e35001757719d0f8338f94dde2860dae784a",
        "title": "How Large Language Models Will Disrupt Data Management"
      },
      {
        "id": "632a758099f412b13d0a8172830a3ac7ee5e0dd5",
        "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT"
      },
      {
        "id": "c24ed03689adc2a075004c6619f43f7f6230fe4b",
        "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop"
      },
      {
        "id": "816f709246530845971c62374c84a2fed8871b8e",
        "title": "Black Box Warning: Large Language Models and the Future of Infectious Diseases Consultation"
      },
      {
        "id": "06f710f59e2119ed892e9291eb942847a34db91c",
        "title": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph"
      },
      {
        "id": "cc8dec40ee2ccce16fb70218f970515338149a09",
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models"
      },
      {
        "id": "9a7b9515b66bf83c9c808626206eabe9a8837c22",
        "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models"
      },
      {
        "id": "68ef6138e007421f1a70e2d0ce1b3308a54cc784",
        "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models"
      },
      {
        "id": "573dad7b2fca7ce72a7f0daf681391d96379ebe3",
        "title": "Low-Parameter Federated Learning with Large Language Models"
      },
      {
        "id": "ba015c5d3f5b44e36363b90070bb3301d21ae57e",
        "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?"
      },
      {
        "id": "a2ccffe67a4ccfb10279dc3f0167fe65ae01e471",
        "title": "Language Models can be Logical Solvers"
      },
      {
        "id": "4f2be887e991efa85f7b874e7ab871080a745c39",
        "title": "CAESURA: Language Models as Multi-Modal Query Planners"
      },
      {
        "id": "8e079a5e525a4cecbccaeff5e04291a8d06d588d",
        "title": "Transformers and Large Language Models for Chemistry and Drug Discovery"
      },
      {
        "id": "15a782b48fd26f2ce63ab1259e647c3656ce43c7",
        "title": "Split-and-Denoise: Protect large language model inference with local differential privacy"
      },
      {
        "id": "01802333cdf7bfe00935ba8a7390f3d028e4fb25",
        "title": "Fine-Tuning Language Models Using Formal Methods Feedback"
      },
      {
        "id": "dab4f70d75a04e62553e583f2450d9bb1f0ead46",
        "title": "CLOMO: Counterfactual Logical Modification with Large Language Models"
      },
      {
        "id": "5e2d50f7745d7d1cd92c1f8fb79ec03735605b08",
        "title": "An appraisal-based chain-of-emotion architecture for affective language model game agents"
      },
      {
        "id": "3f5a434393e560ac0c2c1823f5f620a69f29b5e6",
        "title": "Integration of Large Language Models within Cognitive Architectures for Autonomous Robots"
      },
      {
        "id": "e58b0ee9a1fdb15a72ee721053df3569127cde42",
        "title": "Tackling Vision Language Tasks Through Learning Inner Monologues"
      },
      {
        "id": "1d5ffd4f19355c074da1f9e8b128941ca41d9f11",
        "title": "Make LLM a Testing Expert: Bringing Human-Like Interaction to Mobile GUI Testing via Functionality-Aware Decisions"
      },
      {
        "id": "70a75b05a9410198fd71c3a0fe937a77d15d6bc8",
        "title": "Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text"
      },
      {
        "id": "c47cb9d407b8cfd51699881a804811cc0e5fcade",
        "title": "CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models"
      },
      {
        "id": "c0b2f6e956bf40a59c766f6da5947cb14e7d1d26",
        "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations"
      },
      {
        "id": "4ddc26b3a5fe9044b97b408d163f7464d769ebbf",
        "title": "CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"
      },
      {
        "id": "4776de7b856d3b15eecc4f88666cdc13972df22e",
        "title": "Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking"
      },
      {
        "id": "21510620f0c92dde08741070a00593bcd1815d8c",
        "title": "Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?"
      },
      {
        "id": "86f66498f199d06353c5b8df1b774b80bf29eeac",
        "title": "Agent-based Learning of Materials Datasets from Scientific Literature"
      },
      {
        "id": "c8a0335bad4436ed525135d5e3cad18967ac0793",
        "title": "Using LLMs to Customize the UI of Webpages"
      },
      {
        "id": "4cb4a7304b848e630db504af9d818323fd77fb8b",
        "title": "Data Formulator: AI-Powered Concept-Driven Visualization Authoring"
      },
      {
        "id": "e2d0bd5bfd0d1d35484552abf9f7fbcd276c8669",
        "title": "Effectiveness of Generative Artificial Intelligence for Scientific Content Analysis"
      },
      {
        "id": "caf8c9e52658af46ee6651bf004d545bd379f087",
        "title": "Enhancing Text-to-SQL Translation for Financial System Design"
      },
      {
        "id": "ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"
      },
      {
        "id": "958ed4830ae80a189ecb9b93ab75a6ce2e3926fc",
        "title": "GPT-Driver: Learning to Drive with GPT"
      },
      {
        "id": "3de1d214dc79ecd0b34faf6fb0520dd7d2988c97",
        "title": "ChatGPT as Your Vehicle Co-Pilot: An Initial Attempt"
      },
      {
        "id": "862ed55c8a7d4ef1b23091995d42aae91d906a1b",
        "title": "Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)"
      },
      {
        "id": "c6649c4247282c3e46593e26c5e0429fcb7c7673",
        "title": "Sallm: Security Assessment of Generated Code"
      },
      {
        "id": "873a581320d928249609d3c07229d5af182a379c",
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"
      },
      {
        "id": "ef9b84a57a654888729ce73a3f2400d56e12a300",
        "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations"
      },
      {
        "id": "0d3289eae18ef3a4a88d912bee1e2fe184faa355",
        "title": "Automatically Reproducing Android Bug Reports using Natural Language Processing and Reinforcement Learning"
      },
      {
        "id": "ad959d9c6e108a7b2a08eff62ae4638237e269e4",
        "title": "Inquiring Natural Language Processing Capabilities on Robotic Systems through Virtual Assistants: A Systemic Approach"
      },
      {
        "id": "931cec40538dcec99043b3502ca58e9e0fdf8873",
        "title": "Exploring natural language processing in mechanical engineering education: Implications for academic integrity"
      },
      {
        "id": "b8647eb447102c3df003a39b4768eacdc83d93d7",
        "title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing"
      },
      {
        "id": "df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
        "title": "Can Language Models Solve Graph Problems in Natural Language?"
      },
      {
        "id": "5eab810cc5d90de1c52127d1a5824f0817f46c30",
        "title": "Natural Language Reasoning, A Survey"
      },
      {
        "id": "695d11cfd7838b313935fe5fc7938de3816e5c7c",
        "title": "Understanding Natural Language Understanding Systems. A Critical Analysis"
      },
      {
        "id": "d9bd490aba3a7995f728594403b3358cb79aacd6",
        "title": "Synthesis of Mathematical programs from Natural Language Specifications"
      },
      {
        "id": "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap"
      },
      {
        "id": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      },
      {
        "id": "9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
        "title": "Language Models can Solve Computer Tasks"
      },
      {
        "id": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"
      },
      {
        "id": "03055978e278960de9fbb5c648b1779ef9f26cd1",
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?"
      },
      {
        "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
        "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context"
      },
      {
        "id": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"
      },
      {
        "id": "170c97c7215f42edfb20c2248f954879e91ef86e",
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"
      },
      {
        "id": "12594b6afe01461384d2856d2bf44f1cf8533e3e",
        "title": "ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health"
      },
      {
        "id": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models"
      },
      {
        "id": "2029349c55c1dba3493c5b3bd25152f18ba21ae2",
        "title": "Augmented Language Models: a Survey"
      },
      {
        "id": "1645e93f34ce34c0ff248a7349bf757a416c5312",
        "title": "Health system-scale language models are all-purpose prediction engines"
      },
      {
        "id": "e154dd91de91558f9d671370754eace62a54c911",
        "title": "A study of generative large language model for medical research and healthcare"
      },
      {
        "id": "78c488e2d84bd193a40006b1fceb03e3845b81d4",
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"
      },
      {
        "id": "5dea206e2a36e672f197252bdd27d156d058f48c",
        "title": "FinGPT: Open-Source Financial Large Language Models"
      },
      {
        "id": "70da4fb798a86cbe8cad96c27ced0415885bbd9d",
        "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators"
      },
      {
        "id": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
        "title": "Do Large Language Models Know What They Don't Know?"
      },
      {
        "id": "2b967d82b25088566980aaaf5a7062d90b2fb14f",
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking"
      },
      {
        "id": "109929be7890ef982fb3b6be0d78609cfab1ea13",
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance"
      },
      {
        "id": "8ab4863393fceb41d0fa77d632ace4c80a57a154",
        "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning"
      },
      {
        "id": "20d7965c0b282a0cd7f990e435d0f6bc9535bbc6",
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks"
      },
      {
        "id": "5ef821267fa68d3231ed8135ff8ec09f25bb1398",
        "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"
      },
      {
        "id": "6847b9658f287f430098199cd81bf26308da13f9",
        "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"
      },
      {
        "id": "f4bdec0cf595720bc8ee5df2196324bac8f52ab4",
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources"
      },
      {
        "id": "68850153b0210615c86f9a72624f34e2913bcddf",
        "title": "Document-Level Machine Translation with Large Language Models"
      },
      {
        "id": "1f040c3a8d49f8e54169a0e07013692c7d58de4b",
        "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks"
      },
      {
        "id": "ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3",
        "title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation"
      },
      {
        "id": "3db1219429c3f04e88347d41269bdc83c457fbf9",
        "title": "Symbol tuning improves in-context learning in language models"
      },
      {
        "id": "a8148b9dce6101b080e499b4c48cbe267d745131",
        "title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents"
      },
      {
        "id": "278eae3362fd00b1092051240ab31dfa0bf8b581",
        "title": "An Empirical Study on Information Extraction using Large Language Models"
      },
      {
        "id": "e47e63781c0e7a2c0504b9381b76b5d01b62c53d",
        "title": "InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models"
      },
      {
        "id": "f9bfc6d9ba1665b73af3323d46c7642b852759ef",
        "title": "VideoLLM: Modeling Video Sequence with Large Language Models"
      },
      {
        "id": "c1c98ef93fb6474837961ef300cf3d8e7d3a0cd0",
        "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer"
      },
      {
        "id": "aafae4730b1add0b3e243e011db9ac87428f83cd",
        "title": "BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark"
      },
      {
        "id": "e5754bb65a648f319a02d47c356df0db1e936b7f",
        "title": "Post Hoc Explanations of Language Models Can Improve Language Models"
      },
      {
        "id": "b9a1189f2de7fd5e66551d7c425556e5642b823a",
        "title": "ChessGPT: Bridging Policy Learning and Language Modeling"
      },
      {
        "id": "087a2f4cfea227be944f576f1f049e329316acac",
        "title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models"
      },
      {
        "id": "ef0679f8b3114c339bdf5a0c202403a08d160a88",
        "title": "ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models"
      },
      {
        "id": "327e0290fd71609bfc1a30478a95f690668fe622",
        "title": "Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies"
      },
      {
        "id": "3f758a13d3703b02bdf977f9189230276064da42",
        "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering"
      },
      {
        "id": "a01acc3b62b4e43fd66d17b26d27a7a32623dcf1",
        "title": "ParroT: Translating During Chat Using Large Language Models"
      },
      {
        "id": "e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2",
        "title": "Augmented Large Language Models with Parametric Knowledge Guiding"
      },
      {
        "id": "d01f6e76e67a445f23f807c0d3e68fa0be9a2c9e",
        "title": "The Next Chapter: A Study of Large Language Models in Storytelling"
      },
      {
        "id": "95430a76264a9be7d64633e56831c60041fb2948",
        "title": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery"
      },
      {
        "id": "b8dd3a023b6f3e3bb862d172d84c3f29d3f840d1",
        "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond"
      },
      {
        "id": "a22f3398ea865426c89ee66f4824ec626e56a864",
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models"
      },
      {
        "id": "9d81ec931b85d6c6cf3453126670cd7a30a689e7",
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models"
      },
      {
        "id": "c226a4acb42912054d498bcf771023b0ba2da001",
        "title": "Language Model Self-improvement by Reinforcement Learning Contemplation"
      },
      {
        "id": "4ad189079046bf9df0f8ece7545e42dc00e13b9f",
        "title": "Towards a Robust Detection of Language Model-Generated Text: Is ChatGPT that easy to detect?"
      },
      {
        "id": "e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7",
        "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"
      },
      {
        "id": "258605dc5b00fe66b72091f947642a554e472aee",
        "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task"
      },
      {
        "id": "6ab06565288cd2bd20e701a2daab192f71c0f7e7",
        "title": "A Trip Towards Fairness: Bias and De-Biasing in Large Language Models"
      },
      {
        "id": "f90051f22f263d7c39355f07aea18ae3fcfe344a",
        "title": "An empirical study of pre-trained language models in simple knowledge graph question answering"
      },
      {
        "id": "9884153bbcd20282f32e897363883548e0fd8e50",
        "title": "Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health"
      },
      {
        "id": "a563fa042b16533d23829d76e7e17bf19a05891c",
        "title": "Large Language Models Are Not Strong Abstract Reasoners"
      },
      {
        "id": "93ebfcd6bb0724b3bb8da27edd468514187c446c",
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback"
      },
      {
        "id": "9afa0c3227fd0ec3a76928784e59c4205cbace24",
        "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks"
      },
      {
        "id": "597fa9f6787ef7b0b352e8b9d025dff4a8488c3f",
        "title": "LM4HPC: Towards Effective Language Model Application in High-Performance Computing"
      },
      {
        "id": "c8779da397189ce5b16577db058461c5efb0d2c1",
        "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models"
      },
      {
        "id": "490d8006851b1562cfd9ec1f057471f2868289d1",
        "title": "Rethinking with Retrieval: Faithful Large Language Model Inference"
      },
      {
        "id": "e8f7cc40208f4b6f9eca59715666f320d66d386e",
        "title": "A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models"
      },
      {
        "id": "9f87c8e27a10d71500314e7e21853f5a23efce59",
        "title": "LAraBench: Benchmarking Arabic AI with Large Language Models"
      },
      {
        "id": "a70339b84db86a974335499eb824ebdab3a422b3",
        "title": "Large Language Model Instruction Following: A Survey of Progresses and Challenges"
      },
      {
        "id": "f9338e1b32cb450d95094ce71957375754823e4b",
        "title": "Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models"
      },
      {
        "id": "29203f0b8b9be7fd70d99bf7390c6a78b68a9289",
        "title": "Conceptual Design Generation Using Large Language Models"
      },
      {
        "id": "5882dd04d95c9c88cdec389059fcf44d56cbb789",
        "title": "Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation"
      },
      {
        "id": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
        "title": "Explicit Planning Helps Language Models in Logical Reasoning"
      },
      {
        "id": "7df3595bdb4003589e8ca1757cc39ec03a39a2ff",
        "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models"
      },
      {
        "id": "d7d5d3c76bd71042b976fffe85c37b0df641732a",
        "title": "Massively Multilingual Shallow Fusion with Large Language Models"
      },
      {
        "id": "6a6cbcc596758dd214120a1d51528ce55daa333d",
        "title": "Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation"
      },
      {
        "id": "661e64593fca437e41d4b90bcbc440ba76d988d2",
        "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs"
      },
      {
        "id": "ea1db5796b16c495d12c45a2c34539fd361d856f",
        "title": "Comparative Analysis of CHATGPT and the evolution of language models"
      },
      {
        "id": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"
      },
      {
        "id": "026b3396a63ed5772329708b7580d633bb86bec9",
        "title": "RWKV: Reinventing RNNs for the Transformer Era"
      },
      {
        "id": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a",
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"
      },
      {
        "id": "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "title": "One Fits All: Power General Time Series Analysis by Pretrained LM"
      },
      {
        "id": "e5adc219685c9941b9a3d029480af4a51c0ea05a",
        "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"
      },
      {
        "id": "98b40eb3ce79c24d9726556947e2e2094737fe46",
        "title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models"
      },
      {
        "id": "62ad7ea9467bbcdbfe325b9ee561cab3908e4583",
        "title": "MEGA: Multilingual Evaluation of Generative AI"
      },
      {
        "id": "44f0876dec21a04533587def2add230b878a5006",
        "title": "Prompt Engineering with ChatGPT: A Guide for Academic Writers"
      },
      {
        "id": "302ee27524a717ddc21f332ca634b9211c6ec6aa",
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"
      },
      {
        "id": "30c0cdc414f68211d5d0514df027cec22e005174",
        "title": "A Survey on In-context Learning"
      },
      {
        "id": "c082ccfcfe1afc696e371374146ba9380b84061e",
        "title": "The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field"
      },
      {
        "id": "306c0576750d8ac1298f70474560aa951490b2a1",
        "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization"
      },
      {
        "id": "5a8f34df779227a5cb1f573349556ef479d4359a",
        "title": "Exploring the Feasibility of ChatGPT for Event Extraction"
      },
      {
        "id": "0931888d5cd7c26427cc116af2ac33863552da27",
        "title": "SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model"
      },
      {
        "id": "e3bc7f2bbe6e6c27be6132591d53df308f16ab97",
        "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT"
      },
      {
        "id": "1a01c982aa20c1a1ad1ad94866e3197da99a52a2",
        "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation"
      },
      {
        "id": "9a31b2ce43fe198ab1fd046ca4ec70fded154aee",
        "title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?"
      },
      {
        "id": "da872dfc0934f554311d5f91fa7e5ada44fb8155",
        "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT"
      },
      {
        "id": "8236010c2ecc94d826be6010ff187fdc000e7df6",
        "title": "Deductive Verification of Chain-of-Thought Reasoning"
      },
      {
        "id": "932b9fd1e2aaf3c56841304b7a49e30c804f6234",
        "title": "Inseq: An Interpretability Toolkit for Sequence Generation Models"
      },
      {
        "id": "1b9fc8268b392742ea43c2c017a767cf62386139",
        "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation"
      },
      {
        "id": "ef4cb88b1635b34af15059567dfdf134f79797aa",
        "title": "The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges"
      },
      {
        "id": "18ff1542d5a2a4490c7b3f21522bf1343889f700",
        "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications"
      },
      {
        "id": "6001a5d38df9a043421a670357842d8df71d656b",
        "title": "Grokking of Hierarchical Structure in Vanilla Transformers"
      },
      {
        "id": "197022486b2e2584302bd9b6442e44d15bf3e351",
        "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"
      },
      {
        "id": "09d51a7367fc7fbce643ae5f70c9959ed723009e",
        "title": "Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in Scientific Literature"
      },
      {
        "id": "66f598c974ddc36d16ea70ad49c08ad13a666408",
        "title": "ChatGPT: Systematic Review, Applications, and Agenda for Multidisciplinary Research"
      },
      {
        "id": "4cd9389c227b771ec40cf09d9e7657391098f285",
        "title": "An Opinion on ChatGPT in Health Care—Written by Humans Only"
      },
      {
        "id": "ea647df1d12698114a87cbf043160edbc4cd0722",
        "title": "CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles"
      },
      {
        "id": "6289de84a02f0c27734f295ada565603ac958948",
        "title": "Tab-CoT: Zero-shot Tabular Chain of Thought"
      },
      {
        "id": "f27fcbbc4d449c63164555ff301c3471c616bda2",
        "title": "Performance of Generative Pretrained Transformer on the National Medical Licensing Examination in Japan"
      },
      {
        "id": "ae5e38058e9d622666254fa873c35a449a7bb2e1",
        "title": "Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis"
      },
      {
        "id": "1cdfa7c3465943a295f8df2d2097c4bb3e222426",
        "title": "Rationalization for explainable NLP: a survey"
      },
      {
        "id": "a49687ee1ce6ace8329cfcb693d8f8198c867bcc",
        "title": "ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs"
      },
      {
        "id": "06cece9731273f604d69a73eea5e7ae57fee5056",
        "title": "Slide4N: Creating Presentation Slides from Computational Notebooks with Human-AI Collaboration"
      },
      {
        "id": "d8b2497f1da0dbbd79679f1df9449f0b01c9c4f0",
        "title": "An Iterative Algorithm for Rescaled Hyperbolic Functions Regression"
      },
      {
        "id": "757ed5bd81d6ddf10d3a119d208ba2a9a279c051",
        "title": "Towards Segment Anything Model (SAM) for Medical Image Segmentation: A Survey"
      },
      {
        "id": "b1e67b0cc5705d6aade931e6414ce23dc0ff44b3",
        "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages"
      },
      {
        "id": "174ae9800f6359520d900d19890acfcf46709107",
        "title": "Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference"
      },
      {
        "id": "90df9c6924425d7366d16731a34bfa4e52ad5b2e",
        "title": "What’s the Meaning of Superhuman Performance in Today’s NLU?"
      },
      {
        "id": "4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5",
        "title": "Brainformers: Trading Simplicity for Efficiency"
      },
      {
        "id": "fafb7f456d702147c64356dc9d537c336fa55ae6",
        "title": "Linguistic ambiguity analysis in ChatGPT"
      },
      {
        "id": "9a147712d46d1b01a761987c874b628c34c52cda",
        "title": "Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources"
      },
      {
        "id": "758985395372f5378fcf036094195b2848e13a21",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning"
      },
      {
        "id": "fa7bbdd62c230ba4708661a880ba8b34aad7f491",
        "title": "Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization"
      },
      {
        "id": "8efbd687804a13762f135db30b6077a1b171ae01",
        "title": "Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media"
      },
      {
        "id": "f57afb6c8addfc7a32f9be5916a374a542d1a026",
        "title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram"
      },
      {
        "id": "ccf42104c22432c96d411eeb8f3b9afbfe0dfdd0",
        "title": "Towards Trustworthy Explanation: On Causal Rationalization"
      },
      {
        "id": "d75d11d2c89c01cd284383546ae057cb827dc272",
        "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"
      },
      {
        "id": "c3349d0493a56f76c27d74c5056ab039f665b7b4",
        "title": "Synthesizing Human Gaze Feedback for Improved NLP Performance"
      },
      {
        "id": "df7d7e71eba619363e7dbe8b14f4baeb3100ca73",
        "title": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks"
      },
      {
        "id": "f9d4bfe5618b920a47d39be894658fb4275541ab",
        "title": "FiNER-ORD: Financial Named Entity Recognition Open Research Dataset"
      },
      {
        "id": "544f43e6162bb1c3265ce88c7cb56459d64b7b6a",
        "title": "A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot"
      },
      {
        "id": "6927a5b0152433a199ab4974ad85e787454d6a30",
        "title": "Should We Attend More or Less? Modulating Attention for Fairness"
      },
      {
        "id": "8d8e09ded78cae335e1fa7d5532509f74507c760",
        "title": "Deep Learning in ChatGPT - A Survey"
      },
      {
        "id": "c934f7df636faa36fb0c2074ecdefa6dcb7858f1",
        "title": "Prompt-based for Low-Resource Tibetan Text Classification"
      },
      {
        "id": "dad461379a2645a21089945b6d2fce4124b5de76",
        "title": "Building Intelligent Chatbots: Tools, Technologies, and Approaches"
      },
      {
        "id": "41c271154cb8be6e36ce97ca06cbbadf15c82538",
        "title": "Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives"
      },
      {
        "id": "b8e16ad905ec6d9f2e405ba29769a47834e71db8",
        "title": "Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition"
      },
      {
        "id": "430aa6966c15c4a20a4fb2d8383e136b9cb6cde7",
        "title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine"
      },
      {
        "id": "375a571174ea59b1f4aa62ad2619e9593fc03436",
        "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning"
      },
      {
        "id": "48385ded07af641da331c05f6ea3f93694a08425",
        "title": "Prompting Is All You Need: Automated Android Bug Replay with Large Language Models"
      },
      {
        "id": "0ff8c04c8bdbf93b39b49582c9195cf3fc894d03",
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)"
      },
      {
        "id": "c65fd7dc0558adc8d0ea135914530e26d94fbc91",
        "title": "Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code"
      },
      {
        "id": "362cbfd0d05e139cd6cf049754098a6e1520b910",
        "title": "PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"
      },
      {
        "id": "768850402f991a97ce7a7d063d07050add0254ca",
        "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners"
      },
      {
        "id": "bdbe2c1a430d8a15d20964bfd5d7e828ccae73d0",
        "title": "Querying Large Language Models with SQL"
      },
      {
        "id": "c8fa30492240c406ed773becc7544ce1d09d5ca5",
        "title": "Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy"
      },
      {
        "id": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "id": "502bc7694dbe5dbf2bb7aaf576cd2bc13e6b95d8",
        "title": "Chip-Chat: Challenges and Opportunities in Conversational Hardware Design"
      },
      {
        "id": "8973ad5fc1264594a1fda3bd9e04258074cea9cc",
        "title": "Neural Architecture Search: Insights from 1000 Papers"
      },
      {
        "id": "266d671d5d6bac3c88d7bfb18c5210b46f06e6db",
        "title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT"
      },
      {
        "id": "f178afd3afe6970ff9ed172e8ef5b1946d0c3ba8",
        "title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents"
      },
      {
        "id": "bc283b0e53f749f6c5b8b67eb340ea6a5a4331f5",
        "title": "Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI Testing"
      },
      {
        "id": "5db3257a61d86f302767ae1f21d6fd30567f12e5",
        "title": "Towards Building The Federatedgpt: Federated Instruction Tuning"
      },
      {
        "id": "b2676eac94dc4b138ade95b643534b95f58760d6",
        "title": "Bayesian Optimization of Catalysis With In-Context Learning"
      },
      {
        "id": "2e01fdebbc780d3667ec3bf87a44927f0d9c188a",
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration"
      },
      {
        "id": "19ea368b7f88279899c40813a797dda7adc50c07",
        "title": "Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation"
      },
      {
        "id": "ca0c955699f552e1c2fbda747bd41faf8a2513ce",
        "title": "Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text"
      },
      {
        "id": "1aea23ef6545523e3f0f38d7f62feed11dfe0e44",
        "title": "A Comprehensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT"
      },
      {
        "id": "40c9280d87059c0cc28f2a08d46a7045fa3e9736",
        "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL"
      },
      {
        "id": "10c5079d2baa5a287054d9ddd7806a4c16fd7531",
        "title": "UDAPTER - Efficient Domain Adaptation Using Adapters"
      },
      {
        "id": "c77d908ba29567445a9a4ad1bd4461d441cce174",
        "title": "AutoML-GPT: Automatic Machine Learning with GPT"
      },
      {
        "id": "eda08c6f5919f39979acf0b3bc52e903063b5ba4",
        "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation"
      },
      {
        "id": "3c50ef336232da0885ef61da386c98eac964b7cd",
        "title": "PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology"
      },
      {
        "id": "389ec3e8902a5dcfcde1adec735854e93f845937",
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"
      },
      {
        "id": "e1eb40122b5ddb2a14473da816fb065f4b503538",
        "title": "Towards Human-Bot Collaborative Software Architecting with ChatGPT"
      },
      {
        "id": "966852963a88a28786b798c91b6662d6e501e590",
        "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"
      },
      {
        "id": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "id": "fd7e88a2313e176315d99fc299277e752d7703b7",
        "title": "Efficient Methods for Natural Language Processing: A Survey"
      },
      {
        "id": "6ab6e6f62323132e299fc6717ad0f5ca000414d5",
        "title": "Natural language processing in clinical neuroscience and psychiatry: A review"
      },
      {
        "id": "a8c09c41f39d798dc4201eeec1452fe617e428df",
        "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing"
      },
      {
        "id": "c27c68597065dad70cb0b4f882cd861826bf28fa",
        "title": "Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"
      },
      {
        "id": "5581bf85386737bd3378eec68189759a05280bea",
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic"
      },
      {
        "id": "18b3ab9763ed3c4633ee68aa6dd75f6377837553",
        "title": "Natural Language Deduction with Incomplete Information"
      },
      {
        "id": "e383ee065bb7b9ef798d5aff9db794691f4c0e38",
        "title": "Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook"
      },
      {
        "id": "db4ab91d5675c37795e719e997a2827d3d83cd45",
        "title": "Towards Reasoning in Large Language Models: A Survey"
      },
      {
        "id": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
        "title": "Language Models of Code are Few-Shot Commonsense Learners"
      },
      {
        "id": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
        "title": "Large Language Models are Better Reasoners with Self-Verification"
      },
      {
        "id": "ee8de585183763ff64cb3c81ecda2fc75fa81507",
        "title": "Large Language Models with Controllable Working Memory"
      },
      {
        "id": "2abed82162c47a0cc32cd62afcf46b0745541017",
        "title": "Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"
      },
      {
        "id": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
        "title": "Rationale-Augmented Ensembles in Language Models"
      },
      {
        "id": "e37310ccc5f368355c19b4de45826278aeaff280",
        "title": "Understanding HTML with Large Language Models"
      },
      {
        "id": "b4c80344081d8c548fc4d68868b8262182a146fa",
        "title": "Structured information extraction from complex scientific text with fine-tuned large language models"
      },
      {
        "id": "2a7ae3e98357569c41424dacd60c62d3df78a0db",
        "title": "Limitations of Language Models in Arithmetic and Symbolic Induction"
      },
      {
        "id": "763125fd2befe605b009cdd8d7ee8c8c694bc9e5",
        "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models"
      },
      {
        "id": "ccfa7a251644aafc7f85ca66c6652adfcf93429c",
        "title": "From Word Embeddings to Pre-Trained Language Models: A State-of-the-Art Walkthrough"
      },
      {
        "id": "2b6291eb76e2ff885238e94704bb795046d7d530",
        "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models"
      },
      {
        "id": "f557f3a32d309373e7d31bb93ca1b80b4a6e39e7",
        "title": "Symbolic Math Reasoning with Language Models"
      },
      {
        "id": "44279244407a64431810f982be6d0c7da4429dd7",
        "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"
      },
      {
        "id": "f3a6115e5fb2237df938976e005468f0b18da797",
        "title": "The Stack: 3 TB of permissively licensed source code"
      },
      {
        "id": "df1cc92fba512ce7d28d1d608ea19f18cda185ca",
        "title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence"
      },
      {
        "id": "ca086f4c09cf8de705830ac2b70951737fab93ca",
        "title": "A Review of Sparse Expert Models in Deep Learning"
      },
      {
        "id": "2dbec38fe353ab0e495ad09263389dbc9260824d",
        "title": "A Survey of Deep Learning for Mathematical Reasoning"
      },
      {
        "id": "285d13bf3cbe6a8a0f164f584d84f8b74067271f",
        "title": "Towards Faithful Model Explanation in NLP: A Survey"
      },
      {
        "id": "5aa7bdcae38076b80229c0a024f5b656ac6607af",
        "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter"
      },
      {
        "id": "f7fd184eaa573205dff97d86c836f3038143e87a",
        "title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks"
      },
      {
        "id": "ae441f7305dc2cd58c708528b3ecee3501cc5c46",
        "title": "Plansformer: Generating Symbolic Plans using Transformers"
      },
      {
        "id": "ca2ea26b851fea6914a65b233b7daf8f32e38073",
        "title": "CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation"
      },
      {
        "id": "61d71388025347124a2bbd09804c09bff7023347",
        "title": "Transformers for Tabular Data Representation: A Tutorial on Models and Applications"
      },
      {
        "id": "f4f43a23274cdf74ac017f877f7bd9cb1283b9c8",
        "title": "A transformer-based IDE plugin for vulnerability detection"
      },
      {
        "id": "3e5b7f0b64c60ca0bb5ef547e8e3dcc2a568ab75",
        "title": "MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting"
      },
      {
        "id": "f240b39aa5aa30b3e4f7a8e68d681a13a9a72054",
        "title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\""
      },
      {
        "id": "5b9798dcecb4894e22335a2e5d6980b3b92c6c5f",
        "title": "UGIF: UI Grounded Instruction Following"
      },
      {
        "id": "108ea619cfef2c53b36ace97cef6830944c1d802",
        "title": "Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool"
      },
      {
        "id": "6ac1fccf1e04487d439ee598f51c03ddac5144ca",
        "title": "N-Grammer: Augmenting Transformers with latent n-grams"
      },
      {
        "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
        "title": "Ask Me Anything: A simple strategy for prompting language models"
      },
      {
        "id": "c067664a45dce31411b3052c635c044ad4587db4",
        "title": "Generating Diverse Code Explanations using the GPT-3 Large Language Model"
      },
      {
        "id": "d19bae780d5fe93e4d007e325c278598ec7f9ea4",
        "title": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE"
      },
      {
        "id": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "id": "c8d594f09413b1555970f43e68847c211235d60f",
        "title": "Prompting GPT-3 To Be Reliable"
      },
      {
        "id": "70b98d90767345b15e0569082c0e4ac661279b5d",
        "title": "Is GPT-3 a Good Data Annotator?"
      },
      {
        "id": "b37d57edf4a84da158ab8d77921d4aa39faceb32",
        "title": "FP8 Formats for Deep Learning"
      },
      {
        "id": "49aec6fb44ab52181960512a6067eded0ce4182b",
        "title": "Benchmarking Long-tail Generalization with Likelihood Splits"
      },
      {
        "id": "65feb2a40476bd01bbeea217a6e2afdc657c3a87",
        "title": "Using artificial intelligence to improve pain assessment and pain management: a scoping review"
      },
      {
        "id": "d3e4553f0a1fd465ae358701f1bdc2e8265308d6",
        "title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing"
      },
      {
        "id": "789e8a599feeb431b3cb8a9b47b1624f71a4547c",
        "title": "Markov Models Applications in Natural Language Processing: A Survey"
      },
      {
        "id": "038ece8c19e11cefa1cb205fb2f88cbce836df61",
        "title": "Adversarial attack and defense technologies in natural language processing: A survey"
      },
      {
        "id": "5e8d3c2dc0fc53949794fc00600e25558c4a2441",
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation"
      },
      {
        "id": "47e15941c8b157873c8264e4bf50318d1ba5cd18",
        "title": "Natural Language to Code Translation with Execution"
      },
      {
        "id": "196cc546041cb6db167784f632037f0a1dcf4a79",
        "title": "Generating Natural Language Proofs with Verifier-Guided Search"
      },
      {
        "id": "e5aa2a1e36a2c68fa4aa59afdb8b6e1c419f547c",
        "title": "Natural Language Deduction through Search over Statement Compositions"
      },
      {
        "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "id": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "id": "7cbc2a7843411a1768ab762930707af0a3c33a19",
        "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
      },
      {
        "id": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "id": "332dc8b2ca9d49fad607c7282f3360bb2a9aacf3",
        "title": "A large language model for electronic health records"
      },
      {
        "id": "23c265ba884b92ecbd9d18641078d964697e4590",
        "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"
      },
      {
        "id": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5",
        "title": "Autoformalization with Large Language Models"
      },
      {
        "id": "b92628d13e8d090d042232fe6ae0b8998634b893",
        "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"
      },
      {
        "id": "a0c87ee1b0903c1c9ac72809caf75b6b6997baa0",
        "title": "Human Language Understanding & Reasoning"
      },
      {
        "id": "1bcde55995a957b3e8a595d536b816cb8989cf1d",
        "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning"
      },
      {
        "id": "218956012927bbc0dbbfd94354a56e16a6f38489",
        "title": "CINO: A Chinese Minority Pre-trained Language Model"
      },
      {
        "id": "f8f2b17083c10f730b711a938e2bb5da992086e7",
        "title": "AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models"
      },
      {
        "id": "979eb5c97c49d7979447ed684500895a24d75ac4",
        "title": "The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"
      },
      {
        "id": "cdfe9580f63070f311151444f9df32818cc858bf",
        "title": "An Empirical Evaluation of GitHub Copilot's Code Suggestions"
      },
      {
        "id": "00df5cf0d83c48657d453ab8083d8805a67f744f",
        "title": "Measuring the Carbon Intensity of AI in Cloud Instances"
      },
      {
        "id": "8a8e2c23fd7179d981cbefbbc4844494e7255c53",
        "title": "GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry"
      },
      {
        "id": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17",
        "title": "Designing Effective Sparse Expert Models"
      },
      {
        "id": "f9b94bcaf706892904c058c37d0087628f371df4",
        "title": "e-CARE: a New Dataset for Exploring Explainable Causal Reasoning"
      },
      {
        "id": "5a5b5bd6c644eb43943144410efba704ebb4c083",
        "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists"
      },
      {
        "id": "e8ad4d82f23f88c17f401c785e24e00841719f49",
        "title": "Using Transfer Learning for Code-Related Tasks"
      },
      {
        "id": "a80f2102e5de3ead1b9689b440503f49383ddc94",
        "title": "Is a Question Decomposition Unit All We Need?"
      },
      {
        "id": "eff9d6c7596953b5a61f00ad13d23c2b6fea60ea",
        "title": "Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future"
      },
      {
        "id": "2d0eba854c96b7f52553cb94015dfe93ca906d2d",
        "title": "Technical Perspective of TURL"
      },
      {
        "id": "4857d4de584ed6f15e7ef12a96823ec8b4a17ec1",
        "title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy"
      },
      {
        "id": "7107d06366b48b3593c8128ed2ca67e0b413628c",
        "title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"
      },
      {
        "id": "9038f40c43e7d62d8f1dc4819093083090911f7a",
        "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning"
      },
      {
        "id": "932b6353204e56f20917edadda2fa636ace21090",
        "title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks"
      },
      {
        "id": "3b55412f2d173504417590012676e5c7eb27de6c",
        "title": "Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness"
      },
      {
        "id": "fcac4f97885cc73256c4f64a935927bf1e54cd89",
        "title": "ReCDroid+: Automated End-to-End Crash Reproduction from Bug Reports for Android Apps"
      },
      {
        "id": "db783c480faf87b38e8806d4ef455dfde6e335aa",
        "title": "Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)"
      },
      {
        "id": "1a066314e819d34aa4a836ef59a730ff1a11a1b0",
        "title": "An Empirical Study on Explanations in Out-of-Domain Settings"
      },
      {
        "id": "251e4ba52b277b35b14e3bf618ed599983d1900c",
        "title": "Managing demand volatility of pharmaceutical products in times of disruption through news sentiment analysis"
      },
      {
        "id": "e0180b6ca96283ef8498307f7fd0c6a227081d21",
        "title": "Predicting Issue Types with seBERT"
      },
      {
        "id": "fab5f884301cf5fc6e07907e3d136090d2641923",
        "title": "Making a (Counterfactual) Difference One Rationale at a Time"
      },
      {
        "id": "4a3553941825e7c46eb052e7c3c9fc3e6de895b1",
        "title": "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers"
      },
      {
        "id": "9b2339cfe4640841078073dfa1c1c60e84eeeaa2",
        "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language"
      },
      {
        "id": "d48b29889241551e1ee6622fa78c3fa4159255dd",
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"
      },
      {
        "id": "fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20",
        "title": "Wordcraft: Story Writing With Large Language Models"
      },
      {
        "id": "03013e291fb3192b286147f5bdb5770e434f91b2",
        "title": "Do Language Models Plagiarize?"
      },
      {
        "id": "c7cbbbdba2c942180675c2a69035e506d9378679",
        "title": "Using Pre-Trained Models to Boost Code Review Automation"
      },
      {
        "id": "e98799e709dc93a8ea721dd6b3e1398104797050",
        "title": "Cracking the code: Co-coding with AI in creative programming education"
      },
      {
        "id": "46bd20b3d877848dd38e0039110cca3105d7109c",
        "title": "CELER: A 365-Participant Corpus of Eye Movements in L1 and L2 English Reading"
      },
      {
        "id": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
        "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey"
      },
      {
        "id": "11907f691e9b7fc32a492e1de676a4b788add155",
        "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain"
      },
      {
        "id": "d5784fd3ac7e06ec030abb8f7787faa9279c1a50",
        "title": "Interpreting Deep Learning Models in Natural Language Processing: A Review"
      },
      {
        "id": "a9c5e23c5559bfc4d95dd166c1ed29fa026bbf2e",
        "title": "Fine-tuning large neural language models for biomedical natural language processing"
      },
      {
        "id": "891137d590020214ff703d443cb960a50254ab94",
        "title": "Prediction of 30-Day Readmission After Stroke Using Machine Learning and Natural Language Processing"
      },
      {
        "id": "08460ecff91b8a54358b9c1709d7dc6a77417f62",
        "title": "Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing"
      },
      {
        "id": "80d0116d77beeded0c23cf48946d9d10d4faee14",
        "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"
      },
      {
        "id": "319b84be7a843250bc81d7086f79a4126d550277",
        "title": "ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
      },
      {
        "id": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904",
        "title": "Few-shot Learning with Multilingual Generative Language Models"
      },
      {
        "id": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
        "title": "Primer: Searching for Efficient Transformers for Language Modeling"
      },
      {
        "id": "a3184d40d390793232c99c89b57b8f65c16320b2",
        "title": "ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation"
      },
      {
        "id": "0ab41d455d676542b37ca1499bb19ea6a5d1cf79",
        "title": "Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning"
      },
      {
        "id": "38f683ec0b9fda2069c0b2cb7ee1c71035915723",
        "title": "KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation"
      },
      {
        "id": "234c0e26e011c67fe4e6af2925439eb922104eff",
        "title": "JavaBERT: Training a Transformer-Based Model for the Java Programming Language"
      },
      {
        "id": "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896",
        "title": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"
      },
      {
        "id": "2743e66939b30c43affb3c9e31f20cfac2109045",
        "title": "Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks"
      },
      {
        "id": "4698fc4712f0212c8a3810fd67b41ee8b8896aba",
        "title": "Generate & Rank: A Multi-task Framework for Math Word Problems"
      },
      {
        "id": "04833b92c9002f241b8f8b956d018759eebc85b3",
        "title": "Tailor: Generating and Perturbing Text with Semantic Controls"
      },
      {
        "id": "865bc9d894a55f3e77ea29f11691f8bf550571d5",
        "title": "Emerging trends: A gentle introduction to fine-tuning"
      },
      {
        "id": "1a8c0bc4520f9836d36e1e09fa8df6b54b4b363e",
        "title": "A Statutory Article Retrieval Dataset in French"
      },
      {
        "id": "b6f616e9305e59c9dc7ccf33c311ede47584caf6",
        "title": "Kronecker Decomposition for GPT Compression"
      },
      {
        "id": "8cf3a454556060d6e9aa86dbabf221bd10bf9759",
        "title": "On the Effectiveness of Transfer Learning for Code Search"
      },
      {
        "id": "09ab6e859b71230c9924ba188f41e93b563d1cf9",
        "title": "Generic Neural Architecture Search via Regression"
      },
      {
        "id": "2416459e2ff3124c17af353df582b41beaceeb59",
        "title": "BERT is Robust! A Case Against Synonym-Based Adversarial Examples in Text Classification"
      },
      {
        "id": "2569a7309142e40815cf556b6417059df9abbda8",
        "title": "Protecting Intellectual Property of Language Generation APIs with Lexical Watermark"
      },
      {
        "id": "aa84beaf7d94f09e0fdcb899c7fbc97822050546",
        "title": "EditSum: A Retrieve-and-Edit Framework for Source Code Summarization"
      },
      {
        "id": "3e2234b2b0673fe48e500d4f38bff53767e3369d",
        "title": "A Semantic Web Framework for Automated Smart Assistants: A Case Study for Public Health"
      },
      {
        "id": "a55c399bbb0382650459da59fc545f2dd275012b",
        "title": "FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks"
      },
      {
        "id": "ebdb02f9d0f31d3a13070f61fa07fe2fa11cb531",
        "title": "Model Explainability in Deep Learning Based Natural Language Processing"
      },
      {
        "id": "911b7539e964782670e555930b291de16fa971c5",
        "title": "Flexible Generation of Natural Language Deductions"
      },
      {
        "id": "78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f",
        "title": "PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"
      },
      {
        "id": "1991ea2ec85113cadf38faea840f4b5cf73ae0c7",
        "title": "ELECTRAMed: a new pre-trained language representation model for biomedical NLP"
      },
      {
        "id": "98bb75dcb7dfe8e675781fe2008170e8f00a5dee",
        "title": "FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information"
      },
      {
        "id": "c2c7233293d55f201fe5b496234bed1914eea70e",
        "title": "Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"
      },
      {
        "id": "df157cb42b574c3f46b269504c18375bfa5bc5b1",
        "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders"
      },
      {
        "id": "64902a5077ee68011cd467398dbb66511e8e891a",
        "title": "It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning"
      },
      {
        "id": "d77ad36898a623b2e82d96a0b8d9920204ca60bf",
        "title": "Ungoliant: An Optimized Pipeline for the Generation of a Very Large-Scale Multilingual Web Corpus"
      },
      {
        "id": "6e2a57b46a96694e6dfd3c65796c877dd0094f3d",
        "title": "QASR: QCRI Aljazeera Speech Resource A Large Scale Annotated Arabic Speech Corpus"
      },
      {
        "id": "04b40daa1ca74bdbb578beb314bf662538ecd18e",
        "title": "ZEN 2.0: Continue Training and Adaption for N-gram Enhanced Text Encoders"
      },
      {
        "id": "f386368c0868793b90d7e03b50010af320c21722",
        "title": "deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search"
      },
      {
        "id": "a8debd8f58ee690005d996d223c37239e25273ec",
        "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning"
      },
      {
        "id": "af931c9a67ddda30be86a964c0544cc1a6d54cb5",
        "title": "Text-to-Text Multi-view Learning for Passage Re-ranking"
      },
      {
        "id": "c981b3566140096b79543ba4fef77cbb71e8557c",
        "title": "Extracting Semantic Process Information from the Natural Language in Event Logs"
      },
      {
        "id": "7d9a3b94f78827952b078c664b0da1c02e1c2ee3",
        "title": "What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?"
      },
      {
        "id": "b0010eb52398619fcaafc8343f7611399c00c41d",
        "title": "NBSearch: Semantic Search and Visual Exploration of Computational Notebooks"
      },
      {
        "id": "db233bac71c35e222a78a7e730e14b336bf9915e",
        "title": "Towards Automating Code Review Activities"
      },
      {
        "id": "b668738dde6c770a19f6f7441dd1748d82032d48",
        "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports"
      }
    ],
    "6": [
      {
        "id": "cad0ec273f5af6e5e187606d5c7ff070426314d2",
        "title": "Transformer-Based Named Entity Recognition in Construction Supply Chain Risk Management in Australia"
      },
      {
        "id": "bf93fe733932fd25780ef84911b8a507bec1c372",
        "title": "Neural scaling of deep chemical models"
      },
      {
        "id": "6356e860372addf7278307f490903e32f334ed75",
        "title": "Agile Methodology for the Standardization of Engineering Requirements Using Large Language Models"
      },
      {
        "id": "806d58b835800815d9d6ad0e5731b85c5956c69b",
        "title": "Leveraging molecular structure and bioactivity with chemical language models for de novo drug design"
      },
      {
        "id": "577ebf34eef79357df6d40792dad5e2f21331b96",
        "title": "Stylometric Fake News Detection Based on Natural Language Processing Using Named Entity Recognition: In-Domain and Cross-Domain Analysis"
      },
      {
        "id": "a54761081c2b001c057fb6e1ea9a48058d5aa5e0",
        "title": "CLEX: Continuous Length Extrapolation for Large Language Models"
      },
      {
        "id": "658869b820bfaa919d10378d0a019bff7e56db6c",
        "title": "Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers"
      },
      {
        "id": "a27dced654158b905c7447aae1aa294ebc8ecaf0",
        "title": "Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer"
      },
      {
        "id": "94a47bf3bb8fffaabe75c6aa2b4b1b211b3e40d1",
        "title": "Computers' Interpretations of Knowledge Representation Using Pre-Conceptual Schemas: An Approach Based on the BERT and Llama 2-Chat Models"
      },
      {
        "id": "6cb53afdc2b21236c3957efd4bbeb2aa65338c50",
        "title": "An Interdisciplinary Outlook on Large Language Models for Scientific Research"
      },
      {
        "id": "2ce8bd7295aad4e15296ee098ef0d296c9833c38",
        "title": "Natural Language Processing (NLP) in Aviation Safety: Systematic Review of Research and Outlook into the Future"
      },
      {
        "id": "c7ccb92677afc2738868cf95d38af79a1adf7e9d",
        "title": "Potential of natural language processing for metadata extraction from environmental scientific publications"
      },
      {
        "id": "edfb5696b5431bd20eb57964c083e9118e153e97",
        "title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering"
      },
      {
        "id": "9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5",
        "title": "A Short Survey of Viewing Large Language Models in Legal Aspect"
      },
      {
        "id": "8489b55d992e6a3ac54aec7094a42ec8e333012f",
        "title": "SELFormer: molecular representation learning via SELFIES language models"
      },
      {
        "id": "049288e68caeadf7842df6977e140b47a8a2f89d",
        "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling"
      },
      {
        "id": "60b35c6d68acced19b0c66edcfc0ee0a2c11efed",
        "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers"
      },
      {
        "id": "594d8e1696619f3cebb7c6bffdad8e0a5592f006",
        "title": "Scaling Transformer to 1M tokens and beyond with RMT"
      },
      {
        "id": "27d391d65ab42c30dc35595213ba6585633afa5d",
        "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation"
      },
      {
        "id": "68adb03744692247fb834406798894db9fe77010",
        "title": "A Survey on Long Text Modeling with Transformers"
      },
      {
        "id": "3440687e1fc734baeab1abee4a86c81347d1422a",
        "title": "aeroBERT-Classifier: Classification of Aerospace Requirements Using BERT"
      },
      {
        "id": "1cfe781523b4b3469d822bc52a0721b1c70cee5f",
        "title": "A rule-free workflow for the automated generation of databases from scientific literature"
      },
      {
        "id": "e26197fb0fa409866b287f4bf63abe7997223b51",
        "title": "A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing"
      },
      {
        "id": "cb8374f90c7b61f07b257377b4ce5e6d917c0df3",
        "title": "Privacy-Preserving Models for Legal Natural Language Processing"
      },
      {
        "id": "7d0abebf379383afbf9b7b3f8ab89561d1aa7596",
        "title": "MaterialBERT for natural language processing of materials science texts"
      },
      {
        "id": "299e281cb6e2bfae6e8459d2ac354e11e40931be",
        "title": "ChemBERTa-2: Towards Chemical Foundation Models"
      },
      {
        "id": "732e3faec4e5be4d144256f2c379b9dc49f0b227",
        "title": "Efficient Long-Text Understanding with Short-Text Models"
      },
      {
        "id": "19be53b21a022f7578e073709836c35ff52bec43",
        "title": "Mining legal arguments in court decisions"
      },
      {
        "id": "661e8d555c4424b5953f17434f2ba910bfcf3afe",
        "title": "Efficient Long Sequence Modeling via State Space Augmented Transformer"
      },
      {
        "id": "bcf272c12c55362a92741b1a0b2ac76f066466f9",
        "title": "ClassActionPrediction: A Challenging Benchmark for Legal Judgment Prediction of Class Action Cases in the US"
      },
      {
        "id": "8747b5e9d19d4cf6eab9b93d8a434595e4a7e11f",
        "title": "Traditional Machine Learning and Deep Learning-based Text Classification for Turkish Law Documents using Transformers and Domain Adaptation"
      },
      {
        "id": "6281c40c66febca1d8003bcc6fdfd2189b30c38f",
        "title": "SCROLLS: Standardized CompaRison Over Long Language Sequences"
      },
      {
        "id": "04a90a79bdfe2f9d54811141c104a4c12f74c07a",
        "title": "SsciBERT: a pre-trained language model for social science texts"
      },
      {
        "id": "fcc7a44c19a553998dc471031652531c71a832aa",
        "title": "A Survey on Legal Judgment Prediction: Datasets, Metrics, Models and Challenges"
      },
      {
        "id": "9502d1d429ed2827b533ad3cd19b02580b86aac3",
        "title": "MOFSimplify, machine learning models with extracted stability data of three thousand metal–organic frameworks"
      },
      {
        "id": "5748a0a36fbbd4e24f6503401ebcc51f9394e726",
        "title": "Auto-generated database of semiconductor band gaps using ChemDataExtractor"
      },
      {
        "id": "aed2191647d349c9bb4b12bcd266bfe9ae919930",
        "title": "The Use of NLP-Based Text Representation Techniques to Support Requirement Engineering Tasks: A Systematic Mapping Review"
      },
      {
        "id": "59d68464fdefa57d42ba461cfa7da3004b13be63",
        "title": "Modern Question Answering Datasets and Benchmarks: A Survey"
      },
      {
        "id": "85e3cf70079adb1db8b1b50321a5d336edc1c3fa",
        "title": "Leveraging Locality in Abstractive Text Summarization"
      },
      {
        "id": "379ae767d3a49a32c5223b450e89854a369c5f72",
        "title": "Natural language processing models that automate programming will transform chemistry research and teaching"
      },
      {
        "id": "83c4acba9c532df287e40ef1284146363f1f14be",
        "title": "Natural Language Processing for the identification of Human factors in aviation accidents causes: An application to the SHEL methodology"
      },
      {
        "id": "85474f6154db25cfd2b84e6fe9dadd314fac49d0",
        "title": "Application of natural language processing for systematic requirement management in model‐based systems engineering"
      },
      {
        "id": "0a1ff1d4102d94a50f8862f60bc2ac21f36ad592",
        "title": "ContractNLI: A Dataset for Document-level Natural Language Inference for Contracts"
      },
      {
        "id": "b146be9e80c66a6e062a1525693311fac65ae19e",
        "title": "MatSciBERT: A materials domain language model for text mining and information extraction"
      },
      {
        "id": "20a01009d38d083a49e01ef46005363135453661",
        "title": "The great Transformer: Examining the role of large language models in the political economy of AI"
      },
      {
        "id": "e853ef5d79b5584f4476e82c7a6b736c53a18bfc",
        "title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting"
      },
      {
        "id": "f48752134f8534f6c97ffffed53b15a6b530de31",
        "title": "MOFSimplify: Machine Learning Models with Extracted Stability Data of Three Thousand Metal-Organic Frameworks"
      },
      {
        "id": "0293c0ab2871f42e46a3a2e9e77196ad544326ff",
        "title": "The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification"
      },
      {
        "id": "305c1173a2a10b3f90e4c65d64d19d19c73c7a77",
        "title": "Chemical language models enable navigation in sparsely populated chemical space"
      },
      {
        "id": "879eaab2275a364549809560b42f0fef357ebbce",
        "title": "BERT: A Review of Applications in Natural Language Processing and Understanding"
      },
      {
        "id": "bc9fcbd70dc0f5f4c3c90fac20bc348e9f29035b",
        "title": "Machine Learning and Natural Language Processing for Prediction of Human Factors in Aviation Incident Reports"
      },
      {
        "id": "76f3d302537f3a46167472d9ffaaac0c7fe9fbee",
        "title": "Looking through glass: Knowledge discovery from materials science literature using natural language processing"
      },
      {
        "id": "df59d0098c1b2c1ee8995da802dd6b12d158c2b8",
        "title": "Large-scale chemical language representations capture molecular structure and properties"
      },
      {
        "id": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
        "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers"
      },
      {
        "id": "ef03f3910685dda98279aa2bcefbeada3362a626",
        "title": "When does pretraining help?: assessing self-supervised learning for law and the CaseHOLD dataset of 53,000+ legal holdings"
      },
      {
        "id": "0ccbb1e3800dfe764d6cabb86c065b1d30b5ab92",
        "title": "Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal-Organic Frameworks"
      },
      {
        "id": "1f39385e515e68b3b4a94582c0f8daf70aaf55f7",
        "title": "Using Domain-Specific Corpora for Improved Handling of Ambiguity in Requirements"
      },
      {
        "id": "f4566761fe39c4b5273d696d9bc3f4195c9325bb",
        "title": "Long-Span Summarization via Local Attention and Content Selection"
      }
    ],
    "9": [
      {
        "id": "a01b925d5a0a0d3b3cee5953726db60ef5491925",
        "title": "Employing synthetic data for addressing the class imbalance in aspect-based sentiment classification"
      },
      {
        "id": "ff37cc6a33004ccabb6840da725681873e9d24f3",
        "title": "Exploring the Latest Highlights in Medical Natural Language Processing across Multiple Languages: A Survey"
      },
      {
        "id": "f190aeb5ff68a51d963680403c3d160a72e47e63",
        "title": "Multimodal Data Hybrid Fusion and Natural Language Processing for Clinical Prediction Models"
      },
      {
        "id": "5bda9200b6466da0779116385a277be6dbc746f4",
        "title": "Hierarchical Pretraining on Multimodal Electronic Health Records"
      },
      {
        "id": "68d63fb8a7002e956644467af39db20fddc64661",
        "title": "Extraction of Unstructured Electronic Healthcare Records using Natural Language Processing"
      },
      {
        "id": "3bc5a1fd0a746d544be88879f64da60c21251083",
        "title": "A survey on clinical natural language processing in the United Kingdom from 2007 to 2022"
      },
      {
        "id": "308b0117b5fb147ab0b9d1c20ed242809170bd6d",
        "title": "Analyzing patient experiences using natural language processing: development and validation of the artificial intelligence patient reported experience measure (AI-PREM)"
      },
      {
        "id": "9ab6727fade0fa3f7a121801069eae9cc1ab6f50",
        "title": "DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing"
      },
      {
        "id": "126cf588a125ba8038d2eef3da9b3d04988aa4ca",
        "title": "The natural language processing of radiology requests and reports of chest imaging: Comparing five transformer models’ multilabel classification and a proof-of-concept study"
      },
      {
        "id": "115d7d411f221295b483534eb7faef651f18451c",
        "title": "Summarizing Patients’ Problems from Hospital Progress Notes Using Pre-trained Sequence-to-Sequence Models"
      },
      {
        "id": "4815951f597db5a9c74ab71195ef5bfadd45f452",
        "title": "Cause-and-Effect Analysis of ADAS: A Comparison Study between Literature Review and Complaint Data"
      },
      {
        "id": "301c6b4117580efba62549c87687ac4bcbe701f2",
        "title": "Natural Language Processing Methods and Bipolar Disorder: Scoping Review"
      },
      {
        "id": "cb991711d64f4d92ca339b95d037b3e40b0e2bac",
        "title": "Natural Language Processing for Imaging Protocol Assignment: Machine Learning for Multiclass Classification of Abdominal CT Protocols Using Indication Text Data"
      },
      {
        "id": "0db5207510819b9956849eb84bfe8703f8f3688d",
        "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model"
      },
      {
        "id": "f30e95be411456a709e7cb9a8b3a3e557bd0356a",
        "title": "Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences"
      },
      {
        "id": "9d11c43c7870920d3a2aa45ddf84e543dbd4fb98",
        "title": "Automated clinical coding: what, why, and where we are?"
      },
      {
        "id": "aa14eb5c8eb4021b073b473b0c0e2e60eb1f14f5",
        "title": "GERNERMED++: Transfer Learning in German Medical NLP"
      },
      {
        "id": "82bef8b5fc633475bc5278c1dd98f180f1a5d471",
        "title": "Ontology-driven and weakly supervised rare disease identification from clinical notes"
      },
      {
        "id": "a4af31815e1efdbbeacc091c62f78ed4401f2fae",
        "title": "Natural Language Processing for Smart Healthcare"
      },
      {
        "id": "dc1402d05c6a18843d1fc6b31f1bd4fcdaa8ad30",
        "title": "Benchmarking for biomedical natural language processing tasks with a domain specific ALBERT"
      },
      {
        "id": "67691afe29958fefe70cc903dc78856f63ade678",
        "title": "Natural Language Processing as an Emerging Tool to Detect Late-Life Depression"
      },
      {
        "id": "caf3df05598d26698c700d57fd044c0bcd34a0d6",
        "title": "Neural Natural Language Processing for Unstructured Data in Electronic Health Records: a Review"
      },
      {
        "id": "1deab15a5a30c1da962804527c1139f9d38c5f5b",
        "title": "A Scoping Review of Publicly Available Language Tasks in Clinical Natural Language Processing"
      },
      {
        "id": "f47ef903bc325eef4c2d57e819dbce79308bb2ae",
        "title": "Deep Learning-Based Natural Language Processing in Radiology: The Impact of Report Complexity, Disease Prevalence, Dataset Size, and Algorithm Type on Model Performance"
      },
      {
        "id": "5572fd371a3ac287fb0d68f90f4a3f8574c9547b",
        "title": "Listening to Mental Health Crisis Needs at Scale: Using Natural Language Processing to Understand and Evaluate a Mental Health Crisis Text Messaging Service"
      },
      {
        "id": "f5e38f2a2150ff5733e151976641516f369fd637",
        "title": "Natural language processing enabling COVID-19 predictive analytics to support data-driven patient advising and pooled testing"
      },
      {
        "id": "95d87b6be48c8ad22ea8a5ff3d5b787b946b6463",
        "title": "A Review of Recent Work in Transfer Learning and Domain Adaptation for Natural Language Processing of Electronic Health Records"
      },
      {
        "id": "f28f61ae2a8339f496991e7cbc69ff91b3d6d67e",
        "title": "Using natural language processing to understand, facilitate and maintain continuity in patient experience across transitions of care"
      },
      {
        "id": "b15469d0ab3dc3a9dec037d761817b3fe546bed6",
        "title": "Pre-trained Language Models in Biomedical Domain: A Systematic Survey"
      },
      {
        "id": "25b3aacaa9b98f0e0d41588cf0e36efe4fb7dc43",
        "title": "BERTax: taxonomic classification of DNA sequences with Deep Neural Networks"
      },
      {
        "id": "2a9a8102602d23954e0e8dd457209e5ef9e4b052",
        "title": "Deployment of a Free-Text Analytics Platform at a UK National Health Service Research Hospital: CogStack at University College London Hospitals"
      },
      {
        "id": "b42d20ec9580ebd76860890a1d7a7fdcc742677e",
        "title": "Modeling Protein Using Large-scale Pretrain Language Model"
      },
      {
        "id": "a2412fdebd53bd25476f834ae2b8aa8cb44cb1e1",
        "title": "The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design"
      },
      {
        "id": "cc375e2130cae2bc1f390a6c84a0258bbf9971a8",
        "title": "Applying natural language processing and machine learning techniques to patient experience feedback: a systematic review"
      },
      {
        "id": "00e1d284260e2f8be8cd47c0bb22a1892d91deba",
        "title": "Combat COVID-19 infodemic using explainable natural language processing models"
      },
      {
        "id": "ed57d8fea73b2c480aca5e3696316554b07a284c",
        "title": "Deep Learning-Based Natural Language Processing for Screening Psychiatric Patients"
      },
      {
        "id": "059524838ac1db807ecf3ec4145e69d9b9976ee4",
        "title": "A natural language processing approach for identifying temporal disease onset information from mental healthcare text"
      },
      {
        "id": "58fe64beb45b18f63cbc001849a0dee3e4e60482",
        "title": "Improving Biomedical Pretrained Language Models with Knowledge"
      },
      {
        "id": "31852f9fc732c0868af12d631c72693702d80521",
        "title": "Text Data Augmentation for Deep Learning"
      },
      {
        "id": "474c63c4238e830ae395cad69bb4533e69731ff3",
        "title": "Limitations of Transformers on Clinical Text Classification"
      },
      {
        "id": "1dd525b5af40e613ae1665cf15a193b5ef23431b",
        "title": "Improving BERT Model Using Contrastive Learning for Biomedical Relation Extraction"
      },
      {
        "id": "d13a0c8d49cb268d8d245925baee0316c1fe1875",
        "title": "Which transformer architecture fits my data? A vocabulary bottleneck in self-attention"
      },
      {
        "id": "cd287288c9fbaa97cbd3f4a070e9552592626fa5",
        "title": "Rare Disease Identification from Clinical Notes with Ontologies and Weak Supervision"
      },
      {
        "id": "44a422a2514c1cd6828423b5edce53d0dbdabd73",
        "title": "Structure-inducing pre-training"
      }
    ],
    "3": [
      {
        "id": "d62c4d00b277e948956b6610ce2644e88fe1577b",
        "title": "Large Language Models"
      },
      {
        "id": "2b6cefcedf5ab2b747e4c4279666cb100fff8506",
        "title": "Morphosyntactic probing of multilingual BERT models"
      },
      {
        "id": "7821e7639ffaeea175422f35fae2eb1c095ed1a6",
        "title": "Protein Language Models and Structure Prediction: Connection and Progression"
      },
      {
        "id": "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
        "title": "Explainability for Large Language Models: A Survey"
      },
      {
        "id": "0f92d5a01baa449edc5592716dd639ec7868c44f",
        "title": "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations"
      },
      {
        "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
        "title": "All in One: Multi-Task Prompting for Graph Neural Networks"
      },
      {
        "id": "6912a12f3710bf4bbdfd9f55f90311426fc1c32c",
        "title": "Graph Foundation Models: Concepts, Opportunities and Challenges"
      },
      {
        "id": "9ad4911a7923e19df9fb36cd03de5a63cb86ba63",
        "title": "Graph Prompt Learning: A Comprehensive Survey and Beyond"
      },
      {
        "id": "9a123fa65c393e47e2bbc4dd1f00a830d8fdb7bc",
        "title": "Object detection using convolutional neural networks and transformer-based models: a review"
      },
      {
        "id": "b5cccb9a2a0c1e2c22fd1efe1cda7f9beb57bcbe",
        "title": "HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks"
      },
      {
        "id": "5eadf15ddd1a11476dc059bff3e5cbef35ca76a1",
        "title": "Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals"
      },
      {
        "id": "c7084e6778222430b1b52bd522d0dad93121acc7",
        "title": "XplainLLM: A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs"
      },
      {
        "id": "ca83ab4a4379b6ab6bf4ce128bc4ba74977b2b4d",
        "title": "Efficient Transformer Knowledge Distillation: A Performance Review"
      },
      {
        "id": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
        "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"
      },
      {
        "id": "f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0",
        "title": "A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"
      },
      {
        "id": "9d12916dd46df7a6446cbec0bc4d054f7dafcdab",
        "title": "Scaling Vision-Language Models with Sparse Mixture of Experts"
      },
      {
        "id": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
        "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks"
      },
      {
        "id": "d203076c28587895aa344d088b2788dbab5e82a1",
        "title": "Transformer-Based Visual Segmentation: A Survey"
      },
      {
        "id": "283de2b58c8c79c213feec9c39a9d29b673c9634",
        "title": "Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey"
      },
      {
        "id": "148501704f8c218347ace30a45a5aa7bfa9c20b2",
        "title": "Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding"
      },
      {
        "id": "4ed548d30ed06a296ecd4a734e0a3d45772f71c4",
        "title": "Convolutional Neural Networks or Vision Transformers: Who Will Win the Race for Action Recognitions in Visual Data?"
      },
      {
        "id": "c0d5eaf398abf67f11d8230bffff8d89051e0dab",
        "title": "BioSeq-Diabolo: Biological sequence similarity analysis using Diabolo"
      },
      {
        "id": "f35f5aedc30e2c5ded210d9c91ba6e84bd029425",
        "title": "Toeplitz Neural Network for Sequence Modeling"
      },
      {
        "id": "6209b614db0065de89331196a1ae8aa59404f0db",
        "title": "Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training"
      },
      {
        "id": "50073733a6f8ddafd7ef9a8221cd940fa910b6e2",
        "title": "Explore In-Context Learning for 3D Point Cloud Understanding"
      },
      {
        "id": "54450cceb117d7f43d160085dac8d0a1858d78d5",
        "title": "Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection"
      },
      {
        "id": "86b6e42e2ce957f6497d4aa578c9bb4d2b4e4ba3",
        "title": "Blockwise Parallel Transformer for Large Context Models"
      },
      {
        "id": "e61b124bf7893b4ace01df4b72d6efcf8d8b999e",
        "title": "Vision Transformers with Mixed-Resolution Tokenization"
      },
      {
        "id": "c82db788f3473ef312937aca5810a387ddb51d49",
        "title": "A Framework and Operational Procedures for Metaverses-Based Industrial Foundation Models"
      },
      {
        "id": "4790b411835089fc593b154d3c661f5ccfc05ae6",
        "title": "Explainability of Text Processing and Retrieval Methods: A Critical Survey"
      },
      {
        "id": "4988b3d378b79eb8669112620baf1ff4e3e536fd",
        "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango"
      },
      {
        "id": "5152b9391762aefc532f85a801093bd38a6688c6",
        "title": "Hierarchical Graph Transformer with Adaptive Node Sampling"
      },
      {
        "id": "b5e4ebe31c8a5975f04d04e4cc53764aff2f21f1",
        "title": "A Study on Different Deep Learning Algorithms Used in Deep Neural Nets: MLP SOM and DBN"
      },
      {
        "id": "94a96f64bd93ad91642fa04da09bb709a26ac277",
        "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting"
      },
      {
        "id": "9b8a94f3b3ca2a598645970a4dfe248fff20d9b8",
        "title": "Enhance the Visual Representation via Discrete Adversarial Training"
      },
      {
        "id": "ac40f3197f77397fdf07a73f788056efc99028df",
        "title": "3D Vision with Transformers: A Survey"
      },
      {
        "id": "f461669f4e6e843af6478ddcb805093511f649d2",
        "title": "Rethinking Attention Mechanism in Time Series Classification"
      },
      {
        "id": "cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6",
        "title": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI"
      },
      {
        "id": "8e2930ac8ae9a758b367513cccb7d562f354afea",
        "title": "Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production"
      },
      {
        "id": "1350574ef040fa5313687c601e34720d5ebc4bf4",
        "title": "Vision Transformers: State of the Art and Research Challenges"
      },
      {
        "id": "fa0872e45bf6c0fee528c86120f52490142efba6",
        "title": "Exploring Stochastic Autoregressive Image Modeling for Visual Representation"
      },
      {
        "id": "1d589e172084a1451398f0d6c30592baa8224732",
        "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization"
      },
      {
        "id": "e0d9d80dd0ef1a9996feac61d3315f44ec31aca3",
        "title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers"
      },
      {
        "id": "bbfe80a347aab7e5d3cb82c6d37e13393fdf8a78",
        "title": "ASiT: Local-Global Audio Spectrogram Vision Transformer for Event Classification"
      },
      {
        "id": "936b00941c6f11ef4102ba21664fbd2fef15dcac",
        "title": "Link of Transformers in CV and NLP: A Brief Survey"
      },
      {
        "id": "dcb31b98ec58f3fff9f94f148e2952595f017fd9",
        "title": "ProtGPT2 is a deep unsupervised language model for protein design"
      },
      {
        "id": "025d4b73c1d0f267ca39fa9649413d88ed3cc5bc",
        "title": "Going Beyond XAI: A Systematic Survey for Explanation-Guided Learning"
      },
      {
        "id": "a2cd34174ffd3822ec04d00f261ee1690e1318b1",
        "title": "An Extensive Study on Pretrained Models for Natural Language Processing Based on Transformers"
      },
      {
        "id": "b66e776502b8484a09f8759c8dc3737ebe714f34",
        "title": "Controllable protein design with language models"
      },
      {
        "id": "f7410f535bda5b5a9888512fb954193245c1d0b2",
        "title": "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images"
      },
      {
        "id": "ed4087f6e8d77452810979f58246c5b2ad846cf8",
        "title": "Transformers in Time Series: A Survey"
      },
      {
        "id": "a601e71127d95bdae30fd818d2a0cc34b80b13f7",
        "title": "Masked Autoencoders for Point Cloud Self-supervised Learning"
      },
      {
        "id": "c49ac1f916d6d2edeb187e6619c8d23acd95eb21",
        "title": "cosFormer: Rethinking Softmax in Attention"
      },
      {
        "id": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
        "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"
      },
      {
        "id": "226fcbe55235d873bedb2fcf5b981bd5ec860d4f",
        "title": "SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer"
      },
      {
        "id": "bdacaef16d45e1e7826bf5191637d436d5a98451",
        "title": "Transformers in Time-Series Analysis: A Tutorial"
      },
      {
        "id": "75c642ebdcfbd4c16d6c3161130b72ff9af5c311",
        "title": "Three things everyone should know about Vision Transformers"
      },
      {
        "id": "258e214522232a479b103d358969edf5a25d61ad",
        "title": "A Data-scalable Transformer for Medical Image Segmentation: Architecture, Model Efficiency, and Benchmark"
      },
      {
        "id": "022b3d5f684dd6e1b74e0455b5b78a3986c8b69f",
        "title": "Transformers in 3D Point Clouds: A Survey"
      },
      {
        "id": "cc73da69eb495a122ed24bc680bf9e2f1a420e0c",
        "title": "A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications"
      },
      {
        "id": "a9c72a0aedd209c2f565d09fd46dd27c78585a91",
        "title": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives"
      },
      {
        "id": "daaa79be8d02cd3f748b6a3465fc1f09b5068880",
        "title": "Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models"
      },
      {
        "id": "fdf23f096e4721b18041c5ccc0a984ebfae72f01",
        "title": "DCT-Former: Efficient Self-Attention with Discrete Cosine Transform"
      },
      {
        "id": "37e2c7c89325a1a4685a46ff830fe7ecca8f1f80",
        "title": "Learning to Scaffold: Optimizing Model Explanations for Teaching"
      },
      {
        "id": "c794bf23dced7bf90f9d4b471592487f0fd4a920",
        "title": "ReSTR: Convolution-free Referring Image Segmentation Using Transformers"
      },
      {
        "id": "202967f77c4384bce80eaf2fa5737259008267d3",
        "title": "Learning to Merge Tokens in Vision Transformers"
      },
      {
        "id": "106176d796eda5efcd8a4d84ce98267350d679b9",
        "title": "An introduction to Deep Learning in Natural Language Processing: Models, techniques, and tools"
      },
      {
        "id": "c16d35a6d36a3688f222e1f7ebb858da67f5da7d",
        "title": "BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models"
      },
      {
        "id": "454304628bf10f02aba1c2cfc95891e94d09208e",
        "title": "Graph Neural Networks with Learnable Structural and Positional Representations"
      },
      {
        "id": "a2a0fc5014423d6283c80511b9272d941109ece2",
        "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation"
      },
      {
        "id": "bd9ab344da99022cbbbfd3f5c9c82a0b21c60ad9",
        "title": "nnFormer: Volumetric Medical Image Segmentation via a 3D Transformer"
      },
      {
        "id": "a9c214e846188adb645021cd7b1964b8ea1fef6f",
        "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"
      },
      {
        "id": "333212e246fb65f7c9d43862021e78f007c48449",
        "title": "A Survey of Visual Transformers"
      },
      {
        "id": "c215987b9fb31c2152773368102b9e45f75181a1",
        "title": "End-to-End Referring Video Object Segmentation with Multimodal Transformers"
      },
      {
        "id": "2e644c67a697073d561da4f4dad35e5ad5316cfd",
        "title": "SOFT: Softmax-free Transformer with Linear Complexity"
      },
      {
        "id": "07df27f2c7ecacca99a581efbf4be7a0b3b3ba8b",
        "title": "Gophormer: Ego-Graph Transformer for Node Classification"
      },
      {
        "id": "48af9b314181b04edcc0b7224ffe4689036b755f",
        "title": "Improving Transformers with Probabilistic Attention Keys"
      },
      {
        "id": "203b965e5c9eb1e1c521ec66f82b036335c7cd4d",
        "title": "Shifted Chunk Transformer for Spatio-Temporal Representational Learning"
      },
      {
        "id": "e0cbbca02b332f398c6639b3bea0613f79166220",
        "title": "ABC: Attention with Bounded-memory Control"
      },
      {
        "id": "311e48e1c4a0dcd65a6699376ffc85a24a333a56",
        "title": "Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience"
      },
      {
        "id": "ffcd58f453f207d48075627da011f62782334c8f",
        "title": "Go Wider Instead of Deeper"
      },
      {
        "id": "0084f3cb0a1754272151c5268a783f24bf5676a0",
        "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions"
      },
      {
        "id": "722ad6ac92286507437b31486f47987d6ece05c9",
        "title": "BEiT: BERT Pre-Training of Image Transformers"
      },
      {
        "id": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
        "title": "Transformers in Vision: A Survey"
      },
      {
        "id": "7519a1e9e7371df79bd8a21cee871feb0ec597a5",
        "title": "UNETR: Transformers for 3D Medical Image Segmentation"
      },
      {
        "id": "147164a3905f41a7a5a10f732d086a621c9c5862",
        "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer"
      },
      {
        "id": "8690d62d4bbbd0b1ed5e1f25320d10853bfbeb01",
        "title": "Scaling Vision with Sparse Mixture of Experts"
      },
      {
        "id": "6fa1cfc4f97f03a8485692418c7aa1a06c574a85",
        "title": "Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention"
      },
      {
        "id": "7fff8018bf625447df837c2fda5c58a705fbc038",
        "title": "XCiT: Cross-Covariance Image Transformers"
      },
      {
        "id": "7507603da9711c0e43e29f3094f33d9337e7dfbd",
        "title": "3D Human Pose Estimation with Spatial and Temporal Transformers"
      },
      {
        "id": "2984ab83ade26639c3a82d29628d0d9e4abbebb0",
        "title": "Incorporating Convolution Designs into Visual Transformers"
      },
      {
        "id": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
        "title": "Do Transformers Really Perform Bad for Graph Representation?"
      },
      {
        "id": "3dcfa05a1c162e6cab927c5b08d0444f7b6691f4",
        "title": "Probing Classifiers: Promises, Shortcomings, and Advances"
      },
      {
        "id": "3cbe314cc5407a6c3249815b5173f22ea15173c2",
        "title": "Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding"
      },
      {
        "id": "2e8149dafb864ec3675087c99bf5572fcf4eb170",
        "title": "RegionViT: Regional-to-Local Attention for Vision Transformers"
      },
      {
        "id": "36b9d0f8610a82fd25854889d9327a04da4ff8fd",
        "title": "MST: Masked Self-Supervised Transformer for Visual Representation"
      },
      {
        "id": "166e98317ed9c4687e71bef55a6800431e00b8fa",
        "title": "SiT: Self-supervised vIsion Transformer"
      },
      {
        "id": "f4299e47a76d5d7cf1638ba347d9848903ef5a60",
        "title": "Gated Transformer Networks for Multivariate Time Series Classification"
      },
      {
        "id": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
        "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"
      },
      {
        "id": "f1937a51c1bbb4b5971179f72d4753ccc206bb88",
        "title": "Dual Transformer for Point Cloud Analysis"
      },
      {
        "id": "054e307c1edf4b28137ffcbce980fe81f0647d20",
        "title": "Finetuning Pretrained Transformers into RNNs"
      },
      {
        "id": "1f668aebd03b5150c2c2fddae48f4a65cb4a80a8",
        "title": "Container: Context Aggregation Network"
      },
      {
        "id": "32a6daa76efd00f657e65842771971decf104efc",
        "title": "Improving the Faithfulness of Attention-based Explanations with Task-specific Information for Text Classification"
      },
      {
        "id": "fbcbe5a222786f38a1c69c3487b4edf8ca469934",
        "title": "Fully Transformer Networks for Semantic Image Segmentation"
      },
      {
        "id": "7fa6c0b5fd534ecf214b634f68a85a60d3b3191f",
        "title": "NAST: Non-Autoregressive Spatial-Temporal Transformer for Time Series Forecasting"
      }
    ],
    "2": [
      {
        "id": "1e3ef48abeef882e12f9553a1baf8944f3782c88",
        "title": "Several categories of Large Language Models (LLMs): A Short Survey"
      },
      {
        "id": "f25960465eaaec33f3a16ef9f6b4a0a046a498f8",
        "title": "REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers"
      },
      {
        "id": "8db5adadc0ab39f95a26a2eb6499d340d6c5ea21",
        "title": "From Word Types to Tokens and Back: A Survey of Approaches to Word Meaning Representation and Interpretation"
      },
      {
        "id": "d2aa9772a914b2fd7f193182f44c25b1e1a96107",
        "title": "LMs go Phishing: Adapting Pre-trained Language Models to Detect Phishing Emails"
      },
      {
        "id": "394bd431e522b86581086bcb5cd9be161cf1cdf4",
        "title": "Language Models are Realistic Tabular Data Generators"
      },
      {
        "id": "9b45bd2221b1d91e72939019bfdf3aba0310e6e1",
        "title": "Large Language Models Are Zero-Shot Text Classifiers"
      },
      {
        "id": "a35f1315e91513ff0bec0c488fe175214fd9636c",
        "title": "Recommender Systems in the Era of Large Language Models (LLMs)"
      },
      {
        "id": "3c0c14882318fd7ad3fd51cdc5fc515a7f78effd",
        "title": "LLMaAA: Making Large Language Models as Active Annotators"
      },
      {
        "id": "836906c334159d46a691dfc5466c33caa3d22f65",
        "title": "Unifying Image Processing as Visual Prompting Question Answering"
      },
      {
        "id": "a1081c6fc6921d6b76d9ebda4d712333fd7bbbf5",
        "title": "Large Language Models for Generative Recommendation: A Survey and Visionary Discussions"
      },
      {
        "id": "d9ff3db7a9e37fe0363bb87aa47acd6b6b67977e",
        "title": "GenRec: Large Language Model for Generative Recommendation"
      },
      {
        "id": "69f0c3a693d5f7f1512f2fcb4104692e4ae36184",
        "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models"
      },
      {
        "id": "9aeebfab9e27a4cecf7f488fca2d0bf07e06ae15",
        "title": "A Survey on Large Language Models for Personalized and Explainable Recommendations"
      },
      {
        "id": "0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
        "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian"
      },
      {
        "id": "6c5d706024b0e8a7a0e52e98991a2900d8e52c8d",
        "title": "Exploring In-Context Learning of Textless Speech Language Model for Speech Classification Tasks"
      },
      {
        "id": "856c342606aca05434e48f2e53cdbd6f6b886802",
        "title": "Pre‐trained language models: What do they know?"
      },
      {
        "id": "1b71d50ab66de103a53b3c73615c9638ee72089f",
        "title": "Data-Centric Financial Large Language Models"
      },
      {
        "id": "a9c75cf664f675a1b4034b0256ec3c5168e293df",
        "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "id": "68ae4b28fab8ff641deca48e0d340483d1c691ec",
        "title": "Exploring Fine-tuning ChatGPT for News Recommendation"
      },
      {
        "id": "2172c97edcaeba396dcb863154b4b489d3c2840c",
        "title": "Out of the Cage: How Stochastic Parrots Win in Cyber Security Environments"
      },
      {
        "id": "24288671a2a65bb9bcc155fd28f3b28210b850d6",
        "title": "NusaWrites: Constructing High-Quality Corpora for Underrepresented and Extremely Low-Resource Languages"
      },
      {
        "id": "72b4020f18b56bace32c473aaf2db9e0085f8837",
        "title": "Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models"
      },
      {
        "id": "65b63322cac4f4fd1b2dbc7155be69aed5f95b68",
        "title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching"
      },
      {
        "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models"
      },
      {
        "id": "cf659af952ee98182f9dc7cd6173bad55aa57daa",
        "title": "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation"
      },
      {
        "id": "c6ae95795d3708f3b0f231e2e301b901a3b4aff9",
        "title": "Knowledge Engineering using Large Language Models"
      },
      {
        "id": "47e2978a10aa3895cfbea0ff27c28d0e93102756",
        "title": "An extensive benchmark study on biomedical text generation and mining with ChatGPT"
      },
      {
        "id": "f7d3c17ad1dee97377651e0f5646b3fc6d047fc0",
        "title": "Evaluating ChatGPT as a Recommender System: A Rigorous Approach"
      },
      {
        "id": "c7bb292d86769d883989b4ea1a39cc414f29dbc9",
        "title": "CoLadder: Supporting Programmers with Hierarchical Code Generation in Multi-Level Abstraction"
      },
      {
        "id": "b486982fa7c68a8a08df1111ba9607119419c488",
        "title": "A Survey on Large Language Models for Recommendation"
      },
      {
        "id": "bac54736112098616f0e1c90435888ef3e119d32",
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"
      },
      {
        "id": "c589a3420ba335a05c248f525ea3c6e90215e42b",
        "title": "Pre-train, Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems"
      },
      {
        "id": "85dc7c22aaae5d596e0dc4d732b8f8afd51582bd",
        "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models"
      },
      {
        "id": "0a438980ac42451d6d32dd2ad8ead7b55520408d",
        "title": "A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"
      },
      {
        "id": "a2c8d1c5470435176185bf891c76711a9b44808a",
        "title": "PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models"
      },
      {
        "id": "994a6040fab375669a92cab0e67fb2fd203cd67f",
        "title": "Identifying and Extracting Rare Diseases and Their Phenotypes with Large Language Models"
      },
      {
        "id": "199600ce11fffc299c43717ee57c204d361bbee6",
        "title": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis"
      },
      {
        "id": "dae6d5439f5a7664377c9c10de66321372ef22de",
        "title": "Framing the News: From Human Perception to Large Language Model Inferences"
      },
      {
        "id": "450b5490cc653478c272be50aa986798df828a20",
        "title": "Uncovering ChatGPT’s Capabilities in Recommender Systems"
      },
      {
        "id": "385376b8aa48c25403f17d6206db7c09b67e1314",
        "title": "Prompt Engineering for Healthcare: Methodologies and Applications"
      },
      {
        "id": "60f8a7ac53585aa2c173219e97507d6d963864e7",
        "title": "PALR: Personalization Aware LLMs for Recommendation"
      },
      {
        "id": "26f7785ef8da35820599799549152b9ef695dae2",
        "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation"
      },
      {
        "id": "f4c4e148546089123f8da5db4fb246ab4062bd40",
        "title": "Evaluation of ChatGPT for NLP-based Mental Health Applications"
      },
      {
        "id": "2ee1f98649ff27378fc341cae907eb89aba8fba4",
        "title": "Prompt Learning for News Recommendation"
      },
      {
        "id": "f5de5cbb5563ed5633421d17d894573828c13b2d",
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights"
      },
      {
        "id": "ef91c31d8aab9fe95fec29149e2fe4568ab2fb32",
        "title": "SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains"
      },
      {
        "id": "f5d93cf5d81575aeee61faeddde4437fbcb74cd0",
        "title": "The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development"
      },
      {
        "id": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
        "title": "Task and Motion Planning with Large Language Models for Object Rearrangement"
      },
      {
        "id": "ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3",
        "title": "Is ChatGPT a Good Recommender? A Preliminary Study"
      },
      {
        "id": "6159549f986c63e160a678feef2130a2a4b93feb",
        "title": "Generative Recommendation: Towards Next-generation Recommender Paradigm"
      },
      {
        "id": "f4c01d442f9bbdf74c7b8bda5d916589d6ca845f",
        "title": "Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations"
      },
      {
        "id": "6115bb844813b68ff64188c241cbd1a11a53c88b",
        "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains"
      },
      {
        "id": "11628f656257e75e46447ac21cdaa86c4b340a0a",
        "title": "Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited"
      },
      {
        "id": "c8957181e3905ac8eafacce99c7396571f09ce0c",
        "title": "Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks: Algorithm Development and Validation Study"
      },
      {
        "id": "a7410cfe1a5b966d113a6ef5a2d9022f409d40df",
        "title": "Building Natural Language Processing Applications with EasyNLP"
      },
      {
        "id": "29642e7371f0dd814bfb6d4591c78b83b0dc8b43",
        "title": "Natural language processing in toxicology: Delineating adverse outcome pathways and guiding the application of new approach methodologies"
      },
      {
        "id": "17dcfef70619c0423e0527f0c9d90f4858125f5f",
        "title": "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding"
      },
      {
        "id": "76d688dac9817f2f401e136d1e831c0ec9f9b9f5",
        "title": "Revisiting and Advancing Chinese Natural Language Understanding with Accelerated Heterogeneous Knowledge Pre-training"
      },
      {
        "id": "72fbd9a613d4fad4542c8dda785a74cc36db6d5c",
        "title": "An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks"
      },
      {
        "id": "a26623d52d24e03044a158cddad931ec5ab7304c",
        "title": "A Survey of Knowledge Enhanced Pre-Trained Language Models"
      },
      {
        "id": "c6b30fc2469c4e72a311d91f831cb09ad9345a5d",
        "title": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages"
      },
      {
        "id": "00552729b09cf9430c0289c9c43965b63918cdae",
        "title": "LERT: A Linguistically-motivated Pre-trained Language Model"
      },
      {
        "id": "1139a20f41f8686f9d4e8b32550fe7bc8dc9ca13",
        "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost"
      },
      {
        "id": "1833ad08d52a454a50490fed91181fc7e2cc397a",
        "title": "DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning"
      },
      {
        "id": "dfd461850a228b555487b14ede991adbf85c93aa",
        "title": "Collectively encoding protein properties enriches protein language models"
      },
      {
        "id": "44ebfdb670007b3949507be0d1a1fca93bc3d5d5",
        "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges"
      },
      {
        "id": "180da92f3ba98e12c548dbd5c1d68b066e9b926b",
        "title": "Vision Transformers in Medical Imaging: A Review"
      },
      {
        "id": "bd7d7f7d79df4c628494ee4d6ce19cd7dbebb367",
        "title": "Is this Change the Answer to that Problem?: Correlating Descriptions of Bug and Code Changes for Evaluating Patch Correctness"
      },
      {
        "id": "ea2fb89403ea1cd6af000e761e2f72eb7c150607",
        "title": "TaxoPrompt: A Prompt-based Generation Method with Taxonomic Context for Self-Supervised Taxonomy Expansion"
      },
      {
        "id": "ac5225708efd250d217424ba27885e90f186160d",
        "title": "Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words"
      },
      {
        "id": "13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8",
        "title": "MarianCG: a code generation transformer model inspired by machine translation"
      },
      {
        "id": "bf22ef16a6a912763780aea454198edc3e2bb3c9",
        "title": "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing"
      },
      {
        "id": "c442b4c1a6030b315f8714726239123b05685066",
        "title": "EasyNLP: A Comprehensive and Easy-to-use Toolkit for Natural Language Processing"
      },
      {
        "id": "df602516e28a9ef0ef665ed0aef551984d8d770d",
        "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)"
      },
      {
        "id": "80b2b006ed2f26ec3ddc91e303dc9861fb456a26",
        "title": "On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules"
      },
      {
        "id": "af0fe7f31315a4173de5695887dffb20be458f48",
        "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records"
      },
      {
        "id": "bb7e46f316d319f9819c3554c99995ef8361ae9c",
        "title": "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"
      },
      {
        "id": "bbf3451c9c4eb6d8d84314b584afb0fadd01ffba",
        "title": "A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction"
      },
      {
        "id": "9a4953a8c0df1d7d7b11c8e1747eb156caa9fbcb",
        "title": "PERT: Pre-training BERT with Permuted Language Model"
      },
      {
        "id": "4962ce1242ff905b71e199964eafea3d2ad9688d",
        "title": "A Domain-adaptive Pre-training Approach for Language Bias Detection in News"
      },
      {
        "id": "11f64ec047782cada21d50efea1e0dc5843675f6",
        "title": "NusaX: Multilingual Parallel Sentiment Dataset for 10 Indonesian Local Languages"
      },
      {
        "id": "0dac0e73dc0d6f0ebbbd45ea2e3bc60d437200e1",
        "title": "CIRCLE: continual repair across programming languages"
      },
      {
        "id": "aec212c099e8a711367582806d620f7b0867ce91",
        "title": "DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation"
      },
      {
        "id": "e72ce8bd009e1c97fbd896321efe5a63b3d95c34",
        "title": "PTM4Tag: Sharpening Tag Recommendation of Stack Overflow Posts with Pre-trained Models"
      },
      {
        "id": "c963c505ffc4cc8b33315eb967784d0a466b3910",
        "title": "ProQA: Structural Prompt-based Pre-training for Unified Question Answering"
      },
      {
        "id": "eafe58b4a09844418bc8972f890e5d24e9df2fb7",
        "title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation"
      },
      {
        "id": "c3cfcd42b76ec6a18376f3975198c76e964b664f",
        "title": "yosm: A new yoruba sentiment corpus for movie reviews"
      },
      {
        "id": "443f16b95edec746a5259644540c44204f2d91c3",
        "title": "The Potential of Artificial Intelligence as a Method of Software Developer's Productivity Improvement"
      },
      {
        "id": "3c1855cf61d00282c7a41addb416bb4d9206064b",
        "title": "Beyond the Benchmarks: Toward Human-Like Lexical Representations"
      },
      {
        "id": "3f35e2b369cd975224675016cf44db0b450c0231",
        "title": "Understanding English as a Foreign Language Students’ Idea Generation Strategies for Creative Writing With Natural Language Generation Tools"
      },
      {
        "id": "c10d0353d8a169d23351f1dbec8cdfdd8c62a60e",
        "title": "Context-Tuning: Learning Contextualized Prompts for Natural Language Generation"
      },
      {
        "id": "5bc61019771a9fe2a12cc41bad1d9ae4222a152c",
        "title": "PromptMaker: Prompt-based Prototyping with Large Language Models"
      },
      {
        "id": "546a91d0957a2c34e083a9d9c3d8784aeac980ae",
        "title": "PSP: Pre-trained Soft Prompts for Few-Shot Abstractive Summarization"
      },
      {
        "id": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
        "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
      },
      {
        "id": "760f807406272b5ede591f19241824f2d17c319a",
        "title": "Multi-Task Learning in Natural Language Processing: An Overview"
      },
      {
        "id": "7cf0d71014b6a1a5397d37481c91efbb1a5660ea",
        "title": "A Comparison of Natural Language Processing and Machine Learning Methods for Phishing Email Detection"
      },
      {
        "id": "d095f9ffcb5905bf0858ad1769d3d90e2e8737e2",
        "title": "Jigsaw: Large Language Models meet Program Synthesis"
      },
      {
        "id": "42fc019b2668c9d9d984154d4c57f6c6d5a91619",
        "title": "Language Models are Few-shot Multilingual Learners"
      },
      {
        "id": "c2a79e2a65b721d4de5f6d4806323174b9f8f393",
        "title": "Towards Zero-Label Language Learning"
      },
      {
        "id": "e55391a9406245584b3e5b3225dad2e171b9a06b",
        "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"
      },
      {
        "id": "5075a0df5fc7ad5f1399450498044627ebe7a9f9",
        "title": "Drop Redundant, Shrink Irrelevant: Selective Knowledge Injection for Language Pretraining"
      },
      {
        "id": "62da76a8dbff4de50495be2f4746f25c4cd7ac0c",
        "title": "CLIN-X: pre-trained language models and a study on cross-task transfer for concept extraction in the clinical domain"
      },
      {
        "id": "48a3184b25a90d6864326aec7950af6aee60ef49",
        "title": "A Survey of Knowledge Enhanced Pre-trained Language Models"
      },
      {
        "id": "614dd2ae6db71cd5b2a6069407d3e0705ab2c19c",
        "title": "OpenPrompt: An Open-source Framework for Prompt-learning"
      },
      {
        "id": "cbf98ebe967e0f3f3236e7932f37013b98244e94",
        "title": "ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning"
      },
      {
        "id": "dca4d9abbc82e57dfa52f932e893d467a63e0682",
        "title": "Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation"
      },
      {
        "id": "13783f9d98b47a5b525bc3bec01d02088d5b1ef7",
        "title": "Assessing Generalizability of CodeBERT"
      },
      {
        "id": "2fab75cfd8394de70bca365572bc5bb04a1b1eb5",
        "title": "DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding"
      },
      {
        "id": "c0f56843ce02f92468fe515af6d5eff966e9aa68",
        "title": "Improving early diagnosis of rare diseases using Natural Language Processing in unstructured medical records: an illustration from Dravet syndrome"
      },
      {
        "id": "fe0b1f6194b490f6bbc41c716a58901c1049ccd8",
        "title": "IndoNLG: Benchmark and Resources for Evaluating Indonesian Natural Language Generation"
      },
      {
        "id": "b58d8579ece27a60432e667bfbdb750590fa65d9",
        "title": "True Few-Shot Learning with Language Models"
      },
      {
        "id": "64a1dbdd7653eaca25c78e87335ee156b6f6959e",
        "title": "Constrained Language Models Yield Few-Shot Semantic Parsers"
      },
      {
        "id": "bbfdcbfee1762d48cae9db8637f21ea3c234ba30",
        "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation"
      },
      {
        "id": "5bfb0cc16b871c75e32a6a9d54dd7db225260e04",
        "title": "CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing"
      },
      {
        "id": "e403faa971e2c750999a3d10bef8b01dd3e85f7b",
        "title": "PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains"
      },
      {
        "id": "d8e8e35bf4cf8821ade2d58b34d9ae23a9b08ab2",
        "title": "LET: Linguistic Knowledge Enhanced Graph Transformer for Chinese Short Text Matching"
      },
      {
        "id": "cec62e7a1ad6b6aa7dcab0e27a7d9c44ce6e486e",
        "title": "InferBERT: A Transformer-Based Causal Inference Framework for Enhancing Pharmacovigilance"
      },
      {
        "id": "b9c0d572402d38976d4bf99c78f4cc65e6d8f1e4",
        "title": "MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain"
      },
      {
        "id": "111dbe14083359ab39886790632e7f1421732a8a",
        "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"
      }
    ],
    "8": [
      {
        "id": "b3f272d644fe580d18f635be4d6ac4c520ef0d0f",
        "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning"
      },
      {
        "id": "76b19363b10d7ea783e4a6494eae40d73c8e9628",
        "title": "Parameter-efficient fine-tuning of large-scale pre-trained language models"
      },
      {
        "id": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
        "title": "Pretraining Language Models with Human Preferences"
      },
      {
        "id": "4f63c5a89c7299a864c6c48aa1844fb0fe8c9437",
        "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"
      },
      {
        "id": "3749d7dc47fe4af5bb7d53a0cf67ef99a4f798f8",
        "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training"
      },
      {
        "id": "f1cdf45ac76c7f9d129bcc7ef839f5ec0b3c7b82",
        "title": "OffensEval 2023: Offensive language identification in the age of Large Language Models"
      },
      {
        "id": "ca114b547c06eeb9911b685736679ca4dddf395c",
        "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks"
      },
      {
        "id": "528bfc811a3b4213566134afe2c880f867be5065",
        "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity"
      },
      {
        "id": "83cebf919635504786fc220d569284842b0f0a09",
        "title": "A Survey of Adversarial Defenses and Robustness in NLP"
      },
      {
        "id": "9cefc046ab8159b190f535f1930f0ff1b9460f76",
        "title": "Towards a Robust Deep Neural Network Against Adversarial Texts: A Survey"
      },
      {
        "id": "a2007c352e2051475844aa8ff95f63202b728e79",
        "title": "On Robustness of Finetuned Transformer-based NLP Models"
      },
      {
        "id": "705e49afd92130f2bc1e0d4d0b1f6cb14e88803f",
        "title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection"
      },
      {
        "id": "6079285d8a68a2e80107188126dc496abd0c1900",
        "title": "ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications"
      },
      {
        "id": "656cbdb55b510f9fe462a3993edbd5783e2c2cd4",
        "title": "Material transformers: deep learning language models for generative materials design"
      },
      {
        "id": "41d75ee68c26f1ed0ac42e2567e570aae96f8a4a",
        "title": "SOLD: Sinhala Offensive Language Dataset"
      },
      {
        "id": "85d08a213e9533c515601451cd78f971e547b1ae",
        "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization"
      },
      {
        "id": "4178159a23f9e07ee05a78aa7ddd921cf094c65a",
        "title": "GradMask: Gradient-Guided Token Masking for Textual Adversarial Example Detection"
      },
      {
        "id": "8267e6c9bf8d10013eae45916fc2861a9a09fffa",
        "title": "To Softmax, or not to Softmax: that is the question when applying Active Learning for Transformer Models"
      },
      {
        "id": "3159bd7af555c9612de2e59ea82d100d4140f667",
        "title": "Robust Natural Language Processing: Recent Advances, Challenges, and Future Directions"
      },
      {
        "id": "c9944f7da8aa77e64455ebea7ec488074931bbbf",
        "title": "Interactive Model Cards: A Human-Centered Approach to Model Documentation"
      },
      {
        "id": "ebd1e9d5f26fa817469c10bd0bb9cb5207143aeb",
        "title": "Evaluation Gaps in Machine Learning Practice"
      },
      {
        "id": "8bb9db78b4413b92cdeeae9e24e955aab9c87ae1",
        "title": "A Survey of Adversarial Defences and Robustness in NLP"
      },
      {
        "id": "dcd8e4e17e829640fa8ea4cca9ee5b79f433cda7",
        "title": "Phrase-level Textual Adversarial Attack with Label Preservation"
      },
      {
        "id": "28beacbb4cca9cfa63fb444b73343ec89b624563",
        "title": "A Two-Step Approach to Leverage Contextual Data: Speech Recognition in Air-Traffic Communications"
      },
      {
        "id": "cddf40e579a596d0110b260313adf43470617c4c",
        "title": "Datasets: A Community Library for Natural Language Processing"
      },
      {
        "id": "5e86c1adbe5b7cfccf2201e1c34400c819cdcdab",
        "title": "The King is Naked: on the Notion of Robustness for Natural Language Processing"
      },
      {
        "id": "f659031ceb7bbdcb7b0690742f35e2924fd1ed75",
        "title": "Towards Robustness Against Natural Language Word Substitutions"
      },
      {
        "id": "89daa253cfd707958b1539ec4d8ea9664e8ceb7d",
        "title": "NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation"
      },
      {
        "id": "3b451fa663704f927e1ec602d7c0845a9826922d",
        "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations"
      },
      {
        "id": "77805d75199e7b9e580b4827f56a069ba0ddd13f",
        "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model"
      },
      {
        "id": "99c766f0d71130e7db1290520e48f86cedcebfc4",
        "title": "SMAT: An Attention-Based Deep Learning Solution to the Automation of Schema Matching"
      },
      {
        "id": "96fd89de07a69dd2dc94d71f884e64174c5974e2",
        "title": "Interpreting the Robustness of Neural NLP Models to Textual Perturbations"
      },
      {
        "id": "04bac77744499c79650daeffa37d0f641e17de7f",
        "title": "FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning"
      },
      {
        "id": "8436897e713c2242d6291df9a6a33c1544d4dd39",
        "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"
      },
      {
        "id": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
        "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?"
      },
      {
        "id": "35bbb7e34f709384591ab822a353b4b664dd5c90",
        "title": "Token-modification adversarial attacks for natural language processing: A survey"
      },
      {
        "id": "0f71a4fa9736ae916e6aef53045f6be4c901b0ff",
        "title": "Reliability Testing for Natural Language Processing Systems"
      },
      {
        "id": "fe9cd5bcca161289b0e3da0f49114dcccf62eaa1",
        "title": "Token-Aware Virtual Adversarial Training in Natural Language Understanding"
      },
      {
        "id": "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5",
        "title": "Pretrained Language Models for Text Generation: A Survey"
      },
      {
        "id": "9b54941de1e21826ecc28b32730ac3f69991ede4",
        "title": "Robustness Gym: Unifying the NLP Evaluation Landscape"
      },
      {
        "id": "cbc1e8bbfe98f94c0d13d111b824cf603b62712c",
        "title": "Bad Characters: Imperceptible NLP Attacks"
      },
      {
        "id": "0021b3beb2ee0906b425eef7c0f453623c1c6a03",
        "title": "Certified Robustness to Word Substitution Attack with Differential Privacy"
      },
      {
        "id": "d6357b1c61611f744acbae69484acd7f21c89dff",
        "title": "Substructure Substitution: Structured Data Augmentation for NLP"
      },
      {
        "id": "546daf45b98dcaa1c7b438ec04b2a8b744f279d7",
        "title": "Certified Robustness to Programmable Transformations in LSTMs"
      },
      {
        "id": "1c2e60e0e31d06c909ddffbb2387987b449aceb0",
        "title": "Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice"
      }
    ],
    "4": [
      {
        "id": "d8d578d4ece329f17b025946587b1751721b9144",
        "title": "MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies"
      },
      {
        "id": "af6c6941acecfb23d899cd5266efdf2e27463f12",
        "title": "Causal Language Model Aided Sequential Decoding With Natural Redundancy"
      },
      {
        "id": "dfbfa21a93c3164ae8a033398c8de42b03b1b84d",
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning"
      },
      {
        "id": "3a07a87090a061ca41dd30ac8398a9a5d9d39826",
        "title": "Dense Text Retrieval Based on Pretrained Language Models: A Survey"
      },
      {
        "id": "1c299218d103234f45113f7f927cc0cc6aaefb9f",
        "title": "An End-to-End Contrastive Self-Supervised Learning Framework for Language Understanding"
      },
      {
        "id": "35f4bc71e41ff70bbc752955c1b00e0501bc2cf7",
        "title": "Dial2vec: Self-Guided Contrastive Learning of Unsupervised Dialogue Embeddings"
      },
      {
        "id": "6d42d9c5e8e46d5f45201bd6fb95ecc5b0287839",
        "title": "Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks"
      },
      {
        "id": "6bcc6ab9c28805d4067e99b2cdc7524550fe80e1",
        "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds"
      },
      {
        "id": "f52ea31e37c45e0de7ab4a5324d4d970479c110a",
        "title": "Can Generative Large Language Models Perform ASR Error Correction?"
      },
      {
        "id": "468fc94845b52c6e96ba1f3c3884d0653d5421b4",
        "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding"
      },
      {
        "id": "399cbcf0187197c8c371fcca1bd78cd3e529621c",
        "title": "Named Entity Recognition in Electronic Health Records: A Methodological Review"
      },
      {
        "id": "670b4f3aebb1156f608880e26f710d3dc04fee51",
        "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks"
      },
      {
        "id": "1552eca31fdd2ceb14c6206b0c2f20f1f7a23214",
        "title": "Hate Speech Detection via Dual Contrastive Learning"
      },
      {
        "id": "02d03b97cb76b54107ea2ad8f4ec671ae3aa52ac",
        "title": "A Material Lens on Coloniality in NLP"
      },
      {
        "id": "0318ae7671555eb3b1166888430dbdd76cd32354",
        "title": "Core Building Blocks: Next Gen Geo Spatial GPT Application"
      },
      {
        "id": "04c3fccfe01f42afe18dcdb027385f350ab3c9d1",
        "title": "HexT5: Unified Pre-Training for Stripped Binary Code Information Inference"
      },
      {
        "id": "89689059d0cdcb52d7fbb6007ab953db22936a90",
        "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models"
      },
      {
        "id": "93d6fa92d60938b5bd0e405e159832b91332f169",
        "title": "Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation"
      },
      {
        "id": "1725ad1d8cc0e539ac5d0a85657d5c95b4538c5e",
        "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects"
      },
      {
        "id": "e5dff0d39324dd0bb3fa323f2d256f801043ba4a",
        "title": "A Survey on Time-Series Pre-Trained Models"
      },
      {
        "id": "5c0eb412790a0e47b25714a5857f9a1d0d95c6a6",
        "title": "Sentence Embedding Leaks More Information than You Expect: Generative Embedding Inversion Attack to Recover the Whole Sentence"
      },
      {
        "id": "93a5864a53065f3c865541c4ea6170bbdb16dbb2",
        "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback"
      },
      {
        "id": "a6f7a120f3bdbabe97f8563ce25bfef3fa652400",
        "title": "RankCSE: Unsupervised Sentence Representations Learning via Learning to Rank"
      },
      {
        "id": "db0d67057b41927b5b51d3a393c250be64a405ae",
        "title": "Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods"
      },
      {
        "id": "20fdae9ab2d46c67efbe4baebe67d9f783c1f625",
        "title": "Contrastive Learning Models for Sentence Representations"
      },
      {
        "id": "ade39c39048d66465c7288e4a7f8258a1bce9e60",
        "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation"
      },
      {
        "id": "0925d3158e43917022f3357296c7c18e938bd6a1",
        "title": "Self-Supervised Adversarial Training for Contrastive Sentence Embedding"
      },
      {
        "id": "d1fb9013ed41c67b18755f01f0b7c7c5ef0a047f",
        "title": "System-Level Natural Language Feedback"
      },
      {
        "id": "1c1d1b4e1edae9dca84ea655624a4c8dc7fef2a7",
        "title": "Conformal Language Modeling"
      },
      {
        "id": "48abfc41a0abf023d2037ebb2f274835e0d322d0",
        "title": "Compositional Exemplars for In-context Learning"
      },
      {
        "id": "529e997e0d9730c25ad4347502da7e5a753274b8",
        "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference"
      },
      {
        "id": "0caf0fa48aced139c0d8214be4a795d9576b9990",
        "title": "Natural Test Generation for Precise Testing of Question Answering Software"
      },
      {
        "id": "fe214e738e6dcde26ff375ac7007979808ab554d",
        "title": "Don’t Judge a Language Model by Its Last Layer: Contrastive Learning with Layer-Wise Attention Pooling"
      },
      {
        "id": "3a021e1acc7588897df3f58e3ad928122846122f",
        "title": "Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning"
      },
      {
        "id": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b",
        "title": "In-context Examples Selection for Machine Translation"
      },
      {
        "id": "808e9ce4e86e79098edea7f00b5b91663b87a5e6",
        "title": "A taxonomy and review of generalization research in NLP"
      },
      {
        "id": "29005c5f2a37d8f8c3b4db62e04a2dffd7d9187c",
        "title": "Evaluating Unsupervised Text Classification: Zero-shot and Similarity-based Approaches"
      },
      {
        "id": "5697a0ede5425954d48daa6e1893dc87bd7d8be7",
        "title": "Contrastive Search Is What You Need For Neural Text Generation"
      },
      {
        "id": "bceebd434d7502dcd87004ec7313c2eea2c512fc",
        "title": "A Survey for Efficient Open Domain Question Answering"
      },
      {
        "id": "4e042409c4dc5210787fcd149485a59be3eef0f6",
        "title": "Lecture Notes on Neural Information Retrieval"
      },
      {
        "id": "94b34ad657bcfc9f1a8ed1ab1c3144aae9980901",
        "title": "DPTDR: Deep Prompt Tuning for Dense Passage Retrieval"
      },
      {
        "id": "310c2f0ac40821283a8ccee7ef44792982a6dc72",
        "title": "A survey of contrastive learning in NLP"
      },
      {
        "id": "c205cea82ec5db88be8f466d5b3823e37cb8f341",
        "title": "Feature-Level Debiased Natural Language Understanding"
      },
      {
        "id": "06862ac31ee1d331a08716d6109c4bf6e5bc6bac",
        "title": "From Black Boxes to Conversations: Incorporating XAI in a Conversational Agent"
      },
      {
        "id": "4e0526421da87d88627fef66e9e84ed559fff249",
        "title": "TweetNLP: Cutting-Edge Natural Language Processing for Social Media"
      },
      {
        "id": "802a5d24c78f713e282b003d99b4afd924bd7568",
        "title": "A Survey on Dynamic Neural Networks for Natural Language Processing"
      },
      {
        "id": "b6adc972f4bdbbb4dc7143713d16a3408a71ef7e",
        "title": "Automated Customer Complaint Processing for Water Utilities Based on Natural Language Processing—Case Study of a Dutch Water Utility"
      },
      {
        "id": "3fa0959ed06206b81f6d2125c19e1a5958250e6d",
        "title": "Threats to Pre-trained Language Models: Survey and Taxonomy"
      },
      {
        "id": "e475d7c3b10d548e59590902474ff99206a732f3",
        "title": "The Dark Side of the Language: Pre-trained Transformers in the DarkNet"
      },
      {
        "id": "0ca80cd04d2f485cc29f690befde3f7f532d6ea3",
        "title": "Label Anchored Contrastive Learning for Language Understanding"
      },
      {
        "id": "5bfff0955b511ce4ecb906c67cbe323b60b5c6d3",
        "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias"
      },
      {
        "id": "c090ff3dec01e06f46735b7b9ab133a5db8c73c3",
        "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting"
      },
      {
        "id": "c1ffa773f29e567e13b00495767f8c42e2bb44b2",
        "title": "A Contrastive Framework for Neural Text Generation"
      },
      {
        "id": "ff5c0e3e23a79fbcca95aa6dba1ec7ba71baf204",
        "title": "Unsupervised Sentence Representation via Contrastive Learning with Mixing Negatives"
      },
      {
        "id": "81b234a1e6da7bc8131e8585a9455dca5dd68754",
        "title": "Transkimmer: Transformer Learns to Layer-wise Skim"
      },
      {
        "id": "11500f6a5a5f00969b7600a8047e0c08ece4d9d2",
        "title": "PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings"
      },
      {
        "id": "fb5273032ab997dddd1b2917ed41808fd66fb909",
        "title": "MCSE: Multimodal Contrastive Learning of Sentence Embeddings"
      },
      {
        "id": "55e31baa3ae5f32fb5e695761892319e26dbc639",
        "title": "Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data"
      },
      {
        "id": "15031e6a94f9d6d19f74740c224a5523ec64d975",
        "title": "Improving Biomedical Information Retrieval with Neural Retrievers"
      },
      {
        "id": "eadba0531ba299d2f66fcff0fa07ab37e4ce5d7d",
        "title": "Just Rank: Rethinking Evaluation with Word and Sentence Similarities"
      },
      {
        "id": "43dfcb27404d53a9dfaf80020e028a1599d88639",
        "title": "Text Transformations in Contrastive Self-Supervised Learning: A Review"
      },
      {
        "id": "3d849136e0070f6d038dd96985ed67ead5aedb69",
        "title": "Locally Typical Sampling"
      },
      {
        "id": "6c761cfdb031701072582e434d8f64d436255da6",
        "title": "AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"
      },
      {
        "id": "8b954e1654c6b759a957fd11e66c111e6105fb3f",
        "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding"
      },
      {
        "id": "1cef73714d8ad89e442a9635fcd3061c61067638",
        "title": "RocketQAv2: A Joint Training Method for Dense Passage Retrieval and Passage Re-ranking"
      },
      {
        "id": "d617f51833860dc50d202af7f80be71304b2e994",
        "title": "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP"
      },
      {
        "id": "4097ae9aca6444fd7536bfbed1e62560521b70d3",
        "title": "PAIR: Leveraging Passage-Centric Similarity Relation for Improving Dense Passage Retrieval"
      },
      {
        "id": "37187ceb6008d49e1758bab0d4f86bf39aa175cf",
        "title": "A Survey on Green Deep Learning"
      },
      {
        "id": "8c24ef76d0a15bae316d1b9e6ab526ea5af93530",
        "title": "Graph Neural Networks: Methods, Applications, and Opportunities"
      },
      {
        "id": "f5a3dbc0518df5ca1b6333ae93244dde7f793736",
        "title": "Block-Skim: Efficient Question Answering for Transformer"
      },
      {
        "id": "f9f27e0f196e1b76caa44cf11aef7a40ca95b3f0",
        "title": "Why don’t people use character-level machine translation?"
      },
      {
        "id": "b21e1ffeae34d878e21699d142f37b200688ddbf",
        "title": "Identification of Bias Against People with Disabilities in Sentiment Analysis and Toxicity Detection Models"
      },
      {
        "id": "1982899ca875375227be5c131249cf3107bb9560",
        "title": "Virtual Augmentation Supported Contrastive Learning of Sentence Representations"
      },
      {
        "id": "ba53ea9e88486b8a0eda94e1cd84f6b0c33dffe9",
        "title": "SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations"
      },
      {
        "id": "023719b69c1722e35ad4d06e2efe130f630334f0",
        "title": "Simple Contrastive Representation Adversarial Learning for NLP Tasks"
      },
      {
        "id": "d87647784c12517d31964cc508d5b8423cc24f50",
        "title": "Integrating Approaches to Word Representation"
      },
      {
        "id": "01730636fe12bd3c15597e9439aba9b0b27ac150",
        "title": "A Primer on Contrastive Pretraining in Language Processing: Methods, Lessons Learned, and Perspectives"
      },
      {
        "id": "c26759e6c701201af2f62f7ee4eb68742b5bf085",
        "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
      },
      {
        "id": "077c713bccd9d2c7fde68d4cbde06ab0f07a6855",
        "title": "ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"
      },
      {
        "id": "e259ee075998eedc0b0c91c17769bf9dffeba46f",
        "title": "Graph Self-Supervised Learning: A Survey"
      },
      {
        "id": "a2fd50aa4dff5e04ed8535d84550da8bff316208",
        "title": "Whitening Sentence Representations for Better Semantics and Faster Retrieval"
      },
      {
        "id": "db9296eaa252231e24d066e8413bf29fb058ee45",
        "title": "Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering"
      },
      {
        "id": "2b9762e91305986ac8a2d624d0a69521304405f3",
        "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation"
      },
      {
        "id": "e79d1206292bc5e67ba19737d87d4b2ea4a37105",
        "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization"
      },
      {
        "id": "6563251e69e4378c189d0a0c94d8d19508d552c8",
        "title": "MathBERT: A Pre-Trained Model for Mathematical Formula Understanding"
      },
      {
        "id": "5cdd2cc5ef826a10785d3c6aeb56f1ea5fc0075a",
        "title": "B-PROP: Bootstrapped Pre-training with Representative Words Prediction for Ad-hoc Retrieval"
      },
      {
        "id": "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d",
        "title": "Consistent Accelerated Inference via Confident Adaptive Transformers"
      },
      {
        "id": "7707d54987b4ac7a7b94d8b932e3757a92a6a559",
        "title": "BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems"
      },
      {
        "id": "b29ba3d5341f8ce595f40279f47f404d2ffe1edb",
        "title": "Asteria: Deep Learning-based AST-Encoding for Cross-platform Binary Code Similarity Detection"
      },
      {
        "id": "36a2c27ffa72c05c2a17dc90b7c54e492b88ba01",
        "title": "Few-shot Conformal Prediction with Auxiliary Tasks"
      },
      {
        "id": "0f2f9cbb1b3ab8c5718d64359052d5e71f2d0dc7",
        "title": "Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods"
      },
      {
        "id": "dbcff24e72e8360f2026018a5cde646f369767cb",
        "title": "Not All Attention Is All You Need"
      }
    ],
    "5": [
      {
        "id": "2b31bbc4e042a0ff2e24e3e46ac6d1f66b1054b9",
        "title": "A Survey on Artificial Intelligence for Music Generation: Agents, Domains and Perspectives"
      },
      {
        "id": "753c871ae51cafac1184b31cabecdb6a540840fe",
        "title": "Promptvc: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts"
      },
      {
        "id": "8e1868f84091272544cb4209c4ccaad7cc88af27",
        "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration"
      },
      {
        "id": "46f0172525702f68e254fe5922b77c58e6df02bb",
        "title": "Demonstrating GPT-DB: Generating Query-Specific and Customizable Code for SQL Processing with GPT-4"
      },
      {
        "id": "c1dd77e48dd615ee6881b2cc876a00a92cae6eac",
        "title": "Boosting Large Language Model for Speech Synthesis: An Empirical Study"
      },
      {
        "id": "ffa05cb5504ba08254f498223f613b3ebcf87692",
        "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT"
      },
      {
        "id": "374ebdc8240a35820cb7ab8bfca37e180e21b605",
        "title": "Sparks of Large Audio Models: A Survey and Outlook"
      },
      {
        "id": "593cbef6af5736a3b1f498b59f7d741fafdc1e59",
        "title": "miditok: A Python package for MIDI file tokenization"
      },
      {
        "id": "fe3f5c5209557cd93a98de61afeb325db545740e",
        "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS"
      },
      {
        "id": "0e78674016ed80fd0a1c0b79d6ac1842e5c49a1f",
        "title": "LeBenchmark 2.0: a Standardized, Replicable and Enhanced Framework for Self-supervised Representations of French Speech"
      },
      {
        "id": "2238c293edcefbfc0b5966f27d82c560e66cae1b",
        "title": "Can Large Language Models Aid in Annotating Speech Emotional Data? Uncovering New Frontiers [Research Frontier]"
      },
      {
        "id": "2efadc1c928c8d92756e573f371b8d46087865ed",
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"
      },
      {
        "id": "f0aa870639e0766ce675e4bf45f65742321dcbe2",
        "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space With Natural Language Style Prompt"
      },
      {
        "id": "12454696085d66beaeb6cd43857de982a8445824",
        "title": "Transformers in Speech Processing: A Survey"
      },
      {
        "id": "66d2021641c2003d8614c898bbddb653ec349b22",
        "title": "Rethinking Semi-supervised Learning with Language Models"
      },
      {
        "id": "f51bc74814a3452009ea5ca262d9768d08149ee6",
        "title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model"
      },
      {
        "id": "b50c27607d7a858297232310bbec9819ade875a8",
        "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks"
      },
      {
        "id": "47ba7df38e24da9bad9266d2b58abbb2b70db6e5",
        "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning"
      },
      {
        "id": "c79852e9c9cc6734c9150847deb5449e489354ea",
        "title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner"
      },
      {
        "id": "a557f42eb0e32e9614d631b2271b36c5cea707d2",
        "title": "Effective Structured Prompting by Meta-Learning and Representative Verbalizer"
      },
      {
        "id": "dce2faa882d4a26da6bc6bc2166132a81388f36f",
        "title": "More Than Simply Masking: Exploring Pre-training Strategies for Symbolic Music Understanding"
      },
      {
        "id": "8c870bef01a4fbb20f60722ffc2f6bee3870b18b",
        "title": "AudioLM: A Language Modeling Approach to Audio Generation"
      },
      {
        "id": "5b870ccad0746225c71cf1167d929d80e6db47f6",
        "title": "IndicSUPERB: A Speech Processing Universal Performance Benchmark for Indian languages"
      },
      {
        "id": "9bf75110ea0923bbed49256b5491f1ec284019ec",
        "title": "From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"
      },
      {
        "id": "fdd69dd6ad2be28c6864b7330ccc1311e284cc5c",
        "title": "USB: A Unified Semi-supervised Learning Benchmark"
      },
      {
        "id": "120f4e798d78c3f6dceed218bee1ca83a5855f55",
        "title": "Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task"
      },
      {
        "id": "1ffe4d4deb090b3340f2aee41f5b691fa848db85",
        "title": "Chapter: Exploiting Convolutional Neural Network Adapters for Self-Supervised Speech Models"
      },
      {
        "id": "dfb22e1bfecafb4e37e6b3c0f4fd547669ced6ea",
        "title": "FeatureCut: An Adaptive Data Augmentation for Automated Audio Captioning"
      },
      {
        "id": "5d1c711c1d5b5f8399938af5e3df904680ce24a7",
        "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks"
      },
      {
        "id": "a3e3a9d878999c7038c275e75f5cd8a232aa4999",
        "title": "SUPERB-SG: Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities"
      },
      {
        "id": "f76a5b176f3435214eb87dd105f730f0b53672c3",
        "title": "Self-Supervised Speech Representation Learning: A Review"
      },
      {
        "id": "3e8ac2a46b83498ddd171c179ad97763271908c6",
        "title": "Speech Emotion Recognition Using Self-Supervised Features"
      },
      {
        "id": "f6919b54a4f06367947f0cf58cda54cdd08cd5f2",
        "title": "Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition"
      },
      {
        "id": "56621fd9a831f39ca3695c753c5f3ae84aae0ac7",
        "title": "Bias in Automated Speaker Recognition"
      },
      {
        "id": "1e482e72b0837a7ed4c0a7370a9af3e2ba2eaf29",
        "title": "Modeling Beats and Downbeats with a Time-Frequency Transformer"
      },
      {
        "id": "72a0736627b2a8448dc653c0d1e4c80960c929b5",
        "title": "Leveraging Pre-trained BERT for Audio Captioning"
      },
      {
        "id": "3db46a625b7e58d2e762668791e1ad6c4d614519",
        "title": "Beyond the Status Quo: A Contemporary Survey of Advances and Challenges in Audio Captioning"
      },
      {
        "id": "416b36ad7e4b89340aaf7dc1f611d9fce3870906",
        "title": "A Brief Overview of Unsupervised Neural Speech Representation Learning"
      },
      {
        "id": "588c15ef53d38c7c0266eddd235d64df68d6a973",
        "title": "Separate What You Describe: Language-Queried Audio Source Separation"
      },
      {
        "id": "46d846aa7df7b5c0cd4b129591b5754272701eae",
        "title": "On Transferability of Prompt Tuning for Natural Language Processing"
      },
      {
        "id": "1067c44e473b6998f89e13f0d4c0de730def43f0",
        "title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing"
      },
      {
        "id": "364621e7e864d3399ee90a7b2a058474e698e6de",
        "title": "Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding"
      },
      {
        "id": "58ad7dd2bba99329bc41363f9741aa01c18e2546",
        "title": "Evaluating Off-the-Shelf Machine Listening and Natural Language Models for Automated Audio Captioning"
      },
      {
        "id": "ebe259796870ebccf26577044d0087884209b884",
        "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training"
      },
      {
        "id": "9b56086e420ecb216f85d408a25264f640e46705",
        "title": "Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners"
      },
      {
        "id": "c397d0e17ced17e72aa3fc0df645eeabcabc32de",
        "title": "Efficient Training of Audio Transformers with Patchout"
      },
      {
        "id": "8f4bc7e92526faeb65fabd60e5d8c86392fce414",
        "title": "MT3: Multi-Task Multitrack Music Transcription"
      },
      {
        "id": "032f7bc2878a3f02922003b9359b70c2c6532c0e",
        "title": "SpecTNT: a Time-Frequency Transformer for Music Audio"
      },
      {
        "id": "537c72a77bce81310fe60b8c3b6cfe8f769d7f68",
        "title": "Multi-View Self-Attention Based Transformer for Speaker Recognition"
      },
      {
        "id": "9c58df84d6eddd4830a430dc5e164429cf89a79a",
        "title": "Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Semantic Information"
      },
      {
        "id": "cd84f79105746e8eadb63b9beef21ef3f9c02766",
        "title": "Diverse Audio Captioning Via Adversarial Training"
      },
      {
        "id": "00ac7891f1024244aa902e14f681a5a7fcd40228",
        "title": "Can Deep Neural Networks Predict Data Correlations from Column Names?"
      },
      {
        "id": "7ef33fd5b0ef0c4de42cf0afdc9f7dfb0f430b20",
        "title": "From Natural Language Processing to Neural Databases"
      },
      {
        "id": "d8e81e80490113434f7ac338c5f8d5a23f05a3de",
        "title": "SUPERB: Speech processing Universal PERformance Benchmark"
      },
      {
        "id": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
        "title": "Measuring and Improving Consistency in Pretrained Language Models"
      },
      {
        "id": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
        "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"
      },
      {
        "id": "ff35f4bcae80eca46aa0eff8a58f1f088ba53f8a",
        "title": "Visually grounded models of spoken language: A survey of datasets, architectures and evaluation techniques"
      },
      {
        "id": "f36e2d783a88ea984e39c89b44de4e843b5c8df2",
        "title": "Where are we in semantic concept extraction for Spoken Language Understanding?"
      },
      {
        "id": "79604e29340a8336003d1e8a348083b6249cc754",
        "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training"
      },
      {
        "id": "80bb30e65c36545f7dcaae8fa9f1e66e550d4731",
        "title": "A survey on deep reinforcement learning for audio-based applications"
      },
      {
        "id": "b257038ef379092509b1dd1d66a351f47363d6eb",
        "title": "LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech"
      },
      {
        "id": "c426ce2819990b2de233f93c11940cfb0161b836",
        "title": "Audio Transformers: Transformer Architectures For Large Scale Audio Understanding. Adieu Convolutions"
      },
      {
        "id": "16529f7194bf7faee8a4e43fd54aefeb8730f236",
        "title": "Database reasoning over text"
      },
      {
        "id": "3d0853eb0429fff9a6ff04fd46ea221e4f84fbf2",
        "title": "The Case for NLP-Enhanced Database Tuning: Towards Tuning Tools that \"Read the Manual\""
      }
    ],
    "7": [
      {
        "id": "eb2c2330177f765038a2b17e2ee3498965865797",
        "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
      },
      {
        "id": "5b0fe50dc6df8f4eba13f8177dcd4bbe5a2b0e23",
        "title": "A Survey of Techniques for Optimizing Transformer Inference"
      },
      {
        "id": "a6a828dcaaf373554b53b8f8002de126f3e38695",
        "title": "An Integer-Only and Group-Vector Systolic Accelerator for Efficiently Mapping Vision Transformer on Edge"
      },
      {
        "id": "eb9b568cc98909af92ae5295a6ca526d8c349b68",
        "title": "Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation"
      },
      {
        "id": "d2a42864605a502325a874bc470481ca1904ea0a",
        "title": "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"
      },
      {
        "id": "705c508f3d698510f45a506d1c12e81a7a75bd98",
        "title": "ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers"
      },
      {
        "id": "0636ddd202cfdbb89276c01aae1e2e29e2bbc859",
        "title": "TaskFusion: An Efficient Transfer Learning Architecture with Dual Delta Sparsity for Multi-Task Natural Language Processing"
      },
      {
        "id": "8df67942e29cba92bb5913b62d1d2df7371842d9",
        "title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation"
      },
      {
        "id": "b7d12aec8a0152ec4921dfa43ab525a63b334385",
        "title": "Speculative Decoding with Big Little Decoder"
      },
      {
        "id": "bd5c24ec7d3c91648fb0beec6508a5302823f97b",
        "title": "X-Former: In-Memory Acceleration of Transformers"
      },
      {
        "id": "84ed091b4f7875323e512f17f5fe46eecbf66174",
        "title": "A 95.6-TOPS/W Deep Learning Inference Accelerator With Per-Vector Scaled 4-bit Quantization in 5 nm"
      },
      {
        "id": "7ce692010c494e20ecd838ca51b10c22ae58c5d5",
        "title": "An Energy-Efficient Transformer Processor Exploiting Dynamic Weak Relevances in Global Attention"
      },
      {
        "id": "687843157b19e750d03e078c3807667c7261a99c",
        "title": "A Review of Deep Learning for Video Captioning"
      },
      {
        "id": "b612fc6af23cccf2133c2ea40597453ab40dc2c3",
        "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      },
      {
        "id": "276a00173085d75232dee025104cc27ad23eaf75",
        "title": "A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks"
      },
      {
        "id": "3f6243097a58e386aea1215fed4f372dee07a100",
        "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
      },
      {
        "id": "200ef1cde362aafbf598a2b5a1c5f35504ca2289",
        "title": "ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design"
      },
      {
        "id": "86891d00499eebe86d3f1e39143d412addf2652b",
        "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"
      },
      {
        "id": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
        "title": "ViA: A Novel Vision-Transformer Accelerator Based on FPGA"
      },
      {
        "id": "f1557a3638b164c632660a8bd4186076a96c7bf1",
        "title": "Bebert: Efficient And Robust Binary Ensemble Bert"
      },
      {
        "id": "f806d16e3efd0f3f5b63491a6f6d4a7334df4cee",
        "title": "TCB: Accelerating Transformer Inference Services with Request Concatenation"
      },
      {
        "id": "17a8bd6a5763f6607863348ce1757ac2ad3417fd",
        "title": "Accelerating Transformer Networks through Recomposing Softmax Layers"
      },
      {
        "id": "43b7437ed33a29d3d90239ad66f325a465ff7e91",
        "title": "Meta Learning for Natural Language Processing: A Survey"
      },
      {
        "id": "6da9a81b75e7ad02867860753d1aa276673a3a77",
        "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"
      },
      {
        "id": "5eeb828685e44ca5b8ebafb34a9fa4d51c9186df",
        "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models"
      },
      {
        "id": "fb2307f7ce7c6868429ee3ee15d6eaf311ecba5c",
        "title": "BiBERT: Accurate Fully Binarized BERT"
      },
      {
        "id": "76d40153acfbb35a7eb8272a4215854cafa10e78",
        "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"
      },
      {
        "id": "2a682d3ecda1a3003d5b249867eae53c35c35e68",
        "title": "A 17–95.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm"
      },
      {
        "id": "a71d57029884e60735f5672d33794e29d0ae3dea",
        "title": "Mokey: enabling narrow fixed-point inference for out-of-the-box floating-point transformer models"
      },
      {
        "id": "557c3e1234a5d8fbde234c03776d12f6c3a67f8e",
        "title": "Federated Split BERT for Heterogeneous Text Classification"
      },
      {
        "id": "258d3488a9784c6829c278b31b202657b45c0655",
        "title": "SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression"
      },
      {
        "id": "bbd5eb0924ec07a83dbc99151a09f463300c0001",
        "title": "Accelerating attention through gradient-based learned runtime pruning"
      },
      {
        "id": "73c722148ed4a5301dc75ae291b647a1915b8ecd",
        "title": "MKQ-BERT: Quantized BERT with 4-bits Weights and Activations"
      },
      {
        "id": "e2b29af251adc9628053966ea2b9cb45b4cdbb4b",
        "title": "REx: Data-Free Residual Quantization Error Expansion"
      },
      {
        "id": "511e2fe08d8ce170736b59d386a3301a6d6e57b0",
        "title": "NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM"
      },
      {
        "id": "9202a718ce05395b6e17d5301e3a2e8b1021f31b",
        "title": "Prune Once for All: Sparse Pre-Trained Language Models"
      },
      {
        "id": "ef18db2a18ac61e72783a613328842ce86ef00bf",
        "title": "AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models"
      },
      {
        "id": "b97c3c370401dc34d2adbeb24f34de5180a14be6",
        "title": "Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture"
      },
      {
        "id": "73bcf4577284fa116ee73487b7cbb85c8266eaa0",
        "title": "Understanding and Overcoming the Challenges of Efficient Transformer Quantization"
      },
      {
        "id": "2b38ddff8e24a07597c8d042ea7b8b85a678e9b2",
        "title": "FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks"
      },
      {
        "id": "c67b1a62b868a758791c88d5465c7b6d53510fc3",
        "title": "Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention"
      },
      {
        "id": "9f840be023309cc957dc741dce85dfc6b1a3b486",
        "title": "NPE: An FPGA-based Overlay Processor for Natural Language Processing"
      },
      {
        "id": "7a320541d7772bedc9b7f537f6bd459675675bb0",
        "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural Language Processing"
      },
      {
        "id": "5b89435357806e89cd428c56ce98aeb930b0df5c",
        "title": "Variance-reduced First-order Meta-learning for Natural Language Processing Tasks"
      },
      {
        "id": "093253653cd0b55970c390d77b75137c4095dc29",
        "title": "A Survey of Quantization Methods for Efficient Neural Network Inference"
      },
      {
        "id": "7b8f3f65a98340d6e5ab94bd9a4ccb8f75704fd8",
        "title": "I-BERT: Integer-only BERT Quantization"
      },
      {
        "id": "5af69480a7ae3b571df6782a11ec4437b386a7d9",
        "title": "ELSA: Hardware-Software Co-design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks"
      },
      {
        "id": "b080ba53a471348e7e76234decdf14e730fea7db",
        "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for Transformers"
      },
      {
        "id": "d6eeb0ca9e2f34f2427866aa864d364ec78e6049",
        "title": "Accelerating Transformer-based Deep Learning Models on FPGAs using Column Balanced Block Pruning"
      },
      {
        "id": "dd0a27aa2285bc64798fa76944400ab6d9ce3025",
        "title": "NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search"
      },
      {
        "id": "452d872edfd7cb5c76f0e93b517322b56b861a46",
        "title": "Exploring Video Captioning Techniques: A Comprehensive Survey on Deep Learning Methods"
      },
      {
        "id": "2a0ae7182b13789056e13dc1887904c923a92675",
        "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with Learned Step Size Quantization"
      },
      {
        "id": "46ce523f8ace9d5cd811ef608d4f19432bdd0f37",
        "title": "Automatic Generation of Descriptive Titles for Video Clips Using Deep Learning"
      }
    ]
  },
  "cluster_keywords": {
    "1": [
      "robot",
      "guided",
      "planning",
      "chatbot",
      "robotics"
    ],
    "0": [
      "llm",
      "multilingual",
      "turing",
      "nlg",
      "solvers"
    ],
    "6": [
      "chemical",
      "aviation",
      "knowledge",
      "bert",
      "classification"
    ],
    "9": [
      "health",
      "hospital",
      "radiology",
      "inducing",
      "multiclass"
    ],
    "3": [
      "graph",
      "visual",
      "deep",
      "convolutional",
      "semantic"
    ],
    "2": [
      "models",
      "prompting",
      "prompts",
      "norwegian",
      "textless"
    ],
    "8": [
      "attacks",
      "textual",
      "offensive",
      "robust",
      "defences"
    ],
    "4": [
      "retrieval",
      "supervised",
      "embeddings",
      "translation",
      "multimodal"
    ],
    "5": [
      "music",
      "supervised",
      "prompt",
      "decoder",
      "python"
    ],
    "7": [
      "accelerator",
      "pruning",
      "quantized",
      "transformer",
      "sparse"
    ]
  }
}