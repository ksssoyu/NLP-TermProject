{
  "window": "2022_2024",
  "num_clusters": 11,
  "cluster_details": {
    "3": [
      {
        "id": "0a44db2f1035eedaba16b4d362437b3e6fe7ef26",
        "title": "LLMs are Also Effective Embedding Models: An In-depth Overview"
      },
      {
        "id": "b233f7d5ebe9c15f39236bcde62faa406c27d506",
        "title": "Watermarking Techniques for Large Language Models: A Survey"
      },
      {
        "id": "00ce8beee350a260395676490915d7ebfa7430d1",
        "title": "Impact of Model Size on Fine-tuned LLM Performance in Data-to-Text Generation: A State-of-the-Art Investigation"
      },
      {
        "id": "ae9a2bcd460354c706aaea8797b1c2c15841a6b6",
        "title": "A Survey on Vision-Language-Action Models for Embodied AI"
      },
      {
        "id": "2eccbf79016451794200c47451de87ff2e0356b7",
        "title": "Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples"
      },
      {
        "id": "e70e92c96a75813db82e8597cadb0ad4c893019a",
        "title": "From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation"
      },
      {
        "id": "552bbc2c5014c5229ed751e474ff7f49d27ee0ba",
        "title": "Generative artificial intelligence in innovation management: A preview of future research developments"
      },
      {
        "id": "9f1b3fdf2af592c850b7e50c0022cb7cce2ae103",
        "title": "An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference"
      },
      {
        "id": "6e3eed2b1ec12cf13bcc3a7427113b832193265a",
        "title": "History, Development, and Principles of Large Language Models-An Introductory Survey"
      },
      {
        "id": "ec67d5f0e236f23c6b48b926f9e25db52194dd71",
        "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey"
      },
      {
        "id": "a419e1d74cfc2b5ff400963476bda5c6ae66e172",
        "title": "TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models"
      },
      {
        "id": "dd41f3d40f89c93ba866ba482c3c8c617a0b0bad",
        "title": "BenLLM-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP"
      },
      {
        "id": "9fec5cf2f06e6fd8c5e6f6028226082d1ecec5b7",
        "title": "Extrapolating Large Language Models to Non-English by Aligning Languages"
      },
      {
        "id": "ff6f2b9e56ee0f3f26bcbdc5079678c059fe24e3",
        "title": "A Theory for Emergence of Complex Skills in Language Models"
      },
      {
        "id": "ca18c18ddd730e4d690431ad6c65035d0f41aed6",
        "title": "Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy"
      },
      {
        "id": "8439616aada33a889f4f52d6a9a3d7f6c5a30b79",
        "title": "Assessing How Large Language Models Can Be Integrated with or Used for Blockchain Technology: Overview and Illustrative Case Study"
      },
      {
        "id": "3e664adb009dce373129a3563e4b2cb08731bc76",
        "title": "PolyLM: An Open Source Polyglot Large Language Model"
      },
      {
        "id": "d62c4d00b277e948956b6610ce2644e88fe1577b",
        "title": "Large Language Models"
      },
      {
        "id": "af067c6f1c12941625e4c3b49b002c7c7c0b2542",
        "title": "BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model"
      },
      {
        "id": "879a7f5abdb7ab803d48172d4f0830965f989d46",
        "title": "Language Model Tokenizers Introduce Unfairness Between Languages"
      },
      {
        "id": "dfd8944d39b378489b878d6e105d040fa0e524db",
        "title": "Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis"
      },
      {
        "id": "f9a7175198a2c9f3ab0134a12a7e9e5369428e42",
        "title": "A Survey of Large Language Models"
      },
      {
        "id": "a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5",
        "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"
      },
      {
        "id": "7389b6ebbf36f4d869a02e305e2ef52ad2c92264",
        "title": "Applications of transformer-based language models in bioinformatics: a survey"
      },
      {
        "id": "964bd39b546f0f6625ff3b9ef1083f797807ef2e",
        "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"
      },
      {
        "id": "5b8b994117d9d3273d91e9c671da495e9e20e889",
        "title": "Generative Artificial Intelligence: A Systematic Review and Applications"
      },
      {
        "id": "473543b2d0cf24af9d04eb9f752506542bf25932",
        "title": "The Evolution of Large Language Model: Models, Applications and Challenges"
      },
      {
        "id": "2b3321ddb2f28015dc677f3fee9ba9e3333c93d7",
        "title": "Recent Advances in Large Language Models for Healthcare"
      },
      {
        "id": "3f392287fc443e855865c9e1a595812fa9ac9bf9",
        "title": "AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs"
      },
      {
        "id": "97b788534bcf27c5c88db30ce0f89cbe2c6d1425",
        "title": "Construction of a Japanese Financial Benchmark for Large Language Models"
      },
      {
        "id": "cfa85c8db829dbd2384ea7f130f462e7e7f1f630",
        "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval"
      },
      {
        "id": "33c40aadd08802cfa4911dc99424922d538999c3",
        "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)"
      },
      {
        "id": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
        "title": "PaLM: Scaling Language Modeling with Pathways"
      },
      {
        "id": "677953fb705d1bb1f170dcee4ffab9824f8626de",
        "title": "M5 - A Diverse Benchmark to Assess the Performance of Large Multimodal Models Across Multilingual and Multicultural Vision-Language Tasks"
      },
      {
        "id": "37e3992869212c6434b0cd251fc6e1d219ac485a",
        "title": "An Empirical Comparison of Vocabulary Expansion and Initialization Approaches For Language Models"
      },
      {
        "id": "88934cf7a03dd8820af75d0302a608705543445f",
        "title": "CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches"
      },
      {
        "id": "566d28817179a68508cf6f3c59468ecf1877e446",
        "title": "Lawyer GPT: A Legal Large Language Model with Enhanced Domain Knowledge and Reasoning Capabilities"
      },
      {
        "id": "f10006d615cc32cf0cedac21a96dabd66508d273",
        "title": "A Field Guide to Automatic Evaluation of LLM-Generated Summaries"
      },
      {
        "id": "419d8b07c01b2bf30f4f92513db86251d2a8734f",
        "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs"
      },
      {
        "id": "0f5f628709cd5119ca42e82ab904ffff02514d29",
        "title": "Propulsion: Steering LLM with Tiny Fine-Tuning"
      },
      {
        "id": "27c350f1f4b1abce8ec45d4523d4340ebe60fe01",
        "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"
      },
      {
        "id": "b2469523d558a9f3efb9eb4b9184b09925dc7dbc",
        "title": "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models"
      },
      {
        "id": "1977056dc5b7cbbc26b2210a6d6d1a1e3ce2dad3",
        "title": "Lean Workbook: A large-scale Lean problem set formalized from natural language math problems"
      },
      {
        "id": "e19485777bce53f2301f1137133fd9436d9896ea",
        "title": "Learning to Compress Prompt in Natural Language Formats"
      },
      {
        "id": "9d107ecb6442ae757b79f10cc7f5f799f2372846",
        "title": "GOLD: Geometry Problem Solver with Natural Language Description"
      },
      {
        "id": "c8b18682965ff9dccc0130dab3d679f78cefa617",
        "title": "A Survey on Large Language Models for Code Generation"
      },
      {
        "id": "5760218e4635cc2841dc7fba1752427a023c2193",
        "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias"
      },
      {
        "id": "be445964370b5ea8c15aa1bf6cf10f951fa90b9a",
        "title": "Multilingual Large Language Model: A Survey of Resources, Taxonomy and Frontiers"
      },
      {
        "id": "cd4e16b5ef043ca851b137db92a5d52513119d7a",
        "title": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study"
      },
      {
        "id": "79030ade9b06efac725626c1d0f097ba776dd081",
        "title": "TriSum: Learning Summarization Ability from Large Language Models with Structured Rationale"
      },
      {
        "id": "4bbc278bc27a399bda7bc3015d2f38bd34f8dff7",
        "title": "Large Language Model Adaptation for Financial Sentiment Analysis"
      },
      {
        "id": "a2ca24ae72fbbc54d41083307fc2a24d12f4f23c",
        "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation"
      },
      {
        "id": "c9629421113a3dda8b544e9eac3b174dc40eda4b",
        "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models"
      },
      {
        "id": "786f8f73415f3d67312427743ccd2d5016479496",
        "title": "The Hallucinations Leaderboard - An Open Effort to Measure Hallucinations in Large Language Models"
      },
      {
        "id": "d8ba2eab5d9463f99b0025299b4f8b83fb83eeba",
        "title": "BioInformatics Agent (BIA): Unleashing the Power of Large Language Models to Reshape Bioinformatics Workflow"
      },
      {
        "id": "7493c3c89710766f87d0af4e62e98aa31f34c83c",
        "title": "Machine Translation with Large Language Models: Prompt Engineering for Persian, English, and Russian Directions"
      },
      {
        "id": "b90abcfbc391c606206b4d32c3887292f0fd3226",
        "title": "Professional Agents - Evolving Large Language Models into Autonomous Experts with Human-Level Competencies"
      },
      {
        "id": "3f5f8059d7b29e7817efe491c6e01e78f1066e59",
        "title": "MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property"
      },
      {
        "id": "a4e996a6a3df1ac76a28b1e0513bbcb5449e4227",
        "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods"
      },
      {
        "id": "727b058bf0c3c3b5d18f1937783e8c7bbddcd03d",
        "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges"
      },
      {
        "id": "881374ebe594e0c412499e8e662a4b8d2a687ade",
        "title": "Machine Learning Empowering Drug Discovery: Applications, Opportunities and Challenges"
      },
      {
        "id": "83539e8b94cde5c7d41863aba4fdca55d00f1841",
        "title": "Privacy Preserving Prompt Engineering: A Survey"
      },
      {
        "id": "5d2a53f3cb4ffaade8f0cfa6d9bfa8dfc0c4290a",
        "title": "Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs"
      },
      {
        "id": "dad44ff015c2e1fa83723ab4b882bcae251d17ec",
        "title": "Effectively Compress KV Heads for LLM"
      },
      {
        "id": "ed5e56bde9c17db5329fd37b7611657077676e20",
        "title": "A publishing infrastructure for Artificial Intelligence (AI)-assisted academic authoring"
      },
      {
        "id": "cc1c48ad6c94aa2b262598311def735e49196dd8",
        "title": "Automated Long Answer Grading with RiceChem Dataset"
      },
      {
        "id": "a161f877b87527e7f411e18a7ee4b3a942ac4fac",
        "title": "LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK Case Law Dataset"
      },
      {
        "id": "233f1440f5db6e73dad35a4eeb920b06ab7af226",
        "title": "Conversational Assistants in Knowledge-Intensive Contexts: An Evaluation of LLM- versus Intent-based Systems"
      },
      {
        "id": "e9e746956d7f22bd844d2975a48d0ff8100e4bf2",
        "title": "Privacy-Preserving Language Model Inference with Instance Obfuscation"
      },
      {
        "id": "4fff661078543f6ffb9fe2c0c04829a877f5cfa2",
        "title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL"
      },
      {
        "id": "675629ff78cef09665a1135fece66195ed80a640",
        "title": "MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline"
      },
      {
        "id": "ae06df762adcc4221162e83a737ea63cff47e65d",
        "title": "MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control"
      },
      {
        "id": "6a4501fefaf73261dc180ff86b52208679f3fb9c",
        "title": "A Survey on Deep Learning for Theorem Proving"
      },
      {
        "id": "edf3ad2b10d8084c5185072b07f0318f8ed110c9",
        "title": "Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning"
      },
      {
        "id": "db0f9376882cf7805f36def33600805d330e5386",
        "title": "Process-Driven Autoformalization in Lean 4"
      },
      {
        "id": "6d42d9c5e8e46d5f45201bd6fb95ecc5b0287839",
        "title": "Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks"
      },
      {
        "id": "ca31b8584b6c022ef15ddfe994fe361e002b7729",
        "title": "A Comprehensive Overview of Large Language Models"
      },
      {
        "id": "ab7d320cbae173aef86c31faa087780cba44551f",
        "title": "SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling"
      },
      {
        "id": "942130a875ccfe55a4c60c27c636f693e25cb13d",
        "title": "Personality Traits in Large Language Models"
      },
      {
        "id": "37cbf656ca8b76f29684c37c2ee43118d5bd8a8c",
        "title": "Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?"
      },
      {
        "id": "64e802ea8e9dbe247c31fb06184c04dbf9e55e4e",
        "title": "EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce"
      },
      {
        "id": "49408c5e1ac75854f1580e561384df2be870d559",
        "title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases"
      },
      {
        "id": "62b4e06f5249d22e4a153ec4a2dc934c6a014372",
        "title": "OWL: A Large Language Model for IT Operations"
      },
      {
        "id": "cf1fd110df9fec1d820032a5264514d88a56b06b",
        "title": "LLaMAntino: LLaMA 2 Models for Effective Text Generation in Italian Language"
      },
      {
        "id": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
        "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models"
      },
      {
        "id": "1146d40d3d01427a008a20530269667b8989750c",
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation"
      },
      {
        "id": "b74be6891cd7f6d81e346852494c08528dbffdc4",
        "title": "TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine"
      },
      {
        "id": "0ae7bc864a66feae9e5a189a9ac24cf4c19aa0dd",
        "title": "Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"
      },
      {
        "id": "537335d9aad0ddbaef93e7f88b0db096671ef6ec",
        "title": "No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function"
      },
      {
        "id": "b411c8f98865565f54642af1a5c010bde6beaedc",
        "title": "Exploring Language Models: A Comprehensive Survey and Analysis"
      },
      {
        "id": "4d3179b1fde10d6e4bf044454d37fbbc6591ba53",
        "title": "Benchmarking Large Language Models with Augmented Instructions for Fine-grained Information Extraction"
      },
      {
        "id": "5fc1a3a49e8f1d106118b69d1d6be3b6caa23da0",
        "title": "Qwen Technical Report"
      },
      {
        "id": "9fcdbfdf28245010c875ce85502351fe05c04b49",
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View"
      },
      {
        "id": "7c629ca898fb8c87dd363a16d43f29f6ed44dfa8",
        "title": "Generative AI Text Classification using Ensemble LLM Approaches"
      },
      {
        "id": "880aa839e9275808573f221988944323bf40ddab",
        "title": "Bias and Fairness in Chatbots: An Overview"
      },
      {
        "id": "c02cded20074fff4310d7bc943d0b8bfff305d58",
        "title": "Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench"
      },
      {
        "id": "afa9f128435501d0f2c3ae524a0a7698d0bc3d21",
        "title": "Fast Quantum Algorithm for Attention Computation"
      },
      {
        "id": "10eb81d069f3654fa234f769852173a9ddadfabd",
        "title": "Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges"
      },
      {
        "id": "8d24e6680a19c2f4c113e45145ec067130069805",
        "title": "Increasing Coverage and Precision of Textual Information in Multilingual Knowledge Graphs"
      },
      {
        "id": "ae10d1462e63fd361550546d2470510e9a93fe75",
        "title": "Crosslingual Retrieval Augmented In-context Learning for Bangla"
      },
      {
        "id": "23cc6b2ed88872fcd3767cf054100e8eddcdb0a1",
        "title": "CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks"
      },
      {
        "id": "28c6ac721f54544162865f41c5692e70d61bccab",
        "title": "A Survey on Large Language Model based Autonomous Agents"
      },
      {
        "id": "65d7663b60d95f98e6281ecc4da9c7a975119b91",
        "title": "GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT"
      },
      {
        "id": "9a7b9515b66bf83c9c808626206eabe9a8837c22",
        "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models"
      },
      {
        "id": "68ef6138e007421f1a70e2d0ce1b3308a54cc784",
        "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models"
      },
      {
        "id": "15a782b48fd26f2ce63ab1259e647c3656ce43c7",
        "title": "Split-and-Denoise: Protect large language model inference with local differential privacy"
      },
      {
        "id": "e26888285436bc7998e5c95102a9beb60144be5e",
        "title": "Textbooks Are All You Need II: phi-1.5 technical report"
      },
      {
        "id": "70a75b05a9410198fd71c3a0fe937a77d15d6bc8",
        "title": "Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text"
      },
      {
        "id": "86f66498f199d06353c5b8df1b774b80bf29eeac",
        "title": "Agent-based Learning of Materials Datasets from Scientific Literature"
      },
      {
        "id": "c8a0335bad4436ed525135d5e3cad18967ac0793",
        "title": "Using LLMs to Customize the UI of Webpages"
      },
      {
        "id": "e2d0bd5bfd0d1d35484552abf9f7fbcd276c8669",
        "title": "Effectiveness of Generative Artificial Intelligence for Scientific Content Analysis"
      },
      {
        "id": "931cec40538dcec99043b3502ca58e9e0fdf8873",
        "title": "Exploring natural language processing in mechanical engineering education: Implications for academic integrity"
      },
      {
        "id": "b8647eb447102c3df003a39b4768eacdc83d93d7",
        "title": "AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing"
      },
      {
        "id": "50c966cdb5e62731f4a5ea03f32b72667dbc753e",
        "title": "Chatbot-Based Natural Language Interfaces for Data Visualisation: A Scoping Review"
      },
      {
        "id": "206400aba5f12f734cdd2e4ab48ef6014ea60773",
        "title": "Evaluating Object Hallucination in Large Vision-Language Models"
      },
      {
        "id": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
        "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation"
      },
      {
        "id": "2029349c55c1dba3493c5b3bd25152f18ba21ae2",
        "title": "Augmented Language Models: a Survey"
      },
      {
        "id": "6d0656d9bb60a2bea50c4b894fbcc5d1e32134e7",
        "title": "Biases in Large Language Models: Origins, Inventory, and Discussion"
      },
      {
        "id": "edfb5696b5431bd20eb57964c083e9118e153e97",
        "title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering"
      },
      {
        "id": "89689059d0cdcb52d7fbb6007ab953db22936a90",
        "title": "M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models"
      },
      {
        "id": "ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3",
        "title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation"
      },
      {
        "id": "a8148b9dce6101b080e499b4c48cbe267d745131",
        "title": "DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents"
      },
      {
        "id": "c1c98ef93fb6474837961ef300cf3d8e7d3a0cd0",
        "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer"
      },
      {
        "id": "a22f3398ea865426c89ee66f4824ec626e56a864",
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models"
      },
      {
        "id": "93ebfcd6bb0724b3bb8da27edd468514187c446c",
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback"
      },
      {
        "id": "78f0d55682fbb90c32e21106d69442a8206312eb",
        "title": "Leveraging transformers‐based language models in proteome bioinformatics"
      },
      {
        "id": "9afa0c3227fd0ec3a76928784e59c4205cbace24",
        "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks"
      },
      {
        "id": "e8f7cc40208f4b6f9eca59715666f320d66d386e",
        "title": "A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models"
      },
      {
        "id": "9f87c8e27a10d71500314e7e21853f5a23efce59",
        "title": "LAraBench: Benchmarking Arabic AI with Large Language Models"
      },
      {
        "id": "63483c9387d17e44eeb70c7321ad0dbb59b994fc",
        "title": "Universal Multimodal Representation for Language Understanding"
      },
      {
        "id": "dae6d5439f5a7664377c9c10de66321372ef22de",
        "title": "Framing the News: From Human Perception to Large Language Model Inferences"
      },
      {
        "id": "131c6f328c11706de2c43cd16e0b7c5d5e610b6a",
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"
      },
      {
        "id": "e5adc219685c9941b9a3d029480af4a51c0ea05a",
        "title": "Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca"
      },
      {
        "id": "62ad7ea9467bbcdbfe325b9ee561cab3908e4583",
        "title": "MEGA: Multilingual Evaluation of Generative AI"
      },
      {
        "id": "44f0876dec21a04533587def2add230b878a5006",
        "title": "Prompt Engineering with ChatGPT: A Guide for Academic Writers"
      },
      {
        "id": "5a8f34df779227a5cb1f573349556ef479d4359a",
        "title": "Exploring the Feasibility of ChatGPT for Event Extraction"
      },
      {
        "id": "8ca5ce6f3e59cdeb31941c3b726fa885746ba5ee",
        "title": "Transformer-based deep learning for predicting protein properties in the life sciences"
      },
      {
        "id": "568160c414aa26284b04cdf3e13d25b9d7a5a290",
        "title": "Compressing Transformers: Features Are Low-Rank, but Weights Are Not!"
      },
      {
        "id": "d8b2497f1da0dbbd79679f1df9449f0b01c9c4f0",
        "title": "An Iterative Algorithm for Rescaled Hyperbolic Functions Regression"
      },
      {
        "id": "b1e67b0cc5705d6aade931e6414ce23dc0ff44b3",
        "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages"
      },
      {
        "id": "fa7bbdd62c230ba4708661a880ba8b34aad7f491",
        "title": "Protecting User Privacy in Remote Conversational Systems: A Privacy-Preserving framework based on text sanitization"
      },
      {
        "id": "8ea61d037bfe3a8e6f0211e9e63f840ead34cc72",
        "title": "Evaluation of the performance of GPT-3.5 and GPT-4 on the Medical Final Examination"
      },
      {
        "id": "1cfe781523b4b3469d822bc52a0721b1c70cee5f",
        "title": "A rule-free workflow for the automated generation of databases from scientific literature"
      },
      {
        "id": "544f43e6162bb1c3265ce88c7cb56459d64b7b6a",
        "title": "A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot"
      },
      {
        "id": "c934f7df636faa36fb0c2074ecdefa6dcb7858f1",
        "title": "Prompt-based for Low-Resource Tibetan Text Classification"
      },
      {
        "id": "dad461379a2645a21089945b6d2fce4124b5de76",
        "title": "Building Intelligent Chatbots: Tools, Technologies, and Approaches"
      },
      {
        "id": "b8e16ad905ec6d9f2e405ba29769a47834e71db8",
        "title": "Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition"
      },
      {
        "id": "0ff8c04c8bdbf93b39b49582c9195cf3fc894d03",
        "title": "Automatic Semantic Augmentation of Language Model Prompts (for Code Summarization)"
      },
      {
        "id": "362cbfd0d05e139cd6cf049754098a6e1520b910",
        "title": "PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"
      },
      {
        "id": "b6d6c33298b852cf63edac233deca70530d69a2a",
        "title": "PaLM 2 Technical Report"
      },
      {
        "id": "8973ad5fc1264594a1fda3bd9e04258074cea9cc",
        "title": "Neural Architecture Search: Insights from 1000 Papers"
      },
      {
        "id": "48abfc41a0abf023d2037ebb2f274835e0d322d0",
        "title": "Compositional Exemplars for In-context Learning"
      },
      {
        "id": "45266096cf073407b3336cf61090ec090446956d",
        "title": "A survey on deep learning approaches for text-to-SQL"
      },
      {
        "id": "40c9280d87059c0cc28f2a08d46a7045fa3e9736",
        "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL"
      },
      {
        "id": "eda08c6f5919f39979acf0b3bc52e903063b5ba4",
        "title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation"
      },
      {
        "id": "fd7e88a2313e176315d99fc299277e752d7703b7",
        "title": "Efficient Methods for Natural Language Processing: A Survey"
      },
      {
        "id": "6ab6e6f62323132e299fc6717ad0f5ca000414d5",
        "title": "Natural language processing in clinical neuroscience and psychiatry: A review"
      },
      {
        "id": "a8c09c41f39d798dc4201eeec1452fe617e428df",
        "title": "Bridging Fairness and Environmental Sustainability in Natural Language Processing"
      },
      {
        "id": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
        "title": "Language Models of Code are Few-Shot Commonsense Learners"
      },
      {
        "id": "e37310ccc5f368355c19b4de45826278aeaff280",
        "title": "Understanding HTML with Large Language Models"
      },
      {
        "id": "b4c80344081d8c548fc4d68868b8262182a146fa",
        "title": "Structured information extraction from complex scientific text with fine-tuned large language models"
      },
      {
        "id": "2a7ae3e98357569c41424dacd60c62d3df78a0db",
        "title": "Limitations of Language Models in Arithmetic and Symbolic Induction"
      },
      {
        "id": "ccfa7a251644aafc7f85ca66c6652adfcf93429c",
        "title": "From Word Embeddings to Pre-Trained Language Models: A State-of-the-Art Walkthrough"
      },
      {
        "id": "f557f3a32d309373e7d31bb93ca1b80b4a6e39e7",
        "title": "Symbolic Math Reasoning with Language Models"
      },
      {
        "id": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b",
        "title": "In-context Examples Selection for Machine Translation"
      },
      {
        "id": "ca086f4c09cf8de705830ac2b70951737fab93ca",
        "title": "A Review of Sparse Expert Models in Deep Learning"
      },
      {
        "id": "2dbec38fe353ab0e495ad09263389dbc9260824d",
        "title": "A Survey of Deep Learning for Mathematical Reasoning"
      },
      {
        "id": "808e9ce4e86e79098edea7f00b5b91663b87a5e6",
        "title": "A taxonomy and review of generalization research in NLP"
      },
      {
        "id": "e0271cb75087ccfd4a8c3351e0f5189a6de04c03",
        "title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers"
      },
      {
        "id": "94a96f64bd93ad91642fa04da09bb709a26ac277",
        "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting"
      },
      {
        "id": "44ebfdb670007b3949507be0d1a1fca93bc3d5d5",
        "title": "The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges"
      },
      {
        "id": "1d589e172084a1451398f0d6c30592baa8224732",
        "title": "A New Linear Scaling Rule for Private Adaptive Hyperparameter Optimization"
      },
      {
        "id": "f240b39aa5aa30b3e4f7a8e68d681a13a9a72054",
        "title": "EleutherAI: Going Beyond \"Open Science\" to \"Science in the Open\""
      },
      {
        "id": "5b9798dcecb4894e22335a2e5d6980b3b92c6c5f",
        "title": "UGIF: UI Grounded Instruction Following"
      },
      {
        "id": "d19bae780d5fe93e4d007e325c278598ec7f9ea4",
        "title": "Toward Efficient Language Model Pretraining and Downstream Adaptation via Self-Evolution: A Case Study on SuperGLUE"
      },
      {
        "id": "b37d57edf4a84da158ab8d77921d4aa39faceb32",
        "title": "FP8 Formats for Deep Learning"
      },
      {
        "id": "49aec6fb44ab52181960512a6067eded0ce4182b",
        "title": "Benchmarking Long-tail Generalization with Likelihood Splits"
      },
      {
        "id": "038ece8c19e11cefa1cb205fb2f88cbce836df61",
        "title": "Adversarial attack and defense technologies in natural language processing: A survey"
      },
      {
        "id": "fc14091bbd7d1eb2c9f23ff9c75a4222f2604143",
        "title": "Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning"
      },
      {
        "id": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
        "title": "Solving Quantitative Reasoning Problems with Language Models"
      },
      {
        "id": "7cbc2a7843411a1768ab762930707af0a3c33a19",
        "title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"
      },
      {
        "id": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5",
        "title": "Autoformalization with Large Language Models"
      },
      {
        "id": "218956012927bbc0dbbfd94354a56e16a6f38489",
        "title": "CINO: A Chinese Minority Pre-trained Language Model"
      },
      {
        "id": "00df5cf0d83c48657d453ab8083d8805a67f744f",
        "title": "Measuring the Carbon Intensity of AI in Cloud Instances"
      },
      {
        "id": "e47da75675b9a3fe02ef1efadca39bc8cdfcdc17",
        "title": "Designing Effective Sparse Expert Models"
      },
      {
        "id": "4857d4de584ed6f15e7ef12a96823ec8b4a17ec1",
        "title": "NAS-Bench-Suite: NAS Evaluation is (Now) Surprisingly Easy"
      },
      {
        "id": "7107d06366b48b3593c8128ed2ca67e0b413628c",
        "title": "Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"
      },
      {
        "id": "5748a0a36fbbd4e24f6503401ebcc51f9394e726",
        "title": "Auto-generated database of semiconductor band gaps using ChemDataExtractor"
      },
      {
        "id": "3b55412f2d173504417590012676e5c7eb27de6c",
        "title": "Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness"
      },
      {
        "id": "4a3553941825e7c46eb052e7c3c9fc3e6de895b1",
        "title": "Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers"
      },
      {
        "id": "45afd4a3552b11de7ef9ac1c22f50716db558c68",
        "title": "Generalists vs. Specialists: Evaluating Large Language Models for Urdu"
      },
      {
        "id": "be47fac07eef720dc9c9de2524f341420167f15c",
        "title": "Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level Textual Adversarial Attack on Tibetan Script"
      },
      {
        "id": "13b19a4ed620268c99e55818a2f2a2026de51949",
        "title": "Native vs Non-Native Language Prompting: A Comparative Analysis"
      },
      {
        "id": "cb0075a7d05ca628675e9c7582667cfd3ad67db4",
        "title": "On the Benchmarking of LLMs for Open-Domain Dialogue Evaluation"
      },
      {
        "id": "1a73efe632b1822917e3ae38de146034d0d6a7d6",
        "title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning"
      },
      {
        "id": "8ca1fdca69d780881f905ef9a4612e62f49a1014",
        "title": "Qwen2-Audio Technical Report"
      },
      {
        "id": "85bfbfab6d247a228648d1522c43f6eaf8ce0cc1",
        "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code"
      },
      {
        "id": "a0d11c7cd68c8735cb5d751b871a17bfffd78f6b",
        "title": "UNIT: Unifying Image and Text Recognition in One Vision Encoder"
      }
    ],
    "2": [
      {
        "id": "92743fca569cdad9ff0aa104eee661aba368a215",
        "title": "LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems"
      },
      {
        "id": "b368528a18d8f7b377e6fc74c1050df8c0348a1f",
        "title": "LLaVA-CoT: Let Vision Language Models Reason Step-by-Step"
      },
      {
        "id": "96f10e92ab92048d965acfbbc25a65955f33871f",
        "title": "ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning"
      },
      {
        "id": "0291e9bf06c80904c26ebb3d4d8e2d981483b715",
        "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models"
      },
      {
        "id": "c4035aed5db5d3c1cffa8205de1e070960e5d860",
        "title": "MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences"
      },
      {
        "id": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
        "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture"
      },
      {
        "id": "04cda88826c63dcd7d19597dfad6b7bd2ae41530",
        "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization"
      },
      {
        "id": "26fbf886ea26591536056b3d4a1724187356789f",
        "title": "Educational data augmentation in physics education research using ChatGPT"
      },
      {
        "id": "1f7540725555c9bed7c60db685ce27457f47a064",
        "title": "Text Alignment Is An Efficient Unified Model for Massive NLP Tasks"
      },
      {
        "id": "7d97c17a75beb89f938eaac1d3ca60ac2245fb2e",
        "title": "Faith and Fate: Limits of Transformers on Compositionality"
      },
      {
        "id": "f7a90d58bb512db47d1051506c2829ef3dcb376b",
        "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application"
      },
      {
        "id": "ff38dbb8adac3bbafe03a586b342e8f5915526d8",
        "title": "Large Language Models Lack Understanding of Character Composition of Words"
      },
      {
        "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "id": "9b45bd2221b1d91e72939019bfdf3aba0310e6e1",
        "title": "Large Language Models Are Zero-Shot Text Classifiers"
      },
      {
        "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
        "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
      },
      {
        "id": "ce6108b91e5948e1871232b9ccc0c583112230f2",
        "title": "Natural Language Processing in medicine and ophthalmology: A review for the 21st-century clinician"
      },
      {
        "id": "324e6cdf975a8a63c23ca10e2504b603cdc7e3a3",
        "title": "Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey"
      },
      {
        "id": "bb5f873632616c2cdc07ef1bb139db0c96c8e5f6",
        "title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models"
      },
      {
        "id": "81b14929edebea495a22ab06b658c284a2ec4710",
        "title": "Causal Inference with Large Language Model: A Survey"
      },
      {
        "id": "e170227c1363b3d4142405948867e49b30bab46e",
        "title": "Graphusion: Leveraging Large Language Models for Scientific Knowledge Graph Fusion and Construction in NLP Education"
      },
      {
        "id": "19b27368d576a93e0a985fe3c81b437c93098cd4",
        "title": "Large Language Models as Code Executors: An Exploratory Study"
      },
      {
        "id": "47ebc7bbeed3f951ad78d2eeed9dada17a826b88",
        "title": "LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis"
      },
      {
        "id": "3d28d20238165e18d305acfdd70e8eb9ac2a2722",
        "title": "LLM4CAD: Multi-Modal Large Language Models for 3D Computer-Aided Design Generation"
      },
      {
        "id": "333613799d39af5d9ea880b0ecbeb904d9cc2b51",
        "title": "Using Prompts to Guide Large Language Models in Imitating a Real Person's Language Style"
      },
      {
        "id": "429bd3d44674573f3de65198fe2bcdf3609a49a5",
        "title": "CLR-Fact: Evaluating the Complex Logical Reasoning Capability of Large Language Models over Factual Knowledge"
      },
      {
        "id": "3448c469724f676f3986468623469af40ced1c26",
        "title": "Evaluation of OpenAI o1: Opportunities and Challenges of AGI"
      },
      {
        "id": "2652069c70a84a8d36a3d743065f9fd2abaab021",
        "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL"
      },
      {
        "id": "19a45c70463edc755d54f3d5343e44cd571fe226",
        "title": "System-Level Defense against Indirect Prompt Injection Attacks: An Information Flow Control Perspective"
      },
      {
        "id": "50a3f0dd12114fb2ca90a5511a6325524c3f6013",
        "title": "Evaluating GPT and BERT models for protein–protein interaction identification in biomedical text"
      },
      {
        "id": "f023ce0fec7d682a8ba657f786ddbd6f6570f183",
        "title": "CodePlan: Unlocking Reasoning Potential in Large Langauge Models by Scaling Code-form Planning"
      },
      {
        "id": "7c03cbd425bf4e80fccda67658a43c6f57f8e22a",
        "title": "LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning"
      },
      {
        "id": "a34957e29ad105d4d170546c1feba8796feb3bea",
        "title": "Assertion Detection in Clinical Natural Language Processing Using Large Language Models"
      },
      {
        "id": "f7c89f1f83595257d6e2bc306d4deee4cf77f573",
        "title": "Adapting Large Language Models for Document-Level Machine Translation"
      },
      {
        "id": "83d721abd07298ef0f5d05575410ae386b77bb4a",
        "title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play"
      },
      {
        "id": "6180615110d86033887d0dc4feebe8e8a162346f",
        "title": "Large Language Models Meet NLP: A Survey"
      },
      {
        "id": "dd5f21a71cf9f8c9e24aa1fbdac86c2cec35d6f9",
        "title": "Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning"
      },
      {
        "id": "b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3",
        "title": "Enhance Reasoning for Large Language Models in the Game Werewolf"
      },
      {
        "id": "bf2f25c96ff0a280d89ba0c6c7c1c7e66eb53bdb",
        "title": "Large Language Models and Causal Inference in Collaboration: A Survey"
      },
      {
        "id": "3f877562995d1408b0b3abd5dfbbe8eeecb6061e",
        "title": "From Understanding to Utilization: A Survey on Explainability for Large Language Models"
      },
      {
        "id": "7873368409757f8577714532a95109e5547bf1a5",
        "title": "Assessing and Understanding Creativity in Large Language Models"
      },
      {
        "id": "a2f5c019c8017e10850fc32fc08e2457c81f8df5",
        "title": "Utilizing Large Language Models for Enhanced Clinical Trial Matching: A Study on Automation in Patient Screening"
      },
      {
        "id": "660ec26b660315ba42e91b6e722836abbafb98b5",
        "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges"
      },
      {
        "id": "77a9c310df0d7896d297da90fc4a1131819c341e",
        "title": "LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models"
      },
      {
        "id": "79ff4eb495094e3b47468515846d507144135ae8",
        "title": "A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction"
      },
      {
        "id": "1ef95907484526088c4370c5a60164fe56399551",
        "title": "Resilience of Large Language Models for Noisy Instructions"
      },
      {
        "id": "e5fa346cf96c8f59b0dc2dc63b515cbe568f09f7",
        "title": "Towards Complex Ontology Alignment using Large Language Models"
      },
      {
        "id": "c290633698501ea83144d61d001eb7ac7a42d853",
        "title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models"
      },
      {
        "id": "fd8d1d62d97a1cc1a9379e7aa5601d9983a2b5ff",
        "title": "Large Language Models have Intrinsic Self-Correction Ability"
      },
      {
        "id": "453ca397e8403ed759f8119c42dd2d1cde422a1a",
        "title": "SportQA: A Benchmark for Sports Understanding in Large Language Models"
      },
      {
        "id": "26e357f72ab72e8875ec8ee3f66e45c0b0094012",
        "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents"
      },
      {
        "id": "379e3ee9a6a92817bd0812848b409eafb9bf9550",
        "title": "Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs"
      },
      {
        "id": "cb26863dd5ee9e39ecfa4f383e61435756c337af",
        "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction"
      },
      {
        "id": "7321797f95230c942bba140eff977a34ff6ea1a9",
        "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey"
      },
      {
        "id": "61d134b99099b21cc88b700229140023507ce848",
        "title": "Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels"
      },
      {
        "id": "b47507f12a8b7d26f78a484e32a08d61d3f87358",
        "title": "BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"
      },
      {
        "id": "05ab4b262a2a343aa505ff3640fdf30b2530ac99",
        "title": "LLM-based NLG Evaluation: Current Status and Challenges"
      },
      {
        "id": "eff9d7ed06f30f121d30ee13802a11f172ef66f4",
        "title": "Demystifying Chains, Trees, and Graphs of Thoughts"
      },
      {
        "id": "6eb905faa9ff2e1f191780695f774250b86d6f2b",
        "title": "Robust Planning with LLM-Modulo Framework: Case Study in Travel Planning"
      },
      {
        "id": "7290d5bd9b0b55f8ac668764d58d466857945936",
        "title": "LLMs for Relational Reasoning: How Far are We?"
      },
      {
        "id": "98ce5d3c6b20acc5fe228397f0ae92b8d72004ed",
        "title": "LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text"
      },
      {
        "id": "579a0b9fae3adf0e18aaf155640db87f09783930",
        "title": "Reinforcement Retrieval Leveraging Fine-grained Feedback for Fact Checking News Claims with Black-Box LLM"
      },
      {
        "id": "0550e63749f0270b5ff7655b50a902f614cfb281",
        "title": "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies"
      },
      {
        "id": "43fd7f890b3c8dba8fc5f876871089640068b6f6",
        "title": "LlamBERT: Large-scale low-cost data annotation in NLP"
      },
      {
        "id": "53dcdf3c6e0640bef458f97332063bbf7389721b",
        "title": "Counterfactual Debating with Preset Stances for Hallucination Elimination of LLMs"
      },
      {
        "id": "66c2ec4d903bc3718893e76b4303ec28e59e5552",
        "title": "Beyond Attention: Breaking the Limits of Transformer Context Length with Recurrent Memory"
      },
      {
        "id": "962c67e5e563e45bb493664941b93822848bb977",
        "title": "Can LLMs Understand the Implication of Emphasized Sentences in Dialogue?"
      },
      {
        "id": "e2e49f2e1d3e9d07b0d6ab8a4f41791ffb242b33",
        "title": "Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning"
      },
      {
        "id": "f6b90730cce60994b03c48d9fb54275514303ed2",
        "title": "ChatBI: Towards Natural Language to Complex Business Intelligence SQL"
      },
      {
        "id": "547c2cc8d45c22eaba7c7eb34d4e11a7d95a9cff",
        "title": "Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark"
      },
      {
        "id": "8679575a82d9a2ba5970a96fcc5ac461aecbc90e",
        "title": "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning"
      },
      {
        "id": "868c06e552be0c9a53d41d18eaa233402a2bb7ee",
        "title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition"
      },
      {
        "id": "090e1f1efb36735171fa1d487d80e93d42b54c6f",
        "title": "AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks"
      },
      {
        "id": "212318b81f99a7dc83929b4fea679b096cdf513d",
        "title": "Memory Sharing for Large Language Model based Agents"
      },
      {
        "id": "7c1adf2bcff5d7ed1cf330680a100060f278e2a3",
        "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams"
      },
      {
        "id": "19499cc52d7945d35811e9959fcec59f3b07f6f7",
        "title": "Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"
      },
      {
        "id": "08f11c8f4ba6f10b4df26ed4376df4f2f3106da9",
        "title": "Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs"
      },
      {
        "id": "dc10d108823630343484c5eecbb5c3011751282e",
        "title": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for Interactive Analog Layout Design"
      },
      {
        "id": "8d3796d80ee1b8b208d511d59df5af397813acbf",
        "title": "Is It Safe to Cross? Interpretable Risk Assessment with GPT-4V for Safety-Aware Street Crossing"
      },
      {
        "id": "e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43",
        "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions"
      },
      {
        "id": "ec592e12f45e20819afe203164bbbd0de8990510",
        "title": "AmadeusGPT: a natural language interface for interactive animal behavioral analysis"
      },
      {
        "id": "26089bdfdbca1e6eaaceca71e3116b715bec6d47",
        "title": "Explainability for Large Language Models: A Survey"
      },
      {
        "id": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
        "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers"
      },
      {
        "id": "eb3e6dc2476c5a377b7c3d1489c014712a17a376",
        "title": "ChatEDA: A Large Language Model Powered Autonomous Agent for EDA"
      },
      {
        "id": "dcf87f11e245b76437c2f551c1ff6a7842585811",
        "title": "ISR-LLM: Iterative Self-Refined Large Language Model for Long-Horizon Sequential Task Planning"
      },
      {
        "id": "8c9b8ba4a44b9736ea9db94f11c3227d5bb91a09",
        "title": "Do Large Language Models Know about Facts?"
      },
      {
        "id": "7c93bc4173c9b4d2a71866e8189c8e35cdb00b34",
        "title": "Large Language Models for Code Analysis: Do LLMs Really Do Their Job?"
      },
      {
        "id": "c94471a213c1b160e9c3f0f85f92a0e8ef9af8ea",
        "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation"
      },
      {
        "id": "72273f7a050529fc71c7d45c0256d2b9754f56bb",
        "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration"
      },
      {
        "id": "dd0612ce863f64b0f69d0d9f708c52e829f6f859",
        "title": "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"
      },
      {
        "id": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
        "title": "Metacognitive Prompting Improves Understanding in Large Language Models"
      },
      {
        "id": "a86d3692af34ce70fd8caaa319009f85a88fdb4d",
        "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models"
      },
      {
        "id": "4014253368133c01bfc0383660c518d11afccad2",
        "title": "Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration"
      },
      {
        "id": "08400a12db21fe4d4f844fec57844e74ac09c9ba",
        "title": "Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction"
      },
      {
        "id": "5fb8dd5e31bf423d456ae580e494b406b4403bbf",
        "title": "A Large Language Model Approach to Educational Survey Feedback Analysis"
      },
      {
        "id": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
        "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems"
      },
      {
        "id": "f221eccdd96122a42c5e65532373e6974b30c20c",
        "title": "Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills"
      },
      {
        "id": "5001630bcc65e8e0e621b19625629a2689724743",
        "title": "Generative Judge for Evaluating Alignment"
      },
      {
        "id": "3c55aa582a2d682eb53e9237589234854e0b92d8",
        "title": "Empirical Study of Zero-Shot NER with ChatGPT"
      },
      {
        "id": "58219d9826f9ddde448c73e7ecc690111f5698f4",
        "title": "ReAcTable: Enhancing ReAct for Table Question Answering"
      },
      {
        "id": "bfeda6c7aa7899a80adb01894555b09d24756a59",
        "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"
      },
      {
        "id": "c3c20340fb3b4c4db30291e3b29a1a7b557d102d",
        "title": "Application of ChatGPT in Routine Diagnostic Pathology: Promises, Pitfalls, and Potential Future Directions"
      },
      {
        "id": "2967ab775f6cdabc6ab59010734f352dd3ebc8d6",
        "title": "Instruct and Extract: Instruction Tuning for On-Demand Information Extraction"
      },
      {
        "id": "4f249487c670263700df7b2269cdb92a265bc21f",
        "title": "Effective Distillation of Table-based Reasoning Ability from LLMs"
      },
      {
        "id": "b4646815d5107489e7660d71e83c6584a926d280",
        "title": "Graph Meets LLMs: Towards Large Graph Models"
      },
      {
        "id": "ec2c330301fa8a9f4b6357d9ca630bf5bcd50996",
        "title": "Can GPT models be Financial Analysts? An Evaluation of ChatGPT and GPT-4 on mock CFA Exams"
      },
      {
        "id": "acd7adcdb914fb2e18a19792af47e59e78cab3f0",
        "title": "Overview of the PromptCBLUE Shared Task in CHIP2023"
      },
      {
        "id": "6ad7b84eba57e9d506444d604e6719d1b419dca0",
        "title": "AMERICANO: Argument Generation with Discourse-driven Decomposition and Agent Interaction"
      },
      {
        "id": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
        "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"
      },
      {
        "id": "94d878ba7eeba4abc4d5e42b4c2c4c98d4e575ce",
        "title": "Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning"
      },
      {
        "id": "632a758099f412b13d0a8172830a3ac7ee5e0dd5",
        "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT"
      },
      {
        "id": "9c392d4532e588f04f94de2ede26d7d6bafe6271",
        "title": "OLaLa: Ontology Matching with Large Language Models"
      },
      {
        "id": "a2ccffe67a4ccfb10279dc3f0167fe65ae01e471",
        "title": "Language Models can be Logical Solvers"
      },
      {
        "id": "4f2be887e991efa85f7b874e7ab871080a745c39",
        "title": "CAESURA: Language Models as Multi-Modal Query Planners"
      },
      {
        "id": "8e079a5e525a4cecbccaeff5e04291a8d06d588d",
        "title": "Transformers and Large Language Models for Chemistry and Drug Discovery"
      },
      {
        "id": "ebbe4354c522029762ec78fb83f32d7bf8836e75",
        "title": "Exploring the Reliability of Large Language Models as Customized Evaluators for Diverse NLP Tasks"
      },
      {
        "id": "5e2d50f7745d7d1cd92c1f8fb79ec03735605b08",
        "title": "An appraisal-based chain-of-emotion architecture for affective language model game agents"
      },
      {
        "id": "3f5a434393e560ac0c2c1823f5f620a69f29b5e6",
        "title": "Integration of Large Language Models within Cognitive Architectures for Autonomous Robots"
      },
      {
        "id": "e58b0ee9a1fdb15a72ee721053df3569127cde42",
        "title": "Tackling Vision Language Tasks Through Learning Inner Monologues"
      },
      {
        "id": "4776de7b856d3b15eecc4f88666cdc13972df22e",
        "title": "Eliminating Reasoning via Inferring with Planning: A New Framework to Guide LLMs' Non-linear Thinking"
      },
      {
        "id": "057122d989a681592c409160dffb74bcb2e42d11",
        "title": "ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection"
      },
      {
        "id": "f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0",
        "title": "A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"
      },
      {
        "id": "ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate"
      },
      {
        "id": "89dd10444b733478452f5700184ac9e398b6764d",
        "title": "Gpt-4: A Review on Advancements and Opportunities in Natural Language Processing"
      },
      {
        "id": "5eab810cc5d90de1c52127d1a5824f0817f46c30",
        "title": "Natural Language Reasoning, A Survey"
      },
      {
        "id": "9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
        "title": "Language Models can Solve Computer Tasks"
      },
      {
        "id": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
        "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning"
      },
      {
        "id": "03055978e278960de9fbb5c648b1779ef9f26cd1",
        "title": "Can Large Language Models Be an Alternative to Human Evaluations?"
      },
      {
        "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
        "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context"
      },
      {
        "id": "170c97c7215f42edfb20c2248f954879e91ef86e",
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"
      },
      {
        "id": "c589ddc6c6fb07189af7c1212f6eb15c5ff72cde",
        "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check"
      },
      {
        "id": "70da4fb798a86cbe8cad96c27ced0415885bbd9d",
        "title": "AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators"
      },
      {
        "id": "278eae3362fd00b1092051240ab31dfa0bf8b581",
        "title": "An Empirical Study on Information Extraction using Large Language Models"
      },
      {
        "id": "e47e63781c0e7a2c0504b9381b76b5d01b62c53d",
        "title": "InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models"
      },
      {
        "id": "e5754bb65a648f319a02d47c356df0db1e936b7f",
        "title": "Post Hoc Explanations of Language Models Can Improve Language Models"
      },
      {
        "id": "327e0290fd71609bfc1a30478a95f690668fe622",
        "title": "Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies"
      },
      {
        "id": "3f758a13d3703b02bdf977f9189230276064da42",
        "title": "T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering"
      },
      {
        "id": "d01f6e76e67a445f23f807c0d3e68fa0be9a2c9e",
        "title": "The Next Chapter: A Study of Large Language Models in Storytelling"
      },
      {
        "id": "b8dd3a023b6f3e3bb862d172d84c3f29d3f840d1",
        "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation and Beyond"
      },
      {
        "id": "c226a4acb42912054d498bcf771023b0ba2da001",
        "title": "Language Model Self-improvement by Reinforcement Learning Contemplation"
      },
      {
        "id": "e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7",
        "title": "SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"
      },
      {
        "id": "258605dc5b00fe66b72091f947642a554e472aee",
        "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task"
      },
      {
        "id": "f227e955e40084f2cee68a5148e404c94d4f1490",
        "title": "Benchmarking Large Language Model Capabilities for Conditional Generation"
      },
      {
        "id": "490d8006851b1562cfd9ec1f057471f2868289d1",
        "title": "Rethinking with Retrieval: Faithful Large Language Model Inference"
      },
      {
        "id": "29203f0b8b9be7fd70d99bf7390c6a78b68a9289",
        "title": "Conceptual Design Generation Using Large Language Models"
      },
      {
        "id": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
        "title": "Explicit Planning Helps Language Models in Logical Reasoning"
      },
      {
        "id": "7df3595bdb4003589e8ca1757cc39ec03a39a2ff",
        "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models"
      },
      {
        "id": "381ab7a640f5b46b62f7e08d1af4a8e0d3eadd55",
        "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment"
      },
      {
        "id": "1a01c982aa20c1a1ad1ad94866e3197da99a52a2",
        "title": "Extractive Summarization via ChatGPT for Faithful Summary Generation"
      },
      {
        "id": "8236010c2ecc94d826be6010ff187fdc000e7df6",
        "title": "Deductive Verification of Chain-of-Thought Reasoning"
      },
      {
        "id": "1b9fc8268b392742ea43c2c017a767cf62386139",
        "title": "Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation"
      },
      {
        "id": "6001a5d38df9a043421a670357842d8df71d656b",
        "title": "Grokking of Hierarchical Structure in Vanilla Transformers"
      },
      {
        "id": "197022486b2e2584302bd9b6442e44d15bf3e351",
        "title": "ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"
      },
      {
        "id": "6289de84a02f0c27734f295ada565603ac958948",
        "title": "Tab-CoT: Zero-shot Tabular Chain of Thought"
      },
      {
        "id": "758985395372f5378fcf036094195b2848e13a21",
        "title": "PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning"
      },
      {
        "id": "d75d11d2c89c01cd284383546ae057cb827dc272",
        "title": "Leveraging Training Data in Few-Shot Prompting for Numerical Reasoning"
      },
      {
        "id": "e817b79f3d1ce0add9cb9b653d91af378f1dea18",
        "title": "CatSQL: Towards Real World Natural Language to SQL Applications"
      },
      {
        "id": "c4e36d205bd7c0a82a368ed56c7d1f737ff16a6a",
        "title": "Beyond Rule-based Named Entity Recognition and Relation Extraction for Process Model Generation from Natural Language Text"
      },
      {
        "id": "430aa6966c15c4a20a4fb2d8383e136b9cb6cde7",
        "title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine"
      },
      {
        "id": "1c1d1b4e1edae9dca84ea655624a4c8dc7fef2a7",
        "title": "Conformal Language Modeling"
      },
      {
        "id": "768850402f991a97ce7a7d063d07050add0254ca",
        "title": "Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners"
      },
      {
        "id": "15dd43ded15e6dbf750278430bd822ee2d1b977f",
        "title": "Large Language Models for Healthcare Data Augmentation: An Example on Patient-Trial Matching."
      },
      {
        "id": "f178afd3afe6970ff9ed172e8ef5b1946d0c3ba8",
        "title": "MathChat: Converse to Tackle Challenging Math Problems with LLM Agents"
      },
      {
        "id": "6115bb844813b68ff64188c241cbd1a11a53c88b",
        "title": "Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains"
      },
      {
        "id": "19ea368b7f88279899c40813a797dda7adc50c07",
        "title": "Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation"
      },
      {
        "id": "c77d908ba29567445a9a4ad1bd4461d441cce174",
        "title": "AutoML-GPT: Automatic Machine Learning with GPT"
      },
      {
        "id": "389ec3e8902a5dcfcde1adec735854e93f845937",
        "title": "LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"
      },
      {
        "id": "966852963a88a28786b798c91b6662d6e501e590",
        "title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"
      },
      {
        "id": "5581bf85386737bd3378eec68189759a05280bea",
        "title": "FOLIO: Natural Language Reasoning with First-Order Logic"
      },
      {
        "id": "529e997e0d9730c25ad4347502da7e5a753274b8",
        "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference"
      },
      {
        "id": "18b3ab9763ed3c4633ee68aa6dd75f6377837553",
        "title": "Natural Language Deduction with Incomplete Information"
      },
      {
        "id": "db4ab91d5675c37795e719e997a2827d3d83cd45",
        "title": "Towards Reasoning in Large Language Models: A Survey"
      },
      {
        "id": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
        "title": "Large Language Models are Better Reasoners with Self-Verification"
      },
      {
        "id": "ee8de585183763ff64cb3c81ecda2fc75fa81507",
        "title": "Large Language Models with Controllable Working Memory"
      },
      {
        "id": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
        "title": "Rationale-Augmented Ensembles in Language Models"
      },
      {
        "id": "f97e89a2cea285d53c6ba3dae7413c98c37425fd",
        "title": "Few-Shot Multi-Modal Sentiment Analysis with Prompt-Based Vision-Aware Language Modeling"
      },
      {
        "id": "af1c871282ec122869d03f5420ef5d9143358a91",
        "title": "Visual Programming: Compositional visual reasoning without training"
      },
      {
        "id": "4988b3d378b79eb8669112620baf1ff4e3e536fd",
        "title": "Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango"
      },
      {
        "id": "ae441f7305dc2cd58c708528b3ecee3501cc5c46",
        "title": "Plansformer: Generating Symbolic Plans using Transformers"
      },
      {
        "id": "ca2ea26b851fea6914a65b233b7daf8f32e38073",
        "title": "CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation"
      },
      {
        "id": "bcf272c12c55362a92741b1a0b2ac76f066466f9",
        "title": "ClassActionPrediction: A Challenging Benchmark for Legal Judgment Prediction of Class Action Cases in the US"
      },
      {
        "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
        "title": "Ask Me Anything: A simple strategy for prompting language models"
      },
      {
        "id": "d480d6f0c81d4b9b9e333af2aeab75ff30fba2d1",
        "title": "NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR"
      },
      {
        "id": "6d7b8a478801bd9d21df82d5f33ae6eced90da5e",
        "title": "Solving math word problems with process- and outcome-based feedback"
      },
      {
        "id": "77f06e3d1dfac471322392bd60b9d048152280a4",
        "title": "Experimental Standards for Deep Learning in Natural Language Processing Research"
      },
      {
        "id": "47e15941c8b157873c8264e4bf50318d1ba5cd18",
        "title": "Natural Language to Code Translation with Execution"
      },
      {
        "id": "196cc546041cb6db167784f632037f0a1dcf4a79",
        "title": "Generating Natural Language Proofs with Verifier-Guided Search"
      },
      {
        "id": "e5aa2a1e36a2c68fa4aa59afdb8b6e1c419f547c",
        "title": "Natural Language Deduction through Search over Statement Compositions"
      },
      {
        "id": "6906eb75b9ad9ca98a6b24d510f246da54d6417c",
        "title": "PET: An Annotated Dataset for Process Extraction from Natural Language Text Tasks"
      },
      {
        "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "id": "1bcde55995a957b3e8a595d536b816cb8989cf1d",
        "title": "MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning"
      },
      {
        "id": "f9b94bcaf706892904c058c37d0087628f371df4",
        "title": "e-CARE: a New Dataset for Exploring Explainable Causal Reasoning"
      },
      {
        "id": "a80f2102e5de3ead1b9689b440503f49383ddc94",
        "title": "Is a Question Decomposition Unit All We Need?"
      },
      {
        "id": "932b6353204e56f20917edadda2fa636ace21090",
        "title": "Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks"
      },
      {
        "id": "9b2339cfe4640841078073dfa1c1c60e84eeeaa2",
        "title": "FaiRR: Faithful and Robust Deductive Reasoning over Natural Language"
      },
      {
        "id": "d48b29889241551e1ee6622fa78c3fa4159255dd",
        "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"
      },
      {
        "id": "e4e9d556e9725a5fdb2e133b61243ff7c1ca8aeb",
        "title": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation Practices for Generated Text"
      },
      {
        "id": "543d253a2cf47423ae1f70cba161b9c223bbf765",
        "title": "Automatic Metrics in Natural Language Generation: A survey of Current Evaluation Practices"
      },
      {
        "id": "a682d6fb6167bdf322d6c50c98faecf26cc386ad",
        "title": "A Universal Prompting Strategy for Extracting Process Model Information from Natural Language Text using Large Language Models"
      },
      {
        "id": "d8e4a6cb80b2ab133f0dd55a80e5223847f0ec97",
        "title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated Shapefile Processing"
      },
      {
        "id": "2e039e720ac937c88efd8bb83c7c2ad0d6350d2a",
        "title": "Larger Language Models Don't Care How You Think: Why Chain-of-Thought Prompting Fails in Subjective Tasks"
      },
      {
        "id": "0455396a569ed3a9c6db69858c70cf457e58fb70",
        "title": "Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models"
      },
      {
        "id": "13c1726e4be0b18a79f1629b782fe81946b968b2",
        "title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models"
      },
      {
        "id": "d614fbd773678d2844a829b0c91716f3ddb3e28a",
        "title": "ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models"
      },
      {
        "id": "cd8758b9ee3db62c3d14d091f0ca72666a6da263",
        "title": "Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge"
      },
      {
        "id": "00764884538ef7e1ef736706dfe9d5c417527899",
        "title": "Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations"
      },
      {
        "id": "cee94cb17cd6e1a2782983f6befc287a9f4c4e3c",
        "title": "Strategies for Improving NL-to-FOL Translation with LLMs: Data Generation, Incremental Fine-Tuning, and Verification"
      },
      {
        "id": "7aee8805ef91275dc720a0322c47e217d6a7eaec",
        "title": "SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL"
      },
      {
        "id": "1d1beece295703c0cb3e545edaa12a4336b407bc",
        "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models"
      },
      {
        "id": "efde8940a0b924e93d35184c4a1e8f9670b94fe7",
        "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML"
      },
      {
        "id": "d8bae6bde1ca8d039acec0ca9fd42e228e870341",
        "title": "OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning"
      }
    ],
    "0": [
      {
        "id": "98cb07d3e50f0718ff17eca4e898f451e0f3381d",
        "title": "MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models"
      },
      {
        "id": "690ea3a58f85a633f25be023bfe5577ccc46a1c4",
        "title": "MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL"
      },
      {
        "id": "44372f8d37af7e1b35749c3958d206de374184d4",
        "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
      },
      {
        "id": "9e4e3132dd293b12b18fb1d7a82b28509d1cd0e6",
        "title": "RULE: Reliable Multimodal RAG for Factuality in Medical Vision Language Models"
      },
      {
        "id": "7c834a7ba43a2b5067817426939c362eb06fdb2a",
        "title": "Imprompter: Tricking LLM Agents into Improper Tool Use"
      },
      {
        "id": "a5f26555194d50955f6b3fdafb04d4330cb272dc",
        "title": "A Survey on Human Preference Learning for Large Language Models"
      },
      {
        "id": "14b588f38a3af6ef1b0186e2bb98c77d3b650093",
        "title": "ATM: Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator"
      },
      {
        "id": "d0840037657abc03765cee36ad837a80ef0769de",
        "title": "A Survey of Multimodal Large Language Model from A Data-centric Perspective"
      },
      {
        "id": "4679e5e54ca62f2a299cd4e16a0ed08feacda701",
        "title": "Text Generation: A Systematic Literature Review of Tasks, Evaluation, and Challenges"
      },
      {
        "id": "a1f76db91c0debcf93ae9889736bce8470902113",
        "title": "Large Language Models: A Survey"
      },
      {
        "id": "a987c21afbfb3bd746d114e248202c074b1c40ca",
        "title": "Large Language Models for Mathematicians"
      },
      {
        "id": "936f7f0fa77efcd322805b93a8d74c48a4108290",
        "title": "ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?"
      },
      {
        "id": "813987d484a9eea03e95e677707fd011947a4154",
        "title": "First Tragedy, then Parse: History Repeats Itself in the New Era of Large Language Models"
      },
      {
        "id": "3613299c54bbea66dd6db1b00573f7ade021a5a9",
        "title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback"
      },
      {
        "id": "b931b242f40a032b9ae7dae9d9fc10c6ab90695e",
        "title": "Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models"
      },
      {
        "id": "0b1131a4ef51cf93da8ba9565e68f68c9ff5e792",
        "title": "The now and future of ChatGPT and GPT in psychiatry"
      },
      {
        "id": "68fd6cc9b41291d625b41761149016be6485c0b3",
        "title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey"
      },
      {
        "id": "99de79d26873100e7cb2791adf87aed130d5012a",
        "title": "Evaluating ChatGPT's Decimal Skills and Feedback Generation in a Digital Learning Game"
      },
      {
        "id": "3dcf2db20082b480c6c091eea025465cc4fe57a6",
        "title": "AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap"
      },
      {
        "id": "b3f272d644fe580d18f635be4d6ac4c520ef0d0f",
        "title": "Preference-grounded Token-level Guidance for Language Model Fine-tuning"
      },
      {
        "id": "f8f6942be75d102a14c6441e0bb31ac7c59235a4",
        "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models"
      },
      {
        "id": "ac47bd3b512301371fc87c68416befce6589912e",
        "title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning"
      },
      {
        "id": "8cf819f6ee33909484ece40d79944c9c37f01a89",
        "title": "A Brief Overview of ChatGPT: The History, Status Quo and Potential Future Development"
      },
      {
        "id": "16d83e930a4dab2d49f5d276838ddce79df3f787",
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"
      },
      {
        "id": "1b492746ee3a304a13950cad1a59861b9ee44645",
        "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?"
      },
      {
        "id": "cff26bda86237d113ed01c812ad8bedd0afbe070",
        "title": "DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"
      },
      {
        "id": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
        "title": "Pretraining Language Models with Human Preferences"
      },
      {
        "id": "e79981c91c1ac40d747377b4af7409793d8e7350",
        "title": "What is the Role of Small Models in the LLM Era: A Survey"
      },
      {
        "id": "3a66e3a6fe1f9e1d95140f0c8fefc4ff964ba89d",
        "title": "Neural Methods for Data-to-text Generation"
      },
      {
        "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "id": "0d1c76d45afa012ded7ab741194baf142117c495",
        "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
      },
      {
        "id": "51f0c3b06861c79fbb283022df109818ce4ebe51",
        "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models"
      },
      {
        "id": "13241422a6fef4e41dbb4d0f3e40e39363e07826",
        "title": "SURf: Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information"
      },
      {
        "id": "c255a9f81d7108af416237d4d6fe6ed48fa50058",
        "title": "BiasDPO: Mitigating Bias in Language Models through Direct Preference Optimization"
      },
      {
        "id": "0bd9873a6f11dc57f9ab4c7d18684b82ba45ba15",
        "title": "NUS-Emo at SemEval-2024 Task 3: Instruction-Tuning LLM for Multimodal Emotion-Cause Analysis in Conversations"
      },
      {
        "id": "6887a2513182a1b7376a8aece2e38cae23c26970",
        "title": "CyberPal.AI: Empowering LLMs with Expert-Driven Cybersecurity Instructions"
      },
      {
        "id": "5b19fb4c441856490cbfe8c3eab05187e4063d33",
        "title": "A Review of the Applications of Deep Learning-Based Emergent Communication"
      },
      {
        "id": "68125cedd3c5a77c8c9268503e83932cc9ac9c5d",
        "title": "A Logical Fallacy-Informed Framework for Argument Generation"
      },
      {
        "id": "96ea9806a0db5d3aef2fbaa00a29f14b71831306",
        "title": "A Survey on Evaluation of Multimodal Large Language Models"
      },
      {
        "id": "1c56edb4bff9801cd551a36529de9331f867d784",
        "title": "A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models"
      },
      {
        "id": "b2bc5c98cf4458855d3e6b53b76d0f073c6a5b28",
        "title": "Natural language processing in the era of large language models"
      },
      {
        "id": "4e92e12e20450aa4e51a04df8b0d3a0229bcd32f",
        "title": "Advanced Natural-based interaction for the ITAlian language: LLaMAntino-3-ANITA"
      },
      {
        "id": "74a2ef37466667c843b6322691c49b0475030cb0",
        "title": "Security and Privacy Challenges of Large Language Models: A Survey"
      },
      {
        "id": "478f71f1cd9bad0435560544a9dce7ca49d97766",
        "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap"
      },
      {
        "id": "5aa4e4d90cac81f8ec7001ae25356d75f02efbb1",
        "title": "Datasets for Large Language Models: A Comprehensive Survey"
      },
      {
        "id": "de4dfb773ab455081e5fb1862e08f581c58d43bc",
        "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems"
      },
      {
        "id": "c4d43fe1b7e44c5e9929d6edf7bd11de4e6d293a",
        "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning"
      },
      {
        "id": "9d4bd6f057fde94e2956a14711655e9044257044",
        "title": "From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility"
      },
      {
        "id": "d9db99fa712edf900873b62143a08f5a14c22986",
        "title": "Advancing entity recognition in biomedicine via instruction tuning of large language models"
      },
      {
        "id": "29c8a097fa1690936fb6ef232996796d3a299e91",
        "title": "Efficient Prompting Methods for Large Language Models: A Survey"
      },
      {
        "id": "31968a970f14beab3cbadf9f6ad45c1a51f4ea95",
        "title": "Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem"
      },
      {
        "id": "95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607",
        "title": "FLAME: Factuality-Aware Alignment for Large Language Models"
      },
      {
        "id": "165b712a8b518bb4f40472ea7f2f482354dedf21",
        "title": "Is a Large Language Model a Good Annotator for Event Extraction?"
      },
      {
        "id": "9a741f33aa4d782639e1f81a7e9c341b58b6ed2a",
        "title": "Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices"
      },
      {
        "id": "027ac1dad95fa9f42c5e9d00a4de31214f437604",
        "title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models"
      },
      {
        "id": "a7447274b7b1d524e0415fe2fe1cae3f307b3f2c",
        "title": "TasTe: Teaching Large Language Models to Translate through Self-Reflection"
      },
      {
        "id": "a8c86a10951e21814606bddb68c18d1980f3f481",
        "title": "Unique Security and Privacy Threats of Large Language Model: A Comprehensive Survey"
      },
      {
        "id": "a90560b3c9f52de21caccd82f151b092b45d9603",
        "title": "Large Language Models for Education: A Survey"
      },
      {
        "id": "f4bb0154e537ce9631ed401060029580e8775aaa",
        "title": "Improving Attributed Text Generation of Large Language Models via Preference Learning"
      },
      {
        "id": "cc93265f436314a8285d2c7a858ccbe47bb2a3d0",
        "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection"
      },
      {
        "id": "109f09386d4785b39ccff0e94a4a04a5bdb6516c",
        "title": "Exploring the True Potential: Evaluating the Black-box Optimization Capability of Large Language Models"
      },
      {
        "id": "5ed6f9208da2d836bbd31a2b5853983e260ef17d",
        "title": "Personalized Wireless Federated Learning for Large Language Models"
      },
      {
        "id": "dcfd0ad2c25e6b0d5943993421a6df4b28fb9e04",
        "title": "A Survey on Automatic Generation of Figurative Language: From Rule-based Systems to Large Language Models"
      },
      {
        "id": "5a8a71a24c3604e847650b3fb0379d49945fd7b6",
        "title": "Stance Detection on Social Media with Fine-Tuned Large Language Models"
      },
      {
        "id": "c362015d426c90ec01e1ad02bf3fd66ab8fd0fd9",
        "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models"
      },
      {
        "id": "2b0ce68cae4203c9f2785b4ce83d0b3aff0017a5",
        "title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models"
      },
      {
        "id": "08436b3ddafd2edc798753ebc87f6ceffed6e8df",
        "title": "KnowTuning: Knowledge-aware Fine-tuning for Large Language Models"
      },
      {
        "id": "a8489211b2fc9065b8e1ce62b6ec3815a32493e0",
        "title": "Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models"
      },
      {
        "id": "835b39cbcff8ef5f6f2203c18ce50f0529a928aa",
        "title": "Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias"
      },
      {
        "id": "3c6a6c8de005ef5722a54847747f65922e79d622",
        "title": "Evaluation of Retrieval-Augmented Generation: A Survey"
      },
      {
        "id": "7c8851cce662351c49da94fa4512e2a6d2c1ace0",
        "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement"
      },
      {
        "id": "0eb8d2e9cdee108b500726d18eaa59fc48ec959b",
        "title": "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models"
      },
      {
        "id": "c11d885b219e817bdb3d4e95c0307e7f987d3bba",
        "title": "Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions"
      },
      {
        "id": "d4656bba3a424a25fcd9e1fbf3966f080ace9c2f",
        "title": "LLM Harmony: Multi-Agent Communication for Problem Solving"
      },
      {
        "id": "e2751c2030d1497b5f99ee2e08e733043a15bffe",
        "title": "Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm"
      },
      {
        "id": "e3778b277248123d769eb43fd3d1987f758b3b77",
        "title": "ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization"
      },
      {
        "id": "2bbd9ad6c989c81c3656af71e64daf144e9b3f87",
        "title": "Exploring the potential of ChatGPT in medical dialogue summarization: a study on consistency with human preferences"
      },
      {
        "id": "eb6f866999cf558b66762ce05083d054cfacdcbf",
        "title": "Protecting Your LLMs with Information Bottleneck"
      },
      {
        "id": "80a57f89860cfd3c6183a18d87431e9ba23875c1",
        "title": "An Aspect-Based Review Analysis Using ChatGPT for the Exploration of Hotel Service Failures"
      },
      {
        "id": "d6699a09a1cc557009fbe6e33224765e6c77ec35",
        "title": "On Zero-Shot Counterspeech Generation by LLMs"
      },
      {
        "id": "086b334545bd1648daa56d602dcae3d872f8a165",
        "title": "(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection"
      },
      {
        "id": "b4ed361980a8557081e3a2139a016a86a5e04a68",
        "title": "Surveying the Dead Minds: Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese"
      },
      {
        "id": "4deeb8ea9fd60858bde847775170168e1ede883e",
        "title": "keqing: knowledge-based question answering is a nature chain-of-thought mentor of LLM"
      },
      {
        "id": "c5e40c7de3ad5cce2bd01fce0f71de313a8ee837",
        "title": "PMC-LLaMA: toward building open-source language models for medicine"
      },
      {
        "id": "30ef6a82ac5aa80f2eea02aebeee2b98ca8ba290",
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment"
      },
      {
        "id": "bd22e78d956d2e06d9f192ddbbe414adeb791717",
        "title": "User Embedding Model for Personalized Language Prompting"
      },
      {
        "id": "5e294d9f89181fb73e552dfd12254543738851b0",
        "title": "Teaching Language Models to Self-Improve by Learning from Language Feedback"
      },
      {
        "id": "99b38b72026775a1e91d83fb71e984b5e8b7b374",
        "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models"
      },
      {
        "id": "4e5d609f0819f7c9c5f98dbbb0144433ff5a5b88",
        "title": "Generating Role-Playing Game Quests With GPT Language Models"
      },
      {
        "id": "92a236a1ca049e186ca53c9ce12219ebca315ba1",
        "title": "Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment"
      },
      {
        "id": "eff0410f7d5d78ea6874596a0a77b184d03ecca5",
        "title": "On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization"
      },
      {
        "id": "d269ad2a38bcbfc533303ce0f9be2537ba7b71c2",
        "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning"
      },
      {
        "id": "62104714e31a9f332287444091c0a8a08bbc6dc8",
        "title": "When to Trust LLMs: Aligning Confidence with Response Quality"
      },
      {
        "id": "c8a6aa5d4ba18f3e38f8de27aa17d6fcfb33b89f",
        "title": "Exploring ChatGPT and its Impact on Society"
      },
      {
        "id": "4dd571c6514d169d0b0a7933f3ba3440301d2e46",
        "title": "Improving Reward Models with Synthetic Critiques"
      },
      {
        "id": "9366d2f3884a0a9197d2d2838c81319be3bd0197",
        "title": "Improving Weak-to-Strong Generalization with Reliability-Aware Alignment"
      },
      {
        "id": "431c85f7cb5436981c798697acc13aa72f3e133b",
        "title": "AI-native Memory: A Pathway from LLMs Towards AGI"
      },
      {
        "id": "fe42e08ebe6cfc098faf90de4ab6fb8d731bde4f",
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF"
      },
      {
        "id": "f23755e22a15071bb8c98e90194abd90bb0aded9",
        "title": "Can We Talk Models Into Seeing the World Differently?"
      },
      {
        "id": "712aa254275c8d888f73f3fe65f83ef0bce76271",
        "title": "We are Who We Cite: Bridges of Influence Between Natural Language Processing and Other Academic Fields"
      },
      {
        "id": "919215590b3301f1154deebd1b64722dd3746b8f",
        "title": "Can ChatGPT’s Responses Boost Traditional Natural Language Processing?"
      },
      {
        "id": "efe90ad08b1d29266874be1b74027df80fea29bc",
        "title": "GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts"
      },
      {
        "id": "dd18782960f9ee4c66b79e1518b342ad3f8d19e7",
        "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
      },
      {
        "id": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8",
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions"
      },
      {
        "id": "b8b8d5655df1c6a71bbb713387863e34cc055332",
        "title": "Detecting Language Model Attacks with Perplexity"
      },
      {
        "id": "4f63c5a89c7299a864c6c48aa1844fb0fe8c9437",
        "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"
      },
      {
        "id": "4a806670b0aeb2b55c1efce5ae294e34ac9c676b",
        "title": "On the application of Large Language Models for language teaching and assessment technology"
      },
      {
        "id": "660db1e60311d7d2a24f39d0c96fb12b69ccdcef",
        "title": "Retrieval-augmented Recommender System: Enhancing Recommender Systems with Large Language Models"
      },
      {
        "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites"
      },
      {
        "id": "bf11f01929afed0ad3ccfe1b5e0fd34d90ef2b4f",
        "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"
      },
      {
        "id": "75059feaaec7dd0a8810d8f4ec6985f5d00b73d5",
        "title": "A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks"
      },
      {
        "id": "4118c8bca76b4bfafc379d80bf91455df88614b6",
        "title": "Aligning Language Models with Human Preferences via a Bayesian Approach"
      },
      {
        "id": "8d34de18ce1c2345e3fa1bff786a2410c2783e6a",
        "title": "FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking"
      },
      {
        "id": "f131b342e3aede46d24afc9b9055a94cceb0936a",
        "title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction"
      },
      {
        "id": "8d9d724e387079743e719b7f1af257c120eae51e",
        "title": "Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention"
      },
      {
        "id": "1366b07120580eaf1badde105b9361806e8f9629",
        "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models"
      },
      {
        "id": "d0ffb09a00b67365efb9e217c3fd45d804733810",
        "title": "Large Language Models are biased to overestimate profoundness"
      },
      {
        "id": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
        "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision"
      },
      {
        "id": "a9c75cf664f675a1b4034b0256ec3c5168e293df",
        "title": "EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers"
      },
      {
        "id": "f80e7b5baa6adf0cd5dddb5ba973fed9d8a216cf",
        "title": "Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education"
      },
      {
        "id": "d1b11a8f4e06eb377febc01ec3f4e4eb3345becf",
        "title": "Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study"
      },
      {
        "id": "b3eed5317463c994f98de7b5b8e4558aa0b623b4",
        "title": "No Need to Lift a Finger Anymore? Assessing the Quality of Code Generation by ChatGPT"
      },
      {
        "id": "bf9e20852c04ea5aa28b5a092f7fe8354878bf92",
        "title": "f-Divergence Minimization for Sequence-Level Knowledge Distillation"
      },
      {
        "id": "c1284ee1ddf29955a1a02bdc45abdaac63745017",
        "title": "Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method"
      },
      {
        "id": "18b75ea107ed166d7120c12c162b94f02e20b417",
        "title": "COLLIE: Systematic Construction of Constrained Text Generation Tasks"
      },
      {
        "id": "d2af7f63861ad683b061b508316624615bff162d",
        "title": "Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-Session Conversations"
      },
      {
        "id": "e594cef29d90e82f681afa6bb7b0aa4e8da2ceae",
        "title": "Defenses to Membership Inference Attacks: A Survey"
      },
      {
        "id": "e06595ebb2fe4d73fe42e566b57d7a109df75615",
        "title": "Instruction-following Evaluation through Verbalizer Manipulation"
      },
      {
        "id": "ca114b547c06eeb9911b685736679ca4dddf395c",
        "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks"
      },
      {
        "id": "f7432217b3ce63f3e3c5d231c69103a8d1feeaa3",
        "title": "Distributional Semantics"
      },
      {
        "id": "0fb61be60088e80e565b84f44e49ba30630b6126",
        "title": "Stabilizing RLHF through Advantage Model and Selective Rehearsal"
      },
      {
        "id": "b5e44d73ccaae49540f10c72cae0d902d6170754",
        "title": "Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety"
      },
      {
        "id": "0de69d234c2e808b11db54d1efc94be53f438831",
        "title": "Evaluating Generative Ad Hoc Information Retrieval"
      },
      {
        "id": "6d122357d5d94f88a65f6d8bf83a95ec4869a12d",
        "title": "DEVELOPMENT OF A QUESTION ANSWERING CHATBOT FOR BLOCKCHAIN DOMAIN"
      },
      {
        "id": "557b10adfad3b9963f18f7438935d8c86109691e",
        "title": "M2C: Towards Automatic Multimodal Manga Complement"
      },
      {
        "id": "cb1b43d25f18ad37d7ca1110dcdb87287b4186c6",
        "title": "An Overview Of Temporal Commonsense Reasoning and Acquisition"
      },
      {
        "id": "4c6fb350e7769cb730a15c62927b6e9b563d0157",
        "title": "DARWIN Series: Domain Specific Large Language Models for Natural Science"
      },
      {
        "id": "8aab972b0c3a3d581536b0d74339794809dc1a64",
        "title": "TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models"
      },
      {
        "id": "529ff7d6441d244212cf2becafd12a7e67ac56d9",
        "title": "FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"
      },
      {
        "id": "84d99893ee24fc825e359598d44d602c45c4865e",
        "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"
      },
      {
        "id": "88cba3a919aa10c18f42ccbf2bab753c17b3d947",
        "title": "Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling"
      },
      {
        "id": "c24ed03689adc2a075004c6619f43f7f6230fe4b",
        "title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop"
      },
      {
        "id": "6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
        "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics"
      },
      {
        "id": "cc8dec40ee2ccce16fb70218f970515338149a09",
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models"
      },
      {
        "id": "573dad7b2fca7ce72a7f0daf681391d96379ebe3",
        "title": "Low-Parameter Federated Learning with Large Language Models"
      },
      {
        "id": "ba015c5d3f5b44e36363b90070bb3301d21ae57e",
        "title": "On the Safety of Open-Sourced Large Language Models: Does Alignment Really Prevent Them From Being Misused?"
      },
      {
        "id": "c0b2f6e956bf40a59c766f6da5947cb14e7d1d26",
        "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations"
      },
      {
        "id": "96a7b0fe722e6d2d5167ef25a6aff714a20233a0",
        "title": "Exploring the Limits of ChatGPT in Software Security Applications"
      },
      {
        "id": "2efadc1c928c8d92756e573f371b8d46087865ed",
        "title": "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning"
      },
      {
        "id": "873a581320d928249609d3c07229d5af182a379c",
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?"
      },
      {
        "id": "ef9b84a57a654888729ce73a3f2400d56e12a300",
        "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations"
      },
      {
        "id": "587ffdfd7229e8e0dbc5250b44df5fad6251f6ad",
        "title": "The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research"
      },
      {
        "id": "695d11cfd7838b313935fe5fc7938de3816e5c7c",
        "title": "Understanding Natural Language Understanding Systems. A Critical Analysis"
      },
      {
        "id": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models"
      },
      {
        "id": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
        "title": "Do Large Language Models Know What They Don't Know?"
      },
      {
        "id": "8ab4863393fceb41d0fa77d632ace4c80a57a154",
        "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning"
      },
      {
        "id": "ccb82158429d82a53b541649d351382267e1e89d",
        "title": "Exploring the Implications of ChatGPT for Language Learning in Higher Education"
      },
      {
        "id": "1f040c3a8d49f8e54169a0e07013692c7d58de4b",
        "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks"
      },
      {
        "id": "3db1219429c3f04e88347d41269bdc83c457fbf9",
        "title": "Symbol tuning improves in-context learning in language models"
      },
      {
        "id": "d4c3e3e3c01afed15926adf81527bf46aa491c6a",
        "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark"
      },
      {
        "id": "9d12916dd46df7a6446cbec0bc4d054f7dafcdab",
        "title": "Scaling Vision-Language Models with Sparse Mixture of Experts"
      },
      {
        "id": "b9a1189f2de7fd5e66551d7c425556e5642b823a",
        "title": "ChessGPT: Bridging Policy Learning and Language Modeling"
      },
      {
        "id": "e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2",
        "title": "Augmented Large Language Models with Parametric Knowledge Guiding"
      },
      {
        "id": "09239dac5b1cded9414c946333eaf619dca9aaa7",
        "title": "ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design"
      },
      {
        "id": "9d81ec931b85d6c6cf3453126670cd7a30a689e7",
        "title": "TrustGPT: A Benchmark for Trustworthy and Responsible Large Language Models"
      },
      {
        "id": "4ad189079046bf9df0f8ece7545e42dc00e13b9f",
        "title": "Towards a Robust Detection of Language Model-Generated Text: Is ChatGPT that easy to detect?"
      },
      {
        "id": "f90051f22f263d7c39355f07aea18ae3fcfe344a",
        "title": "An empirical study of pre-trained language models in simple knowledge graph question answering"
      },
      {
        "id": "9884153bbcd20282f32e897363883548e0fd8e50",
        "title": "Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health"
      },
      {
        "id": "a563fa042b16533d23829d76e7e17bf19a05891c",
        "title": "Large Language Models Are Not Strong Abstract Reasoners"
      },
      {
        "id": "a70339b84db86a974335499eb824ebdab3a422b3",
        "title": "Large Language Model Instruction Following: A Survey of Progresses and Challenges"
      },
      {
        "id": "4cfec8ec51a0ecd7efbc6e6622ae8f930935f714",
        "title": "Bioformer: an efficient transformer language model for biomedical text mining"
      },
      {
        "id": "66d2021641c2003d8614c898bbddb653ec349b22",
        "title": "Rethinking Semi-supervised Learning with Language Models"
      },
      {
        "id": "f9338e1b32cb450d95094ce71957375754823e4b",
        "title": "Adding Instructions during Pretraining: Effective way of Controlling Toxicity in Language Models"
      },
      {
        "id": "3b0c49ca5ac0f441c302c9ca4def4804253552d5",
        "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts"
      },
      {
        "id": "5882dd04d95c9c88cdec389059fcf44d56cbb789",
        "title": "Understanding the Effectiveness of Very Large Language Models on Dialog Evaluation"
      },
      {
        "id": "661e64593fca437e41d4b90bcbc440ba76d988d2",
        "title": "Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs"
      },
      {
        "id": "98b40eb3ce79c24d9726556947e2e2094737fe46",
        "title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models"
      },
      {
        "id": "c082ccfcfe1afc696e371374146ba9380b84061e",
        "title": "The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field"
      },
      {
        "id": "39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f",
        "title": "GPT (Generative Pre-Trained Transformer)— A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions"
      },
      {
        "id": "306c0576750d8ac1298f70474560aa951490b2a1",
        "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization"
      },
      {
        "id": "83cebf919635504786fc220d569284842b0f0a09",
        "title": "A Survey of Adversarial Defenses and Robustness in NLP"
      },
      {
        "id": "e3bc7f2bbe6e6c27be6132591d53df308f16ab97",
        "title": "Will Affective Computing Emerge From Foundation Models and General Artificial Intelligence? A First Evaluation of ChatGPT"
      },
      {
        "id": "da872dfc0934f554311d5f91fa7e5ada44fb8155",
        "title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT"
      },
      {
        "id": "af87339f36f9735bae708369256e0ed9734dcc22",
        "title": "Do ChatGPT and Other AI Chatbots Pose a Cybersecurity Risk? - An Exploratory Study"
      },
      {
        "id": "9cefc046ab8159b190f535f1930f0ff1b9460f76",
        "title": "Towards a Robust Deep Neural Network Against Adversarial Texts: A Survey"
      },
      {
        "id": "09d51a7367fc7fbce643ae5f70c9959ed723009e",
        "title": "Applying BERT and ChatGPT for Sentiment Analysis of Lyme Disease in Scientific Literature"
      },
      {
        "id": "66f598c974ddc36d16ea70ad49c08ad13a666408",
        "title": "ChatGPT: Systematic Review, Applications, and Agenda for Multidisciplinary Research"
      },
      {
        "id": "f27fcbbc4d449c63164555ff301c3471c616bda2",
        "title": "Performance of Generative Pretrained Transformer on the National Medical Licensing Examination in Japan"
      },
      {
        "id": "ae5e38058e9d622666254fa873c35a449a7bb2e1",
        "title": "Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis"
      },
      {
        "id": "1cdfa7c3465943a295f8df2d2097c4bb3e222426",
        "title": "Rationalization for explainable NLP: a survey"
      },
      {
        "id": "90df9c6924425d7366d16731a34bfa4e52ad5b2e",
        "title": "What’s the Meaning of Superhuman Performance in Today’s NLU?"
      },
      {
        "id": "fafb7f456d702147c64356dc9d537c336fa55ae6",
        "title": "Linguistic ambiguity analysis in ChatGPT"
      },
      {
        "id": "c79852e9c9cc6734c9150847deb5449e489354ea",
        "title": "Don't Stop Pretraining? Make Prompt-based Fine-tuning Powerful Learner"
      },
      {
        "id": "ccf42104c22432c96d411eeb8f3b9afbfe0dfdd0",
        "title": "Towards Trustworthy Explanation: On Causal Rationalization"
      },
      {
        "id": "c3349d0493a56f76c27d74c5056ab039f665b7b4",
        "title": "Synthesizing Human Gaze Feedback for Improved NLP Performance"
      },
      {
        "id": "8d8e09ded78cae335e1fa7d5532509f74507c760",
        "title": "Deep Learning in ChatGPT - A Survey"
      },
      {
        "id": "bdbe2c1a430d8a15d20964bfd5d7e828ccae73d0",
        "title": "Querying Large Language Models with SQL"
      },
      {
        "id": "d325bda3a2393ef88cdd4ca245a08f89474d9df7",
        "title": "SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL (extended)"
      },
      {
        "id": "c8fa30492240c406ed773becc7544ce1d09d5ca5",
        "title": "Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy"
      },
      {
        "id": "705e49afd92130f2bc1e0d4d0b1f6cb14e88803f",
        "title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection"
      },
      {
        "id": "f60d3ce156dff3868cf923e03d94eef843eaa13a",
        "title": "How Secure is Code Generated by ChatGPT?"
      },
      {
        "id": "5db3257a61d86f302767ae1f21d6fd30567f12e5",
        "title": "Towards Building The Federatedgpt: Federated Instruction Tuning"
      },
      {
        "id": "2e01fdebbc780d3667ec3bf87a44927f0d9c188a",
        "title": "Decision-Oriented Dialogue for Human-AI Collaboration"
      },
      {
        "id": "3c50ef336232da0885ef61da386c98eac964b7cd",
        "title": "PathAsst: A Generative Foundation AI Assistant towards Artificial General Intelligence of Pathology"
      },
      {
        "id": "3599a236f285af48782fc30b1341d13ec7320735",
        "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT"
      },
      {
        "id": "e1eb40122b5ddb2a14473da816fb065f4b503538",
        "title": "Towards Human-Bot Collaborative Software Architecting with ChatGPT"
      },
      {
        "id": "912a39c2e0e4a35747531669cfa952d2c5627729",
        "title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization"
      },
      {
        "id": "e383ee065bb7b9ef798d5aff9db794691f4c0e38",
        "title": "Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook"
      },
      {
        "id": "a26623d52d24e03044a158cddad931ec5ab7304c",
        "title": "A Survey of Knowledge Enhanced Pre-Trained Language Models"
      },
      {
        "id": "2b6291eb76e2ff885238e94704bb795046d7d530",
        "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models"
      },
      {
        "id": "b0b9cbddb4f564f657b0473a9fbc8995bb0d4564",
        "title": "A comparative study of pre-trained language models for named entity recognition in clinical trial eligibility criteria from multiple corpora"
      },
      {
        "id": "1833ad08d52a454a50490fed91181fc7e2cc397a",
        "title": "DictBERT: Dictionary Description Knowledge Enhanced Language Model Pre-training via Contrastive Learning"
      },
      {
        "id": "fdd69dd6ad2be28c6864b7330ccc1311e284cc5c",
        "title": "USB: A Unified Semi-supervised Learning Benchmark"
      },
      {
        "id": "285d13bf3cbe6a8a0f164f584d84f8b74067271f",
        "title": "Towards Faithful Model Explanation in NLP: A Survey"
      },
      {
        "id": "8e2930ac8ae9a758b367513cccb7d562f354afea",
        "title": "Who Says Elephants Can’t Run: Bringing Large Scale MoE Models into Cloud Scale Production"
      },
      {
        "id": "108ea619cfef2c53b36ace97cef6830944c1d802",
        "title": "Marvista: Exploring the Design of a Human-AI Collaborative News Reading Tool"
      },
      {
        "id": "c8d594f09413b1555970f43e68847c211235d60f",
        "title": "Prompting GPT-3 To Be Reliable"
      },
      {
        "id": "65feb2a40476bd01bbeea217a6e2afdc657c3a87",
        "title": "Using artificial intelligence to improve pain assessment and pain management: a scoping review"
      },
      {
        "id": "301c6b4117580efba62549c87687ac4bcbe701f2",
        "title": "Natural Language Processing Methods and Bipolar Disorder: Scoping Review"
      },
      {
        "id": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
        "title": "Red Teaming Language Models with Language Models"
      },
      {
        "id": "d304d0bdfa81fd10b187aa0e4f41d410eb19d6e3",
        "title": "Fine-tuned Language Models are Continual Learners"
      },
      {
        "id": "a0c87ee1b0903c1c9ac72809caf75b6b6997baa0",
        "title": "Human Language Understanding & Reasoning"
      },
      {
        "id": "8176556a3fefa7c1b16262ba75499a4643682262",
        "title": "Linking Emergent and Natural Languages via Corpus Transfer"
      },
      {
        "id": "cdfe9580f63070f311151444f9df32818cc858bf",
        "title": "An Empirical Evaluation of GitHub Copilot's Code Suggestions"
      },
      {
        "id": "1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3",
        "title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models"
      },
      {
        "id": "220590e7815ea278959329058a5de3e4c9df9f4e",
        "title": "FedBERT: When Federated Learning Meets Pre-training"
      },
      {
        "id": "8a8e2c23fd7179d981cbefbbc4844494e7255c53",
        "title": "GPT-3 and InstructGPT: technological dystopianism, utopianism, and “Contextual” perspectives in AI ethics and industry"
      },
      {
        "id": "67dc33f02c8b5f24cd213b6b5fb5c74cead581aa",
        "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond"
      },
      {
        "id": "eff9d6c7596953b5a61f00ad13d23c2b6fea60ea",
        "title": "Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future"
      },
      {
        "id": "2d0eba854c96b7f52553cb94015dfe93ca906d2d",
        "title": "Technical Perspective of TURL"
      },
      {
        "id": "8bb9db78b4413b92cdeeae9e24e955aab9c87ae1",
        "title": "A Survey of Adversarial Defences and Robustness in NLP"
      },
      {
        "id": "011c2b5a08c32e57f7c55539a1944bc733f83aa3",
        "title": "On Continual Model Refinement in Out-of-Distribution Data Streams"
      },
      {
        "id": "fab5f884301cf5fc6e07907e3d136090d2641923",
        "title": "Making a (Counterfactual) Difference One Rationale at a Time"
      },
      {
        "id": "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2",
        "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization"
      },
      {
        "id": "c7cbbbdba2c942180675c2a69035e506d9378679",
        "title": "Using Pre-Trained Models to Boost Code Review Automation"
      },
      {
        "id": "d1eb051c6b13eba8a9b333d5ee0a55250717195d",
        "title": "Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning"
      },
      {
        "id": "46bd20b3d877848dd38e0039110cca3105d7109c",
        "title": "CELER: A 365-Participant Corpus of Eye Movements in L1 and L2 English Reading"
      },
      {
        "id": "282058c7b1f07b878c154f3b4245992fd2bbb15e",
        "title": "Preference-Guided Reflective Sampling for Aligning Language Models"
      },
      {
        "id": "11bdbfe785b874fab4232c394e2d3a5aa389286a",
        "title": "Towards Explainable Network Intrusion Detection using Large Language Models"
      },
      {
        "id": "2530e6ecbd0198012bb8ee4359acb9241cefec95",
        "title": "Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models"
      },
      {
        "id": "e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d",
        "title": "A Survey of Hallucination Problems Based on Large Language Models"
      },
      {
        "id": "4e112479c28ed33fd291a672d539b098d6684f00",
        "title": "AssistRAG: Boosting the Potential of Large Language Models with an Intelligent Information Assistant"
      },
      {
        "id": "e938456ba60adc873106f27a72d4a302e9a3c6ea",
        "title": "LLMR: Knowledge Distillation with a Large Language Model-Induced Reward"
      },
      {
        "id": "c8f8d62f1e466b8af9bc2b3394ee9970cafd78de",
        "title": "Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused"
      },
      {
        "id": "f712b5b061ec99e1ebf15a1572637abfc98ce20b",
        "title": "Overview of the NLPCC 2024 Shared Task on Chinese Metaphor Generation"
      },
      {
        "id": "b66fd193819c99e0445d2d42f8750ea3fd1e8cbd",
        "title": "Conditioning LLMs with Emotion in Neural Machine Translation"
      },
      {
        "id": "cb1454f820b8daf27031411e3f39ce2c3d304422",
        "title": "CERT-ED: Certifiably Robust Text Classification for Edit Distance"
      },
      {
        "id": "63161be2bd80005ffe5eff673994978b7ea9a4cb",
        "title": "RSA-Control: A Pragmatics-Grounded Lightweight Controllable Text Generation Framework"
      },
      {
        "id": "13a03ba21fd6e3c23efe143da070e2d461007540",
        "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding"
      },
      {
        "id": "850a5c74833054824ce2280cb0d7226cbaac8b46",
        "title": "A Survey of AI-Generated Content (AIGC)"
      }
    ],
    "1": [
      {
        "id": "bb26244813bc8ed3ebd051530b60ab5064550121",
        "title": "TaD: A Plug-and-Play Task-Aware Decoding Method to Better Adapt LLMs on Downstream Tasks"
      },
      {
        "id": "425182f5c96c1d239da5cbe3a24371b2bab6319b",
        "title": "Deep Time Series Models: A Comprehensive Survey and Benchmark"
      },
      {
        "id": "b8d402e3ed98be323b5027a3f5251dff4374d08b",
        "title": "PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple Operators for Forecasting Fluid Dynamics"
      },
      {
        "id": "590088c7bc699fe26507cb1e9f70aac531d1f069",
        "title": "Language Generation with Strictly Proper Scoring Rules"
      },
      {
        "id": "99f6acfc14ee633787f987768b31b240a87c2c0f",
        "title": "Me-LLaMA: Foundation Large Language Models for Medical Applications"
      },
      {
        "id": "4406d933f8aab86d43e6efe1bfdc98a382020634",
        "title": "Open challenges and opportunities in federated foundation models towards biomedical healthcare"
      },
      {
        "id": "f3d7fdcdb1191a4a59e78f54d67fcd46a3462d2b",
        "title": "Temporal Scaling Law for Large Language Models"
      },
      {
        "id": "2046b2da23eb2f79744eb391d902da9cedf87947",
        "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey"
      },
      {
        "id": "d488445bb2bf6719bc48a4d39bd906116274abda",
        "title": "AutoTimes: Autoregressive Time Series Forecasters via Large Language Models"
      },
      {
        "id": "ac45bbf9940512d9d686cf8cd3a95969bc313570",
        "title": "OLMo: Accelerating the Science of Language Models"
      },
      {
        "id": "0448656a78e26b9a2899cf85447f800deda8f5f3",
        "title": "Scalable Pre-training of Large Autoregressive Image Models"
      },
      {
        "id": "c827d0239c7aec8355f9b31d94a9b0180b53b58b",
        "title": "TechGPT-2.0: A large language model project to solve the task of knowledge graph construction"
      },
      {
        "id": "bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3",
        "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"
      },
      {
        "id": "5b038c1a93967072cc76689fd805e756f804cc42",
        "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"
      },
      {
        "id": "5432b77bfb1dced97c5b1fc684b0fa7d0d84c424",
        "title": "Large Language Models in Finance: A Survey"
      },
      {
        "id": "1d266cde7ac654e2a55ff52988f134ff66d17218",
        "title": "Examining the Potential of Generative Language Models for Aviation Safety Analysis: Case Study and Insights Using the Aviation Safety Reporting System (ASRS)"
      },
      {
        "id": "98032f95e274db30570727fb7196c15e325fb35a",
        "title": "Three Bricks to Consolidate Watermarks for Large Language Models"
      },
      {
        "id": "6356e860372addf7278307f490903e32f334ed75",
        "title": "Agile Methodology for the Standardization of Engineering Requirements Using Large Language Models"
      },
      {
        "id": "1e3ef48abeef882e12f9553a1baf8944f3782c88",
        "title": "Several categories of Large Language Models (LLMs): A Short Survey"
      },
      {
        "id": "aff0fe00ed7892d1185e8c5b66d318d3892abe6e",
        "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health"
      },
      {
        "id": "3e4085e5869f1b7959707a1e1d7d273b6057eb4e",
        "title": "StarCoder: may the source be with you!"
      },
      {
        "id": "285dae5c2f2ef55c70971094a1ddd45afe720eee",
        "title": "Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense"
      },
      {
        "id": "57e849d0de13ed5f91d086936296721d4ff75a75",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "id": "933b37b21e9d61139660088adb032ff3fdf56d86",
        "title": "Learning Video Representations from Large Language Models"
      },
      {
        "id": "a9f7761b740c85444b86f581801a471340007a0d",
        "title": "scELMo: Embeddings from Language Models are Good Learners for Single-cell Data Analysis"
      },
      {
        "id": "14ba624e625a28d38c20e4e0c7803a88138cfa72",
        "title": "Fairness Definitions in Language Models Explained"
      },
      {
        "id": "ec3ee1d1edf6f384e92d928af970a9c362ad33e2",
        "title": "MaskMoE: Boosting Token-Level Learning via Routing Mask in Mixture-of-Experts"
      },
      {
        "id": "35da0de18be36c54a2d29efdef38b5b4668ffda9",
        "title": "Training on the Test Task Confounds Evaluation and Emergence"
      },
      {
        "id": "9a290f3d364229c04d7ebfa8a87596d6d149e381",
        "title": "The Fire Thief Is Also the Keeper: Balancing Usability and Privacy in Prompts"
      },
      {
        "id": "73fd85ebd43faa2afc6e88a379a932d8017851fc",
        "title": "Large Scale Transfer Learning for Tabular Data via Language Modeling"
      },
      {
        "id": "a544a830c79cf499baa48df0d0d03eda1dd34f00",
        "title": "From Handcrafted Features to LLMs: A Brief Survey for Machine Translation Quality Estimation"
      },
      {
        "id": "67f833766640752cf5d2a714f3738c634a99d8eb",
        "title": "On the Opportunities and Challenges of Foundation Models for GeoAI (Vision Paper)"
      },
      {
        "id": "bb0799cffdb2d676e1b68d15183371473c03a4fd",
        "title": "EfficientQAT: Efficient Quantization-Aware Training for Large Language Models"
      },
      {
        "id": "1ac7fc5a55ce5843fb8a19d9f62b623e822bb7de",
        "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language Models"
      },
      {
        "id": "bcb40695926a09b12423777606dad1d5cbc8223a",
        "title": "Automated Building Information Modeling Compliance Check through a Large Language Model Combined with Deep Learning and Ontology"
      },
      {
        "id": "d1f6b441e53dc42bc0c44755588198c54b5e5d09",
        "title": "ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models"
      },
      {
        "id": "bf4cc303910ed4917c85c42e97f7380ad9fc0337",
        "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting"
      },
      {
        "id": "6db25af3b87e1fa821d3fefc47185a7ff08739ca",
        "title": "AD-LLM: Benchmarking Large Language Models for Anomaly Detection"
      },
      {
        "id": "77657519bd3b41bccb11f3e70d74e64818ad9c97",
        "title": "DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models"
      },
      {
        "id": "8dfb4349a11c503aa0228791575ee1929f808c14",
        "title": "Conversing with business process-aware large language models: the BPLLM framework"
      },
      {
        "id": "48020e5f1a0d5703f6169c20051eeb056194c25b",
        "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey"
      },
      {
        "id": "a913c98aa32427ec0a876d39b35b235ba483b061",
        "title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models"
      },
      {
        "id": "92056d644aed7caa6c5367fe77774883246af793",
        "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge"
      },
      {
        "id": "491838c049ee5bd53a6d7c719033332c4a001fdc",
        "title": "Autoregressive Models in Vision: A Survey"
      },
      {
        "id": "d05a874742134ecde3f1ca6cc7b710c776e0135b",
        "title": "TourSynbio: A Multi-Modal Large Model and Agent Framework to Bridge Text and Protein Sequences for Protein Engineering"
      },
      {
        "id": "0852af290f29c2f7871f6900f2ea6e288ea5fa32",
        "title": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting"
      },
      {
        "id": "4d053f419e361939abdbcfed49c320e05d87ae98",
        "title": "Comparing Discrete and Continuous Space LLMs for Speech Recognition"
      },
      {
        "id": "011c3f1d62a5e9837b7eeb58071986684807c718",
        "title": "Data Contamination Report from the 2024 CONDA Shared Task"
      },
      {
        "id": "e5e0863cbfa594dd7c42e0b2f461fa04f9000a3b",
        "title": "Realistic Evaluation of Model Merging for Compositional Generalization"
      },
      {
        "id": "3888c44ccecf75dc51060582e2df99411b804600",
        "title": "Guided Profile Generation Improves Personalization with LLMs"
      },
      {
        "id": "2328293b9a9da591f820b2f2b4a5551c3d44bbc2",
        "title": "A Survey for Large Language Models in Biomedicine"
      },
      {
        "id": "1b7492a4b813052146300fa8c03325c4ad7a0a25",
        "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing"
      },
      {
        "id": "361e9549940fc75daeeef3d9b31183cc1cf417da",
        "title": "A Survey on Challenges and Advances in Natural Language Processing with a Focus on Legal Informatics and Low-Resource Languages"
      },
      {
        "id": "e973e6904072336aae499b65a60040c68e37df44",
        "title": "Lessons from the Use of Natural Language Inference (NLI) in Requirements Engineering Tasks"
      },
      {
        "id": "0a470fbfebb5515e8af8d9a09f603df4dfce40a0",
        "title": "A Systematic Evaluation of Large Language Models for Natural"
      },
      {
        "id": "7754ac3e8ff1286f17593159781487543cdddaba",
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns"
      },
      {
        "id": "d80a87f3624461b36acdeb809c50979ba89fb1be",
        "title": "A Survey of Large Language Models in Finance (FinLLMs)"
      },
      {
        "id": "13df472c3fe81bf1b615238fbd7884c8b45d8d1c",
        "title": "Large Language Models for Time Series: A Survey"
      },
      {
        "id": "d9723a47eb4dea5270f4164208a74ef1f715d9c0",
        "title": "On Large Visual Language Models for Medical Imaging Analysis: An Empirical Study"
      },
      {
        "id": "a2339a7e417df89c617ee41eef6e70868da6baac",
        "title": "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact"
      },
      {
        "id": "42dd2d351a0cd2ba599ec8c8b21cfdc4ad4f3323",
        "title": "Gender Bias in Large Language Models across Multiple Languages"
      },
      {
        "id": "781fa8bb5ab3af8de29b7674e44648aa40bd5e77",
        "title": "LogFiT: Log Anomaly Detection Using Fine-Tuned Language Models"
      },
      {
        "id": "e95dd8a8a07470eb19770dad42470a16315b1b94",
        "title": "Utility of artificial intelligence‐based large language models in ophthalmic care"
      },
      {
        "id": "2be319e4ac88c29b4d212bc6efa244321d350498",
        "title": "How Can Large Language Models Understand Spatial-Temporal Data?"
      },
      {
        "id": "d8d0d704446ffaf09b9722360ed76341934ec3a3",
        "title": "TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models"
      },
      {
        "id": "de643836277d108e1f8410ea85b0c3ed0e686163",
        "title": "MaLA-500: Massive Language Adaptation of Large Language Models"
      },
      {
        "id": "ebff0de12d826c3eea09cccf5f717f3ac3489366",
        "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models"
      },
      {
        "id": "bc26a46e2c01bc4e16273d10b0f7d491d6e522d8",
        "title": "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning"
      },
      {
        "id": "29b65faa60b96ecd016d75f6d707012fda8d4df1",
        "title": "Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models"
      },
      {
        "id": "0255001544a130e64802b03ba03d4fdd0cd34dbb",
        "title": "On the Convergence of Zeroth-Order Federated Tuning for Large Language Models"
      },
      {
        "id": "404f2833ad36b95bcd5b717f25ff67cbdcb8505c",
        "title": "A Fine-tuning Dataset and Benchmark for Large Language Models for Protein Understanding"
      },
      {
        "id": "44993ace721ffb93f3b78fe8eb9a082d9b6a6b42",
        "title": "Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies"
      },
      {
        "id": "7a8a69d1ee140274dab4e63109cfd8513be127af",
        "title": "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning"
      },
      {
        "id": "7bf024694ee3d55465538f6c238179b401fd81b8",
        "title": "EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation"
      },
      {
        "id": "ed3057a99ddd416bf5dcaee83688ca7e43da678f",
        "title": "DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain"
      },
      {
        "id": "c20942cedd92dcc4e6270f85af780464da655c4c",
        "title": "Exploring Tokenization Strategies and Vocabulary Sizes for Enhanced Arabic Language Models"
      },
      {
        "id": "6c8eebd04990175911f5d451b37b95e693997dab",
        "title": "TernaryLLM: Ternarized Large Language Model"
      },
      {
        "id": "740356ce4b53b28a7c674ae973a9f8116bcaa827",
        "title": "CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer"
      },
      {
        "id": "ba15e41eac064729c634464851ae0a268de777d4",
        "title": "Prompt Perturbation Consistency Learning for Robust Language Models"
      },
      {
        "id": "16e5d1d62423660004cad97cd2c9ccdf92e3422e",
        "title": "Dynamic Data Sampler for Cross-Language Transfer Learning in Large Language Models"
      },
      {
        "id": "7e59806710a4e7961ba6343f215dd50c402ee300",
        "title": "BuddyBot: AI Powered Chatbot for Enhancing English Language Learning"
      },
      {
        "id": "3d6197e4ab55a3a2785ce5934e48cfbe9fe9bf04",
        "title": "Revolutionizing Finance with LLMs: An Overview of Applications and Insights"
      },
      {
        "id": "22e21f328e3c4aec3abe0006d974a1260d0ee996",
        "title": "SuRe: Summarizing Retrievals using Answer Candidates for Open-domain QA of LLMs"
      },
      {
        "id": "f1961a58caca651d919384f8a513fa19c4d6c2ea",
        "title": "Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate"
      },
      {
        "id": "52258e9110f8e73cca24f6394bfc483341b459a4",
        "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation"
      },
      {
        "id": "664e518fa4873e281180b7ead965608ae14b1aef",
        "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks"
      },
      {
        "id": "d5bc78761a02581b859d3a925364d865b9cfe360",
        "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents"
      },
      {
        "id": "a74a20be53e5767648b5970e30b2d81a9ba8293f",
        "title": "A Survey on Transformer Compression"
      },
      {
        "id": "da21fe0e1f6c10f0f5c8b7990ba0e344d5f670c8",
        "title": "History of generative Artificial Intelligence (AI) chatbots: past, present, and future development"
      },
      {
        "id": "ba4ccb46df4ff77eda1bc7a33065b0c4d6d4ee3a",
        "title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs"
      },
      {
        "id": "8a42e384c414bdd041d65e9044c8fa49626dc407",
        "title": "EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning"
      },
      {
        "id": "ee1b0ce3312fa1d23a3f0e8fd5422fcfdfceb23f",
        "title": "LLMs are Good Action Recognizers"
      },
      {
        "id": "f2e78a574925486d1f13440f55688bcffde80101",
        "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks"
      },
      {
        "id": "b9750286ba2198a406137e0dfee2d545f0d78c13",
        "title": "NumeroLogic: Number Encoding for Enhanced LLMs’ Numerical Reasoning"
      },
      {
        "id": "2607c9fe8485e2ccac184666060485e06bca459b",
        "title": "MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations"
      },
      {
        "id": "7e5340568db83df201d183de65cf9ff68c48eeb8",
        "title": "RT: a Retrieving and Chain-of-Thought framework for few-shot medical named entity recognition"
      },
      {
        "id": "7f8d167e00246f8112cf825ea4ca088594d2ce1d",
        "title": "OMPGPT: A Generative Pre-trained Transformer Model for OpenMP"
      },
      {
        "id": "c427ff6852a3330882e8814c0c030439bdfb23bf",
        "title": "Tailoring Education with GenAI: A New Horizon in Lesson Planning"
      },
      {
        "id": "68a65925ac6d270ca27a4fe99e58cf2ed0795821",
        "title": "Time-FFM: Towards LM-Empowered Federated Foundation Model for Time Series Forecasting"
      },
      {
        "id": "9808c18d1ddc108d746c60b4e554284a3d7a71e9",
        "title": "Exposing Vulnerabilities in Clinical LLMs Through Data Poisoning Attacks: Case Study in Breast Cancer"
      },
      {
        "id": "b6ab52e917dd89b9fb8f272adf16d15751a4d85b",
        "title": "Developing ChatGPT for biology and medicine: a complete review of biomedical question answering"
      },
      {
        "id": "6fa3544f42bc026ac684cf6c7a8cd50f59b3ee7d",
        "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation"
      },
      {
        "id": "954b7958f562554fa92d539f895e615e772baf84",
        "title": "Safe Multi-agent Reinforcement Learning with Natural Language Constraints"
      },
      {
        "id": "5714bd69ffc2e377bcf370c23aec7e6991891762",
        "title": "Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models"
      },
      {
        "id": "c9d98068784df6edf25e2e11a47dcd12c65cf3d9",
        "title": "Bridging the Bosphorus: Advancing Turkish Large Language Models through Strategies for Low-Resource Language Adaptation and Benchmarking"
      },
      {
        "id": "1d34bc857e679b78e63daf39843b317de81ee09c",
        "title": "A Systematic Review of the Limitations and Associated Opportunities of ChatGPT"
      },
      {
        "id": "34c6cf6dcd23cd448a11165dbde28671e995ce8d",
        "title": "Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for Automatic Diagnosis"
      },
      {
        "id": "5502d769595981009e43344f8914e287acca2359",
        "title": "ModaVerse: Efficiently Transforming Modalities with LLMs"
      },
      {
        "id": "b5bb10b374b52a830e96e3c427d8486def7d882c",
        "title": "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning"
      },
      {
        "id": "99e470e72d74bebb31a08ca9d4cd6eca3db6ca7d",
        "title": "Is ChatGPT a Financial Expert? Evaluating Language Models on Financial Natural Language Processing"
      },
      {
        "id": "eb2c2330177f765038a2b17e2ee3498965865797",
        "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models"
      },
      {
        "id": "0b220041eb83c23b7b10d32a5d08c0309d528071",
        "title": "Large Language Models for Information Retrieval: A Survey"
      },
      {
        "id": "6bcc6ab9c28805d4067e99b2cdc7524550fe80e1",
        "title": "PointLLM: Empowering Large Language Models to Understand Point Clouds"
      },
      {
        "id": "33112b58e3eb4a6506fa537d892dc6742c5e794d",
        "title": "Large language models in health care: Development, applications, and challenges"
      },
      {
        "id": "fac468032e0c38ea10dfb95ba6cdeac51a473050",
        "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models"
      },
      {
        "id": "0c8a630657a2cf5dea41472a9b5e20544ce2bd56",
        "title": "Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks"
      },
      {
        "id": "bd09391fbd124dc0c0a6be5d0ab2eb5d9c43fbac",
        "title": "FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets"
      },
      {
        "id": "94dd5c30b052e29039c20e8cbd4214bb2302740a",
        "title": "Composite Backdoor Attacks Against Large Language Models"
      },
      {
        "id": "00a67af3b7dc785b4813b61d232cc76b4fb2b189",
        "title": "TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning"
      },
      {
        "id": "ad44f2392df3c6f2c45d21fe1dbd8ec17003530d",
        "title": "Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models"
      },
      {
        "id": "462041e29f2e4e8d78b3214ee1a286865bc68721",
        "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences"
      },
      {
        "id": "9a2f47777b99a92effb4e998b7082e1e92ae13bc",
        "title": "Improving Language Plasticity via Pretraining with Active Forgetting"
      },
      {
        "id": "3b88526a0f0337e3a6b632b4af8fd0882eb4b470",
        "title": "FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models"
      },
      {
        "id": "5d2b77ae8508e277fe9b840a471b7dfb00e806ff",
        "title": "Large Language Models are Complex Table Parsers"
      },
      {
        "id": "00e527ece4db4857bb7a03bd924341eb2b5000fa",
        "title": "HPC-GPT: Integrating Large Language Model for High-Performance Computing"
      },
      {
        "id": "d0edff5402edeab721b7681643e1ff7c2354de4a",
        "title": "Leveraging pre-trained language models for mining microbiome-disease relationships"
      },
      {
        "id": "1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35",
        "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Trustworthy Response Generation in Chinese"
      },
      {
        "id": "27ae43f78463d6417cd97276375a7e864af7dc60",
        "title": "TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety"
      },
      {
        "id": "f1278443e9df54711b8f55548961862337c193a6",
        "title": "Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization"
      },
      {
        "id": "476de9654c840a080276c4b8b5dadfdbe25c663f",
        "title": "Construction contract risk identification based on knowledge-augmented language model"
      },
      {
        "id": "be7dbac2bcaed4cd034a7371004a011933e1bdca",
        "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model"
      },
      {
        "id": "0ad16e2c1c30d8ed5b63970e5fb3459a08218ea3",
        "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian"
      },
      {
        "id": "a9eb336485e148d0a3f5010693d7752facba2875",
        "title": "CFGPT: Chinese Financial Assistant with Large Language Model"
      },
      {
        "id": "220178fb7f488c6925f4607dc58b599afa6ee967",
        "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction"
      },
      {
        "id": "ff2eecb21972eb287064f98db1a4487c62bd7566",
        "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models"
      },
      {
        "id": "2883358201b604b86b4d11fa5d00653e244bedac",
        "title": "Procedural Text Mining with Large Language Models"
      },
      {
        "id": "9a2ee8c33deef36c2dbc7af5d2a8a7cc7c87d882",
        "title": "Evaluating the Capability of Large-scale Language Models on Chinese Grammatical Error Correction Task"
      },
      {
        "id": "1b90e9e9734bed6b379ae87d688cb3b887baf597",
        "title": "Objaverse-XL: A Universe of 10M+ 3D Objects"
      },
      {
        "id": "83ac79bb8e8695fb3c3c024be74790d862adea74",
        "title": "TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting"
      },
      {
        "id": "f5cc6df1a7d00a5ed93bee3db2e5df00c338a936",
        "title": "LLMs Accelerate Annotation for Medical Information Extraction"
      },
      {
        "id": "7c9bb230946cf48a7b9de97fd0281f42fbc51d31",
        "title": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting"
      },
      {
        "id": "4726d1dc54851db99c29180127d840bd19f20afc",
        "title": "Positional Description Matters for Transformers Arithmetic"
      },
      {
        "id": "cfd7550d6c25f4771549726b289665c8aa5ae1be",
        "title": "Thread of Thought Unraveling Chaotic Contexts"
      },
      {
        "id": "ae87028a650705a02f2f8e7bbba9f2c5718ddb68",
        "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey"
      },
      {
        "id": "644a0aa82f58b7f387fb1b731f0932c84c2f200f",
        "title": "Exploring the Boundaries of GPT-4 in Radiology"
      },
      {
        "id": "247509f5658374678830582dddfdb6bc4d8af25e",
        "title": "Is ChatGPT a Good Personality Recognizer? A Preliminary Study"
      },
      {
        "id": "f57fae177e4e6e0a2b306b7b6bef899821ce9545",
        "title": "Instruct Me More! Random Prompting for Visual In-Context Learning"
      },
      {
        "id": "d76389f4585aef5c8176ff27e5f93b22e7d5dfbf",
        "title": "Vec-Tok Speech: Speech Vectorization and Tokenization for Neural Speech Generation"
      },
      {
        "id": "b40276ce0e3fec1c9ad8bb95e8358e083a925a20",
        "title": "Last One Standing: A Comparative Analysis of Security and Privacy of Soft Prompt Tuning, LoRA, and In-Context Learning"
      },
      {
        "id": "a97123a21f27ebd0e4bd0f6c4def42c87813fd64",
        "title": "Is ChatGPT a Good Multi-Party Conversation Solver?"
      },
      {
        "id": "94a47bf3bb8fffaabe75c6aa2b4b1b211b3e40d1",
        "title": "Computers' Interpretations of Knowledge Representation Using Pre-Conceptual Schemas: An Approach Based on the BERT and Llama 2-Chat Models"
      },
      {
        "id": "a7cb000aa86eb035cdb527349885191fe52baf86",
        "title": "Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation"
      },
      {
        "id": "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
        "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
      },
      {
        "id": "bab5e35001757719d0f8338f94dde2860dae784a",
        "title": "How Large Language Models Will Disrupt Data Management"
      },
      {
        "id": "816f709246530845971c62374c84a2fed8871b8e",
        "title": "Black Box Warning: Large Language Models and the Future of Infectious Diseases Consultation"
      },
      {
        "id": "06f710f59e2119ed892e9291eb942847a34db91c",
        "title": "KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph"
      },
      {
        "id": "6cb53afdc2b21236c3957efd4bbeb2aa65338c50",
        "title": "An Interdisciplinary Outlook on Large Language Models for Scientific Research"
      },
      {
        "id": "01802333cdf7bfe00935ba8a7390f3d028e4fb25",
        "title": "Fine-Tuning Language Models Using Formal Methods Feedback"
      },
      {
        "id": "dab4f70d75a04e62553e583f2450d9bb1f0ead46",
        "title": "CLOMO: Counterfactual Logical Modification with Large Language Models"
      },
      {
        "id": "2d9310132cfe9046f4b61f6e90a5ef92c6e0ba71",
        "title": "BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys"
      },
      {
        "id": "caf8c9e52658af46ee6651bf004d545bd379f087",
        "title": "Enhancing Text-to-SQL Translation for Financial System Design"
      },
      {
        "id": "3de1d214dc79ecd0b34faf6fb0520dd7d2988c97",
        "title": "ChatGPT as Your Vehicle Co-Pilot: An Initial Attempt"
      },
      {
        "id": "2ce8bd7295aad4e15296ee098ef0d296c9833c38",
        "title": "Natural Language Processing (NLP) in Aviation Safety: Systematic Review of Research and Outlook into the Future"
      },
      {
        "id": "1645e93f34ce34c0ff248a7349bf757a416c5312",
        "title": "Health system-scale language models are all-purpose prediction engines"
      },
      {
        "id": "e154dd91de91558f9d671370754eace62a54c911",
        "title": "A study of generative large language model for medical research and healthcare"
      },
      {
        "id": "5dea206e2a36e672f197252bdd27d156d058f48c",
        "title": "FinGPT: Open-Source Financial Large Language Models"
      },
      {
        "id": "109929be7890ef982fb3b6be0d78609cfab1ea13",
        "title": "PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance"
      },
      {
        "id": "5ef821267fa68d3231ed8135ff8ec09f25bb1398",
        "title": "ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"
      },
      {
        "id": "e3ec55e9e6720194a0ed5d4033d93a941c8a4f99",
        "title": "Continual Pre-training of Language Models"
      },
      {
        "id": "c18f2239a4bd8cc68db9a013416167357f5e1353",
        "title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark"
      },
      {
        "id": "f9bfc6d9ba1665b73af3323d46c7642b852759ef",
        "title": "VideoLLM: Modeling Video Sequence with Large Language Models"
      },
      {
        "id": "aafae4730b1add0b3e243e011db9ac87428f83cd",
        "title": "BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark"
      },
      {
        "id": "087a2f4cfea227be944f576f1f049e329316acac",
        "title": "Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models"
      },
      {
        "id": "a01acc3b62b4e43fd66d17b26d27a7a32623dcf1",
        "title": "ParroT: Translating During Chat Using Large Language Models"
      },
      {
        "id": "95430a76264a9be7d64633e56831c60041fb2948",
        "title": "SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery"
      },
      {
        "id": "597fa9f6787ef7b0b352e8b9d025dff4a8488c3f",
        "title": "LM4HPC: Towards Effective Language Model Application in High-Performance Computing"
      },
      {
        "id": "199600ce11fffc299c43717ee57c204d361bbee6",
        "title": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language Selection for Low-Resource Multilingual Sentiment Analysis"
      },
      {
        "id": "6a6cbcc596758dd214120a1d51528ce55daa333d",
        "title": "Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation"
      },
      {
        "id": "ea1db5796b16c495d12c45a2c34539fd361d856f",
        "title": "Comparative Analysis of CHATGPT and the evolution of language models"
      },
      {
        "id": "5b7f5488c380cf5085a5dd93e993ad293b225eee",
        "title": "One Fits All: Power General Time Series Analysis by Pretrained LM"
      },
      {
        "id": "5814bd146b37e13115af4330caf3a751159a156f",
        "title": "BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"
      },
      {
        "id": "18faafb15d7e7a8ad923b293f8c945c35680a2ef",
        "title": "ChatGPT and Open-AI Models: A Preliminary Review"
      },
      {
        "id": "302ee27524a717ddc21f332ca634b9211c6ec6aa",
        "title": "HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"
      },
      {
        "id": "30c0cdc414f68211d5d0514df027cec22e005174",
        "title": "A Survey on In-context Learning"
      },
      {
        "id": "58b98ccd256f1ebe8551faa6798cfbfc80c490a5",
        "title": "Investigating the use of ChatGPT for the scheduling of construction projects"
      },
      {
        "id": "5f61ddc37476acf3741b0bfe5fcb59639cadbb86",
        "title": "What Makes Good Examples for Visual In-Context Learning?"
      },
      {
        "id": "528bfc811a3b4213566134afe2c880f867be5065",
        "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity"
      },
      {
        "id": "79c5f79827c643c1e9464c2a5ff4d1326db16f20",
        "title": "AI chatbots not yet ready for clinical use"
      },
      {
        "id": "3a5162590b8423c81c457be9b0f6f9194e6f970e",
        "title": "In-context operator learning with data prompts for differential equation problems"
      },
      {
        "id": "ef4cb88b1635b34af15059567dfdf134f79797aa",
        "title": "The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges"
      },
      {
        "id": "4cd9389c227b771ec40cf09d9e7657391098f285",
        "title": "An Opinion on ChatGPT in Health Care—Written by Humans Only"
      },
      {
        "id": "ea647df1d12698114a87cbf043160edbc4cd0722",
        "title": "CLASS: A Design Framework for Building Intelligent Tutoring Systems Based on Learning Science principles"
      },
      {
        "id": "a49687ee1ce6ace8329cfcb693d8f8198c867bcc",
        "title": "ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs"
      },
      {
        "id": "3440687e1fc734baeab1abee4a86c81347d1422a",
        "title": "aeroBERT-Classifier: Classification of Aerospace Requirements Using BERT"
      },
      {
        "id": "f57afb6c8addfc7a32f9be5916a374a542d1a026",
        "title": "ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram"
      },
      {
        "id": "e458e0bdfc0377cba060896082e7469fcd2b09c2",
        "title": "T5-Based Model for Abstractive Summarization: A Semi-Supervised Learning Approach with Consistency Loss Functions"
      },
      {
        "id": "ade39c39048d66465c7288e4a7f8258a1bce9e60",
        "title": "Alleviating Over-smoothing for Unsupervised Sentence Representation"
      },
      {
        "id": "df7d7e71eba619363e7dbe8b14f4baeb3100ca73",
        "title": "Zero is Not Hero Yet: Benchmarking Zero-Shot Performance of LLMs for Financial Tasks"
      },
      {
        "id": "f9d4bfe5618b920a47d39be894658fb4275541ab",
        "title": "FiNER-ORD: Financial Named Entity Recognition Open Research Dataset"
      },
      {
        "id": "c30a4fb51684a8f4b23d899c9fc8c99653d3b0aa",
        "title": "GreekBART: The First Pretrained Greek Sequence-to-Sequence Model"
      },
      {
        "id": "390eabbedf9a25d82eefd01a484b1f81fdd801df",
        "title": "InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions"
      },
      {
        "id": "595274b38731caf1df5e07b8c4881dc6394a1d12",
        "title": "Artificial intelligence bot ChatGPT in medical research: the potential game changer as a double-edged sword"
      },
      {
        "id": "1aea23ef6545523e3f0f38d7f62feed11dfe0e44",
        "title": "A Comprehensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT"
      },
      {
        "id": "e26197fb0fa409866b287f4bf63abe7997223b51",
        "title": "A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing"
      },
      {
        "id": "c27c68597065dad70cb0b4f882cd861826bf28fa",
        "title": "Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"
      },
      {
        "id": "7d0abebf379383afbf9b7b3f8ab89561d1aa7596",
        "title": "MaterialBERT for natural language processing of materials science texts"
      },
      {
        "id": "3f6243097a58e386aea1215fed4f372dee07a100",
        "title": "Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models"
      },
      {
        "id": "763125fd2befe605b009cdd8d7ee8c8c694bc9e5",
        "title": "FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models"
      },
      {
        "id": "c6b30fc2469c4e72a311d91f831cb09ad9345a5d",
        "title": "AfroLM: A Self-Active Learning-based Multilingual Pretrained Language Model for 23 African Languages"
      },
      {
        "id": "22fbef2bfef213a7619ee4f307e8f42d1888e638",
        "title": "LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation"
      },
      {
        "id": "60d4da04889eb33268ebf58b3b8be730d2877ceb",
        "title": "RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use"
      },
      {
        "id": "44279244407a64431810f982be6d0c7da4429dd7",
        "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"
      },
      {
        "id": "f3a6115e5fb2237df938976e005468f0b18da797",
        "title": "The Stack: 3 TB of permissively licensed source code"
      },
      {
        "id": "15ecdf371d691c6ba0378f3c43bc8ead70c2d501",
        "title": "CSL: A Large-scale Chinese Scientific Literature Dataset"
      },
      {
        "id": "46d64582890afc296c134c0ba69137bb1d7fbba7",
        "title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator"
      },
      {
        "id": "378831c67fe5f79d91fe3421ac37c044704a80ae",
        "title": "Retraining a BERT Model for Transfer Learning in Requirements Engineering: A Preliminary Study"
      },
      {
        "id": "3e5b7f0b64c60ca0bb5ef547e8e3dcc2a568ab75",
        "title": "MTSMAE: Masked Autoencoders for Multivariate Time-Series Forecasting"
      },
      {
        "id": "70b98d90767345b15e0569082c0e4ac661279b5d",
        "title": "Is GPT-3 a Good Data Annotator?"
      },
      {
        "id": "d3e4553f0a1fd465ae358701f1bdc2e8265308d6",
        "title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing"
      },
      {
        "id": "789e8a599feeb431b3cb8a9b47b1624f71a4547c",
        "title": "Markov Models Applications in Natural Language Processing: A Survey"
      },
      {
        "id": "332dc8b2ca9d49fad607c7282f3360bb2a9aacf3",
        "title": "A large language model for electronic health records"
      },
      {
        "id": "b92628d13e8d090d042232fe6ae0b8998634b893",
        "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"
      },
      {
        "id": "0db5207510819b9956849eb84bfe8703f8f3688d",
        "title": "BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model"
      },
      {
        "id": "af0fe7f31315a4173de5695887dffb20be458f48",
        "title": "GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records"
      },
      {
        "id": "04a90a79bdfe2f9d54811141c104a4c12f74c07a",
        "title": "SsciBERT: a pre-trained language model for social science texts"
      },
      {
        "id": "645a317c9305207e95d03b5756a65e7e850f32d5",
        "title": "Towards a Cleaner Document-Oriented Multilingual Crawled Corpus"
      },
      {
        "id": "aed2191647d349c9bb4b12bcd266bfe9ae919930",
        "title": "The Use of NLP-Based Text Representation Techniques to Support Requirement Engineering Tasks: A Systematic Mapping Review"
      },
      {
        "id": "c3cfcd42b76ec6a18376f3975198c76e964b664f",
        "title": "yosm: A new yoruba sentiment corpus for movie reviews"
      },
      {
        "id": "c36873ca457af16e9d44be609e477d456706b2f5",
        "title": "Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks"
      },
      {
        "id": "fe1a59c047b9f93c34055c51466b15bf0da514ba",
        "title": "PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models"
      },
      {
        "id": "d823a979482348a2bc645af70d9a80e22a0df88a",
        "title": "Investigating LLM Applications in E-Commerce"
      },
      {
        "id": "f72345387ed2604d3510b39da0f39e8618fbd56e",
        "title": "Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer"
      },
      {
        "id": "105271639df3594fa39ab68e5af4197d695a7d8d",
        "title": "Crystal: Illuminating LLM Abilities on Language and Code"
      },
      {
        "id": "627821334ba2b90c2476822ffd8bbb8303022603",
        "title": "Multi-Turn Hidden Backdoor in Large Language Model-powered Chatbot Models"
      },
      {
        "id": "af2006e4abddebe7738527180e0c3ad498184caa",
        "title": "D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data and eXpert model predictions"
      },
      {
        "id": "fc5a66656f5695ae196eeaf15fd5e400f8f83666",
        "title": "Next‐generation human‐robot interaction with ChatGPT and robot operating system"
      }
    ],
    "5": [
      {
        "id": "3594500719d2468b4e29a3b844821ef4f318a5d8",
        "title": "The Price of Prompting: Profiling Energy Use in Large Language Models Inference"
      },
      {
        "id": "218ddd5ca9e959c91ad6b48a379397e4cb0d47d8",
        "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models"
      },
      {
        "id": "06046794314216dbdb53aeb0263ee41026e78574",
        "title": "MECLA: Memory-Compute-Efficient LLM Accelerator with Scaling Sub-matrix Partition"
      },
      {
        "id": "95240dda409e28acccdc5cf619ad0c036cf4292d",
        "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
      },
      {
        "id": "83b90f4a0ae4cc214eb3cc140ccfef9cd99fac05",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      },
      {
        "id": "d4793bcb3fd92b9da9a62b15e0dc93f64e452578",
        "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder"
      },
      {
        "id": "929d4c79b5e2bbb3779e35b754d261cfbc9a740e",
        "title": "On-Device Language Models: A Comprehensive Review"
      },
      {
        "id": "6bcd708d2e49b34f34f157daa6bf1c3e062f57c5",
        "title": "A Survey on Large Language Model Acceleration based on KV Cache Management"
      },
      {
        "id": "0749c4edb3751474ed96bcb0af94a4eb1444da02",
        "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator"
      },
      {
        "id": "ebaf42ee31611dafa4b349ce0a23c8744b8b1bd6",
        "title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models"
      },
      {
        "id": "62a82afa1b2adadb68b4667ede9faab0613a6cd9",
        "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM"
      },
      {
        "id": "6c586ffafcf9bd59544f908e4a045edee7992542",
        "title": "ThinK: Thinner Key Cache by Query-Driven Pruning"
      },
      {
        "id": "6bae288e831c26e0fa8724578558daa2cd0f4780",
        "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
      },
      {
        "id": "3955054d16fdb937b84a01e35819dade35f10f35",
        "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison"
      },
      {
        "id": "47e1206e92dab07cd1b933349633d8911ac16cf1",
        "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective"
      },
      {
        "id": "4d77e3192d1246d6247dc62d418a5d17b95a1f73",
        "title": "Recent Advances in Speech Language Models: A Survey"
      },
      {
        "id": "964440957c030504f6bcab11f514635ece1bf6b2",
        "title": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management"
      },
      {
        "id": "364d04149674df5ae8501007768cd085c4c12cee",
        "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching"
      },
      {
        "id": "f7bc5be4cc305f70b36456a959d7b95669b48ac2",
        "title": "Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference"
      },
      {
        "id": "cf278f48c09c2747f415c0b190c09773673ea959",
        "title": "A Survey on Hardware Accelerators for Large Language Models"
      },
      {
        "id": "4efd7cd724802d7ac3ccef972309691466d4637a",
        "title": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue in Recommendations for Large Language Models"
      },
      {
        "id": "fc5933ced955bb14c8b28a1d401bd00b8e4d08b2",
        "title": "WDMoE: Wireless Distributed Large Language Models with Mixture of Experts"
      },
      {
        "id": "7226e0479d16203abd18c5c5c1c3f1980ef112c1",
        "title": "Optimizing LLM Queries in Relational Data Analytics Workloads"
      },
      {
        "id": "827b0b03f54ba43596193e5d47f1ff13b3afe293",
        "title": "H3D-Transformer: A Heterogeneous 3D (H3D) Computing Platform for Transformer Model Acceleration on Edge Devices"
      },
      {
        "id": "53627c07e8970297391b252afa74dc6edb5b86d3",
        "title": "Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment"
      },
      {
        "id": "f85ec64b14494216702d5218f58e725224ad80aa",
        "title": "Measuring Bargaining Abilities of LLMs: A Benchmark and A Buyer-Enhancement Method"
      },
      {
        "id": "00e18c603e60d861c4e99c541e4d65ef442d5945",
        "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory"
      },
      {
        "id": "00e889fcfaf4396a20f37f681cf8b14f3e878879",
        "title": "LLMCad: Fast and Scalable On-device Large Language Model Inference"
      },
      {
        "id": "836b9658eb81f321de90423b6259b07a398ca79b",
        "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way"
      },
      {
        "id": "5b0fe50dc6df8f4eba13f8177dcd4bbe5a2b0e23",
        "title": "A Survey of Techniques for Optimizing Transformer Inference"
      },
      {
        "id": "a6a828dcaaf373554b53b8f8002de126f3e38695",
        "title": "An Integer-Only and Group-Vector Systolic Accelerator for Efficiently Mapping Vision Transformer on Edge"
      },
      {
        "id": "d2a42864605a502325a874bc470481ca1904ea0a",
        "title": "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"
      },
      {
        "id": "705c508f3d698510f45a506d1c12e81a7a75bd98",
        "title": "ITA: An Energy-Efficient Attention and Softmax Accelerator for Quantized Transformers"
      },
      {
        "id": "f4bdec0cf595720bc8ee5df2196324bac8f52ab4",
        "title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources"
      },
      {
        "id": "d50f023fe0921cabdd6d053c377cdd26c715994c",
        "title": "Tabi: An Efficient Multi-Level Inference System for Large Language Models"
      },
      {
        "id": "b7d12aec8a0152ec4921dfa43ab525a63b334385",
        "title": "Speculative Decoding with Big Little Decoder"
      },
      {
        "id": "bd5c24ec7d3c91648fb0beec6508a5302823f97b",
        "title": "X-Former: In-Memory Acceleration of Transformers"
      },
      {
        "id": "32928378a571b649fa3e670267defb29564e92af",
        "title": "TranCIM: Full-Digital Bitline-Transpose CIM-based Sparse Transformer Accelerator With Pipeline/Parallel Reconfigurable Modes"
      },
      {
        "id": "84ed091b4f7875323e512f17f5fe46eecbf66174",
        "title": "A 95.6-TOPS/W Deep Learning Inference Accelerator With Per-Vector Scaled 4-bit Quantization in 5 nm"
      },
      {
        "id": "b612fc6af23cccf2133c2ea40597453ab40dc2c3",
        "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      },
      {
        "id": "276a00173085d75232dee025104cc27ad23eaf75",
        "title": "A Fast and Flexible FPGA-based Accelerator for Natural Language Processing Neural Networks"
      },
      {
        "id": "86891d00499eebe86d3f1e39143d412addf2652b",
        "title": "DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation"
      },
      {
        "id": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
        "title": "ViA: A Novel Vision-Transformer Accelerator Based on FPGA"
      },
      {
        "id": "22b58dce1a13382418b8372bbd50ed3b2533f899",
        "title": "ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs"
      },
      {
        "id": "f1557a3638b164c632660a8bd4186076a96c7bf1",
        "title": "Bebert: Efficient And Robust Binary Ensemble Bert"
      },
      {
        "id": "17a8bd6a5763f6607863348ce1757ac2ad3417fd",
        "title": "Accelerating Transformer Networks through Recomposing Softmax Layers"
      },
      {
        "id": "f3e70aba917d3adc5a01387f8e76da856c1b55aa",
        "title": "Hardware Acceleration of Transformer Networks using FPGAs"
      },
      {
        "id": "6da9a81b75e7ad02867860753d1aa276673a3a77",
        "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models"
      },
      {
        "id": "7711774cf9f1d8683daf853b9e5773158b064980",
        "title": "Enable Deep Learning on Mobile Devices: Methods, Systems, and Applications"
      },
      {
        "id": "fb2307f7ce7c6868429ee3ee15d6eaf311ecba5c",
        "title": "BiBERT: Accurate Fully Binarized BERT"
      },
      {
        "id": "76d40153acfbb35a7eb8272a4215854cafa10e78",
        "title": "PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance"
      },
      {
        "id": "2a682d3ecda1a3003d5b249867eae53c35c35e68",
        "title": "A 17–95.6 TOPS/W Deep Learning Inference Accelerator with Per-Vector Scaled 4-bit Quantization for Transformers in 5nm"
      },
      {
        "id": "2f644f5953b984f4e4c7c8bb612a572d8aa7895f",
        "title": "Achieving Peak Performance for Large Language Models: A Systematic Review"
      },
      {
        "id": "c49a63dd38dc59adb46e5fd4ea78384d034410e7",
        "title": "Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups"
      },
      {
        "id": "4a9ac49dc374e1f873ff7d993e3afe50097195fc",
        "title": "What do Large Language Models Need for Machine Translation Evaluation?"
      },
      {
        "id": "b12edaa1764ebf64e453d3d608674d6e597c17fb",
        "title": "Instruction-Driven Game Engine: A Poker Case Study"
      },
      {
        "id": "4f12c2f039375512ee69cb2ed4faef550a98827c",
        "title": "Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models"
      },
      {
        "id": "ff0e3b8902121832e8b9518745c394b2c1d1efad",
        "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"
      },
      {
        "id": "45b53401717decd82cea2068fa8b1c4947b70594",
        "title": "Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM"
      }
    ],
    "7": [
      {
        "id": "ee29f87a383fd39fd809c675b88c2db99d74b8e4",
        "title": "A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks"
      },
      {
        "id": "7f6cdece764b56262e910a801f8c133e6c27b7ab",
        "title": "Bridging Language and Action: A Survey of Language-Conditioned Robot Manipulation"
      },
      {
        "id": "e86a0320e4e22b5a4dc673fed10268cdce520526",
        "title": "Cook2LTL: Translating Cooking Recipes to LTL Formulae using Large Language Models"
      },
      {
        "id": "b9203f3844e850ed9096bd935de336405acb905a",
        "title": "A Review on Edge Large Language Models: Design, Execution, and Applications"
      },
      {
        "id": "de3dfd3075597c5f46d423d2c7f1bb95b456d0f8",
        "title": "Re-Mix: Optimizing Data Mixtures for Large Scale Imitation Learning"
      },
      {
        "id": "a0f0ca553a30d115d7337740bbbe9520b606361a",
        "title": "AutoVFX: Physically Realistic Video Editing from Natural Language Instructions"
      },
      {
        "id": "2c0697de3deccb0fb88ca6674cfa3648943a8b00",
        "title": "PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions"
      },
      {
        "id": "754d5164e196ff231786d10a48594f3f27d8721f",
        "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment"
      },
      {
        "id": "e9b40e2cf481ebe5bf40b6958eebaf5ab1274481",
        "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models"
      },
      {
        "id": "a3570e82001666955d319647ba832df4f60a2044",
        "title": "A Survey on Robotics with Foundation Models: toward Embodied AI"
      },
      {
        "id": "a25bc1976ce0a6a2ebd6cf91699c8ee083726018",
        "title": "Application of LLM Agents in Recruitment: A Novel Framework for Resume Screening"
      },
      {
        "id": "1cd15db8b2c8e5b690ef1d2c2bc9aa4ee60267a4",
        "title": "NaturalVLM: Leveraging Fine-Grained Natural Language for Affordance-Guided Visual Manipulation"
      },
      {
        "id": "27d1615f847b3429d947a07ea734426c958b2026",
        "title": "Language-Conditioned Robotic Manipulation with Fast and Slow Thinking"
      },
      {
        "id": "41f84f296558fb4e4ab5b577066c5e1fcc67d007",
        "title": "Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps"
      },
      {
        "id": "43c44948874dd19dbb4df4b3127e6b248cb3c7e2",
        "title": "WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens"
      },
      {
        "id": "cc25dc73b0a479094744b2cbf24e84d0b109e4f4",
        "title": "REAL: Resilience and Adaptation using Large Language Models on Autonomous Aerial Robots"
      },
      {
        "id": "b24488203f61e413fe2497957a33780e149b6feb",
        "title": "WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model"
      },
      {
        "id": "6140211405f9917ded519da50f00eee989eabd7f",
        "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis"
      },
      {
        "id": "99b0c3a18050889c591e1db6d51ca01298638437",
        "title": "Transformers in Reinforcement Learning: A Survey"
      },
      {
        "id": "6c64ddd2190909de2c680dd18abc9b92e80c39f9",
        "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action"
      },
      {
        "id": "d3367dc9a7a1d7ae70a06eadc02b2430f4529f7c",
        "title": "Large Language Models for Robotics: A Survey"
      },
      {
        "id": "21701a348d3a0a695f3c9fef11fdc8d872e844d5",
        "title": "TypeFly: Flying Drones with Large Language Model"
      },
      {
        "id": "55367fbade73f96181ffcf52169d0471d4c014a2",
        "title": "GraphText: Graph Reasoning in Text Space"
      },
      {
        "id": "819f477065088220a6f706cd9ef76dbcb4b4c134",
        "title": "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists"
      },
      {
        "id": "0c474b03e8fd385b08a40df22934c9d9b180ffb7",
        "title": "De-Diffusion Makes Text a Strong Cross-Modal Interface"
      },
      {
        "id": "3ad346ae7af5c30964c4916dbcee798f72e1bdb7",
        "title": "Translating Natural Language to Planning Goals with Large-Language Models"
      },
      {
        "id": "72a2cff51bb9a87bbe4fc41325f5a4afc82a0366",
        "title": "NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models"
      },
      {
        "id": "afc5092a4116f27b4c64733c7815cd662bab78f7",
        "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"
      },
      {
        "id": "a8680b3419f3cbe6650f72b1023aed0ad0becb9e",
        "title": "Chain of Thought Prompt Tuning in Vision Language Models"
      },
      {
        "id": "047e3812854a86b2a2e113219fa956eda860ce24",
        "title": "Introspective Tips: Large Language Model for In-Context Decision Making"
      },
      {
        "id": "e701e4c02a32da186d25b08373ada12d83b73b3d",
        "title": "Scaling Robot Learning with Semantically Imagined Experience"
      },
      {
        "id": "948e8cfae92c2004f2dd5c9316f5972f8baaea21",
        "title": "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"
      },
      {
        "id": "8c88c693d5dc2802d3b76b85740e1f04fdaaf801",
        "title": "Large sequence models for sequential decision-making: a survey"
      },
      {
        "id": "9dee1aceb09f7d4c22fdbaf49d238e1502effd1b",
        "title": "Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought"
      },
      {
        "id": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
        "title": "Task and Motion Planning with Large Language Models for Object Rearrangement"
      },
      {
        "id": "dc135dabef805c7271f53ec4b212bdf8996cfd9d",
        "title": "AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers"
      },
      {
        "id": "119d3beca449efd9096d58674cf01a99c793a9a7",
        "title": "Towards Open-World Interactive Disambiguation for Robotic Grasping"
      },
      {
        "id": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
        "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
      },
      {
        "id": "04f87baf7d1b3eb303a52a8a66c8189f396dd114",
        "title": "Application of Pretrained Large Language Models in Embodied Artificial Intelligence"
      },
      {
        "id": "fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d",
        "title": "RT-1: Robotics Transformer for Real-World Control at Scale"
      },
      {
        "id": "60c8d0619481eaafdd1189af610d0e636271fed5",
        "title": "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"
      },
      {
        "id": "25425e299101b13ec2872417a14f961f4f8aa18e",
        "title": "VIMA: General Robot Manipulation with Multimodal Prompts"
      },
      {
        "id": "060cee8411181e8151ab1e3212b81528accd9b8b",
        "title": "On Transforming Reinforcement Learning With Transformers: The Development Trajectory"
      },
      {
        "id": "7b604cd12bfd735f16d2097357b3d6ca584d53a1",
        "title": "Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning"
      },
      {
        "id": "91deaf9d324c8feafc189da0da03e60a60287bca",
        "title": "Code as Policies: Language Model Programs for Embodied Control"
      },
      {
        "id": "c305ab1bdba79442bec72ec7f5c5ee7c49c2a566",
        "title": "Visual Language Maps for Robot Navigation"
      },
      {
        "id": "d1ad1bfa0bb76002b10e7f211b937842baeb28d9",
        "title": "Meta-Reinforcement Learning via Language Instructions"
      },
      {
        "id": "8b5eab31e1c5689312fff3181a75bfbf5c13e51c",
        "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"
      },
      {
        "id": "4f1d598f919aae55c3cbbc425ef1514a54e2b8cd",
        "title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions"
      },
      {
        "id": "01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8",
        "title": "Behavior Transformers: Cloning k modes with one stone"
      },
      {
        "id": "fe26607ca95c0d1d9005810b4ad12845ee69e9cf",
        "title": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem"
      },
      {
        "id": "b9b220b485d2add79118ffdc2aaa148b67fa53ef",
        "title": "Pre-Trained Language Models for Interactive Decision-Making"
      },
      {
        "id": "15ac70d077bb735eed4a8502ce49aa7782c803fd",
        "title": "What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data"
      },
      {
        "id": "47e135ef718baf6a3a86a1167b67ed96f7932ca4",
        "title": "LEBP - Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents"
      },
      {
        "id": "efb3b4550b7308ddeb2f382d8c1439e6ec7a99ec",
        "title": "Interactive Robotic Grasping with Attribute-Guided Disambiguation"
      },
      {
        "id": "c7ffef6eef0ad7a88c8a5f87eaea4e2df465f234",
        "title": "Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset"
      },
      {
        "id": "d43bfbe6fa818a0e5646c5de3ce2dac7001ccf07",
        "title": "PLATO: Planning with LLMs and Affordances for Tool Manipulation"
      }
    ],
    "9": [
      {
        "id": "fd6e8cd6aeadb12b861cc360bd5413fae221de15",
        "title": "PETA: evaluating the impact of protein transfer learning with sub-word tokenization on downstream applications"
      },
      {
        "id": "f75d602332b7c2040259f73f23bdece1e0d9f0bb",
        "title": "ProLLaMA: A Protein Language Model for Multi-Task Protein Language Processing"
      },
      {
        "id": "1ff54bbd8180ae7fcecf673a206bba3a8c8b8f1d",
        "title": "Generative AI for Controllable Protein Sequence Design: A Survey"
      },
      {
        "id": "f7e755443d665ebc215e7caeda8aa3154466bddc",
        "title": "A Philosophical Introduction to Language Models - Part I: Continuity With Classic Debates"
      },
      {
        "id": "15fe066d4f466c118e83b85f57f1c9bdb1a74281",
        "title": "The promises of large language models for protein design and modeling"
      },
      {
        "id": "d8d578d4ece329f17b025946587b1751721b9144",
        "title": "MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies"
      },
      {
        "id": "0fbf7ea1a3bd1754ed9aa12ed25906b731ece589",
        "title": "Training Data Extraction From Pre-trained Language Models: A Survey"
      },
      {
        "id": "dfbfa21a93c3164ae8a033398c8de42b03b1b84d",
        "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning"
      },
      {
        "id": "835c305e52769a8433f8383e91d33ba6c66ad55b",
        "title": "Large language models generate functional protein sequences across diverse families"
      },
      {
        "id": "7821e7639ffaeea175422f35fae2eb1c095ed1a6",
        "title": "Protein Language Models and Structure Prediction: Connection and Progression"
      },
      {
        "id": "0e8297549b4ec852ce8fd55dee9ae21501805af2",
        "title": "GPT-4 passes the bar exam"
      },
      {
        "id": "e97e2d032999eac1d47ab7ff52ac8a268578ea68",
        "title": "Enabling Auditory Large Language Models for Automatic Speech Quality Evaluation"
      },
      {
        "id": "b778fd5f23b91499e4186539e66596a0ac67a13b",
        "title": "A Survey on Mixture of Experts in Large Language Models"
      },
      {
        "id": "1a41d449f2ed4051ac4c763fb8b1b07357eaf5f4",
        "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model"
      },
      {
        "id": "2d60ebb118d966391b275b5c285bab5dc80bfa8f",
        "title": "SpeechVerse: A Large-scale Generalizable Audio Language Model"
      },
      {
        "id": "49ccd7ec067e685161f8fb106c2e89e8b6dcab3e",
        "title": "Learning the Language of Protein Structure"
      },
      {
        "id": "24a2468a1a16d6c332efc44be90854e4f748eeca",
        "title": "Mechanics of Next Token Prediction with Self-Attention"
      },
      {
        "id": "f52ea31e37c45e0de7ab4a5324d4d970479c110a",
        "title": "Can Generative Large Language Models Perform ASR Error Correction?"
      },
      {
        "id": "856c342606aca05434e48f2e53cdbd6f6b886802",
        "title": "Pre‐trained language models: What do they know?"
      },
      {
        "id": "399cbcf0187197c8c371fcca1bd78cd3e529621c",
        "title": "Named Entity Recognition in Electronic Health Records: A Methodological Review"
      },
      {
        "id": "c2bc931a3f6e2a4fdb9918038f47666188625739",
        "title": "Effect of tokenization on transformers for biological sequences"
      },
      {
        "id": "670b4f3aebb1156f608880e26f710d3dc04fee51",
        "title": "Specialist or Generalist? Instruction Tuning for Specific NLP Tasks"
      },
      {
        "id": "ba198cf0fd16b173fb07736e91fc75e17198bf69",
        "title": "Exploring the Effectiveness of GPT-3 in Translating Specialized Religious Text from Arabic to English: A Comparative Study with Human Translation"
      },
      {
        "id": "64d30d7b8070b89eff29318879367cd4bc55ee77",
        "title": "ChatGPT, GPT-4, and Other Large Language Models: The Next Revolution for Clinical Microbiology?"
      },
      {
        "id": "6847b9658f287f430098199cd81bf26308da13f9",
        "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"
      },
      {
        "id": "68850153b0210615c86f9a72624f34e2913bcddf",
        "title": "Document-Level Machine Translation with Large Language Models"
      },
      {
        "id": "9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5",
        "title": "A Short Survey of Viewing Large Language Models in Legal Aspect"
      },
      {
        "id": "93d6fa92d60938b5bd0e405e159832b91332f169",
        "title": "Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation"
      },
      {
        "id": "db0d67057b41927b5b51d3a393c250be64a405ae",
        "title": "Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods"
      },
      {
        "id": "4b5cbb924f06763a3c785d0ccfb3bc8bd765f4a5",
        "title": "Brainformers: Trading Simplicity for Efficiency"
      },
      {
        "id": "9a147712d46d1b01a761987c874b628c34c52cda",
        "title": "Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources"
      },
      {
        "id": "10c5079d2baa5a287054d9ddd7806a4c16fd7531",
        "title": "UDAPTER - Efficient Domain Adaptation Using Adapters"
      },
      {
        "id": "a25e9cc63e7660b2dc0659adc5319131d3a1e0ab",
        "title": "Deciphering microbial gene function using natural language processing"
      },
      {
        "id": "33651e356366b68874d27d9aba4bb5baa38ff794",
        "title": "Natural language processing approach to model the secretion signal of type III effectors"
      },
      {
        "id": "5697a0ede5425954d48daa6e1893dc87bd7d8be7",
        "title": "Contrastive Search Is What You Need For Neural Text Generation"
      },
      {
        "id": "dcb31b98ec58f3fff9f94f148e2952595f017fd9",
        "title": "ProtGPT2 is a deep unsupervised language model for protein design"
      },
      {
        "id": "b66e776502b8484a09f8759c8dc3737ebe714f34",
        "title": "Controllable protein design with language models"
      },
      {
        "id": "c1ffa773f29e567e13b00495767f8c42e2bb44b2",
        "title": "A Contrastive Framework for Neural Text Generation"
      },
      {
        "id": "03013e291fb3192b286147f5bdb5770e434f91b2",
        "title": "Do Language Models Plagiarize?"
      },
      {
        "id": "3d849136e0070f6d038dd96985ed67ead5aedb69",
        "title": "Locally Typical Sampling"
      },
      {
        "id": "b1371ac9e6901ed366d208a47afcfce8bb6a3ea2",
        "title": "MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders"
      }
    ],
    "10": [
      {
        "id": "95b4be099405394d936281413d5d83e9e3dde837",
        "title": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)"
      },
      {
        "id": "f25960465eaaec33f3a16ef9f6b4a0a046a498f8",
        "title": "REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers"
      },
      {
        "id": "394bd431e522b86581086bcb5cd9be161cf1cdf4",
        "title": "Language Models are Realistic Tabular Data Generators"
      },
      {
        "id": "c5b580b60e24a3738fb5d5570ae19110d6c6e203",
        "title": "Language Modeling on Tabular Data: A Survey of Foundations, Techniques and Evolution"
      },
      {
        "id": "e863156c7e853629a0d16bcb183593ee7fcd5aa0",
        "title": "From Traditional Recommender Systems to GPT-Based Chatbots: A Survey of Recent Developments and Future Directions"
      },
      {
        "id": "d0e09bb06723ec6388f0dcb3f3588fa3e95f7216",
        "title": "RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation"
      },
      {
        "id": "9aa4d8461f9d6169e54e10baf3164212c252c80a",
        "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language Models"
      },
      {
        "id": "a35f1315e91513ff0bec0c488fe175214fd9636c",
        "title": "Recommender Systems in the Era of Large Language Models (LLMs)"
      },
      {
        "id": "a1081c6fc6921d6b76d9ebda4d712333fd7bbbf5",
        "title": "Large Language Models for Generative Recommendation: A Survey and Visionary Discussions"
      },
      {
        "id": "d9ff3db7a9e37fe0363bb87aa47acd6b6b67977e",
        "title": "GenRec: Large Language Model for Generative Recommendation"
      },
      {
        "id": "9aeebfab9e27a4cecf7f488fca2d0bf07e06ae15",
        "title": "A Survey on Large Language Models for Personalized and Explainable Recommendations"
      },
      {
        "id": "68ae4b28fab8ff641deca48e0d340483d1c691ec",
        "title": "Exploring Fine-tuning ChatGPT for News Recommendation"
      },
      {
        "id": "65b63322cac4f4fd1b2dbc7155be69aed5f95b68",
        "title": "Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatial Relation Matching"
      },
      {
        "id": "cf659af952ee98182f9dc7cd6173bad55aa57daa",
        "title": "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation"
      },
      {
        "id": "f7d3c17ad1dee97377651e0f5646b3fc6d047fc0",
        "title": "Evaluating ChatGPT as a Recommender System: A Rigorous Approach"
      },
      {
        "id": "0d3289eae18ef3a4a88d912bee1e2fe184faa355",
        "title": "Automatically Reproducing Android Bug Reports using Natural Language Processing and Reinforcement Learning"
      },
      {
        "id": "b486982fa7c68a8a08df1111ba9607119419c488",
        "title": "A Survey on Large Language Models for Recommendation"
      },
      {
        "id": "bac54736112098616f0e1c90435888ef3e119d32",
        "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey"
      },
      {
        "id": "c589a3420ba335a05c248f525ea3c6e90215e42b",
        "title": "Pre-train, Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems"
      },
      {
        "id": "85dc7c22aaae5d596e0dc4d732b8f8afd51582bd",
        "title": "Zero-Shot Next-Item Recommendation using Large Pretrained Language Models"
      },
      {
        "id": "450b5490cc653478c272be50aa986798df828a20",
        "title": "Uncovering ChatGPT’s Capabilities in Recommender Systems"
      },
      {
        "id": "60f8a7ac53585aa2c173219e97507d6d963864e7",
        "title": "PALR: Personalization Aware LLMs for Recommendation"
      },
      {
        "id": "26f7785ef8da35820599799549152b9ef695dae2",
        "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation"
      },
      {
        "id": "0d230fe2af4fe4fd2257ba2a10cfeb91126e27a3",
        "title": "VIP5: Towards Multimodal Foundation Models for Recommendation"
      },
      {
        "id": "2ee1f98649ff27378fc341cae907eb89aba8fba4",
        "title": "Prompt Learning for News Recommendation"
      },
      {
        "id": "18ff1542d5a2a4490c7b3f21522bf1343889f700",
        "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications"
      },
      {
        "id": "f5de5cbb5563ed5633421d17d894573828c13b2d",
        "title": "Exploring Adapter-based Transfer Learning for Recommender Systems: Empirical Studies and Practical Insights"
      },
      {
        "id": "48385ded07af641da331c05f6ea3f93694a08425",
        "title": "Prompting Is All You Need: Automated Android Bug Replay with Large Language Models"
      },
      {
        "id": "ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3",
        "title": "Is ChatGPT a Good Recommender? A Preliminary Study"
      },
      {
        "id": "6159549f986c63e160a678feef2130a2a4b93feb",
        "title": "Generative Recommendation: Towards Next-generation Recommender Paradigm"
      },
      {
        "id": "11628f656257e75e46447ac21cdaa86c4b340a0a",
        "title": "Where to Go Next for Recommender Systems? ID- vs. Modality-based Recommender Models Revisited"
      },
      {
        "id": "61d71388025347124a2bbd09804c09bff7023347",
        "title": "Transformers for Tabular Data Representation: A Tutorial on Models and Applications"
      },
      {
        "id": "df602516e28a9ef0ef665ed0aef551984d8d770d",
        "title": "Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5)"
      },
      {
        "id": "e72ce8bd009e1c97fbd896321efe5a63b3d95c34",
        "title": "PTM4Tag: Sharpening Tag Recommendation of Stack Overflow Posts with Pre-trained Models"
      },
      {
        "id": "fcac4f97885cc73256c4f64a935927bf1e54cd89",
        "title": "ReCDroid+: Automated End-to-End Crash Reproduction from Bug Reports for Android Apps"
      },
      {
        "id": "a3bd42e366ea8014f845c67ff6ea20887a8584ad",
        "title": "The Moral Case for Using Language Model Agents for Recommendation"
      },
      {
        "id": "cb085ce72c1647d23da2514dc45e74fbf88f9224",
        "title": "Language Representations Can be What Recommenders Need: Findings and Potentials"
      }
    ],
    "4": [
      {
        "id": "19c29bd4888a9a376842ce8b5f862a10ade19a25",
        "title": "Let Your Graph Do the Talking: Encoding Structured Data for LLMs"
      },
      {
        "id": "64809140c38ce9d4bee3a30c59da9a1decd88b97",
        "title": "Large Language Model-Driven Classroom Flipping: Empowering Student-Centric Peer Questioning with Flipped Interaction"
      },
      {
        "id": "88bddfb7d1e0462be8fe99fdbd71c658140cb17b",
        "title": "From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"
      },
      {
        "id": "bf93fe733932fd25780ef84911b8a507bec1c372",
        "title": "Neural scaling of deep chemical models"
      },
      {
        "id": "a2bf0f9d41c550bc7cf69706b322f9c9838d3b00",
        "title": "Large Language Models for Test-Free Fault Localization"
      },
      {
        "id": "8071b53a46b0eb368741afcd90c3f93d95f56fdc",
        "title": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review"
      },
      {
        "id": "d54c45ca2b32577b2e071870d5a621ae2997c275",
        "title": "Comparing SMILES and SELFIES tokenization for enhanced chemical language modeling"
      },
      {
        "id": "2bec8a8a9206dfa2bd3934554b2a352472553b66",
        "title": "Graph Reasoning with Large Language Models via Pseudo-code Prompting"
      },
      {
        "id": "79863f144811650585781e9755bf80a816ac10f5",
        "title": "Amortized Probabilistic Conditioning for Optimization, Simulation and Inference"
      },
      {
        "id": "9a4a3f8ef868d01c785693634d1ade55b3174dc6",
        "title": "Retrieval-Enhanced Machine Learning: Synthesis and Opportunities"
      },
      {
        "id": "432e7fb118b94b10ec514fd7c1ebd67cad65ab62",
        "title": "Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing"
      },
      {
        "id": "1823b8aecd62ccfca0cb6caa8e2a1159754afc5e",
        "title": "LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset"
      },
      {
        "id": "84d401a54c09838faa1a1c80fd54efd019e2f0e4",
        "title": "From Words to Molecules: A Survey of Large Language Models in Chemistry"
      },
      {
        "id": "24e2070921ee948065dbbec60da048affac48946",
        "title": "ChemDFM: A Large Language Foundation Model for Chemistry"
      },
      {
        "id": "fe99095df95638b6768e1c10682034d25353ed89",
        "title": "MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining"
      },
      {
        "id": "4bdc1f0436d8ec356fc5c2bb9e1c510b7951d2ba",
        "title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond"
      },
      {
        "id": "d2a33037619f4c6a6a5ffa8792493c797cd0e24d",
        "title": "SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules"
      },
      {
        "id": "06514afdf7b2fa4d257288eb2164d7038b939dc1",
        "title": "Fine-Tuning GPT on Biomedical NLP Tasks: An Empirical Evaluation"
      },
      {
        "id": "3d3f0d7eb69f063e9539dd45df971f007b094734",
        "title": "Large Language Models to Enhance Bayesian Optimization"
      },
      {
        "id": "a9086398e27327e5cc664cbabf8a7bbb25c2c50f",
        "title": "ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction"
      },
      {
        "id": "c3382fd533b9dd7f8ed7ba7766159079bc1d3935",
        "title": "BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"
      },
      {
        "id": "f43b8a87a96f8abc2467b90538b643a6061416e9",
        "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision"
      },
      {
        "id": "8d65b5940a4dbed8c18e02ca35e3a9d7a14ea76b",
        "title": "Language is All a Graph Needs"
      },
      {
        "id": "67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51",
        "title": "Large Language Models on Graphs: A Comprehensive Survey"
      },
      {
        "id": "83de13bd492b9e72c314e308f0d77014154a6a74",
        "title": "Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment"
      },
      {
        "id": "1153b05709be3dd0dc5e5d4d4ee8f631995ad768",
        "title": "CAT-LM Training Language Models on Aligned Code And Tests"
      },
      {
        "id": "4ae7c4decd1df71c466f19d66d69b555945098c4",
        "title": "Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data"
      },
      {
        "id": "a94b1989594987de5daa19503929f5f34ed6bc47",
        "title": "Large Language Models for Fuzzing Parsers (Registered Report)"
      },
      {
        "id": "211b499ac24a45c3edb5b4ca019fb3e28ffe2329",
        "title": "Exploring Large Language Models for Code Explanation"
      },
      {
        "id": "a3509cef906a4517238c1764676cf637efcd1d5e",
        "title": "Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models"
      },
      {
        "id": "43189527d68e612a911680c7039dddba4f030985",
        "title": "AI-Powered Software Testing: The Impact of Large Language Models on Testing Methodologies"
      },
      {
        "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
        "title": "All in One: Multi-Task Prompting for Graph Neural Networks"
      },
      {
        "id": "6912a12f3710bf4bbdfd9f55f90311426fc1c32c",
        "title": "Graph Foundation Models: Concepts, Opportunities and Challenges"
      },
      {
        "id": "9ad4911a7923e19df9fb36cd03de5a63cb86ba63",
        "title": "Graph Prompt Learning: A Comprehensive Survey and Beyond"
      },
      {
        "id": "37d7fd90943b76657ff88d030a9d28677914160f",
        "title": "Investigating ChatGPT’s Potential to Assist in Requirements Elicitation Processes"
      },
      {
        "id": "b5cccb9a2a0c1e2c22fd1efe1cda7f9beb57bcbe",
        "title": "HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks"
      },
      {
        "id": "5b97c7ba8dc762d917af5a486ca76e352c8f2a33",
        "title": "Assessment of the E3C corpus for the recognition of disorders in clinical texts"
      },
      {
        "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models"
      },
      {
        "id": "958530b2a6267fd0c251a7c82e0267c21dca9cdf",
        "title": "Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing"
      },
      {
        "id": "1d5ffd4f19355c074da1f9e8b128941ca41d9f11",
        "title": "Make LLM a Testing Expert: Bringing Human-Like Interaction to Mobile GUI Testing via Functionality-Aware Decisions"
      },
      {
        "id": "21510620f0c92dde08741070a00593bcd1815d8c",
        "title": "Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why?"
      },
      {
        "id": "06396c7cd5d223a1776abf8811359ec7bc05d420",
        "title": "Knowledge-Augmented Methods for Natural Language Processing"
      },
      {
        "id": "df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
        "title": "Can Language Models Solve Graph Problems in Natural Language?"
      },
      {
        "id": "9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6",
        "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap"
      },
      {
        "id": "2b967d82b25088566980aaaf5a7062d90b2fb14f",
        "title": "GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking"
      },
      {
        "id": "20d7965c0b282a0cd7f990e435d0f6bc9535bbc6",
        "title": "What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks"
      },
      {
        "id": "8489b55d992e6a3ac54aec7094a42ec8e333012f",
        "title": "SELFormer: molecular representation learning via SELFIES language models"
      },
      {
        "id": "746bb45433f6b24d3ae64d6cd51c4e9d00a0ffa7",
        "title": "Large-scale Multi-modal Pre-trained Models: A Comprehensive Survey"
      },
      {
        "id": "e147cc46b7f441a68706ca53549d45e9a9843fb6",
        "title": "GraphPrompt: Unifying Pre-Training and Downstream Tasks for Graph Neural Networks"
      },
      {
        "id": "5fce7d9442b06cab91174fb68ba52ff6bdaa29cc",
        "title": "A Comprehensive Survey on Applications of Transformers for Deep Learning Tasks"
      },
      {
        "id": "ed1353d705eeabc0e916caba5fbae890eefe4f84",
        "title": "MolXPT: Wrapping Molecules with Text for Generative Pre-training"
      },
      {
        "id": "9a31b2ce43fe198ab1fd046ca4ec70fded154aee",
        "title": "What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?"
      },
      {
        "id": "f879657fd80ee47b950f537a1416e25fd4e0a625",
        "title": "Direct Fact Retrieval from Knowledge Graphs without Entity Linking"
      },
      {
        "id": "375a571174ea59b1f4aa62ad2619e9593fc03436",
        "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning"
      },
      {
        "id": "bc283b0e53f749f6c5b8b67eb340ea6a5a4331f5",
        "title": "Chatting with GPT-3 for Zero-Shot Human-Like Mobile Automated GUI Testing"
      },
      {
        "id": "b2676eac94dc4b138ade95b643534b95f58760d6",
        "title": "Bayesian Optimization of Catalysis With In-Context Learning"
      },
      {
        "id": "2341353cae858ce06225e46356c472b71dc63372",
        "title": "A Decade of Knowledge Graphs in Natural Language Processing: A Survey"
      },
      {
        "id": "df1cc92fba512ce7d28d1d608ea19f18cda185ca",
        "title": "No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence"
      },
      {
        "id": "299e281cb6e2bfae6e8459d2ac354e11e40931be",
        "title": "ChemBERTa-2: Towards Chemical Foundation Models"
      },
      {
        "id": "5aa7bdcae38076b80229c0a024f5b656ac6607af",
        "title": "KronA: Parameter Efficient Tuning with Kronecker Adapter"
      },
      {
        "id": "5152b9391762aefc532f85a801093bd38a6688c6",
        "title": "Hierarchical Graph Transformer with Adaptive Node Sampling"
      },
      {
        "id": "f7fd184eaa573205dff97d86c836f3038143e87a",
        "title": "An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks"
      },
      {
        "id": "76b09bf664e10ae3a181ced7f0e1a186e6977a20",
        "title": "A smile is all you need: predicting limiting activity coefficients from SMILES with natural language processing"
      },
      {
        "id": "3b9b1aba877ecd3f7e508cbc78a41b623349902b",
        "title": "Translation between Molecules and Natural Language"
      },
      {
        "id": "24ed74ed29c057cba8b52fff4edd2c0d7f408716",
        "title": "VLP: A Survey on Vision-language Pre-training"
      },
      {
        "id": "66ee488cf3dad5bb83804124367460edddd3c271",
        "title": "Vision-and-Language Pretrained Models: A Survey"
      },
      {
        "id": "97f456643712e9618edd7465676c62af3c8ae690",
        "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models"
      },
      {
        "id": "3b239d232ebb0fdb0515f41fd439e54ed4e8f86a",
        "title": "Vision-Language Intelligence: Tasks, Representation Learning, and Large Models"
      },
      {
        "id": "22a77ab4b79b43c69ce25a272de480b2a025c6a4",
        "title": "BERN2: an advanced neural biomedical named entity recognition and normalization tool"
      },
      {
        "id": "eee7997106834442f1704e4681a9a761df6696a1",
        "title": "Unified Deep Learning Model for Multitask Reaction Predictions with Explanation"
      },
      {
        "id": "e8ad4d82f23f88c17f401c785e24e00841719f49",
        "title": "Using Transfer Learning for Code-Related Tasks"
      },
      {
        "id": "cc73da69eb495a122ed24bc680bf9e2f1a420e0c",
        "title": "A Survey of Pretraining on Graphs: Taxonomy, Methods, and Applications"
      },
      {
        "id": "db783c480faf87b38e8806d4ef455dfde6e335aa",
        "title": "Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)"
      },
      {
        "id": "daaa79be8d02cd3f748b6a3465fc1f09b5068880",
        "title": "Towards Effective and Generalizable Fine-tuning for Pre-trained Molecular Graph Models"
      },
      {
        "id": "b03beb4d4844a9464e11e470801f50ea02c66f4b",
        "title": "LATEX-GCL: Large Language Models (LLMs)-Based Data Augmentation for Text-Attributed Graph Contrastive Learning"
      },
      {
        "id": "5fcfc60e2fdcf84c747a9b011282f20a7cd3fdfc",
        "title": "Learning on Graphs with Large Language Models(LLMs): A Deep Dive into Model Robustness"
      },
      {
        "id": "c9e6a698f32bce4b1d376af502e48f7324387fa1",
        "title": "AUITestAgent: Automatic Requirements Oriented GUI Function Testing"
      },
      {
        "id": "62e4f96797c8c1d7e4fcc2bcb2d4a3b6039cf59b",
        "title": "HITS: High-coverage LLM-based Unit Test Generation via Method Slicing"
      }
    ],
    "6": [
      {
        "id": "73f58b90697f957832f5090946894480849dea3a",
        "title": "Timer: Generative Pre-trained Transformers Are Large Time Series Models"
      },
      {
        "id": "1932269090210059b236559e4272cf84c6784640",
        "title": "A Survey on Data Augmentation in Large Model Era"
      },
      {
        "id": "2b31bbc4e042a0ff2e24e3e46ac6d1f66b1054b9",
        "title": "A Survey on Artificial Intelligence for Music Generation: Agents, Domains and Perspectives"
      },
      {
        "id": "8123b5eb1987c19a12de8596be9e653a9dcbad11",
        "title": "Enhancing Emotional Text-to-Speech Controllability with Natural Language Guidance through Contrastive Learning and Diffusion Models"
      },
      {
        "id": "5535a3cd501e5639657e2683011043813069d89f",
        "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives"
      },
      {
        "id": "4393655baf5a41bf365741bf2b6de89a43c35ad0",
        "title": "dMel: Speech Tokenization made Simple"
      },
      {
        "id": "969e53e9e8714360c029c99fd69cf6522da874d5",
        "title": "Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation"
      },
      {
        "id": "6658e96f67efa90f54444079cd2f802473f0d7f9",
        "title": "Parametric Augmentation for Time Series Contrastive Learning"
      },
      {
        "id": "9cdb98bdcf6a914e0c2caa98e3626708aaa55890",
        "title": "Mapache: Masked Parallel Transformer for Advanced Speech Editing and Synthesis"
      },
      {
        "id": "6427b7f29de1b85855cd110ce3f50bca08235ff1",
        "title": "FENICE: Factuality Evaluation of summarization based on Natural language Inference and Claim Extraction"
      },
      {
        "id": "329960d5f4287077cdc258d49a66bdf277c1ee29",
        "title": "Natural Language Supervision For General-Purpose Audio Representations"
      },
      {
        "id": "753c871ae51cafac1184b31cabecdb6a540840fe",
        "title": "Promptvc: Flexible Stylistic Voice Conversion in Latent Space Driven by Natural Language Prompts"
      },
      {
        "id": "8e1868f84091272544cb4209c4ccaad7cc88af27",
        "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration"
      },
      {
        "id": "c1dd77e48dd615ee6881b2cc876a00a92cae6eac",
        "title": "Boosting Large Language Model for Speech Synthesis: An Empirical Study"
      },
      {
        "id": "2a3da709f2ed0d594f1fa6efce2a8cbba20980a2",
        "title": "Transferable Models for Bioacoustics with Human Language Supervision"
      },
      {
        "id": "f45f85fa1beaa795c24c4ff86f1f2deece72252f",
        "title": "A decoder-only foundation model for time-series forecasting"
      },
      {
        "id": "ffa05cb5504ba08254f498223f613b3ebcf87692",
        "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT"
      },
      {
        "id": "ce9c0935c074a0ca8769f13fd4e8651cee263112",
        "title": "LP-MusicCaps: LLM-Based Pseudo Music Captioning"
      },
      {
        "id": "374ebdc8240a35820cb7ab8bfca37e180e21b605",
        "title": "Sparks of Large Audio Models: A Survey and Outlook"
      },
      {
        "id": "593cbef6af5736a3b1f498b59f7d741fafdc1e59",
        "title": "miditok: A Python package for MIDI file tokenization"
      },
      {
        "id": "401510c31725ea9277a4d2e049682578a0134e1c",
        "title": "Distilled GPT for source code summarization"
      },
      {
        "id": "9e99648c5d4d9ce4fba73007291bbd3f804c83ea",
        "title": "ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation"
      },
      {
        "id": "fe3f5c5209557cd93a98de61afeb325db545740e",
        "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS"
      },
      {
        "id": "eb9b568cc98909af92ae5295a6ca526d8c349b68",
        "title": "Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation"
      },
      {
        "id": "2238c293edcefbfc0b5966f27d82c560e66cae1b",
        "title": "Can Large Language Models Aid in Annotating Speech Emotional Data? Uncovering New Frontiers [Research Frontier]"
      },
      {
        "id": "f6c494234a818b2aec286b257ee6117f2894bcf7",
        "title": "CLAP Learning Audio Concepts from Natural Language Supervision"
      },
      {
        "id": "f0aa870639e0766ce675e4bf45f65742321dcbe2",
        "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space With Natural Language Style Prompt"
      },
      {
        "id": "78c488e2d84bd193a40006b1fceb03e3845b81d4",
        "title": "Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias"
      },
      {
        "id": "12454696085d66beaeb6cd43857de982a8445824",
        "title": "Transformers in Speech Processing: A Survey"
      },
      {
        "id": "f51bc74814a3452009ea5ca262d9768d08149ee6",
        "title": "Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model"
      },
      {
        "id": "8df67942e29cba92bb5913b62d1d2df7371842d9",
        "title": "AugGPT: Leveraging ChatGPT for Text Data Augmentation"
      },
      {
        "id": "1725ad1d8cc0e539ac5d0a85657d5c95b4538c5e",
        "title": "Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects"
      },
      {
        "id": "e5dff0d39324dd0bb3fa323f2d256f801043ba4a",
        "title": "A Survey on Time-Series Pre-Trained Models"
      },
      {
        "id": "b50c27607d7a858297232310bbec9819ade875a8",
        "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks"
      },
      {
        "id": "47ba7df38e24da9bad9266d2b58abbb2b70db6e5",
        "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning"
      },
      {
        "id": "eaf67634116e407774be85a6abcaee491be4ca3a",
        "title": "PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions"
      },
      {
        "id": "83d4b22d803ae856cf6b308482bd504fa151d39e",
        "title": "Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation"
      },
      {
        "id": "aa933e27c470eeecbe7bbec5debdd8c5d2faa4be",
        "title": "Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution"
      },
      {
        "id": "e9bc29cfcfbea4d137652d10715a9c9389349a90",
        "title": "Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"
      },
      {
        "id": "8c870bef01a4fbb20f60722ffc2f6bee3870b18b",
        "title": "AudioLM: A Language Modeling Approach to Audio Generation"
      },
      {
        "id": "484b4e96428a7d3ab46330a15b14278ca7bd68ca",
        "title": "GENIUS: Sketch-based Language Model Pre-training via Extreme and Selective Masking for Text Generation and Augmentation"
      },
      {
        "id": "120f4e798d78c3f6dceed218bee1ca83a5855f55",
        "title": "Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task"
      },
      {
        "id": "1ffe4d4deb090b3340f2aee41f5b691fa848db85",
        "title": "Chapter: Exploiting Convolutional Neural Network Adapters for Self-Supervised Speech Models"
      },
      {
        "id": "c822486b8f1dcbef3b96b136c85d48d0dc580f31",
        "title": "Audio Retrieval with WavText5K and CLAP Training"
      },
      {
        "id": "43b7437ed33a29d3d90239ad66f325a465ff7e91",
        "title": "Meta Learning for Natural Language Processing: A Survey"
      },
      {
        "id": "5e8d3c2dc0fc53949794fc00600e25558c4a2441",
        "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation"
      },
      {
        "id": "5d1c711c1d5b5f8399938af5e3df904680ce24a7",
        "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks"
      },
      {
        "id": "23c265ba884b92ecbd9d18641078d964697e4590",
        "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"
      },
      {
        "id": "c090ff3dec01e06f46735b7b9ab133a5db8c73c3",
        "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting"
      },
      {
        "id": "f76a5b176f3435214eb87dd105f730f0b53672c3",
        "title": "Self-Supervised Speech Representation Learning: A Review"
      },
      {
        "id": "f6919b54a4f06367947f0cf58cda54cdd08cd5f2",
        "title": "Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition"
      },
      {
        "id": "55e31baa3ae5f32fb5e695761892319e26dbc639",
        "title": "Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data"
      },
      {
        "id": "4670fcd13676571a580f8524a8645a581b90a072",
        "title": "AudioComposer: Towards Fine-grained Audio Generation with Natural Language Descriptions"
      },
      {
        "id": "b3f77c0f732b22e1684bb45512d461766fdbd47c",
        "title": "Leveraging Large Language Models for Code-Mixed Data Augmentation in Sentiment Analysis"
      },
      {
        "id": "a97b19f498d2b69d64fdab4f4e5f729db17a0e02",
        "title": "Text2FX: Harnessing CLAP Embeddings for Text-Guided Audio Effects"
      },
      {
        "id": "4d4d46b09d09b66deff211fb4ee779f442d82523",
        "title": "EmoSpeech: A Corpus of Emotionally Rich and Contextually Detailed Speech Annotations"
      },
      {
        "id": "548f71c351d26a608c7db00d8b7abf567b2fda46",
        "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations"
      }
    ],
    "8": [
      {
        "id": "b3c1fad1f5f8f0213b6d3f3458fa86205a3434f7",
        "title": "A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models"
      },
      {
        "id": "7238be6ddad7a199aa160e7a2bb77291f299ee99",
        "title": "Automated Construction of Theme-specific Knowledge Graphs"
      },
      {
        "id": "9410653c8fff820360d2107753c756fe57a6e062",
        "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation"
      },
      {
        "id": "92171cd100b5d61b77dedf69150931eff30fea85",
        "title": "Combining Knowledge Graphs and Large Language Models"
      },
      {
        "id": "6090b400fff46f14e1062dc12953b4b1837db494",
        "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models"
      },
      {
        "id": "f34511cdbe840cc073201f3932a021234ea3db72",
        "title": "AIvril: AI-Driven RTL Generation With Verification In-The-Loop"
      },
      {
        "id": "346fdbda3ecf4775819fced0cfed78357bee8128",
        "title": "Conformal Prediction for Natural Language Processing: A Survey"
      },
      {
        "id": "fb4dc0178e5d7347b1615c48caf05347b6e5eb48",
        "title": "TrustLLM: Trustworthiness in Large Language Models"
      },
      {
        "id": "d288b03efdea01d96dc1666bef13db71ea2d9deb",
        "title": "A Study on Performance Improvement of Prompt Engineering for Generative AI with a Large Language Model"
      },
      {
        "id": "26a624f6a415289a4e4b2e265c2722bf54a1bc54",
        "title": "Large Language Model Supply Chain: A Research Agenda"
      },
      {
        "id": "e3293a200f4f50b9c331023483606072193bafa9",
        "title": "SpecLLM: Exploring Generation and Review of VLSI Design Specification with Large Language Model"
      },
      {
        "id": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
        "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering"
      },
      {
        "id": "0fd55e413b587d3b090093d56665e472da2f63de",
        "title": "Comparative Study of Domain Driven Terms Extraction Using Large Language Models"
      },
      {
        "id": "ef1f443f0143dc36c11f7b08ca5b55c6bc866997",
        "title": "Research Trends for the Interplay between Large Language Models and Knowledge Graphs"
      },
      {
        "id": "de817951a32e94ca8115a9cd57aa441984d2d945",
        "title": "Benchmarking LLMs via Uncertainty Quantification"
      },
      {
        "id": "c4a53f4a12207ecce4ea5102fb0a05d8f8984561",
        "title": "From human experts to machines: An LLM supported approach to ontology and knowledge graph construction"
      },
      {
        "id": "fb3c8846d36eca4698cbcc2ad4b8ce0914578e7b",
        "title": "AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs"
      },
      {
        "id": "9be02ecf206ad7494bb9531aed9491af203a3c3d",
        "title": "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction"
      },
      {
        "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
        "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing"
      },
      {
        "id": "888728745dbb769e29ed475d4f7661eebe1a71cf",
        "title": "A Survey on Evaluation of Large Language Models"
      },
      {
        "id": "bcfa73aedf1b2d1ee4f168e21298a37ac55a37f7",
        "title": "Bias and Fairness in Large Language Models: A Survey"
      },
      {
        "id": "007c3d9b8dab341d2c77c4ee764fd921f7f14956",
        "title": "Adapted large language models can outperform medical experts in clinical text summarization."
      },
      {
        "id": "daaa7d4ffb9265226e4baadd2db9a01aa7b2f6fb",
        "title": "Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts"
      },
      {
        "id": "78b1601c013769294a1927d43e50dfa81d6af75f",
        "title": "LLMs4OL: Large Language Models for Ontology Learning"
      },
      {
        "id": "18664b47516ba5424ba5efa79d3f816224245325",
        "title": "ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning"
      },
      {
        "id": "9582717ca2c7cf7f6d39fff408013d636accf4e6",
        "title": "Conformal Autoregressive Generation: Beam Search with Coverage Guarantees"
      },
      {
        "id": "862ed55c8a7d4ef1b23091995d42aae91d906a1b",
        "title": "Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)"
      },
      {
        "id": "ec7a6d3d930dad2c36088478f2490830f102bd97",
        "title": "Uncertainty in Natural Language Processing: Sources, Quantification, and Applications"
      },
      {
        "id": "f9fd2799591b5980fd9d0efa352fc08eebf552b8",
        "title": "Text-to-Ontology Mapping via Natural Language Processing with Application to Search for Relevant Ontologies in Catalysis"
      },
      {
        "id": "c8779da397189ce5b16577db058461c5efb0d2c1",
        "title": "RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models"
      },
      {
        "id": "385376b8aa48c25403f17d6206db7c09b67e1314",
        "title": "Prompt Engineering for Healthcare: Methodologies and Applications"
      },
      {
        "id": "f4c4e148546089123f8da5db4fb246ab4062bd40",
        "title": "Evaluation of ChatGPT for NLP-based Mental Health Applications"
      },
      {
        "id": "5e8dd82419f78025093acbec3ba2e345fff85d11",
        "title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs"
      },
      {
        "id": "6927a5b0152433a199ab4974ad85e787454d6a30",
        "title": "Should We Attend More or Less? Modulating Attention for Fairness"
      },
      {
        "id": "502bc7694dbe5dbf2bb7aaf576cd2bc13e6b95d8",
        "title": "Chip-Chat: Challenges and Opportunities in Conversational Hardware Design"
      },
      {
        "id": "ca0c955699f552e1c2fbda747bd41faf8a2513ce",
        "title": "Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text"
      },
      {
        "id": "c8957181e3905ac8eafacce99c7396571f09ce0c",
        "title": "Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks: Algorithm Development and Validation Study"
      },
      {
        "id": "551b05734eb2181c4ca009a411144e8447ed1606",
        "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis"
      },
      {
        "id": "af2fba1911f0d0977c731e3918c51428e08741da",
        "title": "Conformal Risk Control"
      },
      {
        "id": "ac5225708efd250d217424ba27885e90f186160d",
        "title": "Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words"
      },
      {
        "id": "bf22ef16a6a912763780aea454198edc3e2bb3c9",
        "title": "HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing"
      },
      {
        "id": "5a5b5bd6c644eb43943144410efba704ebb4c083",
        "title": "Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists"
      },
      {
        "id": "eafe58b4a09844418bc8972f890e5d24e9df2fb7",
        "title": "Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation"
      },
      {
        "id": "17b13a8e347abaf66ec76cec8ce735121f585c01",
        "title": "Speciesist bias in AI: how AI applications perpetuate discrimination and unfair outcomes against animals"
      },
      {
        "id": "632ee6dde052ff14165e2c9511b43c9a2b7e6d97",
        "title": "Speciesism in Natural Language Processing Research"
      },
      {
        "id": "fa5b3bf10c9a9818e4b5c09f4216502a55ad15ac",
        "title": "Are LLMs Any Good for High-Level Synthesis?"
      },
      {
        "id": "01526c6fe95ef3716048181aa3b7edb151429dff",
        "title": "CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization"
      }
    ]
  },
  "cluster_keywords": {
    "3": [
      "models",
      "knowledge",
      "nlp",
      "multimodal",
      "text2kgbench"
    ],
    "2": [
      "models",
      "tasks",
      "visual",
      "automating",
      "nlg"
    ],
    "0": [
      "models",
      "ai",
      "nlp",
      "dialogue",
      "texts"
    ],
    "1": [
      "dataset",
      "multilingual",
      "fingpt",
      "languages",
      "multimodal"
    ],
    "5": [
      "accelerator",
      "efficient",
      "hardware",
      "scalable",
      "sparse"
    ],
    "7": [
      "planning",
      "vision",
      "action",
      "interactive",
      "mapping"
    ],
    "9": [
      "models",
      "english",
      "audio",
      "gene",
      "audiollms"
    ],
    "10": [
      "android",
      "recdroid",
      "ptm4tag",
      "rec",
      "realtabformer"
    ],
    "4": [
      "models",
      "knowledge",
      "code",
      "molecular",
      "nlp"
    ],
    "6": [
      "supervised",
      "augmentation",
      "representations",
      "forecasting",
      "synthesis"
    ],
    "8": [
      "llms",
      "generating",
      "aivril",
      "trustllm",
      "llms4ol"
    ]
  }
}