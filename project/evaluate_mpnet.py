from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import pandas as pd
import pickle
import numpy as np
import random
import ast
from tqdm import tqdm

# --------------------------
# 1. CSV ë¶ˆëŸ¬ì˜¤ê¸° ë° í…ìŠ¤íŠ¸ êµ¬ì„±
# --------------------------
df = pd.read_csv("merged_nlp_papers_for_enrichment.csv").fillna("")

def parse_fields_of_study(fos_str):
    try:
        fos_list = ast.literal_eval(fos_str)
        if isinstance(fos_list, list):
            return ", ".join(fos_list)
        else:
            return ""
    except:
        return ""

paper_texts = {}
for _, row in df.iterrows():
    pid = row["paperId"]
    title = row["title"]
    fos = parse_fields_of_study(row["s2FieldsOfStudy"])
    if pid and title:
        paper_texts[pid] = f"Title: {title} Concepts: {fos}"

# --------------------------
# 2. ê·¸ë˜í”„ ë¡œë“œ
# --------------------------
with open("nlp_similarity_graph_with_cluster.gpickle", "rb") as f:
    G = pickle.load(f)

# --------------------------
# 3. ì„ë² ë”© ìƒì„± (Sentence-BERT ì‚¬ìš©)
# --------------------------
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
nodes = list(G.nodes())

valid_nodes = [n for n in nodes if n in paper_texts]
texts = [paper_texts[n] for n in valid_nodes]

print(f"ğŸ”¢ ì´ ì„ë² ë”©í•  ë…¼ë¬¸ ìˆ˜: {len(texts)}")

embeddings = model.encode(texts, batch_size=16, show_progress_bar=True)

# --------------------------
# 4. ID â†’ ì„ë² ë”© ë§¤í•‘
# --------------------------
id2embedding = {n: e for n, e in zip(valid_nodes, embeddings)}

# --------------------------
# 5. ê·¸ë˜í”„ ì—£ì§€ ìœ ì‚¬ë„ ê³„ì‚°
# --------------------------
similarities = []
for a, b in tqdm(G.edges(), desc="ğŸ“ˆ ê·¸ë˜í”„ ì—£ì§€ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘"):
    if a in id2embedding and b in id2embedding:
        sim = cosine_similarity([id2embedding[a]], [id2embedding[b]])[0][0]
        similarities.append(sim)

# --------------------------
# 6. ëœë¤ ì—£ì§€ ìœ ì‚¬ë„ ê³„ì‚°
# --------------------------
random_similarities = []
sample_size = len(similarities)
node_list = list(id2embedding.keys())
random_edges = random.sample([(a, b) for a in node_list for b in node_list if a != b], sample_size)

for a, b in tqdm(random_edges, desc="ğŸ² ëœë¤ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘"):
    sim = cosine_similarity([id2embedding[a]], [id2embedding[b]])[0][0]
    random_similarities.append(sim)

# --------------------------
# 7. ê²°ê³¼ ì¶œë ¥
# --------------------------
print(f"\nğŸ“Š ê·¸ë˜í”„ ì—£ì§€ í‰ê·  ìœ ì‚¬ë„: {np.mean(similarities):.4f}")
print(f"ğŸ² ëœë¤ ì—£ì§€ í‰ê·  ìœ ì‚¬ë„: {np.mean(random_similarities):.4f}")
print(f"ğŸ” ì°¨ì´: {np.mean(similarities) - np.mean(random_similarities):.4f}")
